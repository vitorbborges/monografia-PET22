Publication Type,Authors,Book Authors,Book Editors,Book Group Authors,Author Full Names,Book Author Full Names,Group Authors,Article Title,Source Title,Book Series Title,Book Series Subtitle,Language,Document Type,Conference Title,Conference Date,Conference Location,Conference Sponsor,Conference Host,Author Keywords,Keywords Plus,Abstract,Addresses,Affiliations,Reprint Addresses,Email Addresses,Researcher Ids,ORCIDs,Funding Orgs,Funding Name Preferred,Funding Text,Cited References,Cited Reference Count,"Times Cited, WoS Core","Times Cited, All Databases",180 Day Usage Count,Since 2013 Usage Count,Publisher,Publisher City,Publisher Address,ISSN,eISSN,ISBN,Journal Abbreviation,Journal ISO Abbreviation,Publication Date,Publication Year,Volume,Issue,Part Number,Supplement,Special Issue,Meeting Abstract,Start Page,End Page,Article Number,DOI,DOI Link,Book DOI,Early Access Date,Number of Pages,WoS Categories,Web of Science Index,Research Areas,IDS Number,Pubmed Id,Open Access Designations,Highly Cited Status,Hot Paper Status,Date of Export,UT (Unique WOS ID),Web of Science Record
C,"Noy, A; Crammer, K",,"Kaski, S; Corander, J",,"Noy, Asaf; Crammer, Koby",,,Robust Forward Algorithms via PAC-Bayes and Laplace Distributions,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"Laplace random variables are commonly used to model extreme noise in many fields, while systems trained to deal with such noises are often characterized by robustness properties. We introduce new learning algorithms that minimize objectives derived directly from PAC-Bayes bounds, incorporating Laplace distributions. The resulting algorithms are regulated by the Huber loss fuiction and are robust to noise, as the Laplace distribution integrated large deviation of parameters. We analyze the convexity properties of the objective, and propose a few bounds which are fully convex, two of which jointly convex in the mean and standard-deviation under certain conditions. We derive new forward algorithms analogous to recent boosting algorithms, providing novel relations between boosting and PAC-Bayes analysis. Experiments show that our algorithms outperform AdaBoost, Li-LogBoost [10], and RobustBoost [11] in a wide range of input noise.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,678,686,,,,,,,,,,,,,,,,WOS:000508355800075,0
C,"Paskov, HS; Mitchell, JC; Hastie, TJ",,"Kaski, S; Corander, J",,"Paskov, Hristo S.; Mitchell, John C.; Hastie, Trevor J.",,,An Efficient Algorithm for Large Scale Compressive Feature Learning,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"This paper focuses on large-scale unsupervised feature selection from text. We expand upon the recently proposed Compressive Feature Learning (CFL) framework, a method that uses dictionary-based compression to select a K-gram representation for a document corpus. We show that CFL is NP-Complete and provide a novel and efficient approximation algorithm based on a homotopy that transforms a convex relaxation of CFL into the original problem. Our algorithm allows CFL to scale to corpuses comprised of millions of documents because each step is linear in the corpus length and highly parallelizable. We use it to extract features from the BeerAdvocate dataset, a corpus of over 1.5 million beer reviews spanning 10 years. CFL uses two orders of magnitude fewer features than the full trigram space. It beats a standard unigram model in a number of prediction tasks and achieves nearly twice the accuracy on an author identification task.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,760,768,,,,,,,,,,,,,,,,WOS:000508355800084,0
C,"Ramdas, A; Poczos, B; Singh, A; Wasserman, L",,"Kaski, S; Corander, J",,"Ramdas, Aaditya; Poczos, Barnabas; Singh, Aarti; Wasserman, Larry",,,An Analysis of Active Learning With Uniform Feature Noise,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"In active learning, the user sequentially chooses values for feature X and an oracle returns the corresponding label Y. In this paper, we consider the effect of feature noise in active learning, which could arise either because X itself is being measured, or it is corrupted in transmission to the oracle, or the oracle returns the label of a noisy version of the query point. In statistics, feature noise is known as errors in variables and has been studied extensively in non-active settings. However, the effect of feature noise in active learning has not been studied before. We consider the well-known Berkson errors-in-variables model with additive uniform noise of width sigma. Our simple but revealing setting is that of one-dimensional binary classification setting where the goal is to learn a threshold (point where the probability of a + label crosses half). We deal with regression functions that are antisymmetric in a region of size a around the threshold and also satisfy Tsybakov's margin condition around the threshold. We prove minimax lower and upper bounds which demonstrate that when a is smaller than the minimiax active/passive noiseless error derived in Castro & Nowak (2007), then noise has no effect on the rates and one achieves the same noiseless rates. For larger sigma, the unfiattening of the regression function on convolution with uniform noise, along with its local antisymmetry around the threshold, together yield a behaviour where noise appears to be beneficial. Our key result is that active learning can buy significant improvement over a passive strategy even in the presence of feature noise.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,805,813,,,,,,,,,,,,,,,,WOS:000508355800089,0
C,"Bao, H; Sugiyama, M",,"Banerjee, A; Fukumizu, K",,"Bao, Han; Sugiyama, Masashi",,,Fenchel-Young Losses with Skewed Entropies for Class-posterior Probability Estimation,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We study class-posterior probability estimation (CPE) for binary responses where one class has much fewer data than the other. For example, events such as species co-occurrence in ecology and wars in political science are often much rarer than non-events. Logistic regression has been widely used for CPE, while it tends to underestimate the probability of rare events. Its main drawback is symmetry of the logit link-symmetric links can be misled by small and imbalanced samples because it is more incentivized to overestimate the majority class with finite samples. Parametric skewed links have been proposed to overcome this limitation, but their estimation usually results in nonconvex optimization unlike the logit link. Such nonconvexity is knotty not only from the computational viewpoint but also in terms of the parameter identifiability. In this paper, we provide a procedure to derive a convex loss for a skewed link based on the recently proposed FenchelYoung losses. The derived losses are always convex and have a nice property suitable for class imbalance. The simulation shows the practicality of the derived losses.",,,,,"Sugiyama, Masashi/AEO-1176-2022","Sugiyama, Masashi/0000-0001-6658-6743",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802003,0
C,"Braun, G; Tyagi, H; Biernacki, C",,"Banerjee, A; Fukumizu, K",,"Braun, Guillaume; Tyagi, Hemant; Biernacki, Christophe",,,Clustering multilayer graphs with missing nodes,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Relationships between agents can be conveniently represented by graphs. When these relationships have different modalities, they are better modelled by multilayer graphs in which each layer is associated with one modality. Such graphs arise naturally in many contexts including biological and social networks. Clustering is a fundamental problem in network analysis where the goal is to regroup nodes with similar connectivity profiles. In the past decade, various clustering methods have been extended from the unilayer setting to multilayer graphs in order to incorporate the information provided by each layer. While most existing works assume - rather restrictively - that all layers share the same set of nodes, we propose a new framework that allows for layers to be defined on different sets of nodes. In particular, the nodes not recorded in a layer are treated as missing. Within this paradigm, we investigate several generalizations of well-known clustering methods in the complete setting to the incomplete one and prove some consistency results under the Multi-Layer Stochastic Block Model assumption. Our theoretical results are complemented by thorough numerical comparisons between our proposed algorithms on synthetic data, and also on real datasets, thus highlighting the promising behaviour of our methods in various settings.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802071,0
C,"Brugge, K; Fischer, A; Igel, C",,"Banerjee, A; Fukumizu, K",,"Brugge, Kai; Fischer, Asja; Igel, Christian",,,On the Convergence of the Metropolis Algorithm with Fixed-Order Updates for Multivariate Binary Probability Distributions,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"The Metropolis algorithm is arguably the most fundamental Markov chain Monte Carlo (MCMC) method. But the algorithm is not guaranteed to converge to the desired distribution in the case of multivariate binary distributions (e.g., Ising models or stochastic neural networks such as Boltzmann machines) if the variables (sites or neurons) are updated in a fixed order, a setting commonly used in practice. The reason is that the corresponding Markov chain may not be irreducible. We propose a modified Metropolis transition operator that behaves almost always identically to the standard Metropolis operator and prove that it ensures irreducibility and convergence to the limiting distribution in the multivariate binary case with fixed-order updates. The result provides an explanation for the behaviour of Metropolis MCMC in that setting and closes a long-standing theoretical gap. We experimentally studied the standard and modified Metropolis operator for models where they actually behave differently. If the standard algorithm also converges, the modified operator exhibits similar (if not better) performance in terms of convergence speed.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,469,+,,,,,,,,,,,,,,,,WOS:000659893800053,0
C,"Cohen, E; Geri, O; Sarlos, T; Stemmer, U",,"Banerjee, A; Fukumizu, K",,"Cohen, Edith; Geri, Ofir; Sarlos, Tamas; Stemmer, Uri",,,Differentially Private Weighted Sampling,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Common datasets have the form of elements with keys (e.g., transactions and products) and the goal is to perform analytics on the aggregated form of key and frequency pairs. A weighted sample of keys by (a function of) frequency is a highly versatile summary that provides a sparse set of representative keys and supports approximate evaluations of query statistics. We propose private weighted sampling (PWS): A method that sanitizes a weighted sample as to ensure elementlevel differential privacy, while retaining its utility to the maximum extent possible. PWS maximizes the reporting probabilities of keys and estimation quality of a broad family of statistics. PWS improves over the state of the art even for the well-studied special case of private histograms, when no sampling is performed. We empirically observe significant performance gains of 20%-300% increase in key reporting for common Zipfian frequency distributions and accurate estimation with x2-8 lower frequencies. PWS is applied as a post-processing of a non-private sample, without requiring the original data. Therefore, it can be a seamless addition to existing implementations, such as those optimizes for distributed or streamed data. We believe that due to practicality and performance, PWS may become a method of choice in applications where privacy is desired.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802087,0
C,"Ghosh, A; Sankararaman, A; Ramchandran, K",,"Banerjee, A; Fukumizu, K",,"Ghosh, Avishek; Sankararaman, Abishek; Ramchandran, Kannan",,,Problem-Complexity Adaptive Model Selection for Stochastic Linear Bandits,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We consider the problem of model selection for two popular stochastic linear bandit settings, and propose algorithms that adapts to the unknown problem complexity. In the first setting, we consider the K armed mixture bandits, where the mean reward of arm i is an element of [K](2), is mu(i) + <alpha(i,t), theta*>, with alpha(i,t) is an element of R-d being the known context vector and mu(i) is an element of [-1, 1] and theta* are unknown parameters. We define(3) parallel to theta*parallel to as the problem complexity and consider a sequence of nested hypothesis classes, each positing a different upper bound on parallel to theta*parallel to. Exploiting this, we propose Adaptive Linear Bandit (ALB), a novel phase based algorithm that adapts to the true problem complexity, parallel to theta*parallel to. We show that ALB achieves regret scaling of(4) (O) over tilde(parallel to theta*parallel to root T), where parallel to theta*parallel to is apriori unknown. As a corollary, when theta* = 0, ALB recovers the minimax regret for the simple bandit algorithm without such knowledge of theta*. ALB is the first algorithm that uses parameter norm as model section criteria for linear bandits. Prior state of art algorithms (Chatterji et al. (2019)) achieve a regret of (O) over tilde (L root T), where L is the upper bound on parallel to theta*parallel to, fed as an input to the problem. In the second setting, we consider the standard linear bandit problem (with possibly an infinite number of arms) where the sparsity of theta*, denoted by d* <= d, is unknown to the algorithm. Defining d* as the problem complexity (similar to Foster et al. (2019)), we show that ALB achieves (O) over tilde (d*root T) regret, matching that of an oracle who knew the true sparsity level. This is the first algorithm that achieves such model selection guarantees. This methodology is then extended to the case of finitely many arms and similar results are proven. We further verify through synthetic and real-data experiments that the performance gains are fundamental and not artifacts of mathematical bounds. In particular, we show 1.5 - 3x drop in cumulative regret over non-adaptive algorithms.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801069,0
C,"Ghosh, S; Birrell, PJ; De Angelis, D",,"Banerjee, A; Fukumizu, K",,"Ghosh, Sanmitra; Birrell, Paul J.; De Angelis, Daniela",,,Variational inference for nonlinear ordinary differential equations,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We apply the reparameterisation trick to obtain a variational formulation of Bayesian inference in nonlinear ODE models. By invoking the linear noise approximation we also extend this variational formulation to a stochastic kinetic model. Our proposed inference method does not depend on any emulation of the ODE solution and only requires the extension of automatic differentiation to an ODE. We achieve this through a novel and holistic approach that uses both forward and adjoint sensitivity analysis techniques. Consequently, this approach can cater to both small and large ODE models efficiently. Upon benchmarking on some widely used mechanistic models, the proposed inference method produced a reliable approximation to the posterior distribution, with a significant reduction in execution time, in comparison to MCMC.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803029,0
C,"Girgis, AM; Data, D; Diggavi, S; Kairouz, P; Suresh, AT",,"Banerjee, A; Fukumizu, K",,"Girgis, Antonious M.; Data, Deepesh; Diggavi, Suhas; Kairouz, Peter; Suresh, Ananda Theertha",,,Shuffed Model of Differential Privacy in Federated Learning,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We consider a distributed empirical risk minimization (ERM) optimization problem with communication efficiency and privacy requirements, motivated by the federated learning (FL) framework. We propose a distributed communication-efficient and local differentially private stochastic gradient descent (CLDP-SGD) algorithm and analyze its communication, privacy, and convergence trade-offs. Since each iteration of the CLDP-SGD aggregates the client-side local gradients, we develop (optimal) communication-efficient schemes for mean estimation for several l(p) spaces under local differential privacy (LDP). To overcome performance limitation of LDP, CLDP-SGD takes advantage of the inherent privacy amplification provided by client sub-sampling and data subsampling at each selected client (through SGD) as well as the recently developed shuffed model of privacy. For convex loss functions, we prove that the proposed CLDP-SGD algorithm matches the known lower bounds on the centralized private ERM while using a finite number of bits per iteration for each client, i.e., effectively getting communication efficiency for free. We also provide preliminary experimental results supporting the theory.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803007,0
C,"Guo, QP; Jin, ZJ; Wang, ZY; Qiu, XP; Zhang, WN; Zhu, J; Zhang, Z; Wipf, D",,"Banerjee, A; Fukumizu, K",,"Guo, Qipeng; Jin, Zhijing; Wang, Ziyu; Qiu, Xipeng; Zhang, Weinan; Zhu, Jun; Zhang, Zheng; Wipf, David",,,Fork or Fail: Cycle-Consistent Training with Many-to-One Mappings,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Cycle-consistent training is widely used for jointly learning a forward and inverse mapping between two domains of interest without the cumbersome requirement of collecting matched pairs within each domain. In this regard, the implicit assumption is that there exists (at least approximately) a ground-truth bijection such that a given input from either domain can be accurately reconstructed from successive application of the respective mappings. But in many applications no such bijection can be expected to exist and large reconstruction errors can compromise the success of cycle-consistent training. As one important instance of this limitation, we consider practically-relevant situations where there exists a many-to-one or surjective mapping between domains. To address this regime, we develop a conditional variational auto-encoder (CVAE) approach that can be viewed as converting surjective mappings to implicit bijections whereby reconstruction errors in both directions can be minimized, and as a natural byproduct, realistic output diversity can be obtained in the one-to-many direction. As theoretical motivation, we analyze a simplified scenario whereby minima of the proposed CVAE-based energy function align with the recovery of ground-truth surjective mappings. On the empirical side, we consider a synthetic image dataset with known ground-truth, as well as a real-world application involving natural language generation from knowledge graphs and vice versa, a prototypical surjective case. For the latter, our CVAE pipeline can capture such many-to-one mappings during cycle training while promoting textural diversity for graph-to-text tasks.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802023,0
C,"Hsu, D; Muthukumar, V; Xu, J",,"Banerjee, A; Fukumizu, K",,"Hsu, Daniel; Muthukumar, Vidya; Xu, Ji",,,On the proliferation of support vectors in high dimensions,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"The support vector machine (SVM) is a well-established classification method whose name refers to the particular training examples, called support vectors, that determine the maximum margin separating hyperplane. The SVM classifier is known to enjoy good generalization properties when the number of support vectors is small compared to the number of training examples. However, recent research has shown that in sufficiently high-dimensional linear classification problems, the SVM can generalize well despite a proliferation of support vectors where all training examples are support vectors. In this paper, we identify new deterministic equivalences for this phenomenon of support vector proliferation, and use them to (1) substantially broaden the conditions under which the phenomenon occurs in high-dimensional settings, and (2) prove a nearly matching converse result.",,,,,,"Muthukumar, Vidya/0000-0003-2786-7360",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,91,+,,,,,,,,,,,,,,,,WOS:000659893800011,0
C,"Kwon, J; Ho, N; Caramanis, C",,"Banerjee, A; Fukumizu, K",,"Kwon, Jeongyeol; Ho, Nhat; Caramanis, Constantine",,,On the Minimax Optimality of the EM Algorithm for Learning Two-Component Mixed Linear Regression,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We study the convergence rates of the EM algorithm for learning two-component mixed linear regression under all regimes of signal-to-noise ratio (SNR). We resolve a long-standing question that many recent results have attempted to tackle: we completely characterize the convergence behavior of EM, and show that the EM algorithm achieves minimax optimal sample complexity under all SNR regimes. In particular, when the SNR is sufficiently large, the EM updates converge to the true parameter theta* at the standard parametric convergence rate O((d/n)(1/2)) after O(log(n/d)) iterations. In the regime where the SNR is above O((d/n)(1/4)) and below some constant, the EM iterates converge to a O(SNR-1 (d/n)(1/2)) neighborhood of the true parameter, when the number of iterations is of the order O(SNR-2 log(n/d)). In the low SNR regime where the SNR is below O((d/n)(1/4)), we show that EM converges to a O((d/n)(1/4)) neighborhood of the true parameters, after O((n/d)(1/2)) iterations. Notably, these results are achieved under mild conditions of either random initialization or an efficiently computable local initialization. By providing tight convergence guarantees of the EM algorithm in middle-to-low SNR regimes, we fill the remaining gap in the literature, and significantly, reveal that in low SNR, EM changes rate, matching the n(-1/4) rate of the MLE, a behavior that previous work had been unable to show.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801070,0
C,"Le, T; Nguyen, T",,"Banerjee, A; Fukumizu, K",,"Le, Tam; Nguyen, Truyen",,,Entropy Partial Transport with Tree Metrics: Theory and Practice,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Optimal transport (OT) theory provides powerful tools to compare probability measures. However, OT is limited to nonnegative measures having the same mass, and suffers serious drawbacks about its computation and statistics. This leads to several proposals of regularized variants of OT in the recent literature. In this work, we consider an entropy partial transport (EPT) problem for nonnegative measures on a tree having different masses. The EPT is shown to be equivalent to a standard complete OT problem on a one-node extended tree. We derive its dual formulation, then leverage this to propose a novel regularization for EPT which admits fast computation and negative definiteness. To our knowledge, the proposed regularized EPT is the first approach that yields a closed-form solution among available variants of unbalanced OT for general nonnegative measures. For practical applications without prior knowledge about the tree structure for measures, we propose tree-sliced variants of the regularized EPT, computed by averaging the regularized EPT between these measures using random tree metrics, built adaptively from support data points. Exploiting the negative definiteness of our regularized EPT, we introduce a positive definite kernel, and evaluate it against other baselines on benchmark tasks such as document classification with word embedding and topological data analysis. In addition, we empirically demonstrate that our regularization also provides effective approximations.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804061,0
C,"Lew, AK; Agrawal, M; Sontag, D; Mansinghka, VK",,"Banerjee, A; Fukumizu, K",,"Lew, Alexander K.; Agrawal, Monica; Sontag, David; Mansinghka, Vikash K.",,,PClean: Bayesian Data Cleaning at Scale with Domain-Specific Probabilistic Programming,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Data cleaning is naturally framed as probabilistic inference in a generative model of ground-truth data and likely errors, but the diversity of real-world error patterns and the hardness of inference make Bayesian approaches difficult to automate. We present PClean, a probabilistic programming language (PPL) for leveraging dataset-specific knowledge to automate Bayesian cleaning. Compared to general-purpose PPLs, PClean tackles a restricted problem domain, enabling three modeling and inference innovations: (1) a non-parametric model of relational database instances, which users' programs customize; (2) a novel sequential Monte Carlo inference algorithm that exploits the structure of PClean's model class; and (3) a compiler that generates near-optimal SMC proposals and blocked-Gibbs rejuvenation kernels based on the user's model and data. We show empirically that short (<50-line) PClean programs can: be faster and more accurate than generic PPL inference on data-cleaning benchmarks; match state-of-the-art data-cleaning systems in terms of accuracy and runtime (unlike generic PPL inference in the same runtime); and scale to real-world datasets with millions of records.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802034,0
C,"Liu, JZ",,"Banerjee, A; Fukumizu, K",,"Liu, Jeremiah Zhe",,,Variable Selection with Rigorous Uncertainty Quantification using Deep Bayesian Neural Networks: Posterior Concentration and Bernstein-von Mises Phenomenon,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"This work develops a theoretical basis for a deep Bayesian neural network (BNN)'s ability in performing high-dimensional variable selection with rigorous uncertainty quantification. We develop new Bayesian non-parametric theorems to show that a properly configured deep BNN (1) learns the variable importance effectively in high dimensions, and its learning rate can sometimes\break the curse of dimensionality. (2) BNN's uncertainty quantification for variable importance is rigorous, in the sense that its 95% credible intervals for variable importance indeed covers the truth 95% of the time (i.e. the Bernstein-von Mises (BvM) phenomenon). The theoretical results suggest a simple variable selection algorithm based on the BNN's credible intervals. Extensive simulation confirms the theoretical findings and shows that the proposed algorithm outperforms existing classic and neural-network-based variable selection methods, particularly in high dimensions.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803074,0
C,"Mroueh, Y; Nguyen, T",,"Banerjee, A; Fukumizu, K",,"Mroueh, Youssef; Truyen Nguyen",,,On the Convergence of Gradient Descent in GANs: MMD GAN As a Gradient Flow,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We consider the maximum mean discrepancy (MMD) GAN problem and propose a parametric kernelized gradient flow that mimics the min-max game in gradient regularized MMD GAN. We show that this flow provides a descent direction minimizing the MMD on a statistical manifold of probability distributions. We then derive an explicit condition which ensures that gradient descent on the parameter space of the generator in gradient regularized MMD GAN is globally convergent to the target distribution. Under this condition, we give non asymptotic convergence results of gradient descent in MMD GAN. Another contribution of this paper is the introduction of a dynamic formulation of a regularization of MMD and demonstrating that the parametric kernelized descent for MMD is the gradient flow of this functional with respect to the new Riemannian structure. Our obtained theoretical result allows ones to treat gradient flows for quite general functionals and thus has potential applications to other types of variational inferences on a statistical manifold beyond GANs. Finally, numerical experiments suggest that our parametric kernelized gradient flow stabilizes GAN training and guarantees convergence.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802011,0
C,"Murakonda, SK; Shokri, R; Theodorakopoulos, G",,"Banerjee, A; Fukumizu, K",,"Murakonda, Sasi Kumar; Shokri, Reza; Theodorakopoulos, George",,,Quantifying the Privacy Risks of Learning High-Dimensional Graphical Models,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Models leak information about their training data. This enables attackers to infer sensitive information about their training sets, notably determine if a data sample was part of the model's training set. The existing works empirically show the possibility of these membership inference (tracing) attacks against complex deep learning models. However, the attack results are dependent on the specific training data, can be obtained only after the tedious process of training the model and performing the attack, and are missing any measure of the confidence and unused potential power of the attack. In this paper, we theoretically analyze the maximum power of tracing attacks against high-dimensional graphical models, with the focus on Bayesian networks. We provide a tight upper bound on the power (true positive rate) of these attacks, with respect to their error (false positive rate), for a given model structure even before learning its parameters. As it should be, the bound is independent of the knowledge and algorithm of any specific attack. It can help in identifying which model structures leak more information, how adding new parameters to the model increases its privacy risk, and what can be gained by adding new data points to decrease the overall information leakage. It provides a measure of the potential leakage of a model given its structure, as a function of the model complexity and the size of the training set.",,,,,,"Shokri, Reza/0000-0001-9816-0173",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802074,0
C,"Rossi, S; Heinonen, M; Bonilla, EV; Shen, ZY; Filippone, M",,"Banerjee, A; Fukumizu, K",,"Rossi, Simone; Heinonen, Markus; Bonilla, Edwin, V; Shen, Zheyang; Filippone, Maurizio",,,Sparse Gaussian Processes Revisited: Bayesian Approaches to Inducing-Variable Approximations,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Variational inference techniques based on inducing variables provide an elegant framework for scalable posterior estimation in Gaussian process (GP) models. Besides enabling scalability, one of their main advantages over sparse approximations using direct marginal likelihood maximization is that they provide a robust alternative for point estimation of the inducing inputs, i.e. the location of the inducing variables. In this work we challenge the common wisdom that optimizing the inducing inputs in the variational framework yields optimal performance. We show that, by revisiting old model approximations such as the fully-independent training conditionals endowed with powerful sampling-based inference methods, treating both inducing locations and GP hyper-parameters in a Bayesian way can improve performance significantly. Based on stochastic gradient Hamiltonian Monte Carlo, we develop a fully Bayesian approach to scalable gp and deep gp models, and demonstrate its state-of-the-art performance through an extensive experimental campaign across several regression and classification problems.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802024,0
C,"Tang, YH; Kucukelbir, A",,"Banerjee, A; Fukumizu, K",,"Tang, Yunhao; Kucukelbir, Alp",,,Hindsight Expectation Maximization for Goal-conditioned Reinforcement Learning,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We propose a graphical model framework for goal-conditioned reinforcement learning (RL), with an expectation maximization (EM) algorithm that operates on the lower bound of the RL objective. The E-step provides a natural interpretation of how 'learning in hindsight' techniques, such as hindsight experience replay (her), handle extremely sparse goal-conditioned rewards. The M-step reduces policy optimization to supervised learning updates, which stabilizes end-to-end training on high-dimensional inputs such as images. Our proposed method, called hindsight expectation maximization (hem), significantly outperforms model-free baselines on a wide range of goal-conditioned benchmarks with sparse rewards.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803045,0
C,"Wagenmaker, A; Katz-Samuels, J; Jamieson, K",,"Banerjee, A; Fukumizu, K",,"Wagenmaker, Andrew; Katz-Samuels, Julian; Jamieson, Kevin",,,Experimental Design for Regret Minimization in Linear Bandits,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"In this paper we propose a novel experimental design-based algorithm to minimize regret in online stochastic linear and combinatorial bandits. While existing literature tends to focus on optimism-based algorithms-which have been shown to be suboptimal in many cases-our approach carefully plans which action to take by balancing the tradeo. between information gain and reward, overcoming the failures of optimism. In addition, we leverage tools from the theory of suprema of empirical processes to obtain regret guarantees that scale with the Gaussian width of the action set, avoiding wasteful union bounds. We provide state-of-the-art finite time regret guarantees and show that our algorithm can be applied in both the bandit and semibandit feedback regime. In the combinatorial semi-bandit setting, we show that our algorithm is computationally efficient and relies only on calls to a linear maximization oracle. In addition, we show that with slight modification our algorithm can be used for pure exploration, obtaining state-of-the-art pure exploration guarantees in the semi-bandit setting. Finally, we provide, to the best of our knowledge, the first example where optimism fails in the semi-bandit regime, and show that in this setting our algorithm succeeds.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803070,0
C,"Xu, SY; Bu, ZQ",,"Banerjee, A; Fukumizu, K",,"Xu, Shiyun; Bu, Zhiqi",,,DebiNet: Debiasing Linear Models with Nonlinear Overparameterized Neural Networks,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Recent years have witnessed strong empirical performance of over-parameterized neural networks on various tasks and many advances in the theory, e.g. the universal approximation and provable convergence to global minimum. In this paper, we incorporate over-parameterized neural networks into semi-parametric models to bridge the gap between inference and prediction, especially in the high dimensional linear problem. By doing so, we can exploit a wide class of networks to approximate the nuisance functions and to estimate the parameters of interest consistently. Therefore, we may offer the best of two worlds: the universal approximation ability from neural networks and the interpretability from classic ordinary linear model, leading to both valid inference and accurate prediction. We show the theoretical foundations that make this possible and demonstrate with numerical experiments. Furthermore, we propose a framework, DebiNet, in which we plug-in arbitrary feature selection methods to our semi-parametric neural network. DebiNet can debias the regularized estimators (e.g. Lasso) and perform well, in terms of the post-selection inference and the generalization error.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803071,0
C,"Yang, KH; Yang, LF; Du, SS",,"Banerjee, A; Fukumizu, K",,"Yang, Kunhe; Yang, Lin F.; Du, Simon S.",,,Q-learning with Logarithmic Regret,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"This paper presents the first non-asymptotic result showing a model-free algorithm can achieve logarithmic cumulative regret for episodic tabular reinforcement learning if there exists a strictly positive sub-optimality gap. We prove that the optimistic Q-learning studied in [Jin et al. 2018] enjoys a O(SA.poly(H)/Delta(min) log (SAT)) cumulative regret bound where S is the number of states, A is the number of actions, H is the planning horizon, T is the total number of steps, and Delta(min) is the minimum sub-optimality gap of the optimal Q-function. This bound matches the lower bound in terms of S, A, T up to a log (SA) factor. We further extend our analysis to the discounted setting and obtain a similar logarithmic cumulative regret bound.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801089,0
C,"Askari, A; d'Aspremont, A; El Ghaoui, L",,"Chiappa, S; Calandra, R",,"Askari, Armin; d'Aspremont, Alex; El Ghaoui, Laurent",,,Naive Feature Selection: Sparsity in Naive Bayes,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Due to its linear complexity, naive Bayes classification remains an attractive supervised learning method, especially in very large-scale settings. We propose a sparse version of naive Bayes, which can be used for feature selection. This leads to a combinatorial maximum-likelihood problem, for which we provide an exact solution in the case of binary data, or a bound in the multinomial case. We prove that our bound becomes tight as the marginal contribution of additional features decreases. Both binary and multinomial sparse models are solvable in time almost linear in problem size, representing a very small extra relative cost compared to the classical naive Bayes. Numerical experiments on text data show that the naive Bayes feature selection method is as statistically effective as state-of-the-art feature selection methods such as recursive feature elimination, l(1)-penalized logistic regression and LASSO, while being orders of magnitude faster. For a large data set, having more than with 1:6 million training points and about 12 million features, and with a non-optimized CPU implementation, our model can be trained in less than 15 seconds(1).",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1813,1821,,,,,,,,,,,,,,,,WOS:000559931300016,0
C,"Awasthi, P; Kleindessner, M; Morgenstern, J",,"Chiappa, S; Calandra, R",,"Awasthi, Pranjal; Kleindessner, Matthaus; Morgenstern, Jamie",,,Equalized odds postprocessing under imperfect group information,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Most approaches aiming to ensure a model's fairness with respect to a protected attribute (such as gender or race) assume to know the true value of the attribute for every data point. In this paper, we ask to what extent fairness interventions can be effective even when only imperfect information about the protected attribute is available. In particular, we study the prominent equalized odds postprocessing method of Hardt et al. (2016) under a perturbation of the attribute. We identify conditions on the perturbation that guarantee that the bias of a classifier is reduced even by running equalized odds with the perturbed attribute. We also study the error of the resulting classifier. We empirically observe that under our identified conditions most often the error does not suffer from a perturbation of the protected attribute. For a special case, we formally prove this observation to be true.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1770,1779,,,,,,,,,,,,,,,,WOS:000559931300019,0
C,"Azizian, W; Mitliagkas, I; Lacoste-Julien, S; Gidel, G",,"Chiappa, S; Calandra, R",,"Azizian, Waiss; Mitliagkas, Ioannis; Lacoste-Julien, Simon; Gidel, Gauthier",,,A Tight and Unified Analysis of Gradient-Based Methods for a Whole Spectrum of Differentiable Games,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We consider differentiable games where the goal is to find a Nash equilibrium. The machine learning community has recently started using variants of the gradient method (GD). Prime examples are extragradient (EG), the optimistic gradient method (OG) and consensus optimization (CO), which enjoy linear convergence in cases like bilinear games, where the standard GD fails. The full benefits of theses relatively new methods are not known as there is no unified analysis for both strongly monotone and bilinear games. We provide new analyses of the EG's local and global convergence properties and use is to get a tighter global convergence rate for OG and CO. Our analysis covers the whole range of settings between bilinear and strongly monotone games. It reveals that these methods converges via different mechanisms at these extremes; in between, it exploits the most favorable mechanism for the given problem. We then prove that EG achieves the optimal rate for a wide class of algorithms with any number of extrapolations. Our tight analysis of EG's convergence rate in games shows that, unlike in convex minimization, EG may be much faster than GD.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2863,2872,,,,,,,,,,,,,,,,WOS:000559931300021,0
C,"Blaas, A; Patane, A; Laurenti, L; Cardelli, L; Kwiatkowska, M; Roberts, S",,"Chiappa, S; Calandra, R",,"Blaas, Arno; Patane, Andrea; Laurenti, Luca; Cardelli, Luca; Kwiatkowska, Marta; Roberts, Stephen",,,Adversarial Robustness Guarantees for Classification with Gaussian Processes,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We investigate adversarial robustness of Gaussian Process Classification (GPC) models. Given a compact subset of the input space T subset of R-d enclosing a test point x* and a GPC trained on a dataset D, we aim to compute the minimum and the maximum classification probability for the GPC over all the points in T. In order to do so, we show how functions lower- and upper-bounding the GPC output in T can be derived, and implement those in a branch and bound optimisation algorithm. For any error threshold epsilon > 0 selected a priori, we show that our algorithm is guaranteed to reach values epsilon-close to the actual values in finitely many iterations. We apply our method to investigate the robustness of GPC models on a 2D synthetic dataset, the SPAM dataset and a subset of the MNIST dataset, providing comparisons of different GPC training techniques, and show how our method can be used for interpretability analysis. Our empirical analysis suggests that GPC robustness increases with more accurate posterior estimation.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3372,3381,,,,,,,,,,,,,,,,WOS:000559931300033,0
C,"Boursier, E; Perchet, V",,"Chiappa, S; Calandra, R",,"Boursier, Etienne; Perchet, Vianney",,,Utility/Privacy Trade-off through the lens of Optimal Transport,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Strategic information is valuable either by remaining private (for instance if it is sensitive) or, on the other hand, by being used publicly to increase some utility. These two objectives are antagonistic and leaking this information might be more rewarding than concealing it. Unlike classical solutions that focus on the first point, we consider instead agents that optimize a natural trade-off between both objectives. We formalize this as an optimization problem where the objective mapping is regularized by the amount of information revealed to the adversary (measured as a divergence between the prior and posterior on the private knowledge). Quite surprisingly, when combined with the entropic regularization, the Sinkhorn loss naturally emerges in the optimization objective, making it efficiently solvable. We apply these techniques to preserve some privacy in online repeated auctions.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,591,600,,,,,,,,,,,,,,,,WOS:000559931300037,0
C,"Goldfeld, Z; Greenewald, K",,"Chiappa, S; Calandra, R",,"Goldfeld, Ziv; Greenewald, Kristjan",,,Gaussian-Smoothed Optimal Transport: Metric Structure and Statistical Efficiency,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Optimal transport (OT), in particular the Wasserstein distance, has seen a surge of interest and applications in machine learning. However, empirical approximation under Wasserstein distances suffers from a severe curse of dimensionality, rendering them impractical in high dimensions. As a result, entropically regularized OT has become a popular workaround. While it enjoys fast algorithms and better statistical properties, it however loses the metric structure that Wasserstein distances enjoy. This work proposes a novel Gaussian-smoothed OT (GOT) framework, that achieves the best of both worlds: preserving the 1-Wasserstein metric structure while alleviating the empirical approximation curse of dimensionality. Furthermore, as the Gaussian-smoothing parameter shrinks to zero, GOT Gamma-converges towards classic OT (with convergence of optimizers), thus serving as a natural extension. An empirical study that supports the theoretical results is provided, promoting Gaussian-smoothed OT as a powerful alternative to entropic OT.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3327,3336,,,,,,,,,,,,,,,,WOS:000559931301023,0
C,"Martens, K; Yau, C",,"Chiappa, S; Calandra, R",,"Martens, Kaspar; Yau, Christopher",,,Neural Decomposition: Functional ANOVA with Variational Autoencoders,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Variational Autoencoders (VAEs) have become a popular approach for dimensionality reduction. However, despite their ability to identify latent low-dimensional structures embedded within high-dimensional data, these latent representations are typically hard to interpret on their own. Due to the black-box nature of VAEs, their utility for healthcare and genomics applications has been limited. In this paper, we focus on characterising the sources of variation in Conditional VAEs. Our goal is to provide a feature-level variance decomposition, i.e. to decompose variation in the data by separating out the marginal additive effects of latent variables z and fixed inputs c from their non-linear interactions. We propose to achieve this through what we call Neural Decomposition an adaptation of the well-known concept of functional ANOVA variance decomposition from classical statistics to deep learning models. We show how identifiability can be achieved by training models subject to constraints on the marginal properties of the decoder networks. We demonstrate the utility of our Neural Decomposition on a series of synthetic examples as well as high-dimensional genomics data.",,,,,,"Yau, Christopher/0000-0001-7615-8523",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2917,2926,,,,,,,,,,,,,,,,WOS:000559931302031,0
C,"Rakhshan, BT; Rabusseau, G",,"Chiappa, S; Calandra, R",,"Rakhshan, Beheshteh T.; Rabusseau, Guillaume",,,Tensorized Random Projections,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We introduce a novel random projection technique for efficiently reducing the dimension of very high-dimensional tensors. Building upon classical results on Gaussian random projections and Johnson-Lindenstrauss transforms (JLT), we propose two tensorized random projection maps relying on the tensor train (TT) and CP decomposition format, respectively. The two maps offer very low memory requirements and can be applied efficiently when the inputs are low rank tensors given in the CP or TT format. Our theoretical analysis shows that the dense Gaussian matrix in JLT can be replaced by a low-rank tensor implicitly represented in compressed form with random factors, while still approximately preserving the Euclidean distance of the projected inputs. In addition, our results reveal that the TT format is substantially superior to CP in terms of the size of the random projection needed to achieve the same distortion ratio. Experiments on synthetic data validate our theoretical analysis and demonstrate the superiority of the TT decomposition.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3306,3315,,,,,,,,,,,,,,,,WOS:000559931302087,0
C,"Ustyuzhaninov, I; Kazlauskaite, I; Ek, CH; Campbell, NDF",,"Chiappa, S; Calandra, R",,"Ustyuzhaninov, Ivan; Kazlauskaite, Ieva; Ek, Carl Henrik; Campbell, Neill D. F.",,,Monotonic Gaussian Process Flows,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We propose a new framework for imposing monotonicity constraints in a Bayesian non-parametric setting based on numerical solutions of stochastic differential equations. We derive a nonparametric model of monotonic functions that allows for interpretable priors and principled quantification of hierarchical uncertainty. We demonstrate the efficacy of the proposed model by providing competitive results to other probabilistic monotonic models on a number of benchmark functions. In addition, we consider the utility of a monotonic random process as a part of a hierarchical probabilistic model; we examine the task of temporal alignment of time-series data where it is beneficial to use a monotonic random process in order to preserve the uncertainty in the temporal warpings.",,,,,,"Kazlauskaite, Ieva/0000-0001-9690-0887",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303048,0
C,"Wang, H; Keskar, NS; Xiong, CM; Socher, R",,"Chiappa, S; Calandra, R",,"Wang, Huan; Keskar, Nitish Shirish; Xiong, Caiming; Socher, Richard",,,Assessing Local Generalization Capability in Deep Models,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"While it has not yet been proven, empirical evidence suggests that model generalization is related to local properties of the optima, which can be described via the Hessian. We connect model generalization with the local property of a solution under the PAC-Bayes paradigm. In particular, we prove that model generalization ability is related to the Hessian, the higher-order smoothness terms characterized by the Lipschitz constant of the Hessian, and the scales of the parameters. Guided by the proof, we propose a metric to score the generalization capability of a model, as well as an algorithm that optimizes the perturbed model accordingly.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303064,0
C,"Xue, SK; Yurochkin, M; Sun, YK",,"Chiappa, S; Calandra, R",,"Xue, Songkai; Yurochkin, Mikhail; Sun, Yuekai",,,Auditing ML Models for Individual Bias and Unfairness,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We consider the task of auditing ML models for individual bias/unfairness. We formalize the task in an optimization problem and develop a suite of inferential tools for the optimal value. Our tools permit us to obtain asymptotic confidence intervals and hypothesis tests that cover the target/control the Type I error rate exactly. To demonstrate the utility of our tools, we use them to reveal the gender and racial biases in Northpointe's COMPAS recidivism prediction instrument.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303089,0
C,"Yang, KG; Dong, XW; Toni, L",,"Chiappa, S; Calandra, R",,"Yang, Kaige; Dong, Xiaowen; Toni, Laura",,,Laplacian-Regularized Graph Bandits: Algorithms and Theoretical Analysis,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We consider a stochastic linear bandit problem with multiple users, where the relationship between users is captured by an underlying graph and user preferences are represented as smooth signals on the graph. We introduce a novel bandit algorithm where the smoothness prior is imposed via the random-walk graph Laplacian, which leads to a single-user cumulative regret scaling as (O) over tilde(Psi d root T) with time horizon T, feature dimensionality d, and the scalar parameter Psi is an element of (0, 1) that depends on the graph connectivity. This is an improvement over (O) over tilde (d root T) in LinUCB [Li et al., 2010], where user relationship is not taken into account. In terms of network regret (sum of cumulative regret over n users), the proposed algorithm leads to a scaling as (O) over tilde(Psi d root nT), which is a significant improvement over (O) over tilde (nd root T) in the state-of-the-art algorithm Gob.Lin [Cesa-Bianchi et al., 2013]. To improve scalability, we further propose a simplified algorithm with a linear computational complexity with respect to the number of users, while maintaining the same regret. Finally, we present a finite-time analysis on the proposed algorithms, and demonstrate their advantage in comparison with state-of-the-art graph-based bandit algorithms on both synthetic and real-world data.",,,,,"Dong, Xiaowen/AAJ-5058-2021",,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303094,0
C,"Zeng, YB; Feng, F; Yin, WT",,"Chiappa, S; Calandra, R",,"Zeng, Yibo; Feng, Fei; Yin, Wotao",,,AsyncQVI: Asynchronous-Parallel Q-Value Iteration for Discounted Markov Decision Processes with Near-Optimal Sample Complexity,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"In this paper, we propose AsyncQVI, an asynchronous-parallel Q-value iteration for discounted Markov decision processes whose transition and reward can only be sampled through a generative model. Given such a problem with vertical bar S vertical bar states, vertical bar A vertical bar actions, and a discounted factor gamma is an element of (0, 1), AsyncQVI uses memory of size O(vertical bar S vertical bar) and returns an e-optimal policy with probability at least 1 - delta using (O) over tilde(vertical bar S vertical bar vertical bar A vertical bar/(1 - gamma)(5)epsilon(2) log(1/delta)) samples.(1) AsyncQVI is also the first asynchronous-parallel algorithm for discounted Markov decision processes that has a sample complexity, which nearly matches the theoretical lower bound. The relatively low memory footprint and parallel ability make AsyncQVI suitable for large-scale applications. In numerical tests, we compare AsyncQVI with four sample-based value iteration methods. The results show that our algorithm is highly efficient and achieves linear parallel speedup.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,713,722,,,,,,,,,,,,,,,,WOS:000559931304006,0
C,"Acharya, J; Canonne, CL; Freitag, C; Tyagi, H",,"Chaudhuri, K; Sugiyama, M",,"Acharya, Jayadev; Canonne, Clement L.; Freitag, Cody; Tyagi, Himanshu",,,Test without Trust: Optimal Locally Private Distribution Testing,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We study the problem of distribution testing when the samples can only be accessed using a locally differentially private mechanism and consider two representative testing questions of identity (goodness-of-fit) and independence testing for discrete distributions. First, we construct tests that use existing, general-purpose locally differentially private mechanisms such as the popular RAPPOR or the recently introduced Hadamard Response for collecting data and propose tests that are sample optimal, when we insist on using these mechanisms. Next, we allow bespoke mechanisms designed specifically for testing and introduce the Randomized Aggregated Private Testing Optimal Response (RAPTOR) mechanism which is remarkably simple and requires only one bit of communication per sample. We show that our proposed mechanism yields sample-optimal tests, and in particular, outperforms any test based on RAPPOR or Hadamard Response. A distinguishing feature of our optimal mechanism is that, in contrast to existing mechanisms, it uses public randomness.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902012,0
C,"Chowdhury, SR; Gopalan, A",,"Chaudhuri, K; Sugiyama, M",,"Chowdhury, Sayak Ray; Gopalan, Aditya",,,Online Learning in Kernelized Markov Decision Processes,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We consider online learning for minimizing regret in unknown, episodic Markov decision processes (MDPs) with continuous states and actions. We develop variants of the UCRL and posterior sampling algorithms that employ non parametric Gaussian process priors to generalize across the state and action spaces. When the transition and reward functions of the true MDP are members of the associated Reproducing Kernel Hilbert Spaces of functions induced by symmetric psd kernels, we show that the algorithms enjoy sublinear regret bounds. The bounds are in terms of explicit structural parameters of the kernels, namely a novel generalization of the information gain metric from kernelized bandit, and highlight the influence of transition and reward function structure on the learning performance. Our results are applicable to multi-dimensional state and action spaces with composite kernel structures, and generalize results from the literature on kernelized bandits, and the adaptive control of parametric linear dynamical systems with quadratic costs.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687903026,0
C,"Hainline, J; Juba, B; Le, HS; Woodruff, DP",,"Chaudhuri, K; Sugiyama, M",,"Hainline, John; Juba, Brendan; Le, Hai S.; Woodruff, David P.",,,Conditional Sparse lp-norm Regression With Optimal Probability,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We consider the following conditional linear regression problem: the task is to identify both (i) a k-DNF condition c and (ii) a linear rule f such that the probability of c is (approximately) at least some given bound, and f minimizes the. loss of predicting the target z in the distribution of examples conditioned on c. Thus, the task is to identify a portion of the distribution on which a linear rule can provide a good fit. Algorithms for this task are useful in cases where simple, learnable rules only accurately model portions of the distribution. The prior state-of-the-art for such algorithms could only guarantee to find a condition of probability S2( /nk) when a condition of probability exists, and achieved an O(nk)-approximation to the target loss, where n is the number of Boolean attributes. Here, we give efficient algorithms for solving this task with a condition c that nearly matches the probability of the ideal condition, while also improving the approximation to the target loss. We also give an algorithm for finding a k-DNF reference class for prediction at a given query point, that obtains a sparse regression fit that has loss within O(nk) of optimal among all sparse regression parameters and sufficiently large k-DNF reference classes containing the query point.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901009,0
C,"Hyvarinen, A; Sasaki, H; Turner, RE",,"Chaudhuri, K; Sugiyama, M",,"Hyvarinen, Aapo; Sasaki, Hiroaki; Turner, Richard E.",,,Nonlinear ICA Using Auxiliary Variables and Generalized Contrastive Learning,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Nonlinear ICA is a fundamental problem for unsupervised representation learning, emphasizing the capacity to recover the underlying latent variables generating the data (i.e., identifiability). Recently, the very first identifiability proofs for nonlinear ICA have been proposed, leveraging the temporal structure of the independent components. Here, we propose a general framework for nonlinear ICA, which, as a special case, can make use of temporal structure. It is based on augmenting the data by an auxiliary variable, such as the time index, the history of the time series, or any other available information. We propose to learn nonlinear ICA by discriminating between true augmented data, or data in which the auxiliary variable has been randomized. This enables the framework to be implemented algorithmically through logistic regression, possibly in a neural network. We provide a comprehensive proof of the identifiability of the model as well as the consistency of our estimation method. The approach not only provides a general theoretical framework combining and generalizing previously proposed nonlinear ICA models and algorithms, but also brings practical advantages.",,,,,"Sasaki, Hiroaki/GXG-5024-2022","Hyvarinen, Aapo/0000-0002-5806-4432",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,859,868,,,,,,,,,,,,,,,,WOS:000509687900089,0
C,"Kazlauskaite, I; Ek, CH; Campbell, NDF",,"Chaudhuri, K; Sugiyama, M",,"Kazlauskaite, Ieva; Ek, Carl Henrik; Campbell, Neill D. F.",,,Gaussian Process Latent Variable Alignment Learning,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We present a model that can automatically learn alignments between high-dimensional data in an unsupervised manner. Our proposed method casts alignment learning in a framework where both alignment and data are modelled simultaneously. Further, we automatically infer groupings of different types of sequences within the same dataset. We derive a probabilistic model built on non-parametric priors that allows for flexible warps while at the same time providing means to specify interpretable constraints. We demonstrate the efficacy of our approach with superior quantitative performance to the state-of-the-art approaches and provide examples to illustrate the versatility of our model in automatic inference of sequence groupings, absent from previous approaches, as well as easy specification of high level priors for different modalities of data.",,,,,,"Kazlauskaite, Ieva/0000-0001-9690-0887",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,748,757,,,,,,,,,,,,,,,,WOS:000509687900078,0
C,"Mucke, N",,"Chaudhuri, K; Sugiyama, M",,"Muecke, Nicole",,,Reducing training time by efficient localized kernel regression,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We study generalization properties of kernel regularized least squares regression based on a partitioning approach. We show that optimal rates of convergence are preserved if the number of local sets grows sufficiently slowly with the sample size. Moreover, the partitioning approach can be efficiently combined with local Nystrom subsampling, improving computational cost twofold.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902067,0
C,"Neykov, M",,"Chaudhuri, K; Sugiyama, M",,"Neykov, Matey",,,Tossing Coins Under Monotonicity,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"This paper considers the following problem: we are given n coin tosses of coins with monotone increasing probability of getting heads (success). We study the performance of the monotone constrained likelihood estimate, which is equivalent to the estimate produced by isotonic regression. We derive adaptive and non-adaptive bounds on the performance of the isotonic estimate, i.e., we demonstrate that for some probability vectors the isotonic estimate converges much faster than in general. As an application of this framework we propose a two step procedure for the binary monotone single index model, which consists of running LASSO and consequently running an isotonic regression. We provide thorough numerical studies in support of our claims.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,21,29,,,,,,,,,,,,,,,,WOS:000509687900003,0
C,"Neykov, M",,"Chaudhuri, K; Sugiyama, M",,"Neykov, Matey",,,Gaussian Regression with Convex Constraints,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"The focus of this paper is the linear model with Gaussian design under convex constraints. Specifically, we study the performance of the constrained least squares estimate. We derive two general results characterizing its performance one requiring a tangent cone structure, and one which holds in a general setting. We use our general results to analyze three functional shape constrained problems where the signal is generated from an underlying Lipschitz, monotone or convex function. In each of the examples we show specific classes of functions which achieve fast adaptive estimation rates, and we also provide non-adaptive estimation rates which hold for any function. Our results demonstrate that the Lipschitz, monotone and convex constraints allow one to analyze regression problems even in high-dimensional settings where the dimension may scale as the square or fourth degree of the sample size respectively.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,30,37,,,,,,,,,,,,,,,,WOS:000509687900004,0
C,"Shaloudegi, K; Gyorgy, A",,"Chaudhuri, K; Sugiyama, M",,"Shaloudegi, Kiarash; Gyorgy, Andras",,,Adaptive MCMC via Combining Local Samplers,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Markov chain Monte Carlo (MCMC) methods are widely used in machine learning. One of the major problems with MCMC is the question of how to design chains that mix fast over the whole state space; in particular, how to select the parameters of an MCMC algorithm. Here we take a different approach and, similarly to parallel MCMC methods, instead of trying to find a single chain that samples from the whole distribution, we combine samples from several chains run in parallel, each exploring only parts of the state space (e.g., a few modes only). The chains are prioritized based on the kernel Stein discrepancy, which provides a good measure of performance locally. The samples from the independent chains are combined using a novel technique for estimating the probability of different regions of the sample space. Experimental results demonstrate that the proposed algorithm may provide significant speedups in different sampling problems. Most importantly, when combined with the state-of-the-art NUTS algorithm as the base MCMC sampler, our method remained competitive with NUTS on sampling from unimodal distributions, while significantly outperformed state-of-the-art competitors on synthetic multimodal problems as well as on a challenging sensor localization task.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902077,0
C,"Soma, T",,"Chaudhuri, K; Sugiyama, M",,"Soma, Tasuku",,,No-regret algorithms for online k-submodular maximization,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We present a polynomial time algorithm for online maximization of k-submodular maximization. For online (nonmonotone) k-submodular maximization, our algorithm achieves a tight approximate factor in the approximate regret. For online monotone k-submodular maximization, our approximate-regret matches to the best-known approximation ratio, which is tight asymptotically as k tends to infinity. Our approach is based on the Blackwell approachability theorem and online linear optimization, and provides simpler and clearner analysis.",,,,,"Soma, Tasuku/AAI-5374-2020","Soma, Tasuku/0000-0001-9519-2487",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901026,0
C,"Sun, S; Yu, YL",,"Chaudhuri, K; Sugiyama, M",,"Sun, Sun; Yu, Yaoliang",,,Least Squares Estimation of Weakly Convex Functions,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Function estimation under shape restrictions, such as convexity, has many practical applications and has drawn a lot of recent interests. In this work we argue that convexity, as a global property, is too strict and prone to outliers. Instead, we propose to use weakly convex functions as a simple alternative to quantify approximate convexity a notion that is perhaps more relevant in practice. We prove that, unlike convex functions, weakly convex functions can exactly interpolate any finite dataset and they are universal approximators. Through regularizing the modulus of convexity, we show that weakly convex functions can be efficiently estimated both statistically and algorithmically, requiring minimal modifications to existing algorithms and theory for estimating convex functions. Our numerical experiments confirm the class of weakly convex functions as another competitive alternative for nonparametric estimation.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902033,0
C,"Zimmert, J; Seldin, Y",,"Chaudhuri, K; Sugiyama, M",,"Zimmert, Julian; Seldin, Yevgeny",,,An Optimal Algorithm for Stochastic and Adversarial Bandits,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We derive an algorithm that achieves the optimal (up to constants) pseudo-regret in both adversarial and stochastic multi-armed bandits without prior knowledge of the regime and time horizon. The algorithm is based on online mirror descent with Tsallis entropy regularizer. We provide a complete characterization of such algorithms and show that Tsallis entropy with power alpha = 1/2 achieves the goal. In addition, the proposed algorithm enjoys improved regret guarantees in two intermediate regimes: the moderately contaminated stochastic regime defined by Seldin and Slivkins [22] and the stochastically constrained adversary studied by Wei and Luo [26]. The algorithm also achieves adversarial and stochastic optimality in the utility-based dueling bandit setting. We provide empirical evaluation of the algorithm demonstrating that it outperforms UcB1 and ExP3 in stochastic environments. In certain adversarial regimes the algorithm significantly outperforms UcB1 and THOMPSON SAMPLING, which exhibit almost linear regret.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,467,475,,,,,,,,,,,,,,,,WOS:000509687900049,0
C,"Duvenaud, D; Maclaurin, D; Adams, RP",,"Gretton, A; Robert, CC",,"Duvenaud, David; Maclaurin, Dougal; Adams, Ryan P.",,,Early Stopping as Nonparametric Variational Inference,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"We show that unconverged stochastic gradient descent can be interpreted as sampling from a nonparametric approximate posterior distribution. This distribution is implicitly defined by the transformation of an initial distribution by a sequence of optimization steps. By tracking the change in entropy of this distribution during optimization, we give a scalable, unbiased estimate of a variational lower bound on the log marginal likelihood. This bound can be used to optimize hyperparameters instead of cross-validation. This Bayesian interpretation of SGD also suggests new overfitting-resistant optimization procedures, and gives a theoretical foundation for early stopping and ensembling. We investigate the properties of this marginal likelihood estimator on neural network models.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,1070,1077,,,,,,,,,,,,,,,,WOS:000508662100116,0
C,"Lucic, M; Bachem, O; Krause, A",,"Gretton, A; Robert, CC",,"Lucic, Mario; Bachem, Olivier; Krause, Andreas",,,Strong Coresets for Hard and Soft Bregman Clustering with Applications to Exponential Family Mixtures,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Coresets are efficient representations of data sets such that models trained on the coreset are provably competitive with models trained on the original data set. As such, they have been successfully used to scale up clustering models such as K-Means and Gaussian mixture models to massive data sets. However, until now, the algorithms and the corresponding theory were usually specific to each clustering problem. We propose a single, practical algorithm to construct strong coresets for a large class of hard and soft clustering problems based on Bregman divergences. This class includes hard clustering with popular distortion measures such as the Squared Euclidean distance, the Mahalanobis distance, KL-divergence and Itakura-Saito distance. The corresponding soft clustering problems are directly related to popular mixture models due to a dual relationship between Bregman divergences and Exponential family distributions. Our theoretical results further imply a randomized polynomial-time approximation scheme for hard clustering. We demonstrate the practicality of the proposed algorithm in an empirical evaluation.",,,,,,"Krause, Andreas/0000-0001-7260-9673",,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,1,9,,,,,,,,,,,,,,,,WOS:000508662100001,0
C,"Yen, IEH; Malioutov, D; Kumar, A",,"Gretton, A; Robert, CC",,"Yen, Ian E. H.; Malioutov, Dmitry; Kumar, Abhishek",,,Scalable Exemplar Clustering and Facility Location via Augmented Block Coordinate Descent with Column Generation,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"In recent years exemplar clustering has become a popular tool for applications in document and video summarization, active learning, and clustering with general similarity, where cluster centroids are required to be a subset of the data samples rather than their linear combinations. The problem is also well-known as facility location in the operations research literature. While the problem has well-developed convex relaxation with approximation and recovery guarantees, its number of variables grows quadratically with the number of samples. Therefore, state-of-the-art methods can hardly handle more than 10(4) samples (i.e. 10(8) variables). In this work, we propose an Augmented-Lagrangian with Block Coordinate Descent (AL-BCD) algorithm that utilizes problem structure to obtain closed-form solution for each block sub-problem, and exploits low-rank representation of the dissimilarity matrix to search active columns without computing the entire matrix. Experiments show our approach to be orders of magnitude faster than existing approaches and can handle problems of up to 10(6) samples. We also demonstrate successful applications of the algorithm on world-scale facility location, document summarization and active learning.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,1260,1269,,,,,,,,,,,,,,,,WOS:000508662100137,0
C,"Lee, J; Choi, S",,"Kaski, S; Corander, J",,"Lee, Juho; Choi, Seungjin",,,Incremental Tree-Based Inference with Dependent Normalized Random Measures,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"Normalized random measures (NRMs) form a broad class of discrete random measures that are used as priors for Bayesian nonparametric models. Dependent normalized random measures (DNRMs) introduce dependencies in a set of NRMs, to facilitate the handling of data where the assumption of exchangeability is violated. Various methods have been developed to construct DNRMs; of particular interest is mixed normalized random measures (MNRMs), where DNRM is represented as a mixture of underlying shared normalized random measures. Emphasis in existing works is placed on the construction methods of DNRMs, but there is a little work on efficient inference for DNRMs. In this paper, we present a tree-based inference method for MNRM mixture models, extending Bayesian hierarchical clustering (BHC) which was originally developed as a deterministic approximate inference for Dirichlet process mixture (DPM) models. We also present an incremental inference for MNRM mixture models, building a tree incrementally in the sense that the tree structure is partially updated whenever a new data point comes in. The tree, when constructed in such a way, allows us to efficiently perform tree-consistent MAP inference in MRNM mixture models, determining a most probable tree-consistent partition, as well as to compute a marginal likelihood approximately. Numerical experiments on both synthetic and real-world datasets demonstrate the usefulness of our algorithm, compared to MCMC methods.",,,,,"Lee, Juho/AAA-2901-2022",,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,558,566,,,,,,,,,,,,,,,,WOS:000508355800062,0
C,"Maji, S; Hazan, T; Jaakkola, T",,"Kaski, S; Corander, J",,"Maji, Subhransu; Hazan, Tamir; Jaakkola, Tommi",,,Active Boundary Annotation using Random MAP Perturbations,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"We address the problem of efficiently annotating labels of objects when they are structured. Often the distribution over labels can be described using a joint potential function over the labels for which sampling is provably hard but efficient maximum a-posteriori (MAP) solvers exist. In this setting we develop novel entropy bounds that are based on the expected amount of perturbation to the potential function that is needed to change MAP decisions. By reasoning about the entropy reduction and cost tradeoff, our algorithm actively selects the next annotation task. As an example of our framework we propose a boundary refinement task which can used to obtain pixel-accurate image boundaries much faster than traditional tools by focussing on parts of the image for refinement in a multi-scale manner.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,604,613,,,,,,,,,,,,,,,,WOS:000508355800067,0
C,"Obermeyer, F; Glidden, J; Jonas, E",,"Kaski, S; Corander, J",,"Obermeyer, Fritz; Glidden, Jonathan; Jonas, Eric",,,Scaling Nonparametric Bayesian Inference via Subsample-Annealing,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"We describe an adaptation of the simulated annealing algorithm to nonparametric clustering and related probabilistic models. This new algorithm learns nonparametric latent structure over a growing and constantly churning subsample of training data, where the portion of data subsampled can be interpreted as the inverse temperature beta(t) in an annealing schedule. Gibbs sampling at high temperature (i.e., with a very small subsample) can more quickly explore sketches of the final latent state by (a) making longer jumps around latent space (as in block Gibbs) and (b) lowering energy barriers (as in simulated annealing). We prove subsample annealing speeds up mixing time N-2 -> N in a simple clustering model and exp(N) -> N in another class of models, where N is data size. Empirically subsample-annealing outperforms naive Gibbs sampling in accuracyper-wallclock time, and can scale to larger datasets and deeper hierarchical models. We demonstrate improved inference on million-row subsamples of US Census data and network log data and a 307-row hospital rating dataset, using a Pitman-Yor generalization of the Cross Categorization model.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,696,705,,,,,,,,,,,,,,,,WOS:000508355800077,0
C,"Ariafar, S; Mariet, Z; Brooks, D; Dy, J; Snoek, J",,"Banerjee, A; Fukumizu, K",,"Ariafar, Setareh; Mariet, Zelda; Brooks, Dana; Dy, Jennifer; Snoek, Jasper",,,Faster & More Reliable Tuning of Neural Networks: Bayesian Optimization with Importance Sampling,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Many contemporary machine learning models require extensive tuning of hyperparameters to perform well. A variety of methods, such as Bayesian optimization, have been developed to automate and expedite this process. However, tuning remains costly as it typically requires repeatedly fully training models. To address this issue, Bayesian optimization has been extended to use cheap, partially trained models to extrapolate to expensive complete models. This approach enlarges the set of explored hyperparameters, but including many low-fidelity observations adds to the intrinsic randomness of the procedure and makes extrapolation challenging. We propose to accelerate tuning of neural networks in a robust way by taking into account the relative amount of information contributed by each training example. To do so, we leverage importance sampling (IS); this significantly increases the quality of the function evaluations, but also their runtime, and so must be done carefully. Casting hyperparameter search as a multi-task Bayesian optimization problem over both hyperparameters and IS design achieves the best of both worlds. By learning a parameterization of IS that tradeso. evaluation complexity and quality, our method improves upon validation error, in the average and worst-case, while using higher fidelity observations with less data. We show that this results in more reliable performance of our method in less wall-clock time across a variety of datasets and neural architectures.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804075,0
C,"Baby, D; Zhao, XD; Wang, YX",,"Banerjee, A; Fukumizu, K",,"Baby, Dheeraj; Zhao, Xuandong; Wang, Yu-Xiang",,,An Optimal Reduction of TV-Denoising to Adaptive Online Learning,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We consider the problem of estimating a function from n noisy samples whose discrete Total Variation (TV) is bounded by C-n. We reveal a deep connection to the seemingly disparate problem of Strongly Adaptive online learning (Daniely et al., 2015) and provide an O(n log n) time algorithm that attains the near minimax optimal rate of (O) over tilde (n(1/3)C(n)(2/3)) under squared error loss. The resulting algorithm runs online and optimally adapts to the unknown smoothness parameter C-n. This leads to a new and more versatile alternative to wavelets-based methods for (1) adaptively estimating TV bounded functions; (2) online forecasting of TV bounded trends in time series.",,,,,,"Wang, Yu-Xiang/0000-0002-6403-212X; Zhao, Xuandong/0000-0001-5976-0360",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803049,0
C,"Bechavod, Y; Ligett, K; Wu, ZS; Ziani, J",,"Banerjee, A; Fukumizu, K",,"Bechavod, Yahav; Ligett, Katrina; Wu, Zhiwei Steven; Ziani, Juba",,,Gaming Helps! Learning from Strategic Interactions in Natural Dynamics,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We consider an online regression setting in which individuals adapt to the regression model: arriving individuals are aware of the current model, and invest strategically in modifying their own features so as to improve the predicted score that the current model assigns to them. Such feature manipulation has been observed in various scenarios-from credit assessment to school admissions-posing a challenge for the learner. Surprisingly, we find that such strategic manipulations may in fact help the learner recover the meaningful variables-that is, the features that, when changed, affect the true label (as opposed to non-meaningful features that have no effect). We show that even simple behavior on the learner's part allows her to simultaneously i) accurately recover the meaningful features, and ii) incentivize agents to invest in these meaningful features, providing incentives for improvement.",,,,,,"Wu, Steven/0000-0002-8125-8227; Ligett, Katrina/0000-0003-2780-6656",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801051,0
C,"Cunningham, E; Fiterau, M",,"Banerjee, A; Fukumizu, K",,"Cunningham, Edmond; Fiterau, Madalina",,,RealMVP: A Change of Variables Method For Rectangular Matrix-Vector Products,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Rectangular matrix-vector products (MVPs) are used extensively throughout machine learning and are fundamental to neural networks such as multi-layer perceptrons. However, rectangular MVPs are notably missing not used as normalizing flow transforms. This paper identifies this methodological gap and plugs it with a tall and wide MVP change of variables formula. Our theory builds up to a scalable algorithm that envelops existing dimensionality increasing flow methods such as augmented flows (Huang et al., 2020). We show that tall MVPs are closely related to the stochastic inverse of wide MVPs and empirically demonstrate that they improve density estimation over existing dimension changing methods.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803033,0
C,"Garber, D; Kretzu, B",,"Banerjee, A; Fukumizu, K",,"Garber, Dan; Kretzu, Ben",,,Revisiting Projection-free Online Learning: the Strongly Convex Case,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Projection-free optimization algorithms, which are mostly based on the classical Frank-Wolfe method, have gained significant interest in the machine learning community in recent years due to their ability to handle convex constraints that are popular in many applications, but for which computing projections is often computationally impractical in high-dimensional settings, and hence prohibit the use of most standard projection-based methods. In particular, a significant research effort was put on projection-free methods for online learning. In this paper we revisit the Online Frank-Wolfe (OFW) method suggested by Hazan and Kale (2012) and fill a gap that has been left unnoticed for several years: OFW achieves a faster rate of O(T-2/3) on strongly convex functions (as opposed to the standard O(T-3/4) for convex but not strongly convex functions), where T is the sequence length. This is somewhat surprising since it is known that for offline optimization, in general, strong convexity does not lead to faster rates for Frank-Wolfe. We also revisit the bandit setting under strong convexity and prove a similar bound of (O) over tilde (T-2/3) (instead of O (T-3/4) without strong convexity). Hence, in the current state-of-affairs, the best projectionfree upper-bounds for the full-information and bandit settings with strongly convex and nonsmooth functions match up to logarithmic factors in T.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804034,0
C,"Garg, VK; Kalai, AT; Ligett, K; Wu, ZS",,"Banerjee, A; Fukumizu, K",,"Garg, Vikas K.; Kalai, Adam Tauman; Ligett, Katrina; Wu, Zhiwei Steven",,,Learn to Expect the Unexpected: Probably Approximately Correct Domain Generalization,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Domain generalization is the problem of machine learning when the training data and the test data come from different domains (data distributions). We propose an elementary theoretical model of the domain generalization problem, introducing the concept of a metadistribution over domains. In our model, the training data available to a learning algorithm consist of multiple datasets, each from a single domain, drawn in turn from the metadistribution. We show that our model can capture a rich range of learning phenomena specific to domain generalization for three different settings: learning with Massart noise, learning decision trees, and feature selection. We demonstrate approaches that leverage domain generalization to reduce computational or data requirements in each of these settings. Experiments demonstrate that our feature selection algorithm indeed ignores spurious correlations and improves generalization.",,,,,,"Kalai, Adam Tauman/0000-0002-4559-8574; Ligett, Katrina/0000-0003-2780-6656",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804032,0
C,"Hao, BT; Lazic, N; Abbasi-Yadkori, Y; Joulani, P; Szepesvari, C",,"Banerjee, A; Fukumizu, K",,"Hao, Botao; Lazic, Nevena; Abbasi-Yadkori, Yasin; Joulani, Pooria; Szepesvari, Csaba",,,Adaptive Approximate Policy Iteration,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Model-free reinforcement learning algorithms combined with value function approximation have recently achieved impressive performance in a variety of application domains. However, the theoretical understanding of such algorithms is limited, and existing results are largely focused on episodic or discounted Markov decision processes (MDPs). In this work, we present adaptive approximate policy iteration (AAPI), a learning scheme which enjoys a O (T-2/3) regret bound for undiscounted, continuing learning in uniformly ergodic MDPs. This is an improvement over the best existing bound of O (T-3/4) for the average-reward case with function approximation. Our algorithm and analysis rely on online learning techniques, where value functions are treated as losses. The main technical novelty is the use of a data-dependent adaptive learning rate coupled with a so-called optimistic prediction of upcoming losses. In addition to theoretical guarantees, we demonstrate the advantages of our approach empirically on several environments.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,523,531,,,,,,,,,,,,,,,,WOS:000659893800059,0
C,"Hong, J; Kveton, B; Zaheer, M; Chow, Y; Ahmed, A",,"Banerjee, A; Fukumizu, K",,"Hong, Joey; Kveton, Branislav; Zaheer, Manzil; Chow, Yinlam; Ahmed, Amr",,,Non-Stationary Off-Policy Optimization,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Off-policy learning is a framework for evaluating and optimizing policies without deploying them, from data collected by another policy. Real-world environments are typically non-stationary and the offline learned policies should adapt to these changes. To address this challenge, we study the novel problem of off-policy optimization in piecewise-stationary contextual bandits. Our proposed solution has two phases. In the offline learning phase, we partition logged data into categorical latent states and learn a near-optimal sub-policy for each state. In the online deployment phase, we adaptively switch between the learned sub-policies based on their performance. This approach is practical and analyzable, and we provide guarantees on both the quality of off-policy optimization and the regret during online deployment. To show the effectiveness of our approach, we compare it to state-of-the-art baselines on both synthetic and real-world datasets. Our approach outperforms methods that act only on observed context.",,,,,"Zaheer, Manzil/ABG-6249-2021",,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803004,0
C,"Jiang, HR; Jiang, QJ; Pacchiano, A",,"Banerjee, A; Fukumizu, K",,"Jiang, Heinrich; Jiang, Qijia; Pacchiano, Aldo",,,Learning the Truth From Only One Side of the Story,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Learning under one-sided feedback (i.e., where we only observe the labels for examples we predicted positively on) is a fundamental problem in machine learning - applications include lending and recommendation systems. Despite this, there has been surprisingly little progress made in ways to mitigate the effects of the sampling bias that arises. We focus on generalized linear models and show that without adjusting for this sampling bias, the model may converge suboptimally or even fail to converge to the optimal solution. We propose an adaptive approach that comes with theoretical guarantees and show that it outperforms several existing methods empirically. Our method leverages variance estimation techniques to efficiently learn under uncertainty, offering a more principled alternative compared to existing approaches.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802088,0
C,"Likhosherstov, V; Davis, J; Choromanski, K; Weller, A",,"Banerjee, A; Fukumizu, K",,"Likhosherstov, Valerii; Davis, Jared; Choromanski, Krzysztof; Weller, Adrian",,,CWY Parametrization: a Solution for Parallelized Optimization of Orthogonal and Stiefel Matrices,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We introduce an efficient approach for optimization over orthogonal groups on highly parallel computation units such as GPUs or TPUs. As in earlier work, we parametrize an orthogonal matrix as a product of Householder reflections. However, to overcome low parallelization capabilities of computing Householder reflections sequentially, we propose employing an accumulation scheme called the compact WY (or CWY) transform - a compact parallelization-friendly matrix representation for the series of Householder reflections. We further develop a novel Truncated CWY (or T-CWY) approach for Stiefel manifold parametrization which has a competitive complexity and, again, yields benefits when computed on GPUs and TPUs. We prove that our CWY and T-CWY methods lead to convergence to a stationary point of the training objective when coupled with stochastic gradient descent. We apply our methods to train recurrent neural network architectures in the tasks of neural machine translation and video prediction.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,55,+,,,,,,,,,,,,,,,,WOS:000659893800007,0
C,"Lin, TY; Zheng, ZY; Chen, EY; Cuturi, M; Jordan, MI",,"Banerjee, A; Fukumizu, K",,"Lin, Tianyi; Zheng, Zeyu; Chen, Elynn Y.; Cuturi, Marco; Jordan, Michael, I",,,On Projection Robust Optimal Transport: Sample Complexity and Model Misspecification,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Optimal transport (OT) distances are increasingly used as loss functions for statistical inference, notably in the learning of generative models or supervised learning. Yet, the behavior of minimum Wasserstein estimators is poorly understood, notably in high-dimensional regimes or under model misspecification. In this work we adopt the viewpoint of projection robust (PR) OT, which seeks to maximize the OT cost between two measures by choosing a k-dimensional sub-space onto which they can be projected. Our first contribution is to establish several fundamental statistical properties of PRW asserstein distances, complementing and improving previous literature that has been restricted to one-dimensional and well-specified cases. Next, we propose the integral PR Wasserstein (IPRW) distance as an alternative to the PRW distance, by averaging rather than optimizing on subspaces. Our complexity bounds can help explain why both PRW and IPRW distances outperform Wasserstein distances empirically in high-dimensional inference tasks. Finally, we consider parametric inference using the PRW distance. We provide an asymptotic guarantee of two types of minimum PRW estimators and formulate a central limit theorem for max-sliced Wasserstein estimator under model misspecification. To enable our analysis on PRW with projection dimension larger than one, we devise a novel combination of variational analysis and statistical theory.",,,,,"Lin, Tianyi/ABE-9693-2021","Lin, Tianyi/0000-0002-5323-1852",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,262,+,,,,,,,,,,,,,,,,WOS:000659893800030,0
C,"Morioka, H; Halva, H; Hyvarinen, A",,"Banerjee, A; Fukumizu, K",,"Morioka, Hiroshi; Halva, Hermanni; Hyvarinen, Aapo",,,Independent Innovation Analysis for Nonlinear Vector Autoregressive Process,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"The nonlinear vector autoregressive (NVAR) model provides an appealing framework to analyze multivariate time series obtained from a nonlinear dynamical system. However, the innovation (or error), which plays a key role by driving the dynamics, is almost always assumed to be additive. Additivity greatly limits the generality of the model, hindering analysis of general NVAR processes which have nonlinear interactions between the innovations. Here, we propose a new general framework called independent innovation analysis (IIA), which estimates the innovations from completely general NVAR. We assume mutual independence of the innovations as well as their modulation by an auxiliary variable (which is often taken as the time index and simply interpreted as non-stationarity). We show that IIA guarantees the identifiability of the innovations with arbitrary nonlinearities, up to a permutation and component-wise invertible nonlinearities. We also propose three estimation frameworks depending on the type of the auxiliary variable. We thus provide the first rigorous identifiability result for general NVAR, as well as very general tools for learning such models.",,,,,,"Hyvarinen, Aapo/0000-0002-5806-4432",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801086,0
C,"Moseley, B; Vassilvitskii, S; Wang, YY",,"Banerjee, A; Fukumizu, K",,"Moseley, Benjamin; Vassilvitskii, Sergei; Wang, Yuyan",,,Hierarchical Clustering in General Metric Spaces using Approximate Nearest Neighbors,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Hierarchical clustering is a widely used data analysis method, but suffers from scalability issues, requiring quadratic time in general metric spaces. In this work, we demonstrate how approximate nearest neighbor (ANN) queries can be used to improve the running time of the popular single-linkage and average-linkage methods. Our proposed algorithms are the first subquadratic time algorithms for non-Euclidean metrics. We complement our theoretical analysis with an empirical evaluation showcasing our methods' efficiency and accuracy.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802091,0
C,"Patil, P; Wei, YT; Rinaldo, A; Tibshirani, RJ",,"Banerjee, A; Fukumizu, K",,"Patil, Pratik; Wei, Yuting; Rinaldo, Alessandro; Tibshirani, Ryan J.",,,Uniform Consistency of Cross-Validation Estimators for High-Dimensional Ridge Regression,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We examine generalized and leave-one-out cross-validation for ridge regression in a proportional asymptotic framework where the dimension of the feature space grows proportionally with the number of observations. Given i.i.d. samples from a linear model with an arbitrary feature covariance and a signal vector that is bounded in l(2) norm, we show that generalized cross-validation for ridge regression converges almost surely to the expected out-of-sample prediction error, uniformly over a range of ridge regularization parameters that includes zero (and even negative values). We prove the analogous result for leave-one-out cross-validation. As a consequence, we show that ridge tuning via minimization of generalized or leave-one-out cross-validation asymptotically almost surely delivers the optimal level of regularization for predictive accuracy, whether it be positive, negative, or zero.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803080,0
C,"Ramchandran, S; Tikhonov, G; Kujanpaa, K; Koskinen, M; Lahdesmaki, H",,"Banerjee, A; Fukumizu, K",,"Ramchandran, Siddharth; Tikhonov, Gleb; Kujanpaa, Kalle; Koskinen, Miika; Lahdesmaki, Harri",,,Longitudinal Variational Autoencoder,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Longitudinal datasets measured repeatedly over time from individual subjects, arise in many biomedical, psychological, social, and other studies. A common approach to analyse high-dimensional data that contains missing values is to learn a low-dimensional representation using variational autoencoders (VAEs). However, standard VAEs assume that the learnt representations are i.i.d., and fail to capture the correlations between the data samples. We propose the Longitudinal VAE (L-VAE), that uses a multi-output additive Gaussian process (GP) prior to extend the VAE's capability to learn structured low-dimensional representations imposed by auxiliary covariate information, and derive a new KL divergence upper bound for such GPs. Our approach can simultaneously accommodate both time-varying shared and random effects, produce structured low-dimensional representations, disentangle effects of individual covariates or their interactions, and achieve highly accurate predictive performance. We compare our model against previous methods on synthetic as well as clinical datasets, and demonstrate the state-of-theart performance in data imputation, reconstruction, and long-term prediction tasks.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804068,0
C,"Richards, D; Mourtada, J; Rosasco, L",,"Banerjee, A; Fukumizu, K",,"Richards, Dominic; Mourtada, Jaouad; Rosasco, Lorenzo",,,Asymptotics of Ridge (less) Regression under General Source Condition,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We analyze the prediction error of ridge regression in an asymptotic regime where the sample size and dimension go to infinity at a proportional rate. In particular, we consider the role played by the structure of the true regression parameter. We observe that the case of a general deterministic parameter can be reduced to the case of a random parameter from a structured prior. The latter assumption is a natural adaptation of classic smoothness assumptions in nonparametric regression, which are known as source conditions in the the context of regularization theory for inverse problems. Roughly speaking, we assume the large coefficients of the parameter are in correspondence to the principal components. In this setting a precise characterisation of the test error is obtained, depending on the inputs covariance and regression parameter structure. We illustrate this characterisation in a simplified setting to investigate the influence of the true parameter on optimal regularisation for overparameterized models. We show that interpolation (no regularisation) can be optimal even with bounded signal-to-noise ratio (SNR), provided that the parameter coefficients are larger on high-variance directions of the data, corresponding to a more regular function than posited by the regularization term. This contrasts with previous work considering ridge regression with isotropic prior, in which case interpolation is only optimal in the limit of infinite SNR.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804067,0
C,"Scetbon, M; Harchaoui, Z",,"Banerjee, A; Fukumizu, K",,"Scetbon, Meyer; Harchaoui, Zaid",,,A Spectral Analysis of Dot-product Kernels,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,We present eigenvalue decay estimates of integral operators associated with compositional dot-product kernels. The estimates improve on previous ones established for power series kernels on spheres. This allows us to obtain the volumes of balls in the corresponding reproducing kernel Hilbert spaces. We discuss the consequences on statistical estimation with compositional dot product kernels and highlight interesting trade-offs between the approximation error and the statistical error depending on the number of compositions and the smoothness of the kernels.,,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804012,0
C,"Sun, Z; Wu, JJ; Li, XX; Yang, WM; Xue, JH",,"Banerjee, A; Fukumizu, K",,"Sun, Zhuo; Wu, Jijie; Li, Xiaoxu; Yang, Wenming; Xue, Jing-Hao",,,Amortized Bayesian Prototype Meta-learning: A New Probabilistic Meta-learning Approach to Few-shot Image Classification,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Probabilistic meta-learning methods recently have achieved impressive success in few-shot image classification. However, they introduce a huge number of random variables for neural network weights and thus severe computational and inferential challenges. In this paper, we propose a novel probabilistic meta-learning method called amortized Bayesian prototype meta-learning. In contrast to previous methods, we introduce only a small number of random variables for latent class prototypes rather than a huge number for network weights; we learn to learn the posterior distributions of these latent prototypes in an amortized inference way with no need for an extra amortization network, such that we can easily approximate their posteriors conditional on few labeled samples, whenever at meta-training or meta-testing stage. The proposed method can be trained end-to-end without any pre-training. Compared with other probabilistic meta-learning methods, our proposed approach is more interpretable with much less random variables, while still be able to achieve competitive performance for few-shot image classification problems on various benchmark datasets. Its excellent robustness and predictive uncertainty are also demonstrated through ablation studies.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801071,0
C,"Turner, P; Liu, JB; Rigollet, P",,"Banerjee, A; Fukumizu, K",,"Turner, Paxton; Liu, Jingbo; Rigollet, Philippe",,,Efficient Interpolation of Density Estimators,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We study the problem of space and time efficient evaluation of a nonparametric estimator that approximates an unknown density. In the regime where consistent estimation is possible, we use a piecewise multivariate polynomial interpolation scheme to give a computationally efficient construction that converts the original estimator to a new estimator that can be queried efficiently and has low space requirements, all without adversely deteriorating the original approximation quality. Our result gives a new statistical perspective on the problem of fast evaluation of kernel density estimators in the presence of underlying smoothness. As a corollary, we give a succinct derivation of a classical result of Kolmogorov-Tikhomirov on the metric entropy of Holder classes of smooth functions.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803005,0
C,"Varici, B; Sihag, S; Tajer, A",,"Banerjee, A; Fukumizu, K",,"Varici, Burak; Sihag, Saurabh; Tajer, Ali",,,Learning Shared Subgraphs in Ising Model Pairs,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Probabilistic graphical models (PGMs) are effective for capturing the statistical dependencies in stochastic databases. In many domains (e.g., working with multimodal data), one faces multiple information layers that can be modeled by structurally similar PGMs. While learning the structures of PGMs in isolation is well-investigated, the algorithmic design and performance limits of learning from multiple coupled PGMs are not well-investigated. This paper considers learning the structural similarities shared by a pair of Ising PGMs. The objective is to learn only the shared structure with no regard for the structures exclusive to either of the graphs. This is significantly different from the existing approaches that focus on learning the entire structures of the graphs. This paper proposes an algorithm for learning the shared structure, evaluates its performance empirically and analytically, and compares the performance with that of the existing approaches.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804074,0
C,"Yang, CR; Ding, LJ; Wu, ZY; Udell, M",,"Banerjee, A; Fukumizu, K",,"Yang, Chengrun; Ding, Lijun; Wu, Ziyang; Udell, Madeleine",,,TenIPS: Inverse Propensity Sampling for Tensor Completion,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Tensors are widely used to represent multiway arrays of data. The recovery of missing entries in a tensor has been extensively studied, generally under the assumption that entries are missing completely at random (MCAR). However, in most practical settings, observations are missing not at random (MNAR): the probability that a given entry is observed (also called the propensity) may depend on other entries in the tensor or even on the value of the missing entry. In this paper, we study the problem of completing a partially observed tensor with MNAR observations, without prior information about the propensities. To complete the tensor, we assume that both the original tensor and the tensor of propensities have low multilinear rank. The algorithm first estimates the propensities using a convex relaxation and then predicts missing values using a higher-order SVD approach, reweighting the observed tensor by the inverse propensities. We provide finite-sample error bounds on the resulting complete tensor. Numerical experiments demonstrate the effectiveness of our approach.",,,,,"Yang, Chengrun/HII-3621-2022",,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803078,0
C,"Zhang, JY; Hong, MY; Wang, MD; Zhang, SZ",,"Banerjee, A; Fukumizu, K",,"Zhang, Junyu; Hong, Mingyi; Wang, Mengdi; Zhang, Shuzhong",,,Generalization Bounds for Stochastic Saddle Point Problems,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"This paper studies the generalization bounds for the empirical saddle point (ESP) solution to stochastic saddle point (SSP) problems. For SSP with Lipschitz continuous and strongly convex-strongly concave objective functions, we establish an O (1/n) generalization bound by using a probabilistic stability argument. We also provide generalization bounds under a variety of assumptions, including the cases without strong convexity and without bounded domains. We illustrate our results in three examples: batch policy learning in Markov decision process, stochastic composite optimization problem, and mixed strategy Nash equilibrium estimation for stochastic games. In each of these examples, we show that a regularized ESP solution enjoys a near-optimal sample complexity. To the best of our knowledge, this is the first set of results on the generalization theory of ESP.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,568,+,,,,,,,,,,,,,,,,WOS:000659893800064,0
C,"Zhuang, V; Sui, YA",,"Banerjee, A; Fukumizu, K",,"Zhuang, Vincent; Sui, Yanan",,,No-Regret Reinforcement Learning with Heavy-Tailed Rewards,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Reinforcement learning algorithms typically assume rewards to be sampled from light-tailed distributions, such as Gaussian or bounded. However, a wide variety of real-world systems generate rewards that follow heavy-tailed distributions. We consider such scenarios in the setting of undiscounted reinforcement learning. By constructing a lower bound, we show that the difficulty of learning heavy-tailed rewards asymptotically dominates the difficulty of learning transition probabilities. Leveraging techniques from robust mean estimation, we propose Heavy-UCRL2 and Heavy-Q-Learning, and show that they achieve near-optimal regret bounds in this setting. Our algorithms also naturally generalize to deep reinforcement learning applications; we instantiate Heavy-DQN as an example of this. We demonstrate that all of our algorithms outperform baselines on both synthetic MDPs and standard RL benchmarks.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804011,0
C,"Cai, YF; Li, P",,"Chiappa, S; Calandra, R",,"Cai, Yunfeng; Li, Ping",,,Solving the Robust Matrix Completion Problem via a System of Nonlinear Equations,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We consider the problem of robust matrix completion, which aims to recover a low rank matrix L-* and a sparse matrix S-* from incomplete observations of their sum M = L-*+S-* is an element of R-mxn. Algorithmically, the robust matrix completion problem is transformed into a problem of solving a system of nonlinear equations, and the alternative direction method is then used to solve the nonlinear equations. In addition, the algorithm is highly parallelizable and suitable for large scale problems. Theoretically, we characterize the sufficient conditions for when L-* can be approximated by a low rank approximation of the observed M-*. And under proper assumptions, it is shown that the algorithm converges to the true solution linearly. Numerical simulations show that the simple method works as expected and is comparable with state-of-the-art methods.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,4162,4171,,,,,,,,,,,,,,,,WOS:000559931300041,0
C,"Dwivedi, R; Khamaru, K; Ho, N; Wainwright, M; Jordan, M; Yu, B",,"Chiappa, S; Calandra, R",,"Dwivedi, Raaz; Khamaru, Koulik; Ho, Nhat; Wainwright, Martin; Jordan, Michael; Yu, Bin",,,Sharp Analysis of Expectation-Maximization for Weakly Identifiable Models,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We study a class of weakly identifiable location-scale mixture models for which the maximum likelihood estimates based on n i.i.d. samples are known to have lower accuracy than the classical n(-12) error. We investigate whether the Expectation-Maximization (EM) algorithm also converges slowly for these models. We provide a rigorous characterization of EM for fitting a weakly identifiable Gaussian mixture in a univariate setting where we prove that the EM algorithm converges in order n(3/4) steps and returns estimates that are at a Euclidean distance of order n(-18) and n(-14) from the true location and scale parameter respectively. Establishing the slow rates in the univariate setting requires a novel localization argument with two stages, with each stage involving an epoch-based argument applied to a different surrogate EM operator at the population level. We demonstrate several multivariate (d >= 2) examples that exhibit the same slow rates as the univariate case. We also prove slow statistical rates in higher dimensions in a special case, when the fitted covariance is constrained to be a multiple of the identity.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1866,1875,,,,,,,,,,,,,,,,WOS:000559931300081,0
C,"Favaro, S; Fortini, S; Peluchetti, S",,"Chiappa, S; Calandra, R",,"Favaro, Stefano; Fortini, Sandra; Peluchetti, Stefano",,,Stable behaviour of infinitely wide deep neural networks,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We consider fully connected feed-forward deep neural networks (NNs) where weights and biases are independent and identically distributed as symmetric centered stable distributions. Then, we show that the infinite wide limit of the NN, under suitable scaling on the weights, is a stochastic process whose finite-dimensional distributions are multivariate stable distributions. The limiting process is referred to as the stable process, and it generalizes the class of Gaussian processes recently obtained as infinite wide limits of NNs (Matthews et al., 2018b). Parameters of the stable process can be computed via an explicit recursion over the layers of the network. Our result contributes to the theory of fully connected feed-forward deep NNs, and it paves the way to expand recent lines of research that rely on Gaussian infinite wide limits.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1137,1145,,,,,,,,,,,,,,,,WOS:000559931302071,0
C,"Frohlich, LP; Klenske, ED; Vinogradska, J; Daniel, C; Zeilinger, MN",,"Chiappa, S; Calandra, R",,"Froehlich, Lukas P.; Klenske, Edgar D.; Vinogradska, Julia; Daniel, Christian; Zeilinger, Melanie N.",,,Noisy-Input Entropy Search for Efficient Robust Bayesian Optimization,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We consider the problem of robust optimization within the well-established Bayesian optimization (BO) framework. While BO is intrinsically robust to noisy evaluations of the objective function, standard approaches do not consider the case of uncertainty about the input parameters. In this paper, we propose Noisy-Input Entropy Search (NES), a novel information-theoretic acquisition function that is designed to find robust optima for problems with both input and measurement noise. NES is based on the key insight that the robust objective in many cases can be modeled as a Gaussian process, however, it cannot be observed directly. We evaluate NES on several benchmark problems from the optimization literature and from engineering. The results show that NES reliably finds robust optima, outperforming existing methods from the literature on all benchmarks.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2262,2271,,,,,,,,,,,,,,,,WOS:000559931301008,0
C,"Hao, BT; Lattimore, T; Szepesvari, C",,"Chiappa, S; Calandra, R",,"Hao, Botao; Lattimore, Tor; Szepesvari, Csaba",,,Adaptive Exploration in Linear Contextual Bandit,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Contextual bandits serve as a fundamental model for many sequential decision making tasks. The most popular theoretically justified approaches are based on the optimism principle. While these algorithms can be practical, they are known to be suboptimal asymptotically. On the other hand, existing asymptotically optimal algorithms for this problem do not exploit the linear structure in an optimal way and suffer from lower-order terms that dominate the regret in all practically interesting regimes. We start to bridge the gap by designing an algorithm that is asymptotically optimal and has good finite-time empirical performance. At the same time, we make connections to the recent literature on when exploration-free methods are effective. Indeed, if the distribution of contexts is well behaved, then our algorithm acts mostly greedily and enjoys sub-logarithmic regret. Furthermore, our approach is adaptive in the sense that it automatically detects the nice case. Numerical results demonstrate significant regret reductions by our method relative to several baselines.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3536,3544,,,,,,,,,,,,,,,,WOS:000559931301036,0
C,"Imberg, H; Jonasson, J; Axelson-Fisk, M",,"Chiappa, S; Calandra, R",,"Imberg, Henrik; Jonasson, Johan; Axelson-Fisk, Marina",,,Optimal sampling in unbiased active learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"A common belief in unbiased active learning is that, in order to capture the most informative instances, the sampling probabilities should be proportional to the uncertainty of the class labels. We argue that this produces suboptimal predictions and present sampling schemes for unbiased pool-based active learning that minimise the actual prediction error, and demonstrate a better predictive performance than competing methods on a number of benchmark datasets. In contrast, both probabilistic and deterministic uncertainty sampling performed worse than simple random sampling on some of the datasets.",,,,,"Imberg, Henrik/ABC-5503-2021","Imberg, Henrik/0000-0001-9447-663X",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,559,568,,,,,,,,,,,,,,,,WOS:000559931301053,0
C,"Kawaguchi, K; Kaelbling, LP",,"Chiappa, S; Calandra, R",,"Kawaguchi, Kenji; Kaelbling, Leslie Pack",,,Elimination of All Bad Local Minima in Deep Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"In this paper, we theoretically prove that adding one special neuron per output unit eliminates all suboptimal local minima of any deep neural network, for multi-class classification, binary classification, and regression with an arbitrary loss function, under practical assumptions. At every local minimum of any deep neural network with these added neurons, the set of parameters of the original neural network (without added neurons) is guaranteed to be a global minimum of the original neural network. The effects of the added neurons are proven to automatically vanish at every local minimum. Moreover, we provide a novel theoretical characterization of a failure mode of eliminating suboptimal local minima via an additional theorem and several examples. This paper also introduces a novel proof technique based on the perturbable gradient basis (PGB) necessary condition of local minima, which provides new insight into the elimination of local minima and is applicable to analyze various models and transformations of objective functions beyond the elimination of local minima.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,853,862,,,,,,,,,,,,,,,,WOS:000559931301068,0
C,"Raj, A; Musco, C; Mackey, L",,"Chiappa, S; Calandra, R",,"Raj, Anant; Musco, Cameron; Mackey, Lester",,,Importance Sampling via Local Sensitivity,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Given a loss function F : chi -> R+ that can be written as the sum of losses over a large set of inputs a(1) ,..., a(n), it is often desirable to approximate F by subsampling the input points. Strong theoretical guarantees require taking into account the importance of each point, measured by how much its individual loss contributes to F(x). Maximizing this importance over all x is an element of chi yields the sensitivity score of a(i). Sampling with probabilities proportional to these scores gives strong guarantees, allowing one to approximately minimize of F using just the subsampled points. Unfortunately, sensitivity sampling is difficult to apply since (1) it is unclear how to efficiently compute the sensitivity scores and (2) the sample size required is often impractically large. To overcome both obstacles we introduce local sensitivity, which measures data point importance in a ball around some center x(0). We show that the local sensitivity can be efficiently estimated using the leverage scores of a quadratic approximation to F and that the sample size required to approximate F around x(0) can be bounded. We propose employing local sensitivity sampling in an iterative optimization method and analyze its convergence when F is smooth and convex.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3099,3108,,,,,,,,,,,,,,,,WOS:000559931302086,0
C,"Sondhi, A; Arbour, D; Dimmery, D",,"Chiappa, S; Calandra, R",,"Sondhi, Arjun; Arbour, David; Dimmery, Drew",,,Balanced Off-Policy Evaluation in General Action Spaces,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Estimation of importance sampling weights for off-policy evaluation of contextual bandits often results in imbalance a mismatch between the desired and the actual distribution of state-action pairs after weighting. In this work we present balanced off-policy evaluation (B-OPE), a generic method for estimating weights which minimize this imbalance. Estimation of these weights reduces to a binary classification problem regardless of action type. We show that minimizing the risk of the classifier implies minimization of imbalance to the desired counterfactual distribution. In turn, this is tied to the error of the off-policy estimate, allowing for easy tuning of hyperparameters. We provide experimental evidence that B-OPE improves weighting-based approaches for offline policy evaluation in both discrete and continuous action spaces.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303025,0
C,"Wilson, A; Kasy, M; Mackey, L",,"Chiappa, S; Calandra, R",,"Wilson, Ashia; Kasy, Maximilian; Mackey, Lester",,,Approximate Cross-validation: Guarantees for Model Assessment and Selection,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Cross-validation (CV) is a popular approach for assessing and selecting predictive models. However, when the number of folds is large, CV suffers from a need to repeatedly refit a learning procedure on a large number of training datasets. Recent work in empirical risk minimization (ERM) approximates the expensive refitting with a single Newton step warm-started from the full training set optimizer. While this can greatly reduce run-time, several open questions remain including whether these approximations lead to faithful model selection and whether they are suitable for non-smooth objectives. We address these quest ions with three main contributions: (i) we provide uniform non-asymptotic, deterministic model assessment guarantees for approximate CV; (ii) we show that (roughly) the same conditions also guarantee model selection performance comparable to CV; (iii) we provide a proximal Newton extension of the approximate CV framework for non-smooth prediction problems and develop improved assessment guarantees for problems such as l(1)-regularized ERM.",,,,,,"Mackey, Lester/0000-0002-1102-0387",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303079,0
C,"Yue, XB; Al Kontar, R",,"Chiappa, S; Calandra, R",,"Yue, Xubo; Al Kontar, Raed",,,Why Non-myopic Bayesian Optimization is Promising and How Far Should We Look-ahead? A Study via Rollout,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Lookahead, also known as non-myopic, Bayesian optimization (BO) aims to find optimal sampling policies through solving a dynamic programming (DP) formulation that maximizes a long-term reward over a rolling horizon. Though promising, lookahead BO faces the risk of error propagation through its increased dependence on a possibly mis-specified model. In this work we focus on the rollout approximation for solving the intractable DP. We first prove the improving nature of rollout in tackling lookahead BO and provide a sufficient condition for the used heuristic to be rollout improving. We then provide both a theoretical and practical guideline to decide on the rolling horizon stagewise. This guideline is built on quantifying the negative effect of a mis-specified model. To illustrate our idea, we provide case studies on both single and multi-information source BO. Empirical results show the advantageous properties of our method over several myopic and non-myopic BO algorithms.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2808,2817,,,,,,,,,,,,,,,,WOS:000559931304002,0
C,"Zhang, RY; Chen, CY; Gan, Z; Wen, Z; Wang, WL; Carin, L",,"Chiappa, S; Calandra, R",,"Zhang, Ruiyi; Chen, Changyou; Gan, Zhe; Wen, Zheng; Wang, Wenlin; Carin, Lawrence",,,Nested-Wasserstein Self-Imitation Learning for Sequence Generation,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Reinforcement learning (RL) has been widely studied for improving sequence-generation models. However, the conventional rewards used for RL training typically cannot capture sufficient semantic information and therefore manifest model bias. Further, the sparse and delayed rewards make RL exploration inefficient. To alleviate these issues, we propose the concept of nested-Wasserstein distance for distributional semantic matching. To further exploit it, a novel nested-Wasserstein self-imitation learning framework is developed, encouraging the model to exploit historical high-reward sequences for enhanced exploration and better semantic matching. Our solution can be understood as approximately executing proximal policy optimization with Wasserstein trust-regions. Experiments on a variety of unconditional and conditional sequence-generation tasks demonstrate the proposed approach consistently leads to improved performance.",,,,,"Zhang, Ruiyi/AAB-8402-2021; wen, zheng/HII-3705-2022",,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,422,432,,,,,,,,,,,,,,,,WOS:000559931304008,0
C,"Ablin, P; Gramfort, A; Cardoso, JF; Bach, F",,"Chaudhuri, K; Sugiyama, M",,"Ablin, Pierre; Gramfort, Alexandre; Cardoso, Jean-Francois; Bach, Francis",,,Stochastic algorithms with descent guarantees for ICA,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Independent component analysis (ICA) is a widespread data exploration technique, where observed signals are modeled as linear mixtures of independent components. From a machine learning point of view, it amounts to a matrix factorization problem with a statistical independence criterion. Infomax is one of the most used ICA algorithms. It is based on a loss function which is a non-convex log-likelihood. We develop a new majorizationminimization framework adapted to this loss function. We derive an online algorithm for the streaming setting, and an incremental algorithm for the finite sum setting, with the following benefits. First, unlike most algorithms found in the literature, the proposed methods do not rely on any critical hyper-parameter like a step size, nor do they require a line-search technique. Second, the algorithm for the finite sum setting, although stochastic, guarantees a decrease of the loss function at each iteration. Experiments demonstrate progress on the state-of-the-art for large scale datasets, without the necessity for any manual parameter tuning.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901063,0
C,"Anari, N; Haghtalab, N; Naor, J; Pokutta, S; Singh, M; Torrico, A",,"Chaudhuri, K; Sugiyama, M",,"Anari, Nima; Haghtalab, Nika; Naor, Joseph (Seffi); Pokutta, Sebastian; Singh, Mohit; Torrico, Alfredo",,,Structured Robust Submodular Maximization: Offline and Online Algorithms,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Constrained submodular function maximization has been used in subset selection problems such as selection of most informative sensor locations. While these models have been quite popular, the solutions obtained via this approach are unstable to perturbations in data defining the submodular functions. Robust submodular maximization has been proposed as a richer model that aims to overcome this discrepancy as well as increase the modeling scope of submodular optimization. In this work, we consider robust submodular maximization with structured combinatorial constraints and give efficient algorithms with provable guarantees. Our approach is applicable to constraints defined by single or multiple matroids, knapsack as well as distributionally robust criteria. We consider both the offline setting where the data defining the problem is known in advance as well as the online setting where the input data is revealed over time. For the offline setting, we give a nearly optimal bi-criteria approximation algorithm that relies on new extensions of the classical greedy algorithm. For the online version of the problem, we give an algorithm that returns a bi-criteria solution with sub-linear regret.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687903019,0
C,"Chen, C; Lee, J; Kifer, D",,"Chaudhuri, K; Sugiyama, M",,"Chen, Chen; Lee, Jaewoo; Kifer, Daniel",,,Renyi Differentially Private ERM for Smooth Objectives,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"In this paper, we present a Renyi Differentially Private stochastic gradient descent (SGD) algorithm for convex empirical risk minimization. The algorithm uses output perturbation and leverages randomness inside SGD, which creates a randomized sensitivity, in order to reduce the amount of noise that is added. One of the benefits of output perturbation is that we can incorporate a periodic averaging step that serves to further reduce sensitivity while improving accuracy (reducing the well-known oscillating behavior of SGD near the optimum). Renyi Differential Privacy can be used to provide (epsilon, delta)-differential privacy guarantees and hence provide a comparison with prior work. An empirical evaluation demonstrates that the proposed method outperforms prior methods on differentially private ERM.",,,,,,"Kifer, Daniel/0000-0002-4611-7066",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902009,0
C,"Chen, TF; Navratil, J; Iyengar, V; Shanmugam, K",,"Chaudhuri, K; Sugiyama, M",,"Chen, Tongfei; Navratil, Jiri; Iyengar, Vijay; Shanmugam, Karthikeyan",,,Confidence Scoring Using Whitebox Meta-models with Linear Classifier Probes,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We propose a novel confidence scoring mechanism for deep neural networks based on a two-model paradigm involving a base model and a meta-model. The confidence score is learned by the meta-model observing the base model succeeding/failing at its task. As features to the meta-model, we investigate linear classifier probes inserted between the various layers of the base model. Our experiments demonstrate that this approach outperforms multiple baselines in a filtering task, i.e., task of rejecting samples with low confidence. Experimental results are presented using CIFAR-10 and CIFAR-100 dataset with and without added noise. We discuss the importance of confidence scoring to bridge the gap between experimental and real-world applications.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901053,0
C,"Dai, OE; Cullina, D; Kiyavash, N",,"Chaudhuri, K; Sugiyama, M",,"Dai, Osman Emre; Cullina, Daniel; Kiyavash, Negar",,,Database Alignment with Gaussian Features,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We consider the problem of aligning a pair of databases with jointly Gaussian features. We consider two algorithms, complete database alignment via MAP estimation among all possible database alignments, and partial alignment via a thresholding approach of log likelihood ratios. We derive conditions on mutual information between feature pairs, identifying the regimes where the algorithms are guaranteed to perform reliably and those where they cannot be expected to succeed.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687903029,0
C,"Giordano, R; Stephenson, W; Liu, RJ; Jordan, MI; Broderick, T",,"Chaudhuri, K; Sugiyama, M",,"Giordano, Ryan; Stephenson, Will; Liu, Runjing; Jordan, Michael, I; Broderick, Tamara",,,A Swiss Army Infinitesimal Jackknife,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"The error or variability of machine learning algorithms is often assessed by repeatedly refitting a model with different weighted versions of the observed data. The ubiquitous tools of cross-validation (CV) and the bootstrap are examples of this technique. These methods are powerful in large part due to their model agnosticism but can be slow to run on modern, large data sets due to the need to repeatedly re-fit the model. In this work, we use a linear approximation to the dependence of the fitting procedure on the weights producing results that can be faster than repeated re-fitting by an order of magnitude. This linear approximation is sometimes known as the infinitesimal jackknife in the statistics literature, where it is mostly used as a theoretical tool to prove asymptotic results. We provide explicit finite-sample error bounds for the infinitesimal jackknife in terms of a small number of simple, verifiable assumptions. Our results apply whether the weights and data are stochastic or deterministic, and so can be used as a tool for proving the accuracy of the infinitesimal jackknife on a wide variety of problems. As a corollary, we state mild regularity conditions under which our approximation consistently estimates true leavek-out cross-validation for any fixed k. These theoretical results, together with modern automatic differentiation software, support the application of the infinitesimal jackknife to a wide variety of practical problems in machine learning, providing a Swiss Army infinitesimal jackknife. We demonstrate the accuracy of our methods on a range of simulated and real datasets.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901019,0
C,"Jiang, H; Jang, J; Nachum, O",,"Chaudhuri, K; Sugiyama, M",,"Jiang, Heinrich; Jang, Jennifer; Nachum, Ofir",,,Robustness Guarantees for Density Clustering,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Despite the practical relevance of density-based clustering algorithms, there is little understanding in its statistical robustness properties under possibly adversarial contamination of the input data. We show both robustness and consistency guarantees for a simple modification of the popular DBSCAN algorithm. We then give experimental results which suggest that this method may be relevant in practice.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687903041,0
C,"Kleinegesse, S; Gutmann, M",,"Chaudhuri, K; Sugiyama, M",,"Kleinegesse, Steven; Gutmann, Michael",,,Efficient Bayesian Experimental Design for Implicit Models,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Bayesian experimental design involves the optimal allocation of resources in an experiment, with the aim of optimising cost and performance. For implicit models, where the likelihood is intractable but sampling from the model is possible, this task is particularly difficult and therefore largely unexplored. This is mainly due to technical difficulties associated with approximating posterior distributions and utility functions. We devise a novel experimental design framework for implicit models that improves upon previous work in two ways. First, we use the mutual information between parameters and data as the utility function, which has previously not been feasible. We achieve this by utilising Likelihood-Free Inference by Ratio Estimation (LFIRE) to approximate posterior distributions, instead of the traditional approximate Bayesian computation or synthetic likelihood methods. Secondly, we use Bayesian optimisation in order to solve the optimal design problem, as opposed to the typically used grid search or sampling-based methods. We find that this increases efficiency and allows us to consider higher design dimensions.",,,,,,"Kleinegesse, Steven/0000-0003-2681-6991",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,476,485,,,,,,,,,,,,,,,,WOS:000509687900050,0
C,"Lee, C; Zame, WR; Alaa, AM; van der Schaar, M",,"Chaudhuri, K; Sugiyama, M",,"Lee, Changhee; Zame, William R.; Alaa, Ahmed M.; van der Schaar, Mihaela",,,Temporal Quilting for Survival Analysis,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"The importance of survival analysis in many disciplines (especially in medicine) has led to the development of a variety of approaches to modeling the survival function. Models constructed via various approaches offer different strengths and weaknesses in terms of discriminative performance and calibration, but no one model is best across all datasets or even across all time horizons within a single dataset. Because we require both good calibration and good discriminative performance over different time horizons, conventional model selection and ensemble approaches are not applicable. This paper develops a novel approach that combines the collective intelligence of different underlying survival models to produce a valid survival function that is well-calibrated and offers superior discriminative performance at different time horizons. Empirical results show that our approach provides significant gains over the benchmarks on a variety of real-world datasets.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,596,605,,,,,,,,,,,,,,,,WOS:000509687900062,0
C,"Liu, B; Yuan, XT; Wang, LZ; Liu, QS; Huang, JZ; Metaxas, DN",,"Chaudhuri, K; Sugiyama, M",,"Liu, Bo; Yuan, Xiao-Tong; Wang, Lezi; Liu, Qingshan; Huang, Junzhou; Metaxas, Dimitris N.",,,Distributed Inexact Newton-type Pursuit for Non-Convex Sparse Learning,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"In this paper, we present a sample distributed greedy pursuit method for non-convex sparse learning under cardinality constraint. Given the training samples uniformly randomly partitioned across multiple machines, the proposed method alternates between local inexact sparse minimization of a Newton-type approximation and centralized global results aggregation. Theoretical analysis shows that for a general class of convex functions with Lipschitze continues Hessian, the method converges linearly with contraction factor scaling inversely to the local data size; whilst the communication complexity required to reach desirable statistical accuracy scales logarithmically with respect to the number of machines for some popular statistical learning models. For non convex objective functions, up to a local estimation error, our method can be shown to converge to a local stationary sparse solution with sub-linear communication complexity. Numerical results demonstrate the efficiency and accuracy of our method when applied to large-scale sparse learning tasks including deep neural nets pruning.",,,,,"Liu, Qing/GWC-9222-2022; liu, qingqing/HHD-0360-2022",,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,343,352,,,,,,,,,,,,,,,,WOS:000509687900036,0
C,"Marx, A; Vreeken, J",,"Chaudhuri, K; Sugiyama, M",,"Marx, Alexander; Vreeken, Jilles",,,Testing Conditional Independence on Discrete Data using Stochastic Complexity,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Testing for conditional independence is a core aspect of constraint-based causal discovery. Although commonly used tests are perfect in theory, they often fail to reject independence in practice especially when conditioning on multiple variables. We focus on discrete data and propose a new test based on the notion of algorithmic independence that we instantiate using stochastic complexity. Amongst others, we show that our proposed test, SCI, is an asymptotically unbiased as well as L-2 consistent estimator for conditional mutual information (CMI). Further, we show that SCI can be reformulated to find a sensible threshold for CMI that works well on limited samples. Empirical evaluation shows that SCI has a lower type II error than commonly used tests. As a result, we obtain a higher recall when we use SCI in causal discovery algorithms, without compromising the precision.",,,,,"Marx, Alexander/T-2796-2019","Marx, Alexander/0000-0002-1575-824X",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,496,505,,,,,,,,,,,,,,,,WOS:000509687900052,0
C,"O'Connor, P; Gavves, E; Welling, M",,"Chaudhuri, K; Sugiyama, M",,"O'Connor, Peter; Gavves, Efstratios; Welling, Max",,,Training a Spiking Neural Network with Equilibrium Propagation,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Backpropagation is almost universally used to train artificial neural networks. However, there are several reasons that backpropagation could not be plausibly implemented by biological neurons. Among these are the facts that (1) biological neurons appear to lack any mechanism for sending gradients backwards across synapses, and (2) biological spiking neurons emit binary signals, whereas back-propagation requires that neurons communicate continuous values between one another. Recently Scellier and Bengio [2017], demonstrated an alternative to backpropagation, called Equilibrium Propagation, wherein gradients are implicitly computed by the dynamics of the neural network, so that neurons do not need an internal mechanism for backpropagation of gradients. This provides an interesting solution to problem (1). In this paper, we address problem (2) by proposing a way in which Equilibrium Propagation can be implemented with neurons which are constrained to just communicate binary values at each time step. We show that with appropriate step-size annealing, we can converge to the same fixed-point as a real-valued neural network, and that with predictive coding, we can make this convergence much faster. We demonstrate that the resulting model can be used to train a spiking neural network using the update scheme from Equilibrium propagation.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901058,0
C,"Ok, J; Oh, S; Jang, Y; Shin, J; Yi, Y",,"Chaudhuri, K; Sugiyama, M",,"Ok, Jungseul; Oh, Sewoong; Jang, Yunhun; Shin, Jinwoo; Yi, Yung",,,Iterative Bayesian Learning for Crowdsourced Regression,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Crowdsourcing platforms emerged as popular venues for purchasing human intelligence at low cost for large volume of tasks. As many low-paid workers are prone to give noisy answers, a common practice is to add redundancy by assigning multiple workers to each task and then simply average out these answers. However, to fully harness the wisdom of the crowd, one needs to learn the heterogeneous quality of each worker. We resolve this fundamental challenge in crowdsourced regression tasks, i.e., the answer takes continuous labels, where identifying good or bad workers becomes much more non-trivial compared to a classification setting of discrete labels. In particular, we introduce a Bayesian iterative scheme and show that it provably achieves the optimal mean squared error. Our evaluations on synthetic and real-world datasets support our theoretical results and show the superiority of the proposed scheme.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901055,0
C,"Schmon, SM; Doucet, A; Deligiannidis, G",,"Chaudhuri, K; Sugiyama, M",,"Schmon, Sebastian M.; Doucet, Arnaud; Deligiannidis, George",,,Bernoulli Race Particle Filters,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"When the weights in a particle filter are not available analytically, standard resampling methods cannot be employed. To circumvent this problem state-of-the-art algorithms replace the true weights with non-negative unbiased estimates. This algorithm is still valid but at the cost of higher variance of the resulting filtering estimates in comparison to a particle filter using the true weights. We propose here a novel algorithm that allows for resampling according to the true intractable weights when only an unbiased estimator of the weights is available. We demonstrate our algorithm on several examples.",,,,,"Schmon, Sebastian/AAT-5935-2021","Schmon, Sebastian/0000-0002-1513-1439",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902041,0
C,"Wang, SJ; Bai, WR; Lavania, C; Bilmes, JA",,"Chaudhuri, K; Sugiyama, M",,"Wang, Shengjie; Bai, Wenruo; Lavania, Chandrashekhar; Bilmes, Jeffrey A.",,,Fixing Mini-batch Sequences with Hierarchical Robust Partitioning,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We propose a general and efficient hierarchical robust partitioning framework to generate a deterministic sequence of mini-batches, one that offers assurances of being high quality, unlike a randomly drawn sequence. We compare our deterministically generated mini-batch sequences to randomly generated sequences; we show that, on a variety of deep learning tasks, the deterministic sequences significantly beat the mean and worst case performance of the random sequences, and often outperforms the best of the random sequences. Our theoretical contributions include a new algorithm for the robust submodular partition problem subject to cardinality constraints (which is used to construct mini-batch sequences), and show in general that the algorithm is fast and has good theoretical guarantees; we also show a more efficient hierarchical variant of the algorithm with similar guarantees under mild assumptions.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687903042,0
C,"Zou, DF; Xu, P; Gu, QQ",,"Chaudhuri, K; Sugiyama, M",,"Zou, Difan; Xu, Pan; Gu, Quanquan",,,Sampling from Non-Log-Concave Distributions via Stochastic Variance-Reduced Gradient Langevin Dynamics,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We study stochastic variance reduction-based Langevin dynamic algorithms, SVRG-LD and SAGA-LD (Dubey et al., 2016), for sampling from non-log-concave distributions. Under certain assumptions on the log density function, we establish the convergence guarantees of SVRG-LD and SAGA-LD in 2-Wasserstein distance. More specifically, we show that both SVRG-LD and SAGA-LD require (O) over tilde (n+n(3/4)/epsilon(2)+n(1/2)/epsilon(4)).exp ((O) over tilde (d+ gamma)) stochastic gradient evaluations to achieve epsilon-accuracy in 2-Wasserstein distance, which outperforms the (O) over tilde (n/epsilon(4)).exp ((O) over tilde (d + gamma)) gradient complexity achieved by Langevin Monte Carlo Method (Raginsky et al., 2017). Experiments on synthetic data and real data back up our theory.",,,,,"Xu, Pan/AAH-3620-2019; X, Pan/GVS-4402-2022","Xu, Pan/0000-0002-2559-8622; ",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902101,0
C,"Chen, YX; Mac Aodha, O; Su, SH; Perona, P; Yue, YS",,"Storkey, A; PerezCruz, F",,"Chen, Yuxin; Mac Aodha, Oisin; Su, Shihan; Perona, Pietro; Yue, Yisong",,,Near-Optimal Machine Teaching via Explanatory Teaching Sets,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Modern applications of machine teaching for humans often involve domain-specific, nontrivial target hypothesis classes. To facilitate understanding of the target hypothesis, it is crucial for the teaching algorithm to use examples which are interpretable to the human learner. In this paper, we propose NOTES, a principled framework for constructing interpretable teaching sets, utilizing explanations to accelerate the teaching process. Our algorithm is built upon a natural stochastic model of learners and a novel submodular surrogate objective function which greedily selects interpretable teaching examples. We prove that NOTES is competitive with the optimal explanation-based teaching strategy. We further instantiate NOTES with a specific hypothesis class, which can be viewed as an interpretable approximation of any hypothesis class, allowing us to handle complex hypothesis in practice. We demonstrate the effectiveness of NOTES on several image classification tasks, for both simulated and real human learners. Our experimental results suggest that by leveraging explanations, one can significantly speed up teaching.",,,,,,"Chen, Yuxin/0000-0003-2133-140X",,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300206,0
C,"Chowdhury, A; Jermaine, C",,"Storkey, A; PerezCruz, F",,"Chowdhury, Arkabandhu; Jermaine, Chris",,,Parallel and Distributed MCMC via Shepherding Distributions,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"In this paper, we present a general algorithmic framework for developing easily parallelizable/distributable Markov Chain Monte Carlo (MCMC) algorithms. Our framework relies on the introduction of an auxiliary distribution called a shepherding distribution (SD) that is used to control several MCMC chains that run in parallel. The SD is an introduced prior on one or more key parameters (or hyperparameters) of the target distribution. The shepherded chains then collectively explore the space of samples, communicating via the shepherding distribution, to reach high likelihood regions faster. The method of SDs is simple, and it is often easy to develop a shepherded sampler for a particular problem. Other advantages include wide applicability-the method can easily be used to draw samples from discrete distributions, or distributions on the simplex. Further, the method is asymptotically correct, since the method of SDs trivially maintains detailed balance.",,,,,"Chowdhury, Arkabandhu/GLU-6715-2022",,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300190,0
C,"Fan, XH; Li, B; Sisson, SA",,"Storkey, A; PerezCruz, F",,"Fan, Xuhui; Li, Bin; Sisson, Scott A.",,,The Binary Space Partitioning-Tree Process,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"The Mondrian process represents an elegant and powerful approach for space partition modelling. However, as it restricts the partitions to be axis-aligned, its modelling flexibility is limited. In this work, we propose a self-consistent Binary Space Partitioning (BSP)-Tree process to generalize the Mondrian process. The BSP-Tree process is an almost surely right continuous Markov jump process that allows uniformly distributed oblique cuts in a two-dimensional convex polygon. The BSP-Tree process can also be extended using a non-uniform probability measure to generate direction differentiated cuts. The process is also self-consistent, maintaining distributional invariance under a restricted subdomain. We use Conditional-Sequential Monte Carlo for inference using the tree structure as the high-dimensional variable. The BSP-Tree process's performance on synthetic data partitioning and relational modelling demonstrates clear inferential improvements over the standard Mondrian process and other related methods.",,,,,"Fan, Xu/GSE-2196-2022",,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300194,0
C,"Koanantakool, P; Ali, A; Azad, A; Buluc, A; Morozov, D; Oliker, L; Yelick, K; Oh, SY",,"Storkey, A; PerezCruz, F",,"Koanantakool, Penporn; Ali, Alnur; Azad, Ariful; Buluc, Aydn; Morozov, Dmitriy; Oliker, Leonid; Yelick, Katherine; Oh, Sang-Yun",,,Communication-Avoiding Optimization Methods for Distributed Massive-Scale Sparse Inverse Covariance Estimation,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Across a variety of scientific disciplines, sparse inverse covariance estimation is a popular tool for capturing the underlying dependency relationships in multivariate data. Unfortunately, most estimators are not scalable enough to handle the sizes of modern high-dimensional data sets (often on the order of terabytes), and assume Gaussian samples. To address these deficiencies, we introduce HP-CONCORD, a highly scalable optimization method for estimating a sparse inverse covariance matrix based on a regularized pseudolikelihood framework, without assuming Gaussianity. Our parallel proximal gradient method uses a novel communication-avoiding linear algebra algorithm and runs across a multi-node cluster with up to 1k nodes (24k cores), achieving parallel scalability on problems with up to approximate to 819 billion parameters (1.28 million dimensions); even on a single node, HP-CONCORD demonstrates scalability, outperforming a state-of-the-art method. We also use HP-CONCORD to estimate the underlying dependency structure of the brain from fMRI data, and use the result to identify functional regions automatically. The results show good agreement with a clustering from the neuroscience literature.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300144,0
C,"Rolland, P; Scarlett, J; Bogunovic, I; Cevher, V",,"Storkey, A; PerezCruz, F",,"Rolland, Paul; Scarlett, Jonathan; Bogunovic, Ilija; Cevher, Volkan",,,High-Dimensional Bayesian Optimization via Additive Models with Overlapping Groups,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Bayesian optimization (BO) is a popular technique for sequential black-box function optimization, with applications including parameter tuning, robotics, environmental monitoring, and more. One of the most important challenges in BO is the development of algorithms that scale to high dimensions, which remains a key open problem despite recent progress. In this paper, we consider the approach of Kandasamy et al. (2015), in which the high-dimensional function decomposes as a sum of lower-dimensional functions on subsets of the underlying variables. In particular, we significantly generalize this approach by lifting the assumption that the subsets are disjoint, and consider additive models with arbitrary overlap among the subsets. By representing the dependencies via a graph, we deduce an efficient message passing algorithm for optimizing the acquisition function. In addition, we provide an algorithm for learning the graph from samples based on Gibbs sampling. We empirically demonstrate the effectiveness of our methods on both synthetic and real-world data.",,,,,"Scarlett, Jonathan/AGK-0892-2022",,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300032,0
C,"Runge, J",,"Storkey, A; PerezCruz, F",,"Runge, Jakob",,,Conditional independence testing based on a nearest-neighbor estimator of conditional mutual information,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Conditional independence testing is a fundamental problem in causal discovery and a particularly challenging task in the presence of nonlinear dependencies. Here a fully non-parametric test for continuous data based on conditional mutual information combined with a local permutation scheme is presented. Numerical experiments covering sample sizes from 50 to 2, 000 and dimensions up to 10 demonstrate that the test reliably generates the null distribution. For smooth nonlinear dependencies, the test has higher power than kernel-based tests in lower dimensions and similar power in higher dimensions. For highly non-smooth densities the data-adaptive nearest neighbor approach is particularly well-suited while kernel methods yield much lower power. The experiments also show that kernel methods utilizing an analytical approximation of the null distribution are not well-calibrated for sample sizes below 1, 000. Combining the local permutation scheme with these kernel tests leads to better calibration but lower power. For smaller sample sizes and lower dimensions, the proposed test is faster than random fourier feature-based kernel tests if (embarrassingly) parallelized, but the runtime increases more sharply with sample size and dimensionality. Thus, more theoretical research to analytically approximate the null distribution and speed up estimation is desirable. As illustrated on real data, the test is ideally suited in combination with causal discovery algorithms.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300099,0
C,"Bernstein, G; Sheldon, D",,"Gretton, A; Robert, CC",,"Bernstein, Garrett; Sheldon, Daniel",,,Consistently Estimating Markov Chains with Noisy Aggregate Data,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"We address the problem of estimating the parameters of a time-homogeneous Markov chain given only noisy, aggregate data. This arises when a population of individuals behave independently according to a Markov chain, but individual sample paths cannot be observed due to limitations of the observation process or the need to protect privacy. Instead, only population-level counts of the number of individuals in each state at each time step are available. When these counts are exact, a conditional least squares (CLS) estimator is known to be consistent and asymptotically normal. We initiate the study of method of moments estimators for this problem to handle the more realistic case when observations are additionally corrupted by noise. We show that CLS can be interpreted as a simple plug-in method of moments estimator. However, when observations are noisy, it is not consistent because it fails to account for additional variance introduced by the noise. We develop a new, simpler method of moments estimator that bypasses this problem and is consistent under noisy observations.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,1142,1150,,,,,,,,,,,,,,,,WOS:000508662100124,0
C,"Nguyen, DT; Kumar, A; Lau, HC; Sheldon, D",,"Gretton, A; Robert, CC",,"Duc Thien Nguyen; Kumar, Akshat; Lau, Hoong Chuin; Sheldon, Daniel",,,Approximate Inference Using DC Programming For Collective Graphical Models,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Collective graphical models (CGMs) provide a framework for reasoning about a population of independent and identically distributed individuals when only noisy and aggregate observations are given. Previous approaches for inference in CGMs work on a junction-tree representation, thereby highly limiting their scalability. To remedy this, we show how the Bethe entropy approximation naturally arises for the inference problem in CGMs. We reformulate the resulting optimization problem as a difference-of-convex functions program that can capture different types of CGM noise models. Using the concave-convex procedure, we then develop a scalable message-passing algorithm. Empirically, we show our approach is highly scalable and accurate for large graphs, more than an order-of-magnitude faster than a generic optimization solver, and is guaranteed to converge unlike the previous message-passing approach NLBP that fails in several loopy graphs.",,,,,"LAU, Hoong Chuin/E-8556-2012","LAU, Hoong Chuin/0000-0002-5326-411X",,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,685,693,,,,,,,,,,,,,,,,WOS:000508662100075,0
C,"Li, P",,"Gretton, A; Robert, CC",,"Li, Ping",,,One Scan 1-Bit Compressed Sensing,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Based on a-stable random projections with small a, we develop a simple algorithm for compressed sensing (sparse signal recovery) by utilizing only the signs (i.e., 1-bit) of the measurements. Using only 1-bit information of the measurements results in substantial cost reduction in collection, storage, communication, and decoding for compressed sensing. The proposed algorithm is efficient in that the decoding procedure requires only one scan of the coordinates. Our analysis can precisely show that, for a K-sparse signal of length N, 12.3K logN/delta measurements (where delta is the confidence) would be sufficient for recovering the support and the signs of the signal. While the method is highly robust against typical measurement noises, we also provide the analysis of the scheme under random flipping of the signs of measurements. Compared to the well-known work on 1-bit marginal regression (which can also be viewed as a one-scan method), the proposed algorithm requires orders of magnitude fewer measurements. Compared to 1-bit Iterative Hard Thresholding (IHT) (which is not a one-scan algorithm), our method is still significantly more accurate. Furthermore, the proposed method is reasonably robust against random sign flipping while IHT is known to be very sensitive to this type of noise.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,1515,1523,,,,,,,,,,,,,,,,WOS:000508662100164,0
C,"Li, XG; Zhao, T; Arora, R; Liu, H; Hong, MY",,"Gretton, A; Robert, CC",,"Li, Xingguo; Zhao, Tuo; Arora, Raman; Liu, Han; Hong, Mingyi",,,An Improved Convergence Analysis of Cyclic Block Coordinate Descent-type Methods for Strongly Convex Minimization,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"The cyclic block coordinate descent-type (CBCD-type) methods have shown remarkable computational performance for solving strongly convex minimization problems. Typical applications include many popular statistical machine learning methods such as elastic-net regression, ridge penalized logistic regression, and sparse additive regression. Existing optimization literature has shown that the CBCD-type methods attain iteration complexity of O(p . log(1/epsilon)), where epsilon is a pre-specified accuracy of the objective value, and p is the number of blocks. However, such iteration complexity explicitly depends on p, and therefore is at least p times worse than those of gradient descent methods. To bridge this theoretical gap, we propose an improved convergence analysis for the CBCD-type methods. In particular, we first show that for a family of quadratic minimization problems, the iteration complexity of the CBCD-type methods matches that of the GD methods in term of dependency on p (up to a log 2 p factor). Thus our complexity bounds are sharper than the existing bounds by at least a factor of p/log(2) p. We also provide a lower bound to confirm that our improved complexity bounds are tight (up to a log(2) p factor) if the largest and smallest eigen-values of the Hessian matrix do not scale with p. Finally, we generalize our analysis to other strongly convex minimization problems beyond quadratic ones.",,,,,"Hong, Mingyi/H-6274-2013",,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,491,499,,,,,,,,,,,,,,,,WOS:000508662100054,0
C,"McCarter, C; Kim, S",,"Gretton, A; Robert, CC",,"McCarter, Calvin; Kim, Seyoung",,,Large-Scale Optimization Algorithms for Sparse Conditional Gaussian Graphical Models,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"This paper addresses the problem of scalable optimization for l(1)-regularized conditional Gaussian graphical models. Conditional Gaussian graphical models generalize the well-known Gaussian graphical models to conditional distributions to model the output network influenced by conditioning input variables. While highly scalable optimization methods exist for sparse Gaussian graphical model estimation, state-of-the-art methods for conditional Gaussian graphical models are not efficient enough and more importantly, fail due to memory constraints for very large problems. In this paper, we propose a new optimization procedure based on a Newton method that efficiently iterates over two sub-problems, leading to drastic improvement in computation time compared to the previous methods. We then extend our method to scale to large problems under memory constraints, using block coordinate descent to limit memory usage while achieving fast convergence. Using synthetic and genomic data, we show that our methods can solve problems with millions of variables and tens of billions of parameters to high accuracy on a single machine.",,,,,"McCarter, Calvin/AAE-4434-2020; Kim, SeYoung/GSE-5296-2022","McCarter, Calvin/0000-0002-7257-1350; Kim, SeYoung/0000-0001-9188-868X",,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,529,537,,,,,,,,,,,,,,,,WOS:000508662100058,0
C,"Schneider, M",,"Gretton, A; Robert, CC",,"Schneider, Markus",,,Probability Inequalities for Kernel Embeddings in Sampling without Replacement,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"The kernel embedding of distributions is a popular machine learning technique to manipulate probability distributions and is an integral part of numerous applications. Its empirical counterpart is an estimate from a finite set of samples from the distribution under consideration. However, for large-scale learning problems the empirical kernel embedding becomes infeasible to compute and approximate, constant time solutions are necessary. One can use a random subset of smaller size as a proxy for the exhaustive set of samples to calculate the empirical kernel embedding which is known as sampling without replacement. In this work we generalize the results of Serfling (1974) to quantify the difference between the full empirical kernel embedding and the one estimated from random subsets. Furthermore, we derive probability inequalities for Banach space valued martingales in the setting of sampling without replacement.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,66,74,,,,,,,,,,,,,,,,WOS:000508662100008,0
C,"Sheth, R; Khardon, R",,"Gretton, A; Robert, CC",,"Sheth, Rishit; Khardon, Roni",,,A Fixed-Point Operator for Inference in Variational Bayesian Latent Gaussian Models,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Latent Gaussian Models (LGM) provide a rich modeling framework with general inference procedures. The variational approximation offers an effective solution for such models and has attracted a significant amount of interest. Recent work proposed a fixed-point (FP) update procedure to optimize the covariance matrix in the variational solution and demonstrated its efficacy in specific models. The paper makes three contributions. First, it shows that the same approach can be used more generally in extensions of LGM. Second, it provides an analysis identifying conditions for the convergence of the FP method. Third, it provides an extensive experimental evaluation in Gaussian processes, sparse Gaussian processes, and generalized linear models, with several non-conjugate observation likelihoods, showing wide applicability of the FP method and a significant advantage over gradient-based optimization.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,761,769,,,,,,,,,,,,,,,,WOS:000508662100083,0
C,"Wang, JL; Kolar, M",,"Gretton, A; Robert, CC",,"Wang, Jialei; Kolar, Mladen",,,Inference for High-dimensional Exponential Family Graphical Models,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Probabilistic graphical models have been widely used to model complex systems and aid scientific discoveries. Most existing work on high-dimensional estimation of exponential family graphical models, including Gaussian and Ising models, is focused on consistent model selection. However, these results do not characterize uncertainty in the estimated structure and are of limited value to scientists who worry whether their findings will be reproducible and if the estimated edges are present in the model due to random chance. In this paper, we propose a novel estimator for edge parameters in an exponential family graphical models. We prove that the estimator is root n-consistent and asymptotically Normal. This result allows us to construct confidence intervals for edge parameters, as well as, hypothesis tests. We establish our results under conditions that are typically assumed in the literature for consistent estimation. However, we do not require that the estimator consistently recovers the graph structure. In particular, we prove that the asymptotic distribution of the estimator is robust to model selection mistakes and uniformly valid for a large number of data-generating processes. We illustrate validity of our estimator through extensive simulation studies.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,1042,1050,,,,,,,,,,,,,,,,WOS:000508662100113,0
C,"Yasuda, M",,"Gretton, A; Robert, CC",,"Yasuda, Muneki",,,Relationship between Pretraining and Maximum Likelihood Estimation in Deep Boltzmann Machines,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"A pretraining algorithm, which is a layer-by-layer greedy learning algorithm, for a deep Boltzmann machine (DBM) is presented in this paper. By considering the deep belief net type of pretraining for the DBM, which is a simplified version of the original pretraining of the DBM, two interesting theoretical facts about pretraining can be obtained. (1) By applying two different types of approximation, a replacing approximation by using a Bayesian network and a Bethe type of approximation based on the cluster variation method, to two different parts of the true log-likelihood function of the DBM, pretraining can be derived from a variational approximation of the original maximum likelihood estimation. (2) It can be ensured that the pretraining improves the variational bound of the true log-likelihood function of the DBM. These two theoretical results will help deepen our understanding of deep learning. Moreover, on the basis of the theoretical results, we discuss the original pretraining of the DBM in the latter part of this paper.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,582,590,,,,,,,,,,,,,,,,WOS:000508662100064,0
C,"Ding, XH; Jiang, YY; Huang, Y; Paisley, J",,"Kaski, S; Corander, J",,"Ding, Xinghao; Jiang, Yiyong; Huang, Yue; Paisley, John",,,Pan-Sharpening with a Bayesian Nonparametric Dictionary Learning Model,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"Pan-sharpening, a method for constructing high resolution images from low resolution observations, has recently been explored from the perspective of compressed sensing and sparse representation theory. We present a new pan-sharpening algorithm that uses a Bayesian non-parametric dictionary learning model to give an underlying sparse representation for image reconstruction. In contrast to existing dictionary learning methods, the proposed method infers parameters such as dictionary size, patch sparsity and noise variances. In addition, our regularization includes image constraints such as a total variation penalization term and a new gradient penalization on the reconstructed PAN image. Our method does not require high resolution multiband images for dictionary learning, which are unavailable in practice, but rather the dictionary is learned directly on the reconstructed image as part of the inversion process. We present experiments on several images to validate our method and compare with several other well-known approaches.",,,,,"Huang, Yue/ABD-7847-2021",,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,176,184,,,,,,,,,,,,,,,,WOS:000508355800020,0
C,"Kivinen, JJ; Williams, CKI; Heess, N",,"Kaski, S; Corander, J",,"Kivinen, Jyri J.; Williams, Christopher K., I; Heess, Nicolas",,,Visual Boundary Prediction: A Deep Neural Prediction Network and Quality Dissection,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"This paper investigates visual boundary detection, i.e. prediction of the presence of a boundary at a given image location. We develop a novel neurally-inspired deep architecture for the task. Notable aspects of our work are (i) the use of covariance features [Ranzato and Hinton, 2010] which depend on the squared response of a filter to the input image, and (ii) the integration of image information from multiple scales and semantic levels via multiple streams of inter-linked, layered, and non-linear deep processing. Our results on the Berkeley Segmentation Data Set 500 (BSDS500) show comparable or better performance to the topperforming methods [Arbelaez et al., 2011, Ren and Bo, 2012, Lim et al., 2013, Dollhr and Zitnick, 2013] with effective inference times. We also propose novel quantitative assessment techniques for improved method understanding and comparison. We carefully dissect the performance of our architecture, feature-types used and training methods, providing clear signals for model understanding and development.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,512,521,,,,,,,,,,,,,,,,WOS:000508355800057,0
C,"Wang, ZY; Shakibi, B; Jin, L; de Freitas, N",,"Kaski, S; Corander, J",,"Wang, Ziyu; Shakibi, Babak; Jin, Lin; de Freitas, Nando",,,Bayesian Multi-Scale Optimistic Optimization,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"Bayesian optimization is a powerful global optimization technique for expensive black-box functions. One of its shortcomings is that it requires auxiliary optimization of an acquisition function at each iteration. This auxiliary optimization can be costly and very hard to carry out in practice. Moreover, it creates serious theoretical concerns, as most of the convergence results assume that the exact optimum of the acquisition function can be found. In this paper, we introduce a new technique for efficient global optimization that combines Gaussian process confidence bounds and treed simultaneous optimistic optimization to eliminate the need for auxiliary optimization of acquisition functions. The experiments with global optimization benchmarks and a novel application to automatic information extraction demonstrate that the resulting technique is more efficient than the two approaches from which it draws inspiration. Unlike most theoretical analyses of Bayesian optimization with Gaussian processes, our finite-time convergence rate proofs do not require exact optimization of an acquisition function. That is, our approach eliminates the unsatisfactory assumption that a difficult, potentially NP-hard, problem has to be solved in order to obtain vanishing regret rates.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,1005,1014,,,,,,,,,,,,,,,,WOS:000508355800111,0
C,"Yang, JJ; Han, QY; Airoldi, EM",,"Kaski, S; Corander, J",,"Yang, Justin J.; Han, Qiuyi; Airoldi, Edoardo M.",,,Nonparametric estimation and testing of exchangeable graph models,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"Exchangeable graph models (ExGM) are a non-parametric approach to modeling network data that subsumes a number of popular models. The key object that defines an ExGM is often referred to as a graphon, or graph kernel. Here, we make three contributions to advance the theory of estimation of graphons. We determine conditions under which a unique canonical representation for a graphon exists and it is identifiable. We propose a 3-step procedure to estimate the canonical graphon of any ExGM that satisfies these conditions. We then focus on a specific estimator, built using the proposed 3-step procedure, which combines probability matrix estimation by Universal Singular Value Thresholding (USVT) and empirical degree sorting of the observed adjacency matrix. We prove that this estimator is consistent. We illustrate how the proposed theory and methods can be used to develop hypothesis testing procedures for models of network data.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,1060,1067,,,,,,,,,,,,,,,,WOS:000508355800117,0
C,"Liu, WY; Lin, RM; Liu, Z; Xiong, L; Scholkopf, B; Weller, A",,"Banerjee, A; Fukumizu, K",,"Liu, Weiyang; Lin, Rongmei; Liu, Zhen; Xiong, Li; Schoelkopf, Bernhard; Weller, Adrian",,,Learning with Hyperspherical Uniformity,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Due to the over-parameterization nature, neural networks are a powerful tool for nonlinear function approximation. In order to achieve good generalization on unseen data, a suitable inductive bias is of great importance for neural networks. One of the most straightforward ways is to regularize the neural network with some additional objectives. l(2) regularization serves as a standard regularization for neural networks. Despite its popularity, it essentially regularizes one dimension of the individual neuron, which is not strong enough to control the capacity of highly over-parameterized neural networks. Motivated by this, hyperspherical uniformity is proposed as a novel family of relational regularizations that impact the interaction among neurons. We consider several geometrically distinct ways to achieve hyperspherical uniformity. The effectiveness of hyperspherical uniformity is justified by theoretical insights and empirical evaluations.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801045,0
C,"Matsuoka, T; Ito, S; Ohsaka, N",,"Banerjee, A; Fukumizu, K",,"Matsuoka, Tatsuya; Ito, Shinji; Ohsaka, Naoto",,,Tracking Regret Bounds for Online Submodular Optimization,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"In this paper, we propose algorithms for online submodular optimization with tracking regret bounds. Online submodular optimization is a generic framework for sequential decision making used to select subsets. Existing algorithms for online submodular optimization have been shown to achieve small (static) regret, which means that the algorithm's performance is comparable to the performance of a fixed optimal action. Such algorithms, however, may perform poorly in an environment that changes over time. To overcome this problem, we apply a tracking-regret-analysis framework to online submodular optimization, one by which output is assessed through comparison with time-varying optimal subsets. We propose algorithms for submodular minimization, monotone submodular maximization under a size constraint, and unconstrained submodular maximization, and we show tracking regret bounds. In addition, we show that our tracking regret bound for submodular minimization is nearly tight.",,,,,,"Ohsaka, Naoto/0000-0001-9584-4764",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804015,0
C,"Morningstar, WR; Vikram, SM; Ham, C; Gallagher, AG; Dillon, JV",,"Banerjee, A; Fukumizu, K",,"Morningstar, Warren R.; Vikram, Sharad M.; Ham, Cusuh; Gallagher, Andrew G.; Dillon, Joshua, V",,,Automatic Differentiation Variational Inference with Mixtures,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Automatic Differentiation Variational Inference (ADVI) is a useful tool for efficiently learning probabilistic models in machine learning. Traditionally, approximate posteriors learned by ADVI are forced to be unimodal in order to facilitate use of the reparameterization trick. In this paper, we show how stratified sampling may be used to enable mixture distributions as the approximate posterior, and derive a new lower bound on the evidence analogous to the importance weighted autoencoder (IWAE). We show that this SIWAE is a tighter bound than both IWAE and the traditional ELBO, both of which are special instances of this bound. We verify empirically that the traditional ELBO objective disfavors the presence of multimodal posterior distributions and may therefore not be able to fully capture structure in the latent space. Our experiments show that using the SIWAE objective allows the encoder to learn more complex distributions which contain multimodality, resulting in higher accuracy, better calibration, and improved generative model performance in the presence of incomplete, limited, or corrupted data.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803088,0
C,"Schwedes, T; Calderhead, B",,"Banerjee, A; Fukumizu, K",,"Schwedes, Tobias; Calderhead, Ben",,,Rao-Blackwellised parallel MCMC,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Multiple proposal Markov chain Monte Carlo (MP-MCMC) as introduced by Calderhead (2014) allow for computationally efficient and parallelisable inference, whereby multiple states are proposed and computed simultaneously. In this paper, we improve the resulting integral estimators by sequentially using the multiple states within a RaoBlackwellised estimator. We further propose a novel adaptive Rao-Blackwellised MP-MCMC algorithm, which generalises the adaptive MCMC algorithm introduced by Haario et al. (2001) to allow for multiple proposals. We prove its asymptotic unbiasedness, and demonstrate significant improvements in sampling efficiency through numerical studies.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804018,0
C,"Sudarshan, M; Puli, A; Subramanian, L; Sankararaman, S; Ranganath, R",,"Banerjee, A; Fukumizu, K",,"Sudarshan, Mukund; Puli, Aahlad; Subramanian, Lakshmi; Sankararaman, Sriram; Ranganath, Rajesh",,,CONTRA: Contrarian statistics for controlled variable selection,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"The holdout randomization test (HRT) discovers a set of covariates most predictive of a response. Given the covariate distribution, HRTS can explicitly control the false discovery rate (FDR). However, if this distribution is unknown and must be estimated from data, HRTS can inflate the FDR. To alleviate the inflation of FDR, we propose the contrarian randomization test (CONTRA), which is designed explicitly for scenarios where the covariate distribution must be estimated from data and may even be misspecified. Our key insight is to use an equal mixture of two contrarian probabilistic models in determining the importance of a covariate. One model is fit with the real data, while the other is fit using the same data, but with the covariate being tested replaced with samples from an estimate of the covariate distribution. CONTRA is flexible enough to achieve a power of 1 asymptotically, can reduce the FDR compared to state-of-the-art CVS methods when the covariate distribution is misspecified, and is computationally efficient in high dimensions and large sample sizes. We further demonstrate the effectiveness of CONTRA on numerous synthetic benchmarks, and highlight its capabilities on a genetic dataset.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,34522887,,,,,WOS:000659893802031,0
C,"Tang, W; Ho, CJ; Liu, Y",,"Banerjee, A; Fukumizu, K",,"Tang, Wei; Ho, Chien-Ju; Liu, Yang",,,Linear Models are Robust Optimal Under Strategic Behavior,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"There is an ubiquitous use of algorithms to inform decisions nowadays, from student evaluations, college admissions, to credit scoring. These decisions are made by applying a decision rule to individual's observed features. Given the impacts of these decisions on individuals, decision makers are increasingly required to be transparent on their decision making to offer the right to explanation. Meanwhile, being transparent also invites potential manipulations, also known as gaming, that individuals can utilize the knowledge to strategically alter their features in order to receive a more beneficial decision. In this work, we study the problem of robust decision-making under strategic behavior. Prior works often assume that the decision maker has full knowledge of individuals' cost structure for manipulations. We study the robust variant that relaxes this assumption: The decision maker does not have full knowledge but knows only a subset of the individuals' available actions and associated costs. To approach this non-quantifiable uncertainty, we define robustness based on the worst-case guarantee of a decision, over all possible actions (including actions unknown to the decision maker) individuals might take. A decision rule is called robust optimal if its worst case performance is (weakly) better than that of all other decision rules. Our main contributions are two-fold. First, we provide a crisp characterization of the above robust optimality: For any decision rules under mild conditions that are robust optimal, there exists a linear decision rule that is equally robust optimal. Second, we explore the computational problem of searching for the robust optimal decision rule and demonstrate its connection to distributionally robust optimization. We believe our results promote the use of simple linear decisions with uncertain individual manipulations.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803014,0
C,"Vainstein, D; Chatziafratis, V; Citovsky, G; Rajagopalan, A; Mahdian, M; Azar, Y",,"Banerjee, A; Fukumizu, K",,"Vainstein, Danny; Chatziafratis, Vaggos; Citovsky, Gui; Rajagopalan, Anand; Mahdian, Mohammad; Azar, Yossi",,,Hierarchical Clustering via Sketches and Hierarchical Correlation Clustering,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Recently, Hierarchical Clustering (HC) has been considered through the lens of optimization. In particular, two maximization objectives have been defined. Moseley and Wang defined the Revenue objective to handle similarity information given by a weighted graph on the data points (w.l.o.g., [0, 1] weights), while Cohen-Addad et al. defined the Dissimilarity objective to handle dissimilarity information. In this paper, we prove structural lemmas for both objectives allowing us to convert any HC tree to a tree with constant number of internal nodes while incurring an arbitrarily small loss in each objective. Although the best-known approximations are 0.585 and 0.667 respectively, using our lemmas we obtain approximations arbitrarily close to 1, if not all weights are small (i.e., there exist constants epsilon, delta such that the fraction of weights smaller than delta, is at most 1 - epsilon); such instances encompass many metric-based similarity instances, thereby improving upon prior work. Finally, we introduce Hierarchical Correlation Clustering (HCC) to handle instances that contain similarity and dissimilarity information simultaneously. For HCC, we provide an approximation of 0.4767 and for complementary similarity/dissimilarity weights (analogous to +/- correlation clustering), we again present nearly-optimal approximations.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,559,+,,,,,,,,,,,,,,,,WOS:000659893800063,0
C,"Vakili, S; Khezeli, K; Picheny, V",,"Banerjee, A; Fukumizu, K",,"Vakili, Sattar; Khezeli, Kia; Picheny, Victor",,,On Information Gain and Regret Bounds in Gaussian Process Bandits,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Consider the sequential optimization of an expensive to evaluate and possibly non-convex objective function integral from noisy feedback, that can be regarded as a continuum-armed bandit problem. Upper bounds on the regret performance of several learning algorithms (GP-UCB, GP-TS, and their variants) are known under both a Bayesian (when f is a sample from a Gaussian process (GP)) and a frequentist (when f lives in a reproducing kernel Hilbert space) setting. The regret bounds often rely on the maximal information gain gamma(T) between T observations and the underlying GP (surrogate) model. We provide general bounds on gamma(T) based on the decay rate of the eigenvalues of the GP kernel, whose specialisation for commonly used kernels improves the existing bounds on gamma(T), and subsequently the regret bounds relying on gamma(T) under numerous settings. For the Matern family of kernels, where the lower bounds on gamma(T), and regret under the frequentist setting, are known, our results close a huge polynomial in T gap between the upper and lower bounds (up to logarithmic in T factors).",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,82,+,,,,,,,,,,,,,,,,WOS:000659893800010,0
C,"Wehenkel, A; Louppe, G",,"Banerjee, A; Fukumizu, K",,"Wehenkel, Antoine; Louppe, Gilles",,,Graphical Normalizing Flows,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Normalizing flows model complex probability distributions by combining a base distribution with a series of bijective neural networks. State-of-the-art architectures rely on coupling and autoregressive transformations to lift up invertible functions from scalars to vectors. In this work, we revisit these transformations as probabilistic graphical models, showing they reduce to Bayesian networks with a pre-defined topology and a learnable density at each node. From this new perspective, we propose the graphical normalizing flow, a new invertible transformation with either a prescribed or a learnable graphical structure. This model provides a promising way to inject domain knowledge into normalizing flows while preserving both the interpretability of Bayesian networks and the representation capacity of normalizing flows. We show that graphical conditioners discover relevant graph structure when we cannot hypothesize it. In addition, we analyze the effect of `1-penalization on the recovered structure and on the quality of the resulting density estimation. Finally, we show that graphical conditioners lead to competitive white box density estimators.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,37,+,,,,,,,,,,,,,,,,WOS:000659893800005,0
C,"Xu, WK; Reinert, G",,"Banerjee, A; Fukumizu, K",,"Xu, Wenkai; Reinert, Gesine",,,A Stein Goodness-of-fit Test for Exponential Random Graph Models,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We propose and analyse a novel nonparametric goodness-of-fit testing procedure for exchangeable exponential random graph models (ERGMs) when a single network realisation is observed. The test determines how likely it is that the observation is generated from a target unnormalised ERGM density. Our test statistics are derived from a kernel Stein discrepancy, a divergence constructed via Stein's method using functions in a reproducing kernel Hilbert space, combined with a discrete Stein operator for ERGMs. The test is a Monte Carlo test based on simulated networks from the target ERGM. We show theoretical properties for the testing procedure for a class of ERGMs. Simulation studies and real network applications are presented.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,415,+,,,,,,,,,,,,,,,,WOS:000659893800047,0
C,"Zhu, SX; Zhang, MH; Ding, RY; Xie, Y",,"Banerjee, A; Fukumizu, K",,"Zhu, Shixiang; Zhang, Minghe; Ding, Ruyi; Xie, Yao",,,Deep Fourier Kernel for Self-Attentive Point Processes,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We present a novel attention-based model for discrete event data to capture complex nonlinear temporal dependence structures. We borrow the idea from the attention mechanism and incorporate it into the point processes' conditional intensity function. We further introduce a novel score function using Fourier kernel embedding, whose spectrum is represented using neural networks, which drastically differs from the traditional dot-product kernel and can capture a more complex similarity structure. We establish our approach's theoretical properties and demonstrate our approach's competitive performance compared to the state-of-the-art for synthetic and real data.",,,,,"ZHANG, MINGHE/GWN-0933-2022","Zhu, Shixiang/0000-0002-2241-6096",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801009,0
C,"Aden-Ali, I; Ashtiani, H",,"Chiappa, S; Calandra, R",,"Aden-Ali, Ishaq; Ashtiani, Hassan",,,On the Sample Complexity of Learning Sum-Product Networks,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Sum-Product Networks (SPNs) can be regarded as a form of deep graphical models that compactly represent deeply factored and mixed distributions. An SPN is a rooted directed acyclic graph (DAG) consisting of a set of leaves (corresponding to base distributions), a set of sum nodes (which represent mixtures of their children distributions) and a set of product nodes (representing the products of its children distributions). In this work, we initiate the study of the sample complexity of PAC-learning the set of distributions that correspond to SPNs. We show that the sample complexity of learning tree structured SPNs with the usual type of leaves (i.e., Gaussian or discrete) grows at most linearly (up to logarithmic factors) with the number of parameters of the SPN. More specifically, we show that the class of distributions that corresponds to tree structured Gaussian SPNs with k mixing weights and e (d-dimensional Gaussian) leaves can be learned within Total Variation error epsilon using at most (O) over tilde (ed(2)+k/epsilon(2)) samples. A similar result holds for tree structured SPNs with discrete leaves. We obtain the upper bounds based on the recently proposed notion of distribution compression schemes. More specifically, we show that if a (base) class of distributions F admits an efficient compression, then the class of tree structured SPNs with leaves from F also admits an efficient compression.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,4508,4518,,,,,,,,,,,,,,,,WOS:000559931300005,0
C,"Balda, ER; Koep, N; Behboodi, A; Mathar, R",,"Chiappa, S; Calandra, R",,"Balda, Emilio Rafael; Koep, Niklas; Behboodi, Arash; Mathar, Rudolf",,,Adversarial Risk Bounds through Sparsity based Compression,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Neural networks have been shown to be vulnerable against minor adversarial perturbations of their inputs, especially for high dimensional data under l(infinity) attacks. To combat this problem, techniques like adversarial training have been employed to obtain models that are robust on the training set. However, the robustness of such models against adversarial perturbations may not generalize to unseen data. To study how robustness generalizes, recent works assume that the inputs have bounded l(2)-norm in order to bound the adversarial risk for l(infinity) attacks with no explicit dimension dependence. In this work, we focus on l(infinity) attacks with l(infinity) bounded inputs and prove margin-based bounds. Specifically, we use a compression-based approach that relies on efficiently compressing the set of tunable parameters without distorting the adversarial risk. To achieve this, we apply the concept of effective sparsity and effective joint sparsity on the weight matrices of neural networks. This leads to bounds with no explicit dependence on the input dimension, neither on the number of classes. Our results show that neural networks with approximately sparse weight matrices not only enjoy enhanced robustness but also better generalization. Finally, empirical simulations show that the notion of effective joint sparsity plays a significant role in generalizing robustness to l(infinity) attacks(1).",,,,,"Behboodi, Arash/AAH-2889-2022; Behboodi, Arash/AFM-0515-2022",,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3816,3824,,,,,,,,,,,,,,,,WOS:000559931300022,0
C,"Boluki, S; Ardywibowo, R; Dadaneh, SZ; Zhou, MY; Qian, XN",,"Chiappa, S; Calandra, R",,"Boluki, Shahin; Ardywibowo, Randy; Dadaneh, Siamak Zamani; Zhou, Mingyuan; Qian, Xiaoning",,,Learnable Bernoulli Dropout for Bayesian Deep Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"In this work, we propose learnable Bernoulli dropout (LBD), a new model-agnostic dropout scheme that considers the dropout rates as parameters jointly optimized with other model parameters. By probabilistic modeling of Bernoulli dropout, our method enables more robust prediction and uncertainty quantification in deep models. Especially, when combined with variational auto-encoders (VAEs), LBD enables flexible semi-implicit posterior representations, leading to new semi-implicit VAE (SIVAE) models. We solve the optimization for training with respect to the dropout parameters using Augment-REINFORCE-Merge (ARM), an unbiased and low-variance gradient estimator. Our experiments on a range of tasks show the superior performance of our approach compared with other commonly used dropout schemes. Overall, LBD leads to improved accuracy and uncertainty estimates in image classification and semantic segmentation. Moreover, using SIVAE, we can achieve state-of-the-art performance on collaborative filtering for implicit feedback on several public datasets.",,,,,"Zhou, Mingyuan/AAE-8717-2021",,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3905,3915,,,,,,,,,,,,,,,,WOS:000559931300035,0
C,"Cai, YF; Li, P",,"Chiappa, S; Calandra, R",,"Cai, Yunfeng; Li, Ping",,,An Inverse-free Truncated Rayleigh-Ritz Method for Sparse Generalized Eigenvalue Problem,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"This paper considers the sparse generalized eigenvalue problem (SGEP), which aims to find the leading eigenvector with at most k nonzero entries. SGEP naturally arises in many applications in machine learning, statistics, and scientific computing, for example, the sparse principal component analysis (SPCA), the sparse discriminant analysis (SDA), and the sparse canonical correlation analysis (SCCA). In this paper, we focus on the development of a three-stage algorithm named inverse-free truncated Rayleigh-Ritz method (IFTRR) to efficiently solve SGEP. In each iteration of IFTRR, only a small number of matrix-vector products is required. This makes IFTRR well-suited for large scale problems. Particularly, a new truncation strategy is proposed, which is able to find the support set of the leading eigenvector effectively. Theoretical results are developed to explain why IFTRR works well. Numerical simulations demonstrate the merits of IFTRR.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3460,3469,,,,,,,,,,,,,,,,WOS:000559931300040,0
C,"Derezinski, M; Liang, F; Mahoney, MW",,"Chiappa, S; Calandra, R",,"Derezinski, Michal; Liang, Feynman; Mahoney, Michael W.",,,Bayesian experimental design using regularized determinantal point processes,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We establish a fundamental connection between Bayesian experimental design and determinantal point processes (DPPs). Experimental design is a classical task in combinatorial optimization, where we wish to select a small subset of d-dimensional vectors to minimize a statistical optimality criterion. We show that a new regularized variant of DPPs can be used to design efficient algorithms for finding (1 + epsilon)-approximate solutions to experimental design under four commonly used optimality criteria: A-, C-, D- and V-optimality. A key novelty is that we offer improved guarantees under the Bayesian framework. Our algorithm returns a (1 + epsilon)-approximate solution when the subset size k is Omega (d(A)/epsilon + log 1/epsilon/epsilon(2)), where d(A) << d is an effective dimension determined by prior knowledge (via a precision matrix A). This is the first approximation guarantee where the dependence on d is replaced by an effective dimension. Moreover, the time complexity of our algorithm significantly improves on existing approaches with comparable guarantees.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3197,3206,,,,,,,,,,,,,,,,WOS:000559931300073,0
C,"Ding, Y; Toulis, P",,"Chiappa, S; Calandra, R",,"Ding, Yi; Toulis, Panos",,,Dynamical Systems Theory for Causal Inference with Application to Synthetic Control Methods,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"In this paper, we adopt results in nonlinear time series analysis for causal inference in dynamical settings. Our motivation is policy analysis with panel data, particularly through the use of synthetic control methods. These methods regress pre-intervention outcomes of the treated unit to outcomes from a pool of control units, and then use the fitted regression model to estimate causal effects post-intervention. In this setting, we propose to screen out control units that have a weak dynamical relationship to the treated unit. In simulations, we show that this method can mitigate bias from cherry-picking of control units, which is usually an important concern. We illustrate on real-world applications, including the tobacco legislation example of Abadie et al. (2010), and Brexit.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1888,1897,,,,,,,,,,,,,,,,WOS:000559931300075,0
C,"Dubeyu, A; Zhangu, MM; Xing, EP; Williamson, SA",,"Chiappa, S; Calandra, R",,"Dubeyu, Avinava; Zhangu, Michael M.; Xing, Eric P.; Williamson, Sinead A.",,,"Distributed, partially collapsed MCMC for Bayesian nonparametrics","INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Bayesian nonparametric (BNP) models provide elegant methods for discovering underlying latent features within a data set, but inference in such models can be slow. We exploit the fact that completely random measures, which commonly-used models like the Dirichlet process and the beta-Bernoulli process can be expressed using, are decomposable into independent sub-measures. We use this decomposition to partition the latent measure into a finite measure containing only instantiated components, and an infinite measure containing all other components. We then select different inference algorithms for the two components: uncollapsed samplers mix well on the finite measure, while collapsed samplers mix well on the infinite, sparsely occupied tail. The resulting hybrid algorithm can be applied to a wide class of models, and can be easily distributed to allow scalable inference without sacrificing asymptotic convergence guarantees.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3685,3694,,,,,,,,,,,,,,,,WOS:000559931300078,0
C,"Esfandiari, H; HajiAghayi, MT; Lucier, B; Mitzenmacher, M",,"Chiappa, S; Calandra, R",,"Esfandiari, Hossein; HajiAghayi, Mohammad Taghi; Lucier, Brendan; Mitzenmacher, Michael",,,"Prophets, Secretaries, and Maximizing the Probability of Choosing the Best","INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Suppose a customer is faced with a sequence of fluctuating prices, such as for airfare or a product sold by a large online retailer. Given distributional information about what price they might face each day, how should they choose when to purchase in order to maximize the likelihood of getting the best price in retrospect? This is related to the classical secretary problem, but with values drawn from known distributions. In their pioneering work, Gilbert and Mosteller [J. Amer. Statist. Assoc. 1966] showed that when the values are drawn i.i.d., there is a thresholding algorithm that selects the best value with probability approximately 0.5801. However, the more general problem with non-identical distributions has remained unsolved. In this paper we provide an algorithm for the case of non-identical distributions that selects the maximum element with probability 1/e, and we show that this is tight. We further show that if the observations arrive in a random order, this barrier of 1/e can be broken using a static threshold algorithm, and we show that our success probability is the best possible for any single-threshold algorithm under random observation order. Moreover, we prove that one can achieve a strictly better success probability using more general multi-threshold algorithms, unlike the non-randomorder case. Along the way, we show that the best achievable success probability for the random-order case matches that of the i.i.d. case, which is approximately 0:5801, under a no-superstars condition that no single distribution is very likely ex ante to generate the maximum value. We also extend our results to the problem of selecting one of the k best values.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3717,3726,,,,,,,,,,,,,,,,WOS:000559931300085,0
C,"Farquhar, S; Osborne, MA; Gal, Y",,"Chiappa, S; Calandra, R",,"Farquhar, Sebastian; Osborne, Michael A.; Gal, Yarin",,,Radial Bayesian Neural Networks: Beyond Discrete Support In Large-Scale Bayesian Deep Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We propose Radial Bayesian Neural Networks (BNNs): a variational approximate posterior for BNNs which scales well to large models. Unlike scalable Bayesian deep learning methods like deep ensembles that have discrete support (assign exactly zero probability almost everywhere in weight-space) Radial BNNs maintain full support: letting them act as a prior for continual learning and avoiding the a priori implausibility of discrete support. Our method avoids a sampling problem in mean-field variational inference (MFVI) caused by the so-called 'soap-bubble' pathology of multivariate Gaussians. We show that, unlike MFVI, Radial BNNs are robust to hyperparameters and can be efficiently applied to challenging real-world tasks without needing ad-hoc tweaks and intensive tuning: on a real-world medical imaging task Radial BNNs outperform MC dropout and deep ensembles.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1352,1361,,,,,,,,,,,,,,,,WOS:000559931300091,0
C,"Kacham, P; Woodruff, DP",,"Chiappa, S; Calandra, R",,"Kacham, Praneeth; Woodruff, David P.",,,Optimal Deterministic Coresets for Ridge Regression,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We consider the ridge regression problem, for which we are given an n x d matrix A of examples and a corresponding n x d' matrix B of labels, as well as a ridge parameter lambda >= 0, and would like to output an X' is an element of R-dxd' for which parallel to AX' - B parallel to(2)(F) + lambda parallel to X'parallel to(2)(F) <= (1 + epsilon)OPT, where OPT = min(Y is an element of Rdxd)' parallel to AY - B parallel to(2)(F) + lambda parallel to Y parallel to(2)(F). In the special case of lambda = 0, this is ordinary multi-response linear regression. Our focus is on deterministically constructing coresets for this problem. Here the goal is to select and re-weight a small subset of rows of A and corresponding labels of B, denoted by SA and SB, so that if X' is the minimizer to min(X)' parallel to SAX' - SB parallel to(2)(F) + lambda parallel to X'parallel to(2)(F), then parallel to AX' - B parallel to(2)(F) + lambda parallel to X'parallel to(2)(F) <= (1 + epsilon)OPT. We show how to efficiently (in poly(n; d; 1/epsilon) time) and deterministically select O(sd(lambda)/epsilon) rows of A and B to achieve this property, and prove a matching lower bound, showing that it is necessary to select Omega(sd(lambda)/epsilon) rows no matter what the weights are, for any 1 < 1/epsilon <= sd(lambda). Here sd(lambda) is the statistical dimension of the input, and we assume d' = O(sd lambda) <= d. In the case of ordinary regression, this gives a deterministic algorithm achieving O (d/epsilon) rows and a matching lower bound for any 1 <= 1/epsilon <= d; for 1/epsilon > d we show Theta(d(2)) rows are sufficient. Finally we show our new coresets are mergeable, giving a deterministic protocol for ridge regression with O(sd(lambda)/epsilon) words of communication per server in a distributed setting, in the important case when the rows of A and B have a constant number of non-zero entries and there are a constant number of servers. Prior to our work the best deterministic protocols in this setting required Omega(min(sd(lambda)(2); sd(lambda)/epsilon(2))) communication.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,4141,4149,,,,,,,,,,,,,,,,WOS:000559931301064,0
C,"Kong, W; Valiant, G; Brunskill, E",,"Chiappa, S; Calandra, R",,"Kong, Weihao; Valiant, Gregory; Brunskill, Emma",,,Sublinear Optimal Policy Value Estimation in Contextual Bandits,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We study the problem of estimating the expected reward of the optimal policy in the stochastic disjoint linear bandit setting. We prove that for certain settings it is possible to obtain an accurate estimate of the optimal policy value even with a number of samples that is sublinear in the number that would be required to find a policy that realizes a value close to this optima. We establish nearly matching information theoretic lower bounds, showing that our algorithm achieves near optimal estimation error. Finally, we demonstrate the effectiveness of our algorithm on joke recommendation and cancer inhibition dosage selection problems using real datasets.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,4377,4386,,,,,,,,,,,,,,,,WOS:000559931301082,0
C,"Lengerich, B; Tan, S; Chang, CH; Hooker, G; Caruana, R",,"Chiappa, S; Calandra, R",,"Lengerich, Benjamin; Tan, Sarah; Chang, Chun-Hao; Hooker, Giles; Caruana, Rich",,,Purifying Interaction Effects with the Functional ANOVA: An Efficient Algorithm for Recovering Identifiable Additive Models,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Models which estimate main effects of individual variables alongside interaction effects have an identifiability challenge: effects can be freely moved between main effects and interaction effects without changing the model prediction. This is a critical problem for interpretability because it permits contradictory models to represent the same function. To solve this problem, we propose pure interaction effects: variance in the outcome which cannot be represented by any subset of features. This definition has an equivalence with the Functional ANOVA decomposition. To compute this decomposition, we present a fast, exact algorithm that transforms any piecewise-constant function (such as a tree-based model) into a purified, canonical representation. We apply this algorithm to Generalized Additive Models with interactions trained on several datasets and show large disparity, including contradictions, between the apparent and the purified effects. These results underscore the need to specify data distributions and ensure identifiability before interpreting model parameters.",,,,,,"Lengerich, Ben/0000-0001-8690-9554",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2402,2411,,,,,,,,,,,,,,,,WOS:000559931301100,0
C,"Pan, ZM; Wang, Z; Zhe, SDA",,"Chiappa, S; Calandra, R",,"Pan, Zhimeng; Wang, Zheng; Zhe, Shandian",,,Scalable Nonparametric Factorization for High-Order Interaction Events,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Interaction events among multiple entities are ubiquitous in real-world applications. Although these interactions can be naturally represented by tensors and analyzed by tensor decomposition, most existing approaches are limited to multilinear decomposition forms, and cannot estimate complex, nonlinear relationships in data. More importantly, the existing approaches severely underexploit the time stamps information. They either drop/discretize the time stamps or set a local window to ignore the long-term dependency between the events. To address these issues, we propose a Bayesian nonparametric factorization model for high-order interaction events, which can flexibly estimate/embed the static, nonlinear relationships and capture various long-term and short-term excitations effects, encoding these effects and their decaying patterns into the latent factors. Specifically, we use the latent factors to construct a set of mutually excited Hawkes processes, where we place a Gaussian process prior over the background rates to estimate the static, nonlinear relationships of the entities and propose novel triggering kernels to embed the excitation strengths and their time decaying rates among the interactions. For scalable inference, we derive a fully-decomposed model evidence lower bound to dispose of the huge covariance matrix and expensive log summation terms. Then we develop an efficient stochastic optimization algorithm. We show the advantage of our approach in four real-world applications.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,4325,4334,,,,,,,,,,,,,,,,WOS:000559931302063,0
C,"Poignard, B; Yamada, M",,"Chiappa, S; Calandra, R",,"Poignard, Benjamin; Yamada, Makoto",,,Sparse Hilbert-Schmidt Independence Criterion Regression,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Feature selection is a fundamental problem for machine learning and statistics, and it has been widely studied over the past decades. However, the majority of feature selection algorithms are based on linear models, and the nonlinear feature selection problem has not been well studied compared to linear models, in particular for the high-dimensional case. In this paper, we propose the sparse Hilbert-Schmidt Independence Criterion (SpHSIC) regression, which is a versatile nonlinear feature selection algorithm based on the HSIC and is a continuous optimization variant of the well-known minimum redundancy maximum relevance (mRMR) feature selection algorithm. More specifically, the SpHSIC consists of two parts: the convex HSIC loss function on the one hand and the regularization term on the other hand, where we consider the Lasso, Bridge, MCP, and SCAD penalties. We prove that the sparsity based HSIC regression estimator satisfies the oracle property; that is, the sparsity-based estimator recovers the true underlying sparse model and is asymptotically normally distributed. On the basis of synthetic and real-world experiments, we illustrate this theoretical property and highlight the fact that the proposed algorithm performs well in the high-dimensional setting.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,538,547,,,,,,,,,,,,,,,,WOS:000559931302076,0
C,"Qian, ZZ; Alaa, AM; Bellot, A; Rashbass, J; van der Schaar, M",,"Chiappa, S; Calandra, R",,"Qian, Zhaozhi; Alaa, Ahmed M.; Bellot, Alexis; Rashbass, Jem; van der Schaar, Mihaela",,,Learning Dynamic and Personalized Comorbidity Networks from Event Data using Deep Diffusion Processes,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Comorbid diseases co-occur and progress via complex temporal patterns that vary among individuals. In electronic health records we can observe the different diseases a patient has, but can only infer the temporal relationship between each co-morbid condition. Learning such temporal patterns from event data is crucial for understanding disease pathology and predicting prognoses. To this end, we develop deep diffusion processes (DDP) to model dynamic comorbidity networks, i.e., the temporal relationships between comorbid disease onsets expressed through a dynamic graph. A DDP comprises events modelled as a multi-dimensional point process, with an intensity function parameterized by the edges of a dynamic weighted graph. The graph structure is modulated by a neural network that maps patient history to edge weights, enabling rich temporal representations for disease trajectories. The DDP parameters decouple into clinically meaningful components, which enables serving the dual purpose of accurate risk prediction and intelligible representation of disease pathology. We illustrate these features in experiments using cancer registry data.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3295,3304,,,,,,,,,,,,,,,,WOS:000559931302082,0
C,"Su, X; Lee, WS; Zhang, Z",,"Chiappa, S; Calandra, R",,"Su, Xuan; Lee, Wee Sun; Zhang, Zhen",,,Multiplicative Gaussian Particle Filter,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We propose a new sampling-based approach for approximate inference in filtering problems. Instead of approximating conditional distributions with a finite set of states, as done in particle filters, our approach approximates the distribution with a weighted sum of functions from a set of continuous functions. Central to the approach is the use of sampling to approximate multiplications in the Bayes filter. We provide theoretical analysis, giving conditions for sampling to give good approximation. We next specialize to the case of weighted sums of Gaussians, and show how properties of Gaussians enable closed-form transition and efficient multiplication. Lastly, we conduct preliminary experiments on a robot localization problem and compare performance with the particle filter, to demonstrate the potential of the proposed method.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303033,0
C,"Tang, YH; Choromanski, K; Kucukelbir, A",,"Chiappa, S; Calandra, R",,"Tang, Yunhao; Choromanski, Krzysztof; Kucukelbir, Alp",,,Variance Reduction for Evolutionary Strategies via Structured Control Variates,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Evolution strategies (ES) are a powerful class of blackbox optimization techniques that recently became a competitive alternative to state-of-the-art policy gradient (PG) algorithms for reinforcement learning (RL). We propose a new method for improving accuracy of the ES algorithms, that as opposed to recent approaches utilizing only Monte Carlo structure of the gradient estimator, takes advantage of the underlying Markov decision process (MDP) structure to reduce the variance. We observe that the gradient estimator of the ES objective can be alternatively computed using reparametrization and PG estimators, which leads to new control variate techniques for gradient estimation in ES optimization. We provide theoretical insights and show through extensive experiments that this RL-specific variance reduction approach outperforms general purpose variance reduction methods.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303037,0
C,"Uehara, M; Kanamori, T; Takneouchi, T; Matsuda, T",,"Chiappa, S; Calandra, R",,"Uehara, Masatoshi; Kanamori, Takafumi; Takneouchi, Takashi; Matsuda, Takeru",,,A Unified Statistically Efficient Estimation Framework for Unnormalized Models,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"The parameter estimation of unnormalized models is a challenging problem. The maximum likelihood estimation (MLE) is computationally infeasible for these models since normalizing constants are not explicitly calculated. Although some consistent estimators have been proposed earlier, the problem of statistical efficiency remains. In this study, we propose a unified, statistically efficient estimation framework for unnormalized models and several efficient estimators, whose asymptotic variance is the same as the MLE. The computational cost of these estimators is also reasonable and they can be employed whether the sample space is discrete or continuous. The loss functions of the proposed estimators are derived by combining the following two methods: (1) density-ratio matching using Bregman divergence, and (2) plugging-in non-parametric estimators. We also analyze the properties of the proposed estimators when the unnormalized models are misspecified. The experimental results demonstrate the advantages of our method over existing approaches.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303046,0
C,"Wang, Y; Jang, B; Hero, A",,"Chiappa, S; Calandra, R",,"Wang, Yu; Jang, Byoungwook; Hero, Alfred",,,The Sylvester Graphical Lasso (SyGlasso),"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"This paper introduces the Sylvester graphical lasso (SyGlasso) that captures multiway dependencies present in tensor-valued data. The model is based on the Sylvester equation that defines a generative model. The proposed model complements the tensor graphical lasso (Greenewald et al., 2019) that imposes a Kronecker sum model for the inverse covariance matrix by providing an alternative Kronecker sum model that is generative and interpretable. A nodewise regression approach is adopted for estimating the conditional independence relationships among variables. The statistical convergence of the method is established, and empirical studies are provided to demonstrate the recovery of meaningful conditional dependency graphs. We apply the SyGlasso to an electroencephalography (EEG) study to compare the brain connectivity of alcoholic and nonalcoholic subjects. We demonstrate that our model can simultaneously estimate both the brain connectivity and its temporal dependencies.",,,,,,"Wang, Yu/0000-0002-6287-4710; Hero, Alfred/0000-0002-2531-9670",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303062,0
C,"Wu, PZ; Fukumizu, K",,"Chiappa, S; Calandra, R",,"Wu, Pengzhou (Abel); Fukumizu, Kenji",,,Causal Mosaic: Cause-Effect Inference via Nonlinear ICA and Ensemble Method,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We address the problem of distinguishing cause from effect in bivariate setting. Based on recent developments in nonlinear independent component analysis (ICA), we train general nonlinear causal models that are implemented by neural networks and allow nonadditive noise. Further, we build an ensemble framework, namely Causal Mosaic, which models a causal pair by a mixture of nonlinear models. We compare this method with other recent methods on artificial and real world benchmark datasets, and our method shows state-of-the-art performance.",,,,,,"Fukumizu, Kenji/0000-0002-3488-2625",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303082,0
C,"Zhou, DR; Gu, QQ",,"Chiappa, S; Calandra, R",,"Zhou, Dongruo; Gu, Quanquan",,,Stochastic Recursive Variance-Reduced Cubic Regularization Methods,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Stochastic Variance-Reduced Cubic regularization (SVRC) algorithms have received increasing attention due to its improved gradient/Hessian complexities (i.e., number of queries to stochastic gradient/Hessian oracles) to find local minima for nonconvex finite-sum optimization. However, it is unclear whether existing SVRC algorithms can be further improved. Moreover, the semi-stochastic Hessian estimator adopted in existing SVRC algorithms prevents the use of Hessian-vector product-based fast cubic subproblem solvers, which makes SVRC algorithms computationally intractable for high-dimensional problems. In this paper, we first present a Stochastic Recursive Variance-Reduced Cubic regularization method (SRVRC) using a recursively updated semi-stochastic gradient and Hessian estimators. It enjoys improved gradient and Hessian complexities to find an (epsilon, root epsilon)-approximate local minimum, and outperforms the state-of-the-art SVRC algorithms. Built upon SRVRC, we further propose a Hessian-free SRVRC algorithm, namely SRVRC free, which only needs (O) over tilde (n epsilon(-2) boolean AND epsilon(-3)) stochastic gradient and Hessian-vector product computations, where n is the number of component functions in the finite-sum objective and epsilon is the optimization precision. This outperforms the best-known result (O) over tilde(epsilon(-3.5)) achieved by stochastic cubic regularization algorithm proposed in Tripuraileili et al. (2018).",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3980,3989,,,,,,,,,,,,,,,,WOS:000559931304022,0
C,"Ali, A; Kolter, JZ; Tibshirani, RJ",,"Chaudhuri, K; Sugiyama, M",,"Ali, Alnur; Kolter, J. Zico; Tibshirani, Ryan J.",,,A Continuous-Time View of Early Stopping for Least Squares,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We study the statistical properties of the iterates generated by gradient descent, applied to the fundamental problem of least squares regression. We take a continuous-time view, i.e., consider infinitesimal step sizes in gradient descent, in which case the iterates form a trajectory called gradient flow. Our primary focus is to compare the risk of gradient flow to that of ridge regression. Under the calibration t = 1/lambda-where t is the time parameter in gradient flow, and lambda the tuning parameter in ridge regression-we prove that the risk of gradient flow is no more than 1.69 times that of ridge, along the entire path (for all t >= 0). This holds in finite samples with very weak assumptions on the data model (in particular, with no assumptions on the features X). We prove that the same relative risk bound holds for prediction risk, in an average sense over the underlying signal beta(0). Finally, we examine limiting risk expressions (under standard Marchenko-Pastur asymptotics), and give supporting numerical experiments.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901043,0
C,"Belkin, M; Rakhlin, A; Tsybakov, AB",,"Chaudhuri, K; Sugiyama, M",,"Belkin, Mikhail; Rakhlin, Alexander; Tsybakov, Alexandre B.",,,Does data interpolation contradict statistical optimality?,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,We show that classical learning methods interpolating the training data can achieve optimal rates for the problems of nonparametric regression and prediction with square loss.,,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901068,0
C,"Fontaine, X; Berthet, Q; Perchet, V",,"Chaudhuri, K; Sugiyama, M",,"Fontaine, Xavier; Berthet, Quentin; Perchet, Vianney",,,Regularized Contextual Bandits,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We consider the stochastic contextual bandit problem with additional regularization. The motivation comes from problems where the policy of the agent must be close to some baseline policy known to perform well on the task. To tackle this problem we use a nonparametric model and propose an algorithm splitting the context space into bins, solving simultaneously and independently regularized multi-armed bandit instances on each bin. We derive slow and fast rates of convergence, depending on the unknown complexity of the problem. We also consider a new relevant margin condition to get problem-independent convergence rates, yielding intermediate rates interpolating between the aforementioned slow and fast rates.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902020,0
C,"Letarte, G; Morvant, E; Germain, P",,"Chaudhuri, K; Sugiyama, M",,"Letarte, Gael; Morvant, Emilie; Germain, Pascal",,,Pseudo-Bayesian Learning with Kernel Fourier Transform as Prior,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We revisit Rahimi and Recht (2007)'s kernel random Fourier features (RFF) method through the lens of the PAC-Bayesian theory. While the primary goal of RFF is to approximate a kernel, we look at the Fourier transform as a prior distribution over trigonometric hypotheses. It naturally suggests learning a posterior on these hypotheses. We derive generalization bounds that are optimized by learning a pseudo-posterior obtained from a closed-form expression. Based on this study, we consider two learning strategies: The first one finds a compact landmarks-based representation of the data where each landmark is given by a distribution-tailored similarity measure, while the second one provides a PAC-Bayesian justification to the kernel alignment method of Sinha and Duchi (2016).",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,768,776,,,,,,,,,,,,,,,,WOS:000509687900080,0
C,"Miyaguchi, K; Yamanishi, K",,"Chaudhuri, K; Sugiyama, M",,"Miyaguchi, Kohei; Yamanishi, Kenji",,,Adaptive Minimax Regret against Smooth Logarithmic Losses over High-Dimensional 11-Balls via Envelope Complexity,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We develop a new theoretical framework, the envelope complexity, to analyze the minimax regret with logarithmic loss functions. Within the framework, we derive a Bayesian predictor that adaptively achieves the minimax regret over high-dimensional I-1-balls within a factor of two. The prior is newly derived for achieving the minimax regret and called the spike-and-tails (ST) prior as it looks like. The resulting regret bound is so simple that it is completely determined with the smoothness of the loss function and the radius of the balls except with logarithmic factors, and it has a generalized form of existing regret/risk bounds.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687903051,0
C,"Nacson, MS; Srebro, N; Soudry, D",,"Chaudhuri, K; Sugiyama, M",,"Nacson, Mor Shpigel; Srebro, Nathan; Soudry, Daniel",,,Stochastic Gradient Descent on Separable Data: Exact Convergence with a Fixed Learning Rate,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Stochastic Gradient Descent (SGD) is a central tool in machine learning. We prove that SGD converges to zero loss, even with a fixed (non vanishing) learning rate in the special case of homogeneous linear classifiers with smooth monotone loss functions, optimized on linearly separable data. Previous works assumed either a vanishing learning rate, iterate averaging, or loss assumptions that do not hold for monotone loss functions used for classification, such as the logistic loss. We prove our result on a fixed dataset, both for sampling with or without replacement. Furthermore, for logistic loss (and similar exponentially-tailed losses), we prove that with SGD the weight vector converges in direction to the L-2 max margin vector as O(1/ log(t)) for almost all separable datasets, and the loss converges as O(1/t) - similarly to gradient descent. Lastly, we examine the case of a fixed learning rate proportional to the minibatch size. We prove that in this case, the asymptotic convergence rate of SGD (with replacement) does not depend on the minibatch size in terms of epochs, if the support vectors span the data. These results may suggest an explanation to similar behaviors observed in deep networks, when trained with SGD.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687903011,0
C,"Wang, Z; Zhou, Y; Liang, YB; Lan, GH",,"Chaudhuri, K; Sugiyama, M",,"Wang, Zhe; Zhou, Yi; Liang, Yingbin; Lan, Guanghui",,,Stochastic Variance-Reduced Cubic Regularization for Nonconvex Optimization,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Cubic regularization (CR) is an optimization method with emerging popularity due to its capability to escape saddle points and converge to second-order stationary solutions for nonconvex optimization. However, CR encounters a high sample complexity issue for finite-sum problems with a large data size. In this paper, we propose a stochastic variance-reduced cubic-regularization (SVRC) method under random sampling, and study its convergence guarantee as well as sample complexity. We show that the iteration complexity of SVRC for achieving a second-order stationary solution within accuracy is O(epsilon(-3/2)), which matches the state-of-art result on CR types of methods. Moreover, our proposed variance reduction scheme significantly reduces the periteration sample complexity. The resulting total Hessian sample complexity of our SVRC is (O) over tilde (N-2/3 epsilon(-3/2)), which outperforms the state-of-art result by a factor of (O) over tilde (N-2/15). We also study our SVRC under random sampling without replacement scheme, which yields a lower per-iteration sample complexity, and hence justifies its practical applicability.",,,,,", /GYJ-1551-2022","Liang, Yingbin/0000-0002-8635-2992",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902080,0
C,"Chen, S; Banerjee, A",,"Storkey, A; PerezCruz, F",,"Chen, Sheng; Banerjee, Arindam",,,Sparse Linear Isotonic Models,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"In machine learning and data mining, linear models have been widely used to model the response as parametric linear functions of the predictors. To relax such stringent assumptions made by parametric linear models, additive models consider the response to be a summation of unknown transformations applied on the predictors; in particular, additive isotonic models (AIMs) assume the unknown transformations to be monotone. In this paper, we introduce sparse linear isotonic models (SLIMs) for high-dimensional problems by hybridizing ideas in parametric sparse linear models and AIMs, which enjoy a few appealing advantages over both. In the high-dimensional setting, a two-step algorithm is proposed for estimating the sparse parameters as well as the monotone functions over predictors. Under mild statistical assumptions, we show that the algorithm can accurately estimate the parameters. Promising preliminary experiments are presented to support the theoretical results.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300133,0
C,"Jiang, B; Wu, TY; Wong, WH",,"Storkey, A; PerezCruz, F",,"Jiang, Bai; Wu, Tung-Yu; Wong, Wing Hung",,,Approximate Bayesian Computation with Kullback-Leibler Divergence as Data Discrepancy,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Complex simulator-based models usually have intractable likelihood functions, rendering the likelihood-based inference methods inapplicable. Approximate Bayesian Computation (ABC) emerges as an alternative framework of likelihood-free inference methods. It identifies a quasi-posterior distribution by finding values of parameter that simulate the synthetic data resembling the observed data. A major ingredient of ABC is the discrepancy measure between the observed and the simulated data, which conventionally involves a fundamental difficulty of constructing effective summary statistics. To bypass this difficulty, we adopt a Kullback-Leibler divergence estimator to assess the data discrepancy. Our method enjoys the asymptotic consistency and linearithmic time complexity as the data size increases. In experiments on five benchmark models, this method achieves a comparable or higher quasi-posterior quality, compared to the existing methods using other discrepancy measures.",,,,,"Wu, Tung-Yu/CAH-2223-2022",,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300179,0
C,"Kundu, A; Bach, F; Bhattacharyya, C",,"Storkey, A; PerezCruz, F",,"Kundu, Achintya; Bach, Francis; Bhattacharyya, Chiranjib",,,Convex Optimization over Intersection of Simple Sets: improved Convergence Rate Guarantees via an Exact Penalty Approach,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"We consider the problem of minimizing a convex function over the intersection of finitely many simple sets which are easy to project onto. This is an important problem arising in various domains such as machine learning. The main difficulty lies in finding the projection of a point in the intersection of many sets. Existing approaches yield an infeasible point with an iteration-complexity of O(1/epsilon(2)) for nonsmooth problems with no guarantees on the in-feasibility. By reformulating the problem through exact penalty functions, we derive first-order algorithms which not only guarantees that the distance to the intersection is small but also improve the complexity to O(1/epsilon) and O(1/root epsilon) for smooth functions. For composite and smooth problems, this is achieved through a saddle-point reformulation where the proximal operators required by the primal-dual algorithms can be computed in closed form. We illustrate the benefits of our approach on a graph transduction problem and on graph matching.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300101,0
C,"Lakshminarayanan, C; Szepesvari, C",,"Storkey, A; PerezCruz, F",,"Lakshminarayanan, Chandrashekar; Szepesvari, Csaba",,,Linear Stochastic Approximation: How Far Does Constant Step-Size and Iterate Averaging Go?,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"In this paper we study study constant step-size averaged linear stochastic approximation. With an eye towards linear value estimation in reinforcement learning, we ask whether for a given class of linear estimation problems i) a single universal constant step-size with ii) a C/t worst-case expected error with a class-dependent constant C > 0 can be guaranteed when the error is measured via an appropriate weighted squared norm. Such a result has recently been obtained in the context of linear least squares regression. We give examples that show that the answer to these questions in general is no. On the positive side, we also characterize the instance dependent behavior of the error of the said algorithms, identify some conditions under which the answer to the above questions can be changed to the positive, and in particular show instance-dependent error bounds of magnitude O(1/t) for the constant step-size iterate averaged versions of TD(0) and a novel variant of GTD, where the stepsize is chosen independently of the value estimation instance. Computer simulations are used to illustrate and complement the theory.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300141,0
C,"Shen, YN; Chen, TY; Giannakis, GB",,"Storkey, A; PerezCruz, F",,"Shen, Yanning; Chen, Tianyi; Giannakis, Georgios B.",,,Online Ensemble Multi-kernel Learning Adaptive to Non-stationary and Adversarial Environments,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Kernel-based methods exhibit well-documented performance in various nonlinear learning tasks. Most of them rely on a preselected kernel, whose prudent choice presumes task-specific prior information. To cope with this limitation, multi-kernel learning has gained popularity thanks to its flexibility in choosing kernels from a prescribed kernel dictionary. Leveraging the random feature approximation and its recent orthogonality-promoting variant, the present contribution develops an online multi-kernel learning scheme to infer the intended nonlinear function 'on the fly.' To further boost performance in non-stationary environments, an adaptive multi-kernel learning scheme (termed AdaRaker) is developed with affordable computation and memory complexity. Performance is analyzed in terms of both static and dynamic regret. To our best knowledge, AdaRaker is the first algorithm that can optimally track nonlinear functions in non-stationary settings with theoretical guarantees. Numerical tests on real datasets are carried out to showcase the effectiveness of the proposed algorithms.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300213,0
C,"Suzuki, T",,"Storkey, A; PerezCruz, F",,"Suzuki, Taiji",,,Fast generalization error bound of deep learning from a kernel perspective,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"We develop a new theoretical framework to analyze the generalization error of deep learning, and derive a new fast learning rate for two representative algorithms: empirical risk minimization and Bayesian deep learning. The series of theoretical analyses of deep learning has revealed its high expressive power and universal approximation capability. Our point of view is to deal with the ordinary finite dimensional deep neural network as a finite approximation of the infinite dimensional one. Our formulation of the infinite dimensional model naturally defines a reproducing kernel Hilbert space corresponding to each layer. The approximation error is evaluated by the degree of freedom of the reproducing kernel Hilbert space in each layer. We derive the generalization error bound of both of empirical risk minimization and Bayesian deep learning and it is shown that there appears bias-variance trade-off in terms of the number of parameters of the finite dimensional approximation. We show that the optimal width of the internal layers can be determined through the degree of freedom and derive the optimal convergence rate that is faster than O(1/root n) rate which has been shown in the existing studies.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300146,0
C,"Wang, YN; Du, SS; Balakrishnan, S; Singh, A",,"Storkey, A; PerezCruz, F",,"Wang, Yining; Du, Simon S.; Balakrishnan, Sivaraman; Singh, Aarti",,,Stochastic Zeroth-order Optimization in High Dimensions,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"We consider the problem of optimizing a high-dimensional convex function using stochastic zeroth-order queries. Under sparsity assumptions on the gradients or function values, we present two algorithms: a successive component/feature selection algorithm and a noisy mirror descent algorithm using Lasso gradient estimates, and show that both algorithms have convergence rates that depend only logarithmically on the ambient dimension of the problem. Empirical results confirm our theoretical findings and show that the algorithms we design outperform classical zeroth-order optimization methods in the high-dimensional setting.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300142,0
C,"Gonzalez, J; Osborne, M; Lawrence, ND",,"Gretton, A; Robert, CC",,"Gonzalez, Javier; Osborne, Michael; Lawrence, Neil D.",,,GLASSES: Relieving The Myopia Of Bayesian Optimisation,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"We present glasses: Global optimisation with Look-Ahead through Stochastic Simulation and Expected-loss Search. The majority of global optimisation approaches in use are myopic, in only considering the impact of the next function value; the non-myopic approaches that do exist are able to consider only a handful of future evaluations. Our novel algorithm, glasses, permits the consideration of dozens of evaluations into the future. This is done by approximating the ideal look-ahead loss function, which is expensive to evaluate, by a cheaper alternative in which the future steps of the algorithm are simulated beforehand. An Expectation Propagation algorithm is used to compute the expected value of the loss. We show that the far-horizon planning thus enabled leads to substantive performance gains in empirical tests.",,,,,,"Osborne, Michael/0000-0003-1959-012X",,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,790,799,,10.1016/j.jpedsurg.2016.02.024,0,,,,,,,,,,,,,WOS:000508662100086,0
C,"Herlands, W; Wilson, A; Nickisch, H; Flaxman, S; Neill, D; van Panhuis, W; Xing, E",,"Gretton, A; Robert, CC",,"Herlands, William; Wilson, Andrew; Nickisch, Hannes; Flaxman, Seth; Neill, Daniel; van Panhuis, Wilbert; Xing, Eric",,,Scalable Gaussian Processes for Characterizing Multidimensional Change Surfaces,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"We present a scalable Gaussian process model for identifying and characterizing smooth multidimensional changepoints, and automatically learning changes in expressive covariance structure. We use Random Kitchen Sink features to flexibly define a change surface in combination with expressive spectral mixture kernels to capture the complex statistical structure. Finally, through the use of novel methods for additive non-separable kernels, we can scale the model to large datasets. We demonstrate the model on numerical and real world data, including a large spatio-temporal disease dataset where we identify previously unknown heterogeneous changes in space and time.",,,,,"Nickisch, Hannes/I-7049-2017","Nickisch, Hannes/0000-0003-1604-6647",,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,1013,1021,,,,,,,,,,,,,,,,WOS:000508662100110,0
C,"Hu, CW; Rai, P; Carin, L",,"Gretton, A; Robert, CC",,"Hu, Changwei; Rai, Piyush; Carin, Lawrence",,,Topic-Based Embeddings for Learning from Large Knowledge Graphs,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"We present a scalable probabilistic framework for learning from multi-relational data, given in form of entity-relation-entity triplets, with a potentially massive number of entities and relations (e.g., in multi-relational networks, knowledge bases, etc.). We define each triplet via a relation-specific bilinear function of the embeddings of entities associated with it (these embeddings correspond to topics). To handle massive number of relations and the data sparsity problem (very few observations per relation), we also extend this model to allow sharing of parameters across relations, which leads to a substantial reduction in the number of parameters to be learned. In addition to yielding excellent predictive performance (e.g., for knowledge base completion tasks), the interpretability of our topic-based embedding framework enables easy qualitative analyses. Computational cost of our models scales in the number of positive triplets, which makes it easy to scale to massive real-world multi-relational data sets, which are usually extremely sparse. We develop simple-to-implement batch as well as online Gibbs sampling algorithms and demonstrate the effectiveness of our models on tasks such as multi-relational link-prediction, and learning from large knowledge bases.",,,,,,"Carin, Lawrence/0000-0001-6277-7948",,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,1133,1141,,,,,,,,,,,,,,,,WOS:000508662100123,0
C,"Li, Y; Liu, H; Powell, WB",,"Gretton, A; Robert, CC",,"Li, Yan; Liu, Han; Powell, Warren B.",,,A Lasso-based Sparse Knowledge Gradient Policy for Sequential Optimal Learning,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"We propose a sequential learning policy for noisy discrete global optimization and ranking and selection (R&S) problems with high dimensional sparse belief functions, where there are hundreds or even thousands of features, but only a small portion of these features contain explanatory power. Our problem setting, motivated by the experimental sciences, arises where we have to choose which experiment to run next. Here the experiments are time-consuming and expensive. We derive a sparse knowledge gradient (SpKG) decision policy based on the l(1)-penalized regression Lasso to identify the sparsity pattern before our budget is exhausted. This policy is a unique and novel hybrid of Bayesian R&S with a frequentist learning approach. Theoretically, we provide the error bound of the posterior mean estimate, which has shown to be at the minimax optimal root s log p/n rate. Controlled experiments on both synthetic data and real application for automatically designing experiments to identify the structure of an RNA molecule show that the algorithm efficiently learns the correct set of nonzero parameters. It also outperforms several other learning policies.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,417,425,,,,,,,,,,,,,,,,WOS:000508662100046,0
C,"Mohri, M; Yang, S",,"Gretton, A; Robert, CC",,"Mohri, Mehryar; Yang, Scott",,,Accelerating Online Convex Optimization via Adaptive Prediction,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"We present a powerful general framework for designing data-dependent online convex optimization algorithms, building upon and unifying recent techniques in adaptive regularization, optimistic gradient predictions, and problem-dependent randomization. We first present a series of new regret guarantees that hold at any time and under very minimal assumptions, and then show how different relaxations recover existing algorithms, both basic as well as more recent sophisticated ones. Finally, we show how combining adaptivity, optimism, and problem-dependent randomization can guide the design of algorithms that benefit from more favorable guarantees than recent state-of-the-art methods.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,848,856,,,,,,,,,,,,,,,,WOS:000508662100092,0
C,"Perolat, J; Piot, B; Scherrer, B; Pietquin, O",,"Gretton, A; Robert, CC",,"Perolat, Julien; Piot, Bilal; Scherrer, Bruno; Pietquin, Olivier",,,On the Use of Non-Stationary Strategies for Solving Two-Player Zero-Sum Markov Games,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"The main contribution of this paper consists in extending several non-stationary Reinforcement Learning (RL) algorithms and their theoretical guarantees to the case of gamma-discounted zero-sum Markov Games (MGs). As in the case of Markov Decision Processes (MDPs), non-stationary algorithms are shown to exhibit better performance bounds compared to their stationary counterparts. The obtained bounds are generically composed of three terms: 1) a dependency over. (discount factor), 2) a concentrability coefficient and 3) a propagation error term. This error, depending on the algorithm, can be caused by a regression step, a policy evaluation step or a best-response evaluation step. As a second contribution, we empirically demonstrate, on generic MGs (called Garnets), that non-stationary algorithms outperform their stationary counterparts. In addition, it is shown that their performance mostly depends on the nature of the propagation error. Indeed, algorithms where the error is due to the evaluation of a best-response are penalized (even if they exhibit better concentrability coefficients and dependencies on.) compared to those suffering from a regression error.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,893,901,,,,,,,,,,,,,,,,WOS:000508662100097,0
C,"Zhan, JC; Lois, B; Guo, H; Vaswani, N",,"Gretton, A; Robert, CC",,"Zhan, Jinchun; Lois, Brian; Guo, Han; Vaswani, Namrata",,,Online (and Offline) Robust PCA: Novel Algorithms and Performance Guarantees,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"In this work we develop and study a novel online robust principal components' analysis (RPCA) algorithm based on the recently introduced ReProCS framework. Our algorithm significantly improves upon the original ReProCS algorithm and it also returns even more accurate offline estimates. The key contribution of this work is a correctness result for this algorithm under relatively mild assumptions. By using extra (but usually valid) assumptions we are able to remove one important limitation of batch RPCA results and two important limitations of a recent result for ReProCS for online RPCA. To the best of our knowledge, this work is among the first correctness results for online RPCA.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,1488,1496,,,,,,,,,,,,,,,,WOS:000508662100161,0
C,"Carreira-Perpinan, MA; Wang, Wr",,"Kaski, S; Corander, J",,"Carreira-Perpinan, Miguel A.; Wang, Weiran",,,Distributed Optimization of Deeply Nested Systems,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"Intelligent processing of complex signals such as images is often performed by a hierarchy of non-linear processing layers, such as a deep net or an object recognition cascade. Joint estimation of the parameters of all the layers is a difficult nonconvex optimization. We describe a general strategy to learn the parameters and, to some extent, the architecture of nested systems, which we call the method of auxiliary coordinates (MAC). This replaces the original problem involving a deeply nested function with a constrained problem involving a different function in an augmented space without nesting. The constrained problem may be solved with penalty-based methods using alternating optimization over the parameters and the auxiliary coordinates. MAC has provable convergence, is easy to implement reusing existing algorithms for single layers, can be parallelized trivially and massively, applies even when parameter derivatives are not available or not desirable, can perform some model selection on the fly, and is competitive with state-of-the-art nonlinear optimizers even in the serial computation setting, often providing reasonable models within a few iterations.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,10,19,,,,,,,,,,,,,,,,WOS:000508355800002,0
C,"Hayashi, T; Kuroki, M",,"Kaski, S; Corander, J",,"Hayashi, Takahiro; Kuroki, Manabu",,,On Estimating Causal Effects based on Supplemental Variables,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"This paper considers the problem of estimating causal effects of a treatment on a response using supplementary variables. Under the assumption that a treatment is associated with a response through a univariate supplementary variable in the framework of linear regression models, Cox (1960) showed that the estimation accuracy of the regression coefficient of the treatment on the response in the single linear regression model can be improved by using the recursive linear regression model based on the supplementary variable from the viewpoint of the asymptotic variance. However, such assumptions may not hold in many practical situations. In this paper, we consider the situation where a treatment is associated with a response through a set of supplementary variables in both linear and discrete models. Then, we show that the estimation accuracy of the causal effect can be improved by using the supplementary variables. Different from Cox (1960), the results of this paper are derived without the assumption of Gaussian error terms in linear models or dichotomous variables in discrete models. The results of this paper help us to obtain the reliable evaluation of causal effects from observed data.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,312,319,,,,,,,,,,,,,,,,WOS:000508355800035,0
C,"Wang, SS; Zhang, ZH",,"Kaski, S; Corander, J",,"Wang, Shusen; Zhang, Zhihua",,,Efficient Algorithms and Error Analysis for the Modified Nystrom Method,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"Many kernel methods suffer from high time and space complexities and are thus prohibitive in big-data applications. To tackle the computational challenge, the Nystrom method has been extensively used to reduce time and space complexities by sacrificing some accuracy. The Nystrom method speedups computation by constructing an approximation of the kernel matrix using only a few columns of the matrix. Recently, a variant of the Nystrom method called the modified Nystrom method has demonstrated significant improvement over the standard Nystrom method in approximation accuracy, both theoretically and empirically. In this paper, we propose two algorithms that make the modified Nystrom method practical. First, we devise a simple column selection algorithm with a provable error bound. Our algorithm is more efficient and easier to implement than and nearly as accurate as the state-of-the-art algorithm. Second, with the selected columns at hand, we propose an algorithm that computes the approximation in lower time complexity than the approach in the previous work. Furthermore, we prove that the modified Nystrom method is exact under certain conditions, and we establish a lower error bound for the modified Nystrom method.",,,,,,"Wang, Shusen/0000-0003-3928-6782",,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,996,1004,,,,,,,,,,,,,,,,WOS:000508355800110,0
C,"Greenewald, K; Katz-Rogozhnikov, D; Shanmugam, K",,"Banerjee, A; Fukumizu, K",,"Greenewald, Kristjan; Katz-Rogozhnikov, Dmitriy; Shanmugam, Karthik",,,High-Dimensional Feature Selection for Sample Efficient Treatment Effect Estimation,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"The estimation of causal treatment effects from observational data is a fundamental problem in causal inference. To avoid bias, the effect estimator must control for all confounders. Hence practitioners often collect data for as many covariates as possible to raise the chances of including the relevant confounders. While this addresses the bias, this has the side effect of significantly increasing the number of data samples required to accurately estimate the effect due to the increased dimensionality. In this work, we consider the setting where out of a large number of covariates X that satisfy strong ignorability, an unknown sparse subset S is sufficient to include to achieve zero bias, i.e. c-equivalent to X. We propose a common objective function involving outcomes across treatment cohorts with nonconvex joint sparsity regularization that is guaranteed to recover S with high probability under a linear outcome model for Y and subgaussian covariates for each of the treatment cohort. This improves the effect estimation sample complexity so that it scales with the cardinality of the sparse subset S and log vertical bar X vertical bar, as opposed to the cardinality of the full set X. We validate our approach with experiments on treatment effect estimation.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802067,0
C,"Guille-Escuret, C; Goujaud, B; Girotti, M; Mitliagkas, I",,"Banerjee, A; Fukumizu, K",,"Guille-Escuret, Charles; Goujaud, Baptiste; Girotti, Manuela; Mitliagkas, Ioannis",,,A Study of Condition Numbers for First-Order Optimization,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"The study of first-order optimization algorithms (FOA) typically starts with assumptions on the objective functions, most commonly smoothness and strong convexity. These metrics are used to tune the hyperparameters of FOA. We introduce a class of perturbations quantified via a new norm, called *-norm. We show that adding a small perturbation to the objective function has an equivalently small impact on the behavior of any FOA, which suggests that it should have a minor impact on the tuning of the algorithm. However, we show that smoothness and strong convexity can be heavily impacted by arbitrarily small perturbations, leading to excessively conservative tunings and convergence issues. In view of these observations, we propose a notion of continuity of the metrics, which is essential for a robust tuning strategy. Since smoothness and strong convexity are not continuous, we propose a comprehensive study of existing alternative metrics which we prove to be continuous. We describe their mutual relations and provide their guaranteed convergence rates for the Gradient Descent algorithm accordingly tuned. Finally we discuss how our work impacts the theoretical understanding of FOA and their performances.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801054,0
C,"Guo, FR; Perkovic, E",,"Banerjee, A; Fukumizu, K",,"Guo, F. Richard; Perkovic, Emilija",,,Minimal Enumeration of All Possible Total Effects in a Markov Equivalence Class,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"In observational studies, when a total causal effect of interest is not identified, the set of all possible effects can be reported instead. This typically occurs when the underlying causal DAG is only known up to a Markov equivalence class, or a refinement thereof due to background knowledge. As such, the class of possible causal DAGs is represented by a maximally oriented partially directed acyclic graph (MPDAG), which contains both directed and undirected edges. We characterize the minimal additional edge orientations required to identify a given total effect. A recursive algorithm is then developed to enumerate subclasses of DAGs, such that the total effect in each subclass is identified as a distinct functional of the observed distribution. This resolves an issue with existing methods, which often report possible total effects with duplicates, namely those that are numerically distinct due to sampling variability but are in fact causally identical.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802086,0
C,"Hao, BT; Lattimore, T; Szepesvari, C; Wang, MD",,"Banerjee, A; Fukumizu, K",,"Hao, Botao; Lattimore, Tor; Szepesvari, Csaba; Wang, Mengdi",,,Online Sparse Reinforcement Learning,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We investigate the hardness of online reinforcement learning in fixed horizon, sparse linear Markov decision process (MDP), with a special focus on the high-dimensional regime where the ambient dimension is larger than the number of episodes. Our contribution is two-fold. First, we provide a lower bound showing that linear regret is generally unavoidable in this case, even if there exists a policy that collects well-conditioned data. The lower bound construction uses an MDP with a fixed number of states while the number of actions scales with the ambient dimension. Note that when the horizon is fixed to one, the case of linear stochastic bandits, the linear regret can be avoided. Second, we show that if the learner has oracle access to a policy that collects well-conditioned data then a variant of Lasso fitted Q-iteration enjoys a nearly dimension free regret of (O) over tilde (s(2/3)N(2/3)) where N is the number of episodes and s is the sparsity level. This shows that in the large-action setting, the difficulty of learning can be attributed to the difficulty of finding a good exploratory policy.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,316,+,,,,,,,,,,,,,,,,WOS:000659893800036,0
C,"Khan, Q; Wenzel, P; Cremers, D",,"Banerjee, A; Fukumizu, K",,"Khan, Qadeer; Wenzel, Patrick; Cremers, Daniel",,,Self-Supervised Steering Angle Prediction for Vehicle Control Using Visual Odometry,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Vision-based learning methods for self-driving cars have primarily used supervised approaches that require a large number of labels for training. However, those labels are usually difficult and expensive to obtain. In this paper, we demonstrate how a model can be trained to control a vehicle's trajectory using camera poses estimated through visual odometry methods in an entirely self-supervised fashion. We propose a scalable framework that leverages trajectory information from several different runs using a camera setup placed at the front of a car. Experimental results on the CARLA simulator demonstrate that our proposed approach performs at par with the model trained with supervision.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804055,0
C,"Le Priol, R; Harikandeh, RB; Bengio, Y; Lacoste-Julien, S",,"Banerjee, A; Fukumizu, K",,"Le Priol, Remi; Harikandeh, Reza Babanezhad; Bengio, Yoshua; Lacoste-Julien, Simon",,,An Analysis of the Adaptation Speed of Causal Models,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Consider a collection of datasets generated by unknown interventions on an unknown structural causal model G. Recently, Bengio et al. (2020) conjectured that among all candidate models, G is the fastest to adapt from one dataset to another, along with promising experiments. Indeed, intuitively G has less mechanisms to adapt, but this justification is incomplete. Our contribution is a more thorough analysis of this hypothesis. We investigate the adaptation speed of cause-effect SCMs. Using convergence rates from stochastic optimization, we justify that a relevant proxy for adaptation speed is distance in parameter space after intervention. Applying this proxy to categorical and normal causeeffect models, we show two results. When the intervention is on the cause variable, the SCM with the correct causal direction is advantaged by a large factor. When the intervention is on the effect variable, we characterize the relative adaptation speed. Surprisingly, we find situations where the anticausal model is advantaged, falsifying the initial hypothesis.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,775,+,,,,,,,,,,,,,,,,WOS:000659893800087,0
C,"Nguyen, NV; Nguyen, TN; Ha Nguyen, P; Quoc, TD; Nguyen, LM; van Dijk, M",,"Banerjee, A; Fukumizu, K",,"Nguyen, Nhuong, V; Nguyen, Toan N.; Phuong Ha Nguyen; Quoc Tran-Dinh; Nguyen, Lam M.; van Dijk, Marten",,,Hogwild! over Distributed Local Data Sets with Linearly Increasing Mini-Batch Sizes,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Hogwild! implements asynchronous Stochastic Gradient Descent (SGD) where multiple threads in parallel access a common repository containing training data, perform SGD iterations, and update shared state that represents a jointly learned (global) model. We consider big data analysis where training data is distributed among local data sets in a heterogeneous way - and we wish to move SGD computations to local compute nodes where local data resides. The results of these local SGD computations are aggregated by a central aggregator which mimics Hogwild!. We show how local compute nodes can start choosing small mini-batch sizes which increase to larger ones in order to reduce communication cost (round interaction with the aggregator). We improve state-of-the-art literature and show O(root K) communication rounds for heterogeneous data for strongly convex problems, where K is the total number of gradient computations across all local compute nodes. For our scheme, we prove a tight and novel non-trivial convergence analysis for strongly convex problems for heterogeneous data which does not use the bounded gradient assumption as seen in many existing publications. The tightness is a consequence of our proofs for lower and upper bounds of the convergence rate, which show a constant factor difference. We show experimental results for plain convex and non-convex problems for biased (i.e., heterogeneous) and unbiased local data sets.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801048,0
C,"Tangkaratt, V; Charoenphakdee, N; Sugiyama, M",,"Banerjee, A; Fukumizu, K",,"Tangkaratt, Voot; Charoenphakdee, Nontawat; Sugiyama, Masashi",,,Robust Imitation Learning from Noisy Demonstrations,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Robust learning from noisy demonstrations is a practical but highly challenging problem in imitation learning. In this paper, we first theoretically show that robust imitation learning can be achieved by optimizing a classification risk with a symmetric loss. Based on this theoretical finding, we then propose a new imitation learning method that optimizes the classification risk by effectively combining pseudo-labeling with co-training. Unlike existing methods, our method does not require additional labels or strict assumptions about noise distributions. Experimental results on continuous-control benchmarks show that our method is more robust compared to state-of-the-art methods.",,,,,"Sugiyama, Masashi/AEO-1176-2022","Sugiyama, Masashi/0000-0001-6658-6743",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,298,+,,,,,,,,,,,,,,,,WOS:000659893800034,0
C,"Vogel, R; Bellet, A; Clemencon, S",,"Banerjee, A; Fukumizu, K",,"Vogel, Robin; Bellet, Aurelien; Clemencon, Stephan",,,Learning Fair Scoring Functions: Bipartite Ranking under ROC-based Fairness Constraints,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Many applications of AI involve scoring individuals using a learned function of their attributes. These predictive risk scores are then used to take decisions based on whether the score exceeds a certain threshold, which may vary depending on the context. The level of delegation granted to such systems in critical applications like credit lending and medical diagnosis will heavily depend on how questions of fairness can be answered. In this paper, we study fairness for the problem of learning scoring functions from binary labeled data, a classic learning task known as bipartite ranking. We argue that the functional nature of the ROC curve, the gold standard measure of ranking accuracy in this context, leads to several ways of formulating fairness constraints. We introduce general families of fairness definitions based on the AUC and on ROC curves, and show that our ROC-based constraints can be instantiated such that classifiers obtained by thresholding the scoring function satisfy classification fairness for a desired range of thresholds. We establish generalization bounds for scoring functions learned under such constraints, design practical learning algorithms and show the relevance our approach with numerical experiments on real and synthetic data.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801001,0
C,"Wang, HY; Zou, JH",,"Banerjee, A; Fukumizu, K",,"Wang, HaiYing; Zou, Jiahui",,,A comparative study on sampling with replacement vs Poisson sampling in optimal subsampling,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Faced with massive data, subsampling is a commonly used technique to improve computational efficiency, and using nonuniform subsampling probabilities is an effective approach to improve estimation efficiency. For computational efficiency, subsampling is often implemented with replacement or through Poisson subsampling. However, no rigorous investigation has been performed to study the difference between the two subsampling procedures such as their estimation efficiency and computational convenience. In the context of maximizing a general target function, this paper derives optimal subsampling probabilities for both subsampling with replacement and Poisson subsampling. The optimal subsampling probabilities minimize variance functions of the subsampling estimators. Furthermore, they provide deep insights on the theoretical similarities and differences between subsampling with replacement and Poisson subsampling. Practically implementable algorithms are proposed based on the optimal structural results, which are evaluated by both theoretical and empirical analysis.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,289,+,,,,,,,,,,,,,,,,WOS:000659893800033,0
C,"Wei, JH; Fu, ZY; Liu, Y; Li, XY; Yang, ZR; Wang, ZR",,"Banerjee, A; Fukumizu, K",,"Wei, Jiaheng; Fu, Zuyue; Liu, Yang; Li, Xingyu; Yang, Zhuoran; Wang, Zhaoran",,,Sample Elicitation,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"It is important to collect credible training samples (x, y) for building data-intensive learning systems (e.g., a deep learning system). Asking people to report complex distribution p(x), though theoretically viable, is challenging in practice. This is primarily due to the cognitive loads required for human agents to form the report of this highly complicated information. While classical elicitation mechanisms apply to eliciting a complex and generative (and continuous) distribution p(x), we are interested in eliciting samples x(i) similar to p(x) from agents directly. We coin the above problem sample elicitation. This paper introduces a deep learning aided method to incentivize credible sample contributions from self-interested and rational agents. We show that with an accurate estimation of a certain f-divergence function we can achieve approximate incentive compatibility in eliciting truthful samples. We then present an efficient estimator with theoretical guarantees via studying the variational forms of the f-divergence function. We also show a connection between this sample elicitation problem and f-GAN, and how this connection can help reconstruct an estimator of the distribution based on collected samples. Experiments on synthetic data, MNIST, and CIFAR-10 datasets demonstrate that our mechanism elicits truthful samples.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803026,0
C,"Wu, LH; Miller, A; Anderson, L; Pleiss, G; Blei, D; Cunningham, J",,"Banerjee, A; Fukumizu, K",,"Wu, Luhuan; Miller, Andrew; Anderson, Lauren; Pleiss, Geoff; Blei, David; Cunningham, John",,,Hierarchical Inducing Point Gaussian Process for Inter-domain Observations,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We examine the general problem of interdomain Gaussian Processes (GPs): problems where the GP realization and the noisy observations of that realization lie on different domains. When the mapping between those domains is linear, such as integration or differentiation, inference is still closed form. However, many of the scaling and approximation techniques that our community has developed do not apply to this setting. In this work, we introduce the hierarchical inducing point GP (HIP-GP), a scalable interdomain GP inference method that enables us to improve the approximation accuracy by increasing the number of inducing points to the millions. HIP-GP, which relies on inducing points with grid structure and a stationary kernel assumption, is suitable for lowdimensional problems. In developing HIPGP, we introduce (1) a fast whitening strategy, and (2) a novel preconditioner for conjugate gradients which can be helpful in general GP settings.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803052,0
C,"Xu, JM; Zhu, HJ",,"Banerjee, A; Fukumizu, K",,"Xu, Jiaming; Zhu, Hanjing",,,One-pass stochastic gradient descent in overparametrized two-layer neural networks,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"There has been a recent surge of interest in understanding the convergence of gradient descent (GD) and stochastic gradient descent (SGD) in overparameterized neural networks. Most previous work assumes that the training data is provided a priori in a batch, while less attention has been paid to the important setting where the training data arrives in a stream. In this paper, we study the streaming data setup and show that with overparamterization and random initialization, the prediction error of two-layer neural networks under one-pass SGD converges in expectation. The convergence rate depends on the eigen-decomposition of the integral operator associated with the so-called neural tangent kernel (NTK). A key step of our analysis is to show a random kernel function converges to the NTK with high probability using the VC dimension and McDiarmid inequality.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804043,0
C,"Yadav, M; Sheldon, D; Musco, C",,"Banerjee, A; Fukumizu, K",,"Yadav, Mohit; Sheldon, Daniel; Musco, Cameron",,,Faster Kernel Interpolation for Gaussian Processes,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"A key challenge in scaling Gaussian Process (GP) regression to massive datasets is that exact inference requires computation with a dense n x n kernel matrix, where n is the number of data points. Significant work focuses on approximating the kernel matrix via interpolation using a smaller set of m inducing points. Structured kernel interpolation (SKI) is among the most scalable methods: by placing inducing points on a dense grid and using structured matrix algebra, SKI achieves per-iteration time of O(n + m log m) for approximate inference. This linear scaling in n enables inference for very large data sets; however the cost is per-iteration, which remains a limitation for extremely large n. We show that the SKI per-iteration time can be reduced to O(m log m) after a single O(n) time precomputation step by reframing SKI as solving a natural Bayesian linear regression problem with a fixed set of m compact basis functions. With per-iteration complexity independent of the dataset size n for a fixed grid, our method scales to truly massive data sets. We demonstrate speedups in practice for a wide range of m and n and apply the method to GP inference on a three-dimensional weather radar dataset with over 100 million points.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803057,0
C,"Alimisis, F; Orvieto, A; Becigneul, G; Lucchi, A",,"Chiappa, S; Calandra, R",,"Alimisis, Foivos; Orvieto, Antonio; Becigneul, Gary; Lucchi, Aurelien",,,A Continuous-time Perspective for Modeling Acceleration in Riemannian Optimization,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We propose a novel second-order ODE as the continuous-time limit of a Riemannian accelerated gradient-based method on a manifold with curvature bounded from below. This ODE can be seen as a generalization of the ODE derived for Euclidean spaces, and can also serve as an analysis tool. We study the convergence behavior of this ODE for different classes of functions, such as geodesically convex, strongly-convex and weakly-quasi-convex. We demonstrate how such an ODE can be discretized using a semi-implicit and Nesterov-inspired numerical integrator, that empirically yields stable algorithms which are faithful to the continuous-time analysis and exhibit accelerated convergence.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1297,1306,,,,,,,,,,,,,,,,WOS:000559931300009,0
C,"Hazimeh, H; Mazumder, R",,"Chiappa, S; Calandra, R",,"Hazimeh, Hussein; Mazumder, Rahul",,,Learning Hierarchical Interactions at Scale: A Convex Optimization Approach,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"In many learning settings, it is beneficial to augment the main features with pairwise interactions. Such interaction models can be often enhanced by performing variable selection under the so-called strong hierarchy constraint: an interaction is non-zero only if its associated main features are non-zero. Existing convex optimization-based algorithms face difficulties in handling problems where the number of main features p similar to 10(3) (with total number of features similar to p(2)). In this paper, we study a convex relaxation which enforces strong hierarchy and develop a highly scalable algorithm based on proximal gradient descent. We introduce novel screening rules that allow for solving the complicated proximal problem in parallel. In addition, we introduce a specialized active-set strategy with gradient screening for avoiding costly gradient computations. The framework can handle problems having dense design matrices, with p = 50; 000 (similar to 10(9) interactions)-instances that are much larger than state of the art. Experiments on real and synthetic data suggest that our toolkit hierScale outperforms the state of the art in terms of prediction and variable selection and can achieve over a 4900x speed-up.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1833,1842,,,,,,,,,,,,,,,,WOS:000559931301039,0
C,"Janz, D; Burt, DR; Gonzalez, J",,"Chiappa, S; Calandra, R",,"Janz, David; Burt, David R.; Gonzalez, Javier",,,Bandit optimisation of functions in the Matern kernel RKHS,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We consider the problem of optimising functions in the reproducing kernel Hilbert space (RKHS) of a Matern kernel with smoothness parameter nu over the domain [0, 1](d) under noisy bandit feedback. Our contribution, the pi-GP-UCB algorithm, is the first practical approach with guaranteed sublinear regret for all nu > 1 and d >= 1. Empirical validation suggests better performance and drastically improved computational scalablity compared with its predecessor, Improved GP-UCB.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2486,2494,,,,,,,,,,,,,,,,WOS:000559931301059,0
C,"Katz-Samuels, J; Jamieson, K",,"Chiappa, S; Calandra, R",,"Katz-Samuels, Julian; Jamieson, Kevin",,,The True Sample Complexity of Identifying Good Arms,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We consider two multi-armed bandit problems with n arms: (i) given an epsilon > 0, identify an arm with mean that is within. of the largest mean and (ii) given a threshold mu(0) and integer k, identify k arms with means larger than mu(0). Existing lower bounds and algorithms for the PAC framework suggest that both of these problems require Omega(n) samples. However, we argue that the PAC framework not only conflicts with how these algorithms are used in practice, but also that these results disagree with intuition that says (i) requires only Theta(n/m) samples where m = vertical bar{i : mu(i) > max(j is an element of[n]) mu(j) - epsilon}| and (ii) requires Theta(n/m k) samples where m = vertical bar{i : mu(i) > mu(0)}vertical bar. We provide definitions that formalize these intuitions, obtain lower bounds that match the above sample complexities, and develop explicit, practical algorithms that achieve nearly matching upper bounds.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1781,1790,,,,,,,,,,,,,,,,WOS:000559931301066,0
C,"LeJeune, D; Javadi, H; Baraniuk, RG",,"Chiappa, S; Calandra, R",,"LeJeune, Daniel; Javadi, Hamid; Baraniuk, Richard G.",,,The Implicit Regularization of Ordinary Least Squares Ensembles,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Ensemble methods that average over a collection of independent predictors that are each limited to a subsampling of both the examples and features of the training data command a significant presence in machine learning, such as the ever-popular random forest, yet the nature of the subsampling effect, particularly of the features, is not well understood. We study the case of an ensemble of linear predictors, where each individual predictor is fit using ordinary least squares on a random submatrix of the data matrix. We show that, under standard Gaussianity assumptions, when the number of features selected for each predictor is optimally tuned, the asymptotic risk of a large ensemble is equal to the asymptotic ridge regression risk, which is known to be optimal among linear predictors in this setting. In addition to eliciting this implicit regularization that results from subsampling, we also connect this ensemble to the dropout technique used in training deep (neural) networks, another strategy that has been shown to have a ridgelike regularizing effect.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3525,3534,,,,,,,,,,,,,,,,WOS:000559931301099,0
C,"Liu, LT; Mania, H; Jordan, MI",,"Chiappa, S; Calandra, R",,"Liu, Lydia T.; Mania, Horia; Jordan, Michael I.",,,Competing Bandits in Matching Markets,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Stable matching, a classical model for two-sided markets, has long been studied assuming known preferences. In reality agents often have to learn about their preferences through exploration. With the advent of massive on-line markets powered by data-driven matching platforms, it has become necessary to better understand the interplay between learning and market objectives. We propose a statistical learning model in which one side of the market does not have a priori knowledge about its preferences for the other side and is required to learn these from stochastic rewards. Our model extends the standard multi-armed bandits framework to multiple players, with the added feature that arms have preferences over players. We study both centralized and decentralized approaches to this problem and show surprising exploration-exploitation trade-offs compared to the single player multi-armed bandits setting.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1618,1627,,,,,,,,,,,,,,,,WOS:000559931302016,0
C,"Pan, D; Wang, T; Hara, S",,"Chiappa, S; Calandra, R",,"Pan, Danqing; Wang, Tong; Hara, Satoshi",,,Interpretable Companions for Black-Box Models,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We present an interpretable companion model for any pre-trained black-box classifiers. The idea is that for any input, a user can decide to either receive a prediction from the black-box model, with high accuracy but no explanations, or employ a companion rule to obtain an interpretable prediction with slightly lower accuracy. The companion model is trained from data and the predictions of the black-box model, with the objective combining area under the transparency-accuracy curve and model complexity. Our model provides flexible choices for practitioners who face the dilemma of choosing between always using interpretable models and always using black-box models for a predictive task, so users can, for any given input, take a step back to resort to an interpretable prediction if they find the predictive performance satisfying, or stick to the black-box model if the rules are unsatisfying. To show the value of companion models, we design a human evaluation on more than a hundred people to investigate the tolerable accuracy loss to gain interpretability for humans.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2444,2453,,,,,,,,,,,,,,,,WOS:000559931302062,0
C,"Rogers, R; Roth, A; Smith, A; Srebro, N; Thakkar, O; Woodworth, B",,"Chiappa, S; Calandra, R",,"Rogers, Ryan; Roth, Aaron; Smith, Adam; Srebro, Nathan; Thakkar, Om; Woodworth, Blake",,,Guaranteed Validity for Empirical Approaches to Adaptive Data Analysis,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We design a general framework for answering adaptive statistical queries that focuses on providing explicit confidence intervals along with point estimates. Prior work in this area has either focused on providing tight confidence intervals for specific analyses, or providing general worst-case bounds for point estimates. Unfortunately, as we observe, these worst-case bounds are loose in many settings - often not even beating simple baselines like sample splitting. Our main contribution is to design a framework for providing valid, instance-specific confidence intervals for point estimates that can be generated by heuristics. When paired with good heuristics, this method gives guarantees that are orders of magnitude better than the best worst-case bounds. We provide a Python library implementing our method.",,,,,"Smith, Adam/GPS-8322-2022",,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2830,2839,,,,,,,,,,,,,,,,WOS:000559931302092,0
C,"Rowland, M; Harutyunyan, A; van Hasselt, H; Borsa, D; Schaul, T; Munos, R; Dabney, W",,"Chiappa, S; Calandra, R",,"Rowland, Mark; Harutyunyan, Anna; van Hasselt, Hado; Borsa, Diana; Schaul, Tom; Munos, Remi; Dabney, Will",,,Conditional Importance Sampling for Off-Policy Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"The principal contribution of this paper is a conceptual framework for off-policy reinforcement learning, based on conditional expectations of importance sampling ratios. This framework yields new perspectives and understanding of existing off-policy algorithms, and reveals a broad space of unexplored algorithms. We theoretically analyse this space, and concretely investigate several algorithms that arise from this framework.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,45,54,,,,,,,,,,,,,,,,WOS:000559931302096,0
C,"Taheri, H; Pedarsani, R; Thrampoulidis, C",,"Chiappa, S; Calandra, R",,"Taheri, Hossein; Pedarsani, Ramtin; Thrampoulidis, Christos",,,Sharp Asymptotics and Optimal Performance for Inference in Binary Models,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We study convex empirical risk minimization for high-dimensional inference in binary models. Our first result sharply predicts the statistical performance of such estimators in the linear asymptotic regime under isotropic Gaussian features. Importantly, the predictions hold for a wide class of convex loss functions, which we exploit in order to prove a bound on the best achievable performance among them. Notably, we show that the proposed bound is tight for popular binary models (such as Signed, Logistic or Probit), by constructing appropriate loss functions that achieve it. More interestingly, for binary linear classification under the Logistic and Probit models, we prove that the performance of least-squares is no worse than 0.997 and 0.98 times the optimal one. Numerical simulations corroborate our theoretical findings and suggest they are accurate even for relatively small problem dimensions.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303035,0
C,"Terenin, A; Simpson, D; Draper, D",,"Chiappa, S; Calandra, R",,"Terenin, Alexander; Simpson, Daniel; Draper, David",,,Asynchronous Gibbs sampling,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Gibbs sampling is a Markov Chain Monte Carlo (MCMC) method often used in Bayesian learning. MCMC methods can be difficult to deploy on parallel and distributed systems due to their inherently sequential nature. We study asynchronous Gibbs sampling, which achieves parallelism by simply ignoring sequential requirements. This method has been shown to produce good empirical results for some hierarchical models, and is popular in the topic modeling community, but was also shown to diverge for other targets. We introduce a theoretical framework for analyzing asynchronous Gibbs sampling and other extensions of MCMC that do not possess the Markov property. We prove that asynchronous Gibbs can be modified so that it converges under appropriate regularity conditions we call this the exact asynchronous Gibbs algorithm. We study asynchronous Gibbs on a set of examples by comparing the exact and approximate algorithms, including two where it works well, and one where it fails dramatically. We conclude with a set of heuristics to describe settings where the algorithm can be effectively used.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303039,0
C,"Uehara, M; Matsuda, T; Kim, JK",,"Chiappa, S; Calandra, R",,"Uehara, Masatoshi; Matsuda, Takeru; Kim, Jae Kwang",,,Imputation Estimators for Unnormalized Models with Missing Data,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Several statistical models are given in the form of unnormalized densities, and calculation of the normalization constant is intractable. We propose estimation methods for such unnormalized models with missing data. The key concept is to combine imputation techniques with estimators for unnormalized models including noise contrastive estimation and score matching. In addition, we derive asymptotic distributions of the proposed estimators and construct confidence intervals. Simulation results with truncated Gaussian graphical models and the application to real data of wind direction reveal that the proposed methods effectively enable statistical inference with unnormalized models from missing data.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303047,0
C,"Wang, G; Giannakis, GB",,"Chiappa, S; Calandra, R",,"Wang, Gang; Giannakis, Georgios B.",,,Finite-Time Error Bounds for Biased Stochastic Approximation with Application to Q-Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Inspired by the widespread use of Q-learning algorithms in reinforcement learning (RL), this present paper studies a class of biased stochastic approximation (SA) procedures under an 'ergodic-like' assumption on the underlying stochastic noise sequence. Leveraging a multistep Lyapunov function that looks ahead to several future updates to accommodate the gradient bias, we prove a general result on the convergence of the iterates, and use it to derive finite-time bounds on the mean-square error in the case of constant stepsizes. This novel viewpoint renders the finite-time analysis of biased SA algorithms under a broad family of stochastic perturbations possible. For direct comparison with past works, we also demonstrate these bounds by applying them to Q-learning with linear function approximation, under the realistic Markov chain observation model. The resultant finite-time error bound for Q-learning is the first of its kind, in the sense that it holds: i) for the unmodified version (i.e., without making any modifications to the updates), and ii), for Markov chains starting from any initial distribution, at least one of which has to be violated for existing results to be applicable.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303066,0
C,"Blondel, M; Martins, AFT; Niculae, V",,"Chaudhuri, K; Sugiyama, M",,"Blondel, Mathieu; Martins, Andre F. T.; Niculae, Vlad",,,"Learning Classifiers with Fenchel-Young Losses: Generalized Entropies, Margins, and Algorithms","22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"This paper studies Fenchel-Young losses, a generic way to construct convex loss functions from a regularization function. We analyze their properties in depth, showing that they unify many well-known loss functions and allow to create useful new ones easily. Fenchel-Young losses constructed from a generalized entropy, including the Shannon and Tsallis entropies, induce predictive probability distributions. We formulate conditions for a generalized entropy to yield losses with a separation margin, and probability distributions with sparse support. Finally, we derive efficient algorithms, making Fenchel-Young losses appealing both in theory and practice.",,,,,,"Torres Martins, Andre Filipe/0000-0001-8282-625X",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,606,615,,,,,,,,,,,,,,,,WOS:000509687900063,0
C,"Cortes, C; DeSalvo, G; Gentile, C; Mohri, M; Zhang, NS",,"Chaudhuri, K; Sugiyama, M",,"Cortes, Corinna; DeSalvo, Giulia; Gentile, Claudio; Mohri, Mehryar; Zhang, Ningshan",,,Region-Based Active Learning,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We study a scenario of active learning where the input space is partitioned into different regions and where a distinct hypothesis is learned for each region. We first introduce a new active learning algorithm (EIWAL), which is an enhanced version of the IWAL algorithm, based on a finer analysis that results in more favorable learning guarantees. Then, we present a new learning algorithm for region-based active learning, ORIWAL, in which either IWAL or EIWAL serve as a subroutine. ORIWAL optimally allocates points to the subroutine algorithm for each region. We give a detailed theoretical analysis of ORIWAL, including generalization error guarantees and bounds on the number of points labeled, in terms of both the hypothesis set used in each region and the probability mass of that region. We also report the results of several experiments for our algorithm which demonstrate substantial benefits over existing non-region-based active learning algorithms, such as IWAL, and over passive learning.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902087,0
C,"Szabo, Z; Sriperumbudur, BK",,"Chaudhuri, K; Sugiyama, M",,"Szabo, Zoltan; Sriperumbudur, Bharath K.",,,On Kernel Derivative Approximation with Random Fourier Features,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Random Fourier features (RFF) represent one of the most popular and wide-spread techniques in machine learning to scale up kernel algorithms. Despite the numerous successful applications of RFFs, unfortunately, quite little is understood theoretically on their optimality and limitations of their performance. Only recently, precise statistical-computational trade-offs have been established for RFFs in the approximation of kernel values, kernel ridge regression, kernel PCA and SVM classification. Our goal is to spark the investigation of optimality of RFF-based approximations in tasks involving not only function values but derivatives, which naturally lead to optimization problems with kernel derivatives. Particularly, in this paper, we focus on the approximation quality of RFFs for kernel derivatives and prove that the existing finite-sample guarantees can be improved exponentially in terms of the domain where they hold, using recent tools from unbounded empirical process theory. Our result implies that the same approximation guarantee is attainable for kernel derivatives using RFF as achieved for kernel values.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,827,836,,,,,,,,,,,,,,,,WOS:000509687900086,0
C,"Taghia, J; Schon, TB",,"Chaudhuri, K; Sugiyama, M",,"Taghia, Jalil; Schon, Thomas B.",,,Conditionally Independent Multiresolution Gaussian Processes,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"The multiresolution Gaussian process (GP) has gained increasing attention as a viable approach towards improving the quality of approximations in GPs that scale well to large-scale data. Most of the current constructions assume full independence across resolutions. This assumption simplifies the inference, but it underestimates the uncertainties in transitioning from one resolution to another. This in turn results in models which are prone to overfitting in the sense of excessive sensitivity to the chosen resolution, and predictions which are non-smooth at the boundaries. Our contribution is a new construction which instead assumes conditional independence among GPs across resolutions. We show that relaxing the full independence assumption enables robustness against overfitting, and that it delivers predictions that are smooth at the boundaries. Our new model is compared against current state of the art on 2 synthetic and 9 real-world datasets. In most cases, our new conditionally independent construction performed favorably when compared against models based on the full independence assumption. In particular, it exhibits little to no signs of overfitting.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901001,0
C,"Wang, SN; Liu, JS; Shroff, N; Yang, PY",,"Chaudhuri, K; Sugiyama, M",,"Wang, Sinong; Liu, Jiashang; Shroff, Ness; Yang, Pengyu",,,Computation Efficient Coded Linear Transform,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"In large-scale distributed linear transform problems, coded computation plays an important role to reduce the delay caused by slow machines. However, existing coded schemes could end up destroying the significant sparsity that exists in large-scale machine learning problems, and in turn increase the computational delay. In this paper, we propose a coded computation strategy, referred to as diagonal code, that achieves the optimum recovery threshold and the optimum computation load. Furthermore, by leveraging the ideas from random proposal graph theory, we design a random code that achieves a constant computation load, which significantly outperforms the existing best known result. We apply our schemes to the distributed gradient descent problem and demonstrate the advantage of the approach over current fastest coded schemes.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,577,585,,,,,,,,,,,,,,,,WOS:000509687900060,0
C,"Wu, YS; Wang, PA; Lu, CJ",,"Chaudhuri, K; Sugiyama, M",,"Wu, Yi-Shan; Wang, Po-An; Lu, Chi-Jen",,,Lifelong Optimization with Low Regret,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"In this work, we study a problem arising from two lines of works: online optimization and lifelong learning. In the problem, there is a sequence of tasks arriving sequentially, and within each task, we have to make decisions one after one and then suffer corresponding losses. The tasks are related as they share some common representation, but they are different as each requires a different predictor on top of the representation. As learning a representation is usually costly in lifelong learning scenarios, the goal is to learn it continuously through time across different tasks, making the learning of later tasks easier than previous ones. We provide such learning algorithms with good regret bounds which can be seen as natural generalization of prior works on online optimization.",,,,,"Lu, Chi-Jen/AAQ-3728-2021","Wu, Yi-Shan/0000-0002-7949-0115",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,448,456,,,,,,,,,,,,,,,,WOS:000509687900047,0
C,"Bellot, A; van der Schaar, M",,"Storkey, A; PerezCruz, F",,"Bellot, Alexis; van der Schaar, Mihaela",,,Tree-based Bayesian Mixture Model for Competing Risks,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Many chronic diseases possess a shared biology. Therapies designed for patients at risk of multiple diseases need to account for the shared impact they may have on related diseases to ensure maximum overall well-being. Learning from data in this setting differs from classical survival analysis methods since the incidence of an event of interest may be obscured by other related competing events. We develop a semi-parametric Bayesian regression model for survival analysis with competing risks, which can be used for jointly assessing a patient's risk of multiple (competing) adverse outcomes. We construct a Hierarchical Bayesian Mixture (HBM) model to describe survival paths in which a patient's covariates influence both the estimation of the type of adverse event and the subsequent survival trajectory through Multivariate Random Forests. In addition variable importance measures, which are essential for clinical interpretability are induced naturally by our model. We aim with this setting to provide accurate individual estimates but also interpretable conclusions for use as a clinical decision support tool. We compare our method with various state-of-the-art benchmarks on both synthetic and clinical data.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300096,0
C,"Kallus, N; Zhou, A",,"Storkey, A; PerezCruz, F",,"Kallus, Nathan; Zhou, Angela",,,Policy Evaluation and Optimization with Continuous Treatments,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"We study the problem of policy evaluation and learning from batched contextual bandit data when treatments are continuous, going beyond previous work on discrete treatments. Previous work for discrete treatment/action spaces focuses on inverse probability weighting (IPW) and doubly robust (DR) methods that use a rejection sampling approach for evaluation and the equivalent weighted classification problem for learning. In the continuous setting, this reduction fails as we would almost surely reject all observations. To tackle the case of continuous treatments, we extend the IPW and DR approaches to the continuous setting using a kernel function that leverages treatment proximity to attenuate discrete rejection. Our policy estimator is consistent and we characterize the optimal bandwidth. The resulting continuous policy optimizer (CPO) approach using our estimator achieves convergent regret and approaches the best-in-class policy for learnable policy classes. We demonstrate that the estimator performs well and, in particular, outperforms a discretization-based benchmark. We further study the performance of our policy optimizer in a case study on personalized dosing based on a dataset of Warfarin patients, their covariates, and final therapeutic doses. Our learned policy outperforms benchmarks and nears the oracle-best linear policy.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300130,0
C,"Pfeffer, A; Ruttenberg, B; Kretschmer, W; O'Connor, A",,"Storkey, A; PerezCruz, F",,"Pfeffer, Avi; Ruttenberg, Brian; Kretschmer, William; O'Connor, Alison",,,Structured Factored Inference for Probabilistic Programming,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Probabilistic reasoning on complex real-world models is computationally challenging. Inference algorithms have been developed that work well on specific models or on parts of general models, but they require significant hand-engineering to apply to full-scale problems. Probabilistic programming (PP) enables the expression of rich probabilistic models, but inference remains a bottleneck in many applications. Factored inference is one of the main approaches to inference in graphical models, but has trouble scaling up to some hard problems expressible as probabilistic programs. We present structured factored inference (SFI), a framework that enables factored inference algorithms to scale to significantly more complex programs. Using models encoded in a PP language, SFI provides a sound means to decompose a model into submodels, apply an algorithm to each submodel, and combine results to answer a query. Our results show that SFI successfully reasons on models where standard factored inference algorithms fail due to computational complexity. SFI is nearly as accurate as exact inference and is as fast as approximate inference methods.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300128,0
C,"Bartunov, S; Kondrashkin, D; Osokin, A; Vetrov, DP",,"Gretton, A; Robert, CC",,"Bartunov, Sergey; Kondrashkin, Dmitry; Osokin, Anton; Vetrov, Dmitry P.",,,Breaking Sticks and Ambiguities with Adaptive Skip-gram,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"The recently proposed Skip-gram model is a powerful method for learning high-dimensional word representations that capture rich semantic relationships between words. However, Skip-gram as well as most prior work on learning word representations does not take into account word ambiguity and maintain only a single representation per word. Although a number of Skip-gram modifications were proposed to overcome this limitation and learn multi-prototype word representations, they either require a known number of word meanings or learn them using greedy heuristic approaches. In this paper we propose the Adaptive Skip-gram model which is a non-parametric Bayesian extension of Skip-gram capable to automatically learn the required number of representations for all words at desired semantic resolution. We derive efficient online variational learning algorithm for the model and empirically demonstrate its efficiency on word-sense induction task.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,130,138,,,,,,,,,,,,,,,,WOS:000508662100015,0
C,"Chen, ST; Balcan, MF; Chau, DH",,"Gretton, A; Robert, CC",,"Chen, Shang-Tse; Balcan, Maria-Florina; Chau, Duen Horng",,,Communication Efficient Distributed Agnostic Boosting,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"We consider the problem of learning from distributed data in the agnostic setting, i.e., in the presence of arbitrary forms of noise. Our main contribution is a general distributed boosting-based procedure for learning an arbitrary concept space, that is simultaneously noise tolerant, communication efficient, and computationally efficient. This improves significantly over prior works that were either communication efficient only in noisefree scenarios or computationally prohibitive. Empirical results on large synthetic and real-world datasets demonstrate the effectiveness and scalability of the proposed approach.",,,,,,"Chau, Polo/0000-0001-9824-3323",,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,1299,1307,,,,,,,,,,,,,,,,WOS:000508662100141,0
C,"Dasarathy, G; Singh, A; Balcan, MF; Park, JH",,"Gretton, A; Robert, CC",,"Dasarathy, Gautam; Singh, Aarti; Balcan, Maria F.; Park, Jong H.",,,Active Learning Algorithms for Graphical Model Selection,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"The problem of learning the structure of a high dimensional graphical model from data has received considerable attention in recent years. In many applications such as sensor networks and proteomics it is often expensive to obtain samples from all the variables involved simultaneously. For instance, this might involve the synchronization of a large number of sensors or the tagging of a large number of proteins. To address this important issue, we initiate the study of a novel graphical model selection problem, where the goal is to optimize the total number of scalar samples obtained by allowing the collection of samples from only subsets of the variables. We propose a general paradigm for graphical model selection where feedback is used to guide the sampling to high degree vertices, while obtaining only few samples from the ones with the low degrees. We instantiate this framework with two specific active learning algorithms, one of which makes mild assumptions but is computationally expensive, while the other is computationally more efficient but requires stronger (nevertheless standard) assumptions. Whereas the sample complexity of passive algorithms is typically a function of the maximum degree of the graph, we show that the sample complexity of our algorithms is provably smaller and that it depends on a novel local complexity measure that is akin to the average degree of the graph. We finally demonstrate the efficacy of our framework via simulations.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,1356,1364,,,,,,,,,,,,,,,,WOS:000508662100147,0
C,"Shen, J; Li, P",,"Gretton, A; Robert, CC",,"Shen, Jie; Li, Ping",,,Learning Structured Low-Rank Representation via Matrix Factorization,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"A vast body of recent works in the literature have shown that exploring structures beyond data low-rankness can boost the performance of subspace clustering methods such as Low-Rank Representation (LRR). It has also been well recognized that the matrix factorization framework might offer more flexibility on pursuing underlying structures of the data. In this paper, we propose to learn structured LRR by factorizing the nuclear norm regularized matrix, which leads to our proposed non-convex formulation NLRR. Interestingly, this formulation of NLRR provides a general framework for unifying a variety of popular algorithms including LRR, dictionary learning, robust principal component analysis, sparse subspace clustering, etc. Several variants of NLRR are also proposed, for example, to promote sparsity while preserving low-rankness. We design a practical algorithm for NLRR and its variants, and establish theoretical guarantee for the stability of the solution and the convergence of the algorithm. Perhaps surprisingly, the computational and memory cost of NLRR can be reduced by roughly one order of magnitude compared to the cost of LRR. Experiments on extensive simulations and real datasets confirm the robustness of efficiency of NLRR and the variants.",,,,,,"Shen, Jie/0000-0002-4440-7756",,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,500,509,,,,,,,,,,,,,,,,WOS:000508662100055,0
C,"Svensson, A; Solin, A; Sarkka, S; Schon, TB",,"Gretton, A; Robert, CC",,"Svensson, Andreas; Solin, Arno; Sarkka, Simo; Schon, Thomas B.",,,Computationally Efficient Bayesian Learning of Gaussian Process State Space Models,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Gaussian processes allow for flexible specification of prior assumptions of unknown dynamics in state space models. We present a procedure for efficient Bayesian learning in Gaussian process state space models, where the representation is formed by projecting the problem onto a set of approximate eigen-functions derived from the prior covariance structure. Learning under this family of models can be conducted using a carefully crafted particle MCMC algorithm. This scheme is computationally efficient and yet allows for a fully Bayesian treatment of the problem. Compared to conventional system identification tools or existing learning methods, we show competitive performance and reliable quantification of uncertainties in the model.",,,,,"Solin, Arno/G-6859-2012; Schon, Thomas/D-4169-2009","Solin, Arno/0000-0002-0958-7886; Schon, Thomas/0000-0001-5183-234X",,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,213,221,,,,,,,,,,,,,,,,WOS:000508662100024,0
C,"Wang, YL; Brubaker, M; Chaib-draa, B; Urtasun, R",,"Gretton, A; Robert, CC",,"Wang, Yali; Brubaker, Marcus; Chaib-draa, Brahim; Urtasun, Raquel",,,Sequential Inference for Deep Gaussian Process,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"A deep Gaussian process (DGP) is a deep network in which each layer is modelled with a Gaussian process (GP). It is a flexible model that can capture highly-nonlinear functions for complex data sets. However, the network structure of DGP often makes inference computationally expensive. In this paper, we propose an efficient sequential inference framework for DGP, where the data is processed sequentially. We also propose two DGP extensions to handle heteroscedasticity and multi-task learning. Our experimental evaluation shows the effectiveness of our sequential inference framework on a number of important learning tasks.",,,,,"Chaib-draa, Brahim/A-1157-2008","Chaib-draa, Brahim/0000-0001-7615-5154",,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,694,703,,,,,,,,,,,,,,,,WOS:000508662100076,0
C,"Duvenaud, D; Rippel, O; Adams, RP; Ghahramani, Z",,"Kaski, S; Corander, J",,"Duvenaud, David; Rippel, Oren; Adams, Ryan P.; Ghahramani, Zoubin",,,Avoiding pathologies in very deep networks,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"Choosing appropriate architectures and regularization strategies of deep networks is crucial to good predictive performance. To shed light on this problem, we analyze the analogous problem of constructing useful priors on compositions of functions. Specifically, we study the deep Gaussian process, a type of infinitely-wide, deep neural network. We show that in standard architectures, the representational capacity of the network tends to capture fewer degrees of freedom as the number of layers increases, retaining only a single degree of freedom in the limit. We propose an alternate network architecture which does not suffer from this pathology. We also examine deep covariance functions, obtained by composing infinitely many feature transforms. Lastly, we characterize the class of models obtained by performing dropout on Gaussian processes.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,202,210,,,,,,,,,,,,,,,,WOS:000508355800023,0
C,"Bosch, N; Hennig, P; Tronarp, F",,"Banerjee, A; Fukumizu, K",,"Bosch, Nathanael; Hennig, Philipp; Tronarp, Filip",,,Calibrated Adaptive Probabilistic ODE Solvers,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Probabilistic solvers for ordinary differential equations assign a posterior measure to the solution of an initial value problem. The joint covariance of this distribution provides an estimate of the (global) approximation error. The contraction rate of this error estimate as a function of the solver's step size identifies it as a well-calibrated worst-case error, but its explicit numerical value for a certain step size is not automatically a good estimate of the explicit error. Addressing this issue, we introduce, discuss, and assess several probabilistically motivated ways to calibrate the uncertainty estimate. Numerical experiments demonstrate that these calibration methods interact efficiently with adaptive step-size selection, resulting in descriptive, and efficiently computable posteriors. We demonstrate the efficiency of the methodology by benchmarking against the classic, widely used Dormand-Prince 4/5 Runge-Kutta method.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804020,0
C,"Diakonikolas, J; Daskalakis, C; Jordan, MI",,"Banerjee, A; Fukumizu, K",,"Diakonikolas, Jelena; Daskalakis, Constantinos; Jordan, Michael, I",,,Efficient Methods for Structured Nonconvex-Nonconcave Min-Max Optimization,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"The use of min-max optimization in the adversarial training of deep neural network classifiers, and the training of generative adversarial networks has motivated the study of nonconvex-nonconcave optimization objectives, which frequently arise in these applications. Unfortunately, recent results have established that even approximate first-order stationary points of such objectives are intractable, even under smoothness conditions, motivating the study of min-max objectives with additional structure. We introduce a new class of structured nonconvex-nonconcave min-max optimization problems, proposing a generalization of the extragradient algorithm which provably converges to a stationary point. The algorithm applies not only to Euclidean spaces, but also to general l(p)-normed finite-dimensional real vector spaces. We also discuss its stability under stochastic oracles and provide bounds on its sample complexity. Our iteration complexity and sample complexity bounds either match or improve the best known bounds for the same or less general nonconvex-nonconcave settings, such as those that satisfy variational coherence or in which a weak solution to the associated variational inequality problem is assumed to exist.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803032,0
C,"Durmus, A; Jimenez, P; Moulines, E; Said, S",,"Banerjee, A; Fukumizu, K",,"Durmus, Alain; Jimenez, Pablo; Moulines, Eric; Said, Salem",,,On Riemannian Stochastic Approximation Schemes with Fixed Step-Size,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"This paper studies fixed step-size stochastic approximation (SA) schemes, including stochastic gradient schemes, in a Riemannian framework. It is motivated by several applications, where geodesics can be computed explicitly, and their use accelerates crude Euclidean methods. A fixed step-size scheme defines a family of time-homogeneous Markov chains, parametrized by the step-size. Here, using this formulation, non-asymptotic performance bounds are derived, under Lyapunov conditions. Then, for any step-size, the corresponding Markov chain is proved to admit a unique stationary distribution, and to be geometrically ergodic. This result gives rise to a family of stationary distributions indexed by the step-size, which is further shown to converge to a Dirac measure, concentrated at the solution of the problem at hand, as the step-size goes to 0. Finally, the asymptotic rate of this convergence is established, through an asymptotic expansion of the bias, and a central limit theorem.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801027,0
C,"Elkhalil, K; Hasan, A; Ding, J; Farsiu, S; Tarokh, V",,"Banerjee, A; Fukumizu, K",,"Elkhalil, Khalil; Hasan, Ali; Ding, Jie; Farsiu, Sina; Tarokh, Vahid",,,Fisher Auto-Encoders,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"It has been conjectured that the Fisher divergence is more robust to model uncertainty than the conventional Kullback-Leibler (KL) divergence. This motivates the design of a new class of robust generative auto-encoders (AE) referred to as Fisher auto-encoders. Our approach is to design Fisher AEs by minimizing the Fisher divergence between the intractable joint distribution of observed data and latent variables, with that of the postulated/modeled joint distribution. In contrast to KL-based variational AEs (VAEs), the Fisher AE can exactly quantify the distance between the true and the model-based posterior distributions. Qualitative and quantitative results are provided on both MNIST and celebA datasets demonstrating the competitive performance of Fisher AEs in terms of robustness compared to other AEs such as VAEs and Wasserstein AEs.",,,,,"Poor, H. Vincent/S-5027-2016","Poor, H. Vincent/0000-0002-2062-131X",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,352,360,,,,,,,,,,,,,,,,WOS:000659893800040,0
C,"Gitiaux, X; Rangwala, H",,"Banerjee, A; Fukumizu, K",,"Gitiaux, Xavier; Rangwala, Huzefa",,,Learning Smooth and Fair Representations,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"This paper explores the statistical properties of fair representation learning, a preprocessing method that preemptively removes the correlations between features and sensitive attributes by mapping features to a fair representation space. The demographic parity of a representation can be certified from a finite sample if and only if the chi-squared mutual information between features and representations is finite for all features distributions. Empirically, we find that smoothing representations provides generalization guarantees of fairness certificates, which improves upon existing fair representation learning approaches. On four datasets we simulate many downstream users and show that our approach, AGWN, is the only one that generates representations whose fairness properties are robust to many downstream users.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,253,+,,,,,,,,,,,,,,,,WOS:000659893800029,0
C,"Jahani, M; Nazari, M; Tappenden, R; Berahas, AS; Takac, M",,"Banerjee, A; Fukumizu, K",,"Jahani, Majid; Nazari, Mohammadreza; Tappenden, Rachael; Berahas, Albert S.; Takac, Martin",,,SONIA: A Symmetric Blockwise Truncated Optimization Algorithm,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"This work presents a new optimization algorithm for empirical risk minimization. The algorithm bridges the gap between first- and second-order methods by computing a search direction that uses a second-order-type update in one subspace, coupled with a scaled steepest descent step in the orthogonal complement. To this end, partial curvature information is incorporated to help with ill-conditioning, while simultaneously allowing the algorithm to scale to the large problem dimensions often encountered in machine learning applications. Theoretical results are presented to confirm that the algorithm converges to a stationary point in both the strongly convex and non-convex cases. A stochastic variant of the algorithm is also presented, along with corresponding theoretical guarantees. Numerical results confirm the strengths of the new approach on standard machine learning problems.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,487,+,,,,,,,,,,,,,,,,WOS:000659893800055,0
C,"Kalantari, R; Zhou, MY",,"Banerjee, A; Fukumizu, K",,"Kalantari, Rahi; Zhou, Mingyuan",,,Graph Gamma Process Linear Dynamical Systems,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We introduce graph gamma process (GGP) linear dynamical systems to model real-valued multivariate time series. GGP generates S latent states that are shared by K different communities, each of which is characterized by its own pattern of activation probabilities imposed on a SxS directed sparse graph, and allow both S and K to grow without bound. For temporal pattern discovery, the latent representation under the model is used to decompose the time series into a parsimonious set of multivariate sub-sequences generated by formed communities. In each sub-sequence, different data dimensions often share similar temporal patterns but may exhibit distinct magnitudes, and hence allowing the superposition of all sub-sequences to exhibit diverse behaviors at different data dimensions. On both synthetic and real-world time series, the proposed nonparametric Bayesian dynamic models, which are initialized at random, consistently exhibit good predictive performance in comparison to a variety of baseline models, revealing interpretable latent state transition patterns and decomposing the time series into distinctly behaved sub-sequences.",,,,,"Zhou, Mingyuan/AAE-8717-2021",,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804085,0
C,"Krishnaswamy, AK; Jiang, ZH; Wang, KN; Cheng, Y; Munagala, K",,"Banerjee, A; Fukumizu, K",,"Krishnaswamy, Anilesh K.; Jiang, Zhihao; Wang, Kangning; Cheng, Yu; Munagala, Kamesh",,,Fair for All: Best-effort Fairness Guarantees for Classification,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Standard approaches to group-based notions of fairness, such as parity and equalized odds, try to equalize absolute measures of performance across known groups (based on race, gender, etc.). Consequently, a group that is inherently harder to classify may hold back the performance on other groups; and no guarantees can be provided for unforeseen groups. Instead, we propose a fairness notion whose guarantee, on each group g in a class G, is relative to the performance of the best classifier on g. We apply this notion to broad classes of groups, in particular, where (a) G consists of all possible groups (subsets) in the data, and (b) G is more streamlined. For the first setting, which is akin to groups being completely unknown, we devise the PF (Proportional Fairness) classifier, which guarantees, on any possible group g, an accuracy that is proportional to that of the optimal classifier for g, scaled by the relative size of g in the data set. Due to including all possible groups, some of which could be too complex to be relevant, the worst-case theoretical guarantees here have to be proportionally weaker for smaller subsets. For the second setting, we devise the BeFair (Best-effort Fair) framework which seeks an accuracy, on every g is an element of G, which approximates that of the optimal classifier on g, independent of the size of g. Aiming for such a guarantee results in a non-convex problem, and we design novel techniques to get around this difficulty when G is the set of linear hypotheses. We test our algorithms on realworld data sets, and present interesting comparative insights on their performance.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803089,0
C,"Lin, ZN; Sekar, V; Fanti, G",,"Banerjee, A; Fukumizu, K",,"Lin, Zinan; Sekar, Vyas; Fanti, Giulia",,,On the Privacy Properties of GAN-generated Samples,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"The privacy implications of generative adversarial networks (GANs) are a topic of great interest, leading to several recent algorithms for training GANs with privacy guarantees. By drawing connections to the generalization properties of GANs, we prove that under some assumptions, GAN-generated samples inherently satisfy some (weak) privacy guarantees. First, we show that if a GAN is trained on m samples and used to generate n samples, the generated samples are (epsilon, delta)-differentially-private for (epsilon, delta) pairs where delta scales as O(n/m). We show that under some special conditions, this upper bound is tight. Next, we study the robustness of GAN-generated samples to membership inference attacks. We model membership inference as a hypothesis test in which the adversary must determine whether a given sample was drawn from the training dataset or from the underlying data distribution. We show that this adversary can achieve an area under the ROC curve that scales no better than O(m(-1/4)).",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801083,0
C,"Meehan, C; Chaudhuri, K",,"Banerjee, A; Fukumizu, K",,"Meehan, Casey; Chaudhuri, Kamalika",,,Location Trace Privacy Under Conditional Priors,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Providing meaningful privacy to users of location based services is particularly challenging when multiple locations are revealed in a short period of time. This is primarily due to the tremendous degree of dependence that can be anticipated between points. We propose a Renyi divergence based privacy framework for bounding expected privacy loss for conditionally dependent data. Additionally, we demonstrate an algorithm for achieving this privacy under Gaussian process conditional priors. This framework both exemplifies why conditionally dependent data is so challenging to protect and offers a strategy for preserving privacy to within a fixed radius for sensitive locations in a user's trace.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803047,0
C,"Quinzan, F; Doskoc, V; Gobel, A; Friedrich, T",,"Banerjee, A; Fukumizu, K",,"Quinzan, Francesco; Doskoc, Vanja; Goebel, Andreas; Friedrich, Tobias",,,Adaptive Sampling for Fast Constrained Maximization of Submodular Functions,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Several large-scale machine learning tasks, such as data summarization, can be approached by maximizing functions that satisfy submodularity. These optimization problems often involve complex side constraints, imposed by the underlying application. In this paper, we develop an algorithm with poly-logarithmic adaptivity for non-monotone submodular maximization under general side constraints. The adaptive complexity of a problem is the minimal number of sequential rounds required to achieve the objective. Our algorithm is suitable to maximize a nonmonotone submodular function under a psystem side constraint, and it achieves a (p + O(root p)-approximation for this problem, after only poly-logarithmic adaptive rounds and polynomial queries to the valuation oracle function. Furthermore, our algorithm achieves a (p + O (1))-approximation when the given side constraint is a p-extendible system. This algorithm yields an exponential speedup, with respect to the adaptivity, over any other known constant-factor approximation algorithm for this problem. It also competes with previous known results in terms of the query complexity. We perform various experiments on various real-world applications. We find that, in comparison with commonly used heuristics, our algorithm performs better on these instances.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801021,0
C,"Stanton, S; Maddox, WJ; Delbridge, I; Wilson, AG",,"Banerjee, A; Fukumizu, K",,"Stanton, Samuel; Maddox, Wesley J.; Delbridge, Ian; Wilson, Andrew Gordon",,,Kernel Interpolation for Scalable Online Gaussian Processes,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Gaussian processes (GPs) provide a gold standard for performance in online settings, such as sample-efficient control and black box optimization, where we need to update a posterior distribution as we acquire data in a sequential fashion. However, updating a GP posterior to accommodate even a single new observation after having observed n points incurs at least O(n) computations in the exact setting. We show how to use structured kernel interpolation to efficiently reuse computations for constant-time O(1) online updates with respect to the number of points n, while retaining exact inference. We demonstrate the promise of our approach in a range of online regression and classification settings, Bayesian optimization, and active sampling to reduce error in malaria incidence forecasting. Code is available at https://github.com/wjmaddox/online_gp.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803075,0
C,"Vega, R; Gorji, P; Zhang, ZC; Qin, XB; Hareendranathan, AR; Kapur, J; Jaremko, JL; Greiner, R",,"Banerjee, A; Fukumizu, K",,"Vega, Roberto; Gorji, Pouneh; Zhang, Zichen; Qin, Xuebin; Hareendranathan, Abhilash Rakkunedeth; Kapur, Jeevesh; Jaremko, Jacob L.; Greiner, Russell",,,Sample Efficient Learning of Image-Based Diagnostic Classifiers Using Probabilistic Labels,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Deep learning approaches often require huge datasets to achieve good generalization. This complicates its use in tasks like image-based medical diagnosis, where the small training datasets are usually insufficient to learn appropriate data representations. For such sensitive tasks it is also important to provide the confidence in the predictions. Here, we propose a way to learn and use probabilistic labels to train accurate and calibrated deep networks from relatively small datasets. We observe gains of up to 22% in the accuracy of models trained with these labels, as compared with traditional approaches, in three classification tasks: diagnosis of hip dysplasia, fatty liver, and glaucoma. The outputs of models trained with probabilistic labels are calibrated, allowing the interpretation of its predictions as proper probabilities. We anticipate this approach will apply to other tasks where few training instances are available and expert knowledge can be encoded as probabilities.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,739,+,,,,,,,,,,,,,,,,WOS:000659893800083,0
C,"Zhang, JX; Bi, SR; Zhang, GN",,"Banerjee, A; Fukumizu, K",,"Zhang, Jiaxin; Bi, Sirui; Zhang, Guannan",,,A Scalable Gradient-Free Method for Bayesian Experimental Design with Implicit Models,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Bayesian experimental design (BED) is to answer the question that how to choose designs that maximize the information gathering. For implicit models, where the likelihood is intractable but sampling is possible, conventional BED methods have difficulties in efficiently estimating the posterior distribution and maximizing the mutual information (MI) between data and parameters. Recent work proposed the use of gradient ascent to maximize a lower bound on MI to deal with these issues. However, the approach requires a sampling path to compute the pathwise gradient of the MI lower bound with respect to the design variables, and such a pathwise gradient is usually inaccessible for implicit models. In this paper, we propose a novel approach that leverages recent advances in stochastic approximate gradient ascent incorporated with a smoothed variational MI estimator for efficient and robust BED. Without the necessity of pathwise gradients, our approach allows the design process to be achieved through a unified procedure with an approximate gradient for implicit models. Several experiments show that our approach outperforms baseline methods, and significantly improves the scalability of BED in high-dimensional problems.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804051,0
C,"Zhang, YX; Regol, F; Pal, S; Khan, S; Ma, LH; Coates, M",,"Banerjee, A; Fukumizu, K",,"Zhang, Yingxue; Regol, Florence; Pal, Soumyasundar; Khan, Sakif; Ma, Liheng; Coates, Mark",,,Detection and Defense of Topological Adversarial Attacks on Graphs,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Graph neural network (GNN) models achieve superior performance when classifying nodes in graph-structured data. Given that stateof-the-art GNNs share many similarities with their CNN cousins and that CNNs suffer adversarial vulnerabilities, there has also been interest in exploring analogous vulnerabilities in GNNs. Indeed, recent work has demonstrated that node classification performance of several graph models, including the popular graph convolution network (GCN) model, can be severely degraded through adversarial perturbations to the graph structure and the node features. In this work, we take a first step towards detecting adversarial attacks against graph models. We first propose a straightforward single node threshold test for detecting nodes subject to targeted attacks. Subsequently, we describe a kernelbased two-sample test for detecting whether a given subset of nodes within a graph has been maliciously corrupted. The efficacy of our algorithms is established via thorough experiments using commonly used node classification benchmark datasets. We also illustrate the potential practical benefit of our detection method by demonstrating its application to a real-world Bitcoin transaction network.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803059,0
C,"Lam, H; Li, FP; Prusty, S",,"Chiappa, S; Calandra, R",,"Lam, Henry; Li, Fengpei; Prusty, Siddharth",,,Robust Importance Weighting for Covariate Shift,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"In many learning problems, the training and testing data follow different distributions and a particularly common situation is the covariate shift. To correct for sampling biases, most approaches, including the popular kernel mean matching (KMM), focus on estimating the importance weights between the two distributions. Reweighting-based methods, however, are exposed to high variance when the distributional discrepancy is large and the weights are poorly estimated. On the other hand, the alternate approach of using nonparametric regression (NR) incurs high bias when the training size is limited. In this paper, we propose and analyze a new estimator that systematically integrates the residuals of NR with KMM reweighting, based on a control-variate perspective. The proposed estimator can be shown to either strictly outperform or match the best-known existing rates for both KMM and NR, and thus is a robust combination of both estimators. The experiments shows the estimator works well in practice.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,352,361,,,,,,,,,,,,,,,,WOS:000559931302002,0
C,"Mita, G; Papotti, P; Filippone, M; Michiardi, P",,"Chiappa, S; Calandra, R",,"Mita, Graziano; Papotti, Paolo; Filippone, Maurizio; Michiardi, Pietro",,,LIBRE: Learning Interpretable Boolean Rule Ensembles,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We present a novel method LIBRE to learn an interpretable classifier, which materializes as a set of Boolean rules. LIBRE uses an ensemble of bottom-up, weak learners operating on a random subset of features, which allows for the learning of rules that generalize well on unseen data even in imbalanced settings. Weak learners are combined with a simple union so that the final ensemble is also interpretable. Experimental results indicate that LIBRE efficiently strikes the right balance between prediction accuracy, which is competitive with black-box methods, and interpretability, which is often superior to alternative methods from the literature.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,245,254,,,,,,,,,,,,,,,,WOS:000559931302045,0
C,"Pooladian, AA; Finlay, C; Hoheisel, T; Oberman, AM",,"Chiappa, S; Calandra, R",,"Pooladian, Aram-Alexandre; Finlay, Chris; Hoheisel, Tim; Oberman, Adam M.",,,A principled approach for generating adversarial images under non-smooth dissimilarity metrics,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Deep neural networks perform well on real world data but are prone to adversarial perturbations: small changes in the input easily lead to misclassification. In this work, we propose an attack methodology not only for cases where the perturbations are measured by l(p) norms, but in fact any adversarial dissimilarity metric with a closed proximal form. This includes, but is not limited to, l(1), l(2), and l(infinity) perturbations; the l(0) counting norm (i.e. true sparseness); and the total variation seminorm, which is a (non-l(p)) convolutional dissimilarity measuring local pixel changes. Our approach is a natural extension of a recent adversarial attack method, and eliminates the differentiability requirement of the metric. We demonstrate our algorithm, ProxLogBarrier, on the MNIST, CIFAR10, and ImageNet-1k datasets. We consider undefended and defended models, and show that our algorithm easily transfers to various datasets. We observe that ProxLogBarrier outperforms a host of modern adversarial attacks specialized for the l(0) case. Moreover, by altering images in the total variation seminorm, we shed light on a new class of perturbations that exploit neighboring pixel information.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1442,1451,,,,,,,,,,,,,,,,WOS:000559931302078,0
C,"Ribeiro, AH; Tiels, K; Aguirre, LA; Schon, TB",,"Chiappa, S; Calandra, R",,"Ribeiro, Antonio H.; Tiels, Koen; Aguirre, Luis A.; Schoen, Thomas B.",,,Beyond exploding and vanishing gradients: analysing RNN training using attractors and smoothness,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"The exploding and vanishing gradient problem has been the major conceptual principle behind most architecture and training improvements in recurrent neural networks (RNNs) during the last decade. In this paper, we argue that this principle, while powerful, might need some refinement to explain recent developments. We refine the concept of exploding gradients by reformulating the problem in terms of the cost function smoothness, which gives insight into higher-order derivatives and the existence of regions with many close local minima. We also clarify the distinction between vanishing gradients and the need for the RNN to learn attractors to fully use its expressive power. Through the lens of these refinements, we shed new light on recent developments in the RNN field, namely stable RNN and unitary (or orthogonal) RNNs.",,,,,"Aguirre, Luis A./A-2737-2008","Aguirre, Luis A./0000-0002-2746-5102",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2370,2379,,,,,,,,,,,,,,,,WOS:000559931302091,0
C,"Saha, A; Gopalan, A",,"Chiappa, S; Calandra, R",,"Saha, Aadirupa; Gopalan, Aditya",,,Best-item Learning in Random Utility Models with Subset Choices,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We consider the problem of PAC learning the most valuable item from a pool of n items using sequential, adaptively chosen plays of subsets of k items, when, upon playing a subset, the learner receives relative feedback sampled according to a general Random Utility Model (RUM) with independent noise perturbations to the latent item utilities. We identify a new property of such a RUM, termed the minimum advantage, that helps in characterizing the complexity of separating pairs of items based on their relative win/loss empirical counts, and can be bounded as a function of the noise distribution alone. We give a learning algorithm for general RUMs, based on pairwise relative counts of items and hierarchical elimination, along with a new PAC sample complexity guarantee of O(n/c(2)epsilon(2) log k/delta) rounds to identify an 6-optimal item with confidence 1 - delta, when the worst case pairwise advantage in the RUM has sensitivity at least c to the parameter gaps of items. Fundamental lower bounds on PAC sample complexity show that this is near-optimal in terms of its dependence on n, k and c.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,4281,4290,,,,,,,,,,,,,,,,WOS:000559931300001,0
C,"Vankadara, LC; Ghoshdastidar, D",,"Chiappa, S; Calandra, R",,"Vankadara, Leena Chennuru; Ghoshdastidar, Debarghya",,,On the optimality of kernels for high-dimensional clustering,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"This paper studies the optimality of kernel methods in high-dimensional data clustering. Recent works have studied the large sample performance of kernel clustering in the high-dimensional regime, where Euclidean distance becomes less informative. However, it is unknown whether popular methods, such as kernel k-means, are optimal in this regime. We consider the problem of high-dimensional Gaussian clustering and show that, for a class of dot-product kernels, the sufficient conditions for partial recovery of clusters using the NP-hard kernel k-means objective matches the known information-theoretic limit up to a factor of root 2 for large k. It also exactly matches the known upper bounds for the non-kernel setting. We also show that a semidefinite relaxation of the kernel k-means procedure matches upto constant factors, the spectral threshold, below which no polynomial time algorithm is known to succeed. This is the first work that provides such optimality guarantees for the kernel k-means as well as its convex relaxation. Our proofs demonstrate the utility of the less known polynomial concentration results for random variables with exponentially decaying tails in higher-order analysis of kernel methods.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303053,0
C,"Wang, S; Gupta, M",,"Chiappa, S; Calandra, R",,"Wang, Serena; Gupta, Maya",,,Deontological Ethics By Monotonicity Shape Constraints,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We dernoustrate lrow easy it is for modern machine systems to violate common deontological ethical principles and social norms such as favor the less fortunate, and do not penalize good attributes. We propose that in some cases such ethical principles can be incorporated into a machine -learned model by adding shape constraints that constrain the model to respond only positively to relevant inputs. We analyze the relationship between these deontological constraints that act on individuals and the consequentialist group -based fairness goals of one-sided statistical parity and equal opportunity. This strategy works with sensitive attributes that are Boolean or real-valued such as income and age, and can help produce more responsible and trustworthy AL",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303063,0
C,"Wen, YM; Luk, K; Gazeau, M; Zhang, GD; Chan, H; Ba, J",,"Chiappa, S; Calandra, R",,"Wen, Yeming; Luk, Kevin; Gazeau, Maxime; Zhang, Guodong; Chan, Harris; Ba, Jimmy",,,An Empirical Study of Large-Batch Stochastic Gradient Descent with Structured Covariance Noise,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"The choice of batch-size in a stochastic optimization algorithm plays a substantial role for both optimization and generalization. Increasing the batch-size used typically improves optimization but degrades generalization. To address the problem of improving generalization while maintaining optimal convergence in large-batch training, we propose to add covariance noise to the gradients. We demonstrate that the learning performance of our method is more accurately captured by the structure of the covariance matrix of the noise rather than by the variance of gradients. Moreover, over the convex-quadratic, we prove in theory that it can be characterized by the Frobenius norm of the noise matrix. Our empirical studies with standard deep learning model-architectures and datasets shows that our method not only improves generalization performance in large-batch training, but furthermore, does so in a way where the optimization performance remains desirable and the training duration is not elongated.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303077,0
C,"Xu, YC; Chen, X; Singh, A; Dubrawski, A",,"Chiappa, S; Calandra, R",,"Xu, Yichong; Chen, Xi; Singh, Aarti; Dubrawski, Artur",,,Thresholding Bandit Problem with Both Duels and Pulls,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"The Thresholding Bandit Problem (TBP) aims to find the set of arms with mean rewards greater than a given threshold. We consider a new setting of TBP, where in addition to pulling arms, one can also duel two arms and get the arm with a greater mean. In our motivating application from crowdsourcing, dueling two arms can be more cost-effective and time-efficient than direct pulls. We refer to this problem as TBP with Dueling Choices (TBP-DC). This paper provides an algorithm called Rank-Search (RS) for solving TBP-DC by alternating between ranking and binary search. We prove theoretical guarantees for RS, and also give lower bounds to show the optimality of it. Experiments show that RS outperforms previous baseline algorithms that only use pulls or duels.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303088,0
C,"Yan, YM; Ailem, M; Sha, F",,"Chiappa, S; Calandra, R",,"Yan, Yiming; Ailem, Melissa; Sha, Fei",,,Amortized Inference of Variational Bounds for Learning Noisy-OR,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Classical approaches for approximate inference depend on cleverly designed variational distributions and bounds. Modern approaches employ amortized variational inference, which uses a neural network to approximate any posterior without leveraging the structures of the generative models. In this paper, we propose Amortized Conjugate Posterior (ACP), a hybrid approach taking advantages of both types of approaches. Specifically, we use the classical methods to derive specific forms of posterior distributions and then learn the variational parameters using amortized inference. We study the effectiveness of the proposed approach on the NOISY-OR model and compare to both the classical and the modern approaches for approximate inference and parameter learning. Our results show that the proposed method outperforms or are at par with other approaches.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303091,0
C,"Zhang, LJ; Lu, SY; Yang, TB",,"Chiappa, S; Calandra, R",,"Zhang, Lijun; Lu, Shiyin; Yang, Tianbao",,,Minimizing Dynamic Regret and Adaptive Regret Simultaneously,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Regret minimization is treated as the golden rule in the traditional study of online learning. However, regret minimization algorithms tend to converge to the static optimum, thus being suboptimal for changing environments. To address this limitation, new performance measures, including dynamic regret and adaptive regret have been proposed to guide the design of online algorithms. The former one aims to minimize the global regret with respect to a sequence of changing comparators, and the latter one attempts to minimize every local regret with respect to a fixed comparator. Existing algorithms for dynamic regret and adaptive regret are developed independently, and only target one performance measure. In this paper, we bridge this gap by proposing novel online algorithms that are able to minimize the dynamic regret and adaptive regret simultaneously. In fact, our theoretical guarantee is even stronger in the sense that one algorithm is able to minimize the dynamic regret over any interval.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,309,318,,,,,,,,,,,,,,,,WOS:000559931304007,0
C,"Eggeling, R; Viinikka, J; Vuoksenmaa, A; Koivisto, M",,"Chaudhuri, K; Sugiyama, M",,"Eggeling, Ralf; Viinikka, Jussi; Vuoksenmaa, Aleksis; Koivisto, Mikko",,,On Structure Priors for Learning Bayesian Networks,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"To learn a Bayesian network structure from data, one popular approach is to maximize a decomposable likelihood-based score. While various scores have been proposed, they usually assume a uniform prior, or penalty, over the possible directed acyclic graphs (DAGs); relatively little attention has been paid to alternative priors. We investigate empirically several structure priors in combination with different scores, using benchmark data sets and data sets generated from benchmark networks. Our results suggest that, in practice, priors that strongly favor sparsity perform significantly better than the uniform prior or even the informed variant that is conditioned on the correct number of parents for each node. For an analytic comparison of different priors, we generalize a known recurrence equation for the number of DAGs to accommodate modular weightings of DAGs, a result that is also of independent interest.",,,,,"Eggeling, Ralf/P-3403-2019","Eggeling, Ralf/0000-0002-3583-1029",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901076,0
C,"Hsu, H; Salamatian, S; Calmon, FP",,"Chaudhuri, K; Sugiyama, M",,"Hsu, Hsiang; Salamatian, Salman; Calmon, Flavio P.",,,Correspondence Analysis Using Neural Networks,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Correspondence analysis (CA) is a multivariate statistical tool used to visualize and interpret data dependencies. CA has found applications in fields ranging from epidemiology to social sciences. However, current methods used to perform CA do not scale to large, high-dimensional datasets. By reinterpreting the objective in CA using an information-theoretic tool called the principal inertia components, we demonstrate that performing CA is equivalent to solving a functional optimization problem over the space of finite variance functions of two random variable. We show that this optimization problem, in turn, can be efficiently approximated by neural networks. The resulting formulation, called the correspondence analysis neural network (CA-NN), enables CA to be performed at an unprecedented scale. We validate the CA-NN on synthetic data, and demonstrate how it can be used to perform CA on a variety of datasets, including food recipes, wine compositions, and images. Our results outperform traditional methods used in CA, indicating that CA-NN can serve as a new, scalable tool for interpretability and visualization of complex dependencies between random variables.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902074,0
C,"Johansson, FD; Sontag, D; Ranganath, R",,"Chaudhuri, K; Sugiyama, M",,"Johansson, Fredrik D.; Sontag, David; Ranganath, Rajesh",,,Support and Invertibility in Domain-Invariant Representations,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Learning domain-invariant representations has become a popular approach to unsupervised domain adaptation and is often justified by invoking a particular suite of theoretical results. We argue that there are two significant flaws in such arguments. First, the results in question hold only for a fixed representation and do not account for information lost in non-invertible transformations. Second, domain invariance is often a far too strict requirement and does not always lead to consistent estimation, even under strong and favorable assumptions. In this work, we give generalization bounds for unsupervised domain adaptation that hold for any representation function by acknowledging the cost of non-invertibility. In addition, we show that penalizing distance between densities is often wasteful and propose a bound based on measuring the extent to which the support of the source domain covers the target domain. We perform experiments on well-known benchmarks that illustrate the short-comings of current standard practice.",,,,,,"Johansson, Fredrik/0000-0002-4323-3715",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,527,536,,,,,,,,,,,,,,,,WOS:000509687900055,0
C,"Karimi, MR; Krause, A; Lattanzi, S; Vassilvitskii, S",,"Chaudhuri, K; Sugiyama, M",,"Karimi, Mohammad Reza; Krause, Andreas; Lattanzi, Silvio; Vassilvitskii, Sergei",,,Consistent Online Optimization: Convex and Submodular,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Modern online learning algorithms achieve low (sublinear) regret in a variety of diverse settings. These algorithms, however, update their solution at every time step. While these updates are computationally efficient, the very requirement of frequent updates makes the algorithms untenable in some practical applications. In this work, we develop online learning algorithms that update a sublinear number of times. We give a meta-algorithm based on non-homogeneous Poisson Processes that gives a smooth tradeoff between regret and frequency of updates. Empirically, we show that in many cases, we can significantly reduce updates at a minimal increase in regret.",,,,,,"Krause, Andreas/0000-0001-7260-9673",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902030,0
C,"Kerdreux, T; d'Aspremont, A; Pokutta, S",,"Chaudhuri, K; Sugiyama, M",,"Kerdreux, Thomas; d'Aspremont, Alexandre; Pokutta, Sebastian",,,Restarting Frank-Wolfe,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Conditional Gradients (aka Frank-Wolfe algorithms) form a classical set of methods for constrained smooth convex minimization due to their simplicity, the absence of projection step, and competitive numerical performance. While the vanilla Frank-Wolfe algorithm only ensures a worst-case rate of O(1/epsilon), various recent results have shown that for strongly convex functions, the method can be slightly modified to achieve linear convergence. However, this still leaves a huge gap between sublinear O(1/epsilon) convergence and linear O(log 1/epsilon) convergence to reach an epsilon-approximate solution. Here, we present a new variant of Conditional Gradients, that can dynamically adapt to the function's geometric properties using restarts and thus smoothly interpolates between the sublinear and linear regimes.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901033,0
C,"Park, G; Park, H",,"Chaudhuri, K; Sugiyama, M",,"Park, Gunwoong; Park, Hyewon",,,Identifiability of Generalized Hypergeometric Distribution (GHD) Directed Acyclic Graphical Models,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We introduce a new class of identifiable DAG models where the conditional distribution of each node given its parents belongs to a family of generalized hypergeometric distributions (GHD). A family of generalized hypergeometric distributions includes a lot of discrete distributions such as the binomial, Beta-binomial, negative binomial, Poisson, hyper-Poisson, and many more. We prove that if the data drawn from the new class of DAG models, one can fully identify the graph structure. We further present a reliable and polynomial-time algorithm that recovers the graph from finitely many data. We show through theoretical results and numerical experiments that our algorithm is statistically consistent in high-dimensional settings (p > n) if the indegree of the graph is bounded, and out-performs state-of-the-art DAG learning algorithms.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,158,166,,,,,,,,,,,,,,,,WOS:000509687900017,0
C,"Rabusseau, G; Li, TY; Precup, D",,"Chaudhuri, K; Sugiyama, M",,"Rabusseau, Guillaume; Li, Tianyu; Precup, Doina",,,Connecting Weighted Automata and Recurrent Neural Networks through Spectral Learning,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"In this paper, we unravel a fundamental connection between weighted finite automata (WFAs) and second-order recurrent neural networks (2-RNNs): in the case of sequences of discrete symbols, WFAs and 2-RNNs with linear activation functions are expressively equivalent. Motivated by this result, we build upon a recent extension of the spectral learning algorithm to vector-valued WFAs and propose the first provable learning algorithm for linear 2-RNNs defined over sequences of continuous input vectors. This algorithm relies on estimating low rank subblocks of the so-called Hankel tensor, from which the parameters of a linear 2-RNN can be provably recovered. The performances of the proposed method are assessed in a simulation study.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901070,0
C,"Stojanov, P; Gong, MM; Carbonell, JG; Zhang, K",,"Chaudhuri, K; Sugiyama, M",,"Stojanov, Petar; Gong, Mingming; Carbonell, Jaime G.; Zhang, Kun",,,Data-Driven Approach to Multiple-Source Domain Adaptation,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"A key problem in domain adaptation is determining what to transfer across different domains. We propose a data-driven method to represent these changes across multiple source domains and perform unsupervised domain adaptation. We assume that the joint distributions follow a specific generating process and have a small number of identifiable changing parameters, and develop a data-driven method to identify the changing parameters by learning low-dimensional representations of the changing class-conditional distributions across multiple source domains. The learned low-dimensional representations enable us to reconstruct the target-domain joint distribution from unlabeled target-domain data, and further enable predicting the labels in the target domain. We demonstrate the efficacy of this method by conducting experiments on synthetic and real datasets.",,,,,"Stojanov, Petar/GQI-4462-2022","Stojanov, Petar/0000-0001-7815-776X",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687903056,0
C,"Suggala, AS; Prasad, A; Nagarajan, V; Ravikumar, P",,"Chaudhuri, K; Sugiyama, M",,"Suggala, Arun Sai; Prasad, Adarsh; Nagarajan, Vaishnavh; Ravikumar, Pradeep",,,Revisiting Adversarial Risk,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Recent works on adversarial perturbations show that there is an inherent trade-off between standard test accuracy and adversarial accuracy. Specifically, they show that no classifier can simultaneously be robust to adversarial perturbations and achieve high standard test accuracy. However, this is contrary to the standard notion that on tasks such as image classification, humans are robust classifiers with low error rate. In this work, we show that the main reason behind this confusion is the inexact definition of adversarial perturbation that is used in the literature. To fix this issue, we propose a slight, yet important modification to the existing definition of adversarial perturbation. Based on the modified definition, we show that there is no trade-off between adversarial and standard accuracies; there exist classifiers that are robust and achieve high standard accuracy. We further study several properties of this new definition of adversarial risk and its relation to the existing definition.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902039,0
C,"Nguyen, TV; Wong, RKW; Hegde, C",,"Chaudhuri, K; Sugiyama, M",,"Thanh V Nguyen; Wong, Raymond K. W.; Hegde, Chinmay",,,On the Dynamics of Gradient Descent for Autoencoders,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We provide a series of results for unsupervised learning with autoencoders. Specifically, we study shallow two-layer autoencoder architectures with shared weights. We focus on three generative models for data that are common in statistical machine learning: (i) the mixture-of-gaussians model, (ii) the sparse coding model, and (iii) the sparsity model with non-negative coefficients. For each of these models, we prove that under suitable choices of hyperparameters, architectures, and initialization, autoencoders learned by gradient descent can successfully recover the parameters of the corresponding model. To our knowledge, this is the first result that rigorously studies the dynamics of gradient descent for weight-sharing autoencoders. Our analysis can be viewed as theoretical evidence that shallow autoencoder modules indeed can be used as feature learning mechanisms for a variety of data models, and may shed insight on how to train larger stacked architectures with autoencoders as basic building blocks.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902093,0
C,"Weber, T; Heess, N; Buesing, L; Silver, D",,"Chaudhuri, K; Sugiyama, M",,"Weber, Theophane; Heess, Nicolas; Buesing, Lars; Silver, David",,,Credit Assignment Techniques in Stochastic Computation Graphs,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Stochastic computation graphs (SCGs) provide a formalism to represent structured optimization problems arising in artificial intelligence, including supervised, unsupervised, and reinforcement learning. Previous work has shown that an unbiased estimator of the gradient of the expected loss of SCGs can be derived from a single principle. However, this estimator often has high variance and requires a full model evaluation per data point, making this algorithm costly in large graphs. In this work, we address these problems by generalizing concepts from the reinforcement learning literature. We introduce the concepts of value functions, baselines and critics for arbitrary SCGs, and show how to use them to derive lower-variance gradient estimates from partial model evaluations, paving the way towards general and efficient credit assignment for gradient-based optimization. In doing so, we demonstrate how our results unify recent advances in the probabilistic inference and reinforcement learning literature.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902072,0
C,"Xie, JH; Hang, C; Shen, ZB; Mi, C; Qian, H",,"Chaudhuri, K; Sugiyama, M",,"Xie, Jiahao; Hang, Chao Z.; Shen, Zebang; Mi, Chao; Qian, Hui",,,lDecentralized Gradient Tracking for Continuous DR-Submodular Maximization,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"In this paper, we focus on the continuous DR-submodular maximization over a network. By using the gradient tracking technique, two decentralized algorithms are proposed for deterministic and stochastic settings, respectively. The proposed methods attain the epsilon-accuracy tight approximation ratio for monotone continuous DR-submodular functions in only O(1/epsilon) and (O) over tilde (1/epsilon) rounds of communication, respectively, which are superior to the state-of-the-art. Our numerical results show that the proposed methods outperform existing decentralized methods in terms of both computation and communication complexity.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902097,0
C,"Zhang, YC; Liang, P",,"Chaudhuri, K; Sugiyama, M",,"Zhang, Yuchen; Liang, Percy",,,Defending against Whitebox Adversarial Attacks via Randomized Discretization,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Adversarial perturbations dramatically decrease the accuracy of state-of-the-art image classifiers. In this paper, we propose and analyze a simple and computationally efficient defense strategy: inject random Gaussian noise, discretize each pixel, and then feed the result into any pre-trained classifier. Theoretically, we show that our randomized discretization strategy reduces the KL divergence between original and adversarial inputs, leading to a lower bound on the classification accuracy of any classifier against any (potentially whitebox) l(infinity)-bounded adversarial attack. Empirically, we evaluate our defense on adversarial examples generated by a strong iterative PGD attack. On ImageNet, our defense is more robust than adversarially-trained networks and the winning defenses of the NIPS 2017 Adversarial Attacks & Defenses competition.",,,,,"Zhang, Yuchen/GYI-8858-2022",,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,684,693,,,,,,,,,,,,,,,,WOS:000509687900071,0
C,"Chen, CF; Rudin, C",,"Storkey, A; PerezCruz, F",,"Chen, Chaofan; Rudin, Cynthia",,,An Optimization Approach to Learning Falling Rule Lists,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"A falling rule list is a probabilistic decision list for binary classification, consisting of a series of if-then rules with antecedents in the if clauses and probabilities of the desired outcome (1) in the then clauses. Just as in a regular decision list, the order of rules in a falling rule list is important - each example is classified by the first rule whose antecedent it satisfies. Unlike a regular decision list, a falling rule list requires the probabilities of the desired outcome (1) to be monotonically decreasing down the list. We propose an optimization approach to learning falling rule lists and softly falling rule lists, along with Monte-Carlo search algorithms that use bounds on the optimal solution to prune the search space.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300064,0
C,"Chien, I; Lin, CY; Wang, IH",,"Storkey, A; PerezCruz, F",,"Chien, I. (Eli); Lin, Chung-Yi; Wang, I-Hsiang",,,Community Detection in Hypergraphs: Optimal Statistical Limit and Efficient Algorithms,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"In this paper, community detection in hypergraphs is explored. Under a generative hypergraph model called d-wise hypergraph stochastic block model (d-hSBM) which naturally extends the Stochastic Block Model (SBM) from graphs to d-uniform hypergraphs, the fundamental limit on the asymptotic minimax misclassified ratio is characterized. For proving the achievability, we propose a two-step polynomial time algorithm that provably achieves the fundamental limit in the sparse hypergraph regime. For proving the optimality, the lower bound of the minimax risk is set by finding a smaller parameter space which contains the most dominant error events, inspired by the analysis in the achievability part. It turns out that the minimax risk decays exponentially fast to zero as the number of nodes tends to infinity, and the rate function is a weighted combination of several divergence terms, each of which is the Renyi divergence of order 1/2 between two Bernoulli distributions. The Bernoulli distributions involved in the characterization of the rate function are those governing the random instantiation of hyperedges in d-hSBM. Experimental results on both synthetic and real-world data validate our theoretical finding.",,,,,"Wang, I-Hsiang/AAD-8383-2021; Chien, Eli/AHA-3502-2022",,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300092,0
C,"Guo, H; Kara, K; Zhang, C",,"Storkey, A; PerezCruz, F",,"Guo, Heng; Kara, Kaan; Zhang, Ce",,,Layerwise Systematic Scan: Deep Boltzmann Machines and Beyond,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"For Markov chain Monte Carlo methods, one of the greatest discrepancies between theory and system is the scan order - while most theoretical development on the mixing time analysis deals with random updates, real-world systems are implemented with systematic scans. We bridge this gap for models that exhibit a bipartite structure, including, most notably, the Restricted/Deep Boltzmann Machine. The de facto implementation for these models scans variables in a layer-wise fashion. We show that the Gibbs sampler with a layer-wise alternating scan order has its relaxation time (in terms of epochs) no larger than that of a random-update Gibbs sampler (in terms of variable updates). We also construct examples to show that this bound is asymptotically tight. Through standard inequalities, our result also implies a comparison on the mixing times.",,,,,,"Zhang, Ce/0000-0002-8105-7505",,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300020,0
C,"Herlands, W; McFowland, E; Wilson, AG; Neill, DB",,"Storkey, A; PerezCruz, F",,"Herlands, William; McFowland, Edward, III; Wilson, Andrew G.; Neill, Daniel B.",,,Gaussian Process Subset Scanning for Anomalous Pattern Detection in Non-iid Data,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Identifying anomalous patterns in real-world data is essential for understanding where, when, and how systems deviate from their expected dynamics. Yet methods that separately consider the anomalousness of each individual data point have low detection power for subtle, emerging irregularities. Additionally, recent detection techniques based on subset scanning make strong independence assumptions and suffer degraded performance in correlated data. We introduce methods for identifying anomalous patterns in non-iid data by combining Gaussian processes with novel log-likelihood ratio statistic and subset scanning techniques. Our approaches are powerful, interpretable, and can integrate information across multiple data streams. We illustrate their performance on numeric simulations and three open source spatiotemporal datasets of opioid overdose deaths, 311 calls, and storm reports.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300045,0
C,"Hughes, MC; Hope, G; Weiner, L; McCoy, TH; Perlis, RH; Sudderth, E; Doshi-Velez, F",,"Storkey, A; PerezCruz, F",,"Hughes, Michael C.; Hope, Gabriel; Weiner, Leah; McCoy, Thomas H., Jr.; Perlis, Roy H.; Sudderth, Erik; Doshi-Velez, Finale",,,Semi-Supervised Prediction-Constrained Topic Models,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Supervisory signals can help topic models discover low-dimensional data representations which are useful for a specific prediction task. We propose a framework for training supervised latent Dirichlet allocation that balances two goals: faithful generative explanations of high-dimensional data and accurate prediction of associated class labels. Existing approaches fail to balance these goals by not properly handling a fundamental asymmetry: the intended application is always predicting labels from data, not data from labels. Our new prediction-constrained objective for training generative models coherently integrates supervisory signals even when only a small fraction of training examples are labeled. We demonstrate improved prediction quality compared to previous supervised topic models, achieving results competitive with high-dimensional logistic regression on text analysis and electronic health records tasks while simultaneously learning interpretable topics.",,,,,"Hughes, Michael C./AAO-7155-2021","Hughes, Michael C./0000-0003-4859-7400",,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300112,0
C,"Krishnan, RG; Liang, DW; Hoffman, MD",,"Storkey, A; PerezCruz, F",,"Krishnan, Rahul G.; Liang, Dawen; Hoffman, Matthew D.",,,"On the challenges of learning with inference networks on sparse, high-dimensional data","INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"We study parameter estimation in Nonlinear Factor Analysis (NFA) where the generative model is parameterized by a deep neural network. Recent work has focused on learning such models using inference (or recognition) networks; we identify a crucial problem when modeling large, sparse, high-dimensional datasets {underfitting. We study the extent of underfitting, highlighting that its severity increases with the sparsity of the data. We propose methods to tackle it via iterative optimization inspired by stochastic variational inference [Hoffman et al., 2013] and improvements in the data representation used for inference. The proposed techniques drastically improve the ability of these powerful models to fit sparse data, achieving state-of-the-art results on a benchmark text-count dataset and excellent results on the task of top-N recommendation.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300016,0
C,"Lim, WS; Du, RD; Dai, B; Jung, KM; Song, L; Park, H",,"Storkey, A; PerezCruz, F",,"Lim, Woosang; Du, Rundong; Dai, Bo; Jung, Kyomin; Song, Le; Park, Haesun",,,Multi-scale Nystrom Method,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Kernel methods are powerful tools for modeling nonlinear data. However, the amount of computation and memory required for kernel methods becomes the bottleneck when dealing with large-scale problems. In this paper, we propose Nested Nystrom Method (NNM) which achieves a delicate balance between the approximation accuracy and computational efficiency by exploiting the multilayer structure and multiple compressions. Even when the size of the kernel matrix is very large, NNM consistently decomposes very small matrices to update the eigen-decomposition of the kernel matrix. We theoretically show that NNM implicitly updates the principal sub-space through the multiple layers, and also prove that its corresponding errors of rank-k PSD matrix approximation and kernel PCA (KPCA) are decreased by using additional sublayers before the final layer. Finally, we empirically demonstrate the decreasing property of errors of NNM with the additional sublayers through the experiments on the constructed kernel matrices of real data sets, and show that NNM effectively controls the efficiency both for rank-k PSD matrix approximation and KPCA.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300008,0
C,"Mussmann, S; Liang, P",,"Storkey, A; PerezCruz, F",,"Mussmann, Stephen; Liang, Percy",,,Generalized Binary Search For Split-Neighborly Problems,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"In sequential hypothesis testing, Generalized Binary Search (GBS) greedily chooses the test with the highest information gain at each step. It is known that GBS obtains the gold standard query cost of O(log n) for problems satisfying the k-neighborly condition, which requires any two tests to be connected by a sequence of tests where neighboring tests disagree on at most k hypotheses. In this paper, we introduce a weaker condition, split-neighborly, which requires that for the set of hypotheses two neighbors disagree on, any subset is splittable by some test. For four problems that are not k-neighborly for any constant k, we prove that they are splitneighborly, which allows us to obtain the optimal O(log n) worst-case query cost.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300163,0
C,"Shang, FH; Liu, YY; Zhou, KW; Cheng, J; Ng, KKW; Yoshida, Y",,"Storkey, A; PerezCruz, F",,"Shang, Fanhua; Liu, Yuanyuan; Zhou, Kaiwen; Cheng, James; Ng, Kelvin K. W.; Yoshida, Yuichi",,,Guaranteed Sufficient Decrease for Stochastic Variance Reduced Gradient Optimization,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"In this paper, we propose a novel sufficient decrease technique for stochastic variance reduced gradient descent methods such as SVRG and SAGA. In order to make sufficient decrease for stochastic optimization, we design a new sufficient decrease criterion, which yields sufficient decrease versions of stochastic variance reduction algorithms such as SVRG-SD and SAGA-SD as a byproduct. We introduce a coefficient to scale current iterate and to satisfy the sufficient decrease property, which takes the decisions to shrink, expand or even move in the opposite direction, and then give two specific update rules of the coefficient for Lasso and ridge regression. Moreover, we analyze the convergence properties of our algorithms for strongly convex problems, which show that our algorithms attain linear convergence rates. We also provide the convergence guarantees of our algorithms for non-strongly convex problems. Our experimental results further verify that our algorithms achieve significantly better performance than their counterparts.",,,,,"liu, yuanyuan/GWZ-5838-2022",,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300108,0
C,"Turgay, E; Oner, D; Tekin, C",,"Storkey, A; PerezCruz, F",,"Turgay, Eralp; Oner, Doruk; Tekin, Cem",,,Multi-objective Contextual Bandit Problem with Similarity Information,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"In this paper we propose the multi-objective contextual bandit problem with similarity information. This problem extends the classical contextual bandit problem with similarity information by introducing multiple and possibly conflicting objectives. Since the best arm in each objective can be different given the context, learning the best arm based on a single objective can jeopardize the rewards obtained from the other objectives. In order to evaluate the performance of the learner in this setup, we use a performance metric called the contextual Pareto regret. Essentially, the contextual Pareto regret is the sum of the distances of the arms chosen by the learner to the context dependent Pareto front. For this problem, we develop a new online learning algorithm called Pareto Contextual Zooming (PCZ), which exploits the idea of contextual zooming to learn the arms that are close to the Pareto front for each observed context by adaptively partitioning the joint context-arm set according to the observed rewards and locations of the contextarm pairs selected in the past. Then, we prove that PCZ achieves (O) over tilde (T(1+dp)/(2+dp)) Pareto regret where d(p) is the Pareto zooming dimension that depends on the size of the set of near-optimal context-arm pairs. Moreover, we show that this regret bound is nearly optimal by providing an almost matching Omega(T(1+dp)/(2+dp)) lower bound.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300175,0
C,"Wang, ZY; Zhou, RD; Shen, C",,"Storkey, A; PerezCruz, F",,"Wang, Zhiyang; Zhou, Ruida; Shen, Cong",,,Regional Multi-Armed Bandits,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"We consider a variant of the classic multi-armed bandit problem where the expected reward of each arm is a function of an unknown parameter. The arms are divided into different groups, each of which has a common parameter. Therefore, when the player selects an arm at each time slot, information of other arms in the same group is also revealed. This regional bandit model naturally bridges the non-informative bandit setting where the player can only learn the chosen arm, and the global bandit model where sampling one arms reveals information of all arms. We propose an efficient algorithm, UCB-g, that solves the regional bandit problem by combining the Upper Confidence Bound (UCB) and greedy principles. Both parameter-dependent and parameter-free regret upper bounds are derived. We also establish a matching lower bound, which proves the order-optimality of UCB-g. Moreover, we propose SW-UCB-g, which is an extension of UCB-g for a non-stationary environment where the parameters slowly vary over time.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300054,0
C,"Yin, D; Pananjady, A; Lam, M; Papailiopoulos, D; Ramchandran, K; Bartlett, PL",,"Storkey, A; PerezCruz, F",,"Yin, Dong; Pananjady, Ashwin; Lam, Max; Papailiopoulos, Dimitris; Ramchandran, Kannan; Bartlett, Peter L.",,,Gradient Diversity: a Key Ingredient for Scalable Distributed Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"It has been experimentally observed that distributed implementations of mini-batch stochastic gradient descent (SGD) algorithms exhibit speedup saturation and decaying generalization ability beyond a particular batchsize. In this work, we present an analysis hinting that high similarity between concurrently processed gradients may be a cause of this performance degradation. We introduce the notion of gradient diversity that measures the dissimilarity between concurrent gradient updates, and show its key role in the convergence and generalization performance of mini-batch SGD. We also establish that heuristics similar to DropConnect, Langevin dynamics, and quantization, are provably diversity-inducing mechanisms, and provide experimental evidence indicating that these mechanisms can indeed enable the use of larger batches without sacrificing accuracy and lead to faster training in distributed learning. For example, in one of our experiments, for a convolutional neural network to reach 95% training accuracy on MNIST, using the diversity-inducing mechanism can reduce the training time by 30% in the distributed setting.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300209,0
C,"Abbasi-Yadkori, Y; Bartlett, PL; Wright, SJ",,"Gretton, A; Robert, CC",,"Abbasi-Yadkori, Yasin; Bartlett, Peter L.; Wright, Stephen J.",,,A Fast and Reliable Policy Improvement Algorithm,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"We introduce a simple, efficient method that improves stochastic policies for Markov decision processes. The computational complexity is the same as that of the value estimation problem. We prove that when the value estimation error is small, this method gives an improvement in performance that increases with certain variance properties of the initial policy and transition dynamics. Performance in numerical experiments compares favorably with previous policy improvement algorithms.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,1338,1346,,,,,,,,,,,,,,,,WOS:000508662100145,0
C,"Carpentier, A; Valko, M",,"Gretton, A; Robert, CC",,"Carpentier, Alexandra; Valko, Michal",,,Revealing graph bandits for maximizing local influence,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"We study a graph bandit setting where the objective of the learner is to detect the most influential node of a graph by requesting as little information from the graph as possible. One of the relevant applications for this setting is marketing in social networks, where the marketer aims at finding and taking advantage of the most influential customers. The existing approaches for bandit problems on graphs require either partial or complete knowledge of the graph. In this paper, we do not assume any knowledge of the graph, but we consider a setting where it can be gradually discovered in a sequential and active way. At each round, the learner chooses a node of the graph and the only information it receives is a stochastic set of the nodes that the chosen node is currently influencing. To address this setting, we propose BARE, a bandit strategy for which we prove a regret guarantee that scales with the detectable dimension, a problem dependent quantity that is often much smaller than the number of nodes.",,,,,"Carpentier, Alexandra/R-8179-2018","Carpentier, Alexandra/0000-0002-1194-7385",,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,10,18,,,,,,,,,,,,,,,,WOS:000508662100002,0
C,"Chaudhuri, S; Tewari, A",,"Gretton, A; Robert, CC",,"Chaudhuri, Sougata; Tewari, Ambuj",,,Online Learning to Rank with Feedback at the Top,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"We consider an online learning to rank setting in which, at each round, an oblivious adversary generates a list of m documents, pertaining to a query, and the learner ranks the documents according to assigned scores. The adversary then generates a relevance vector and the learner updates its ranker according to the feedback received. We consider the setting where the feedback is restricted to be the relevance levels of only the top k documents in the ranked list, for k << m. However, the performance of learner is judged based on the unrevealed full relevance vectors, using an appropriate learning to rank loss function. We develop efficient algorithms for well known losses in the pointwise, pairwise and listwise families. We also prove that no online algorithm can have sublinear regret, with top 1 feedback, for any loss that is calibrated with respect to NDCG. We apply our algorithms on benchmark datasets demonstrating efficient online learning of a ranking function from highly restricted feedback.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,277,285,,,,,,,,,,,,,,,,WOS:000508662100031,0
C,"Filan, D; Leike, J; Hutter, M",,"Gretton, A; Robert, CC",,"Filan, Daniel; Leike, Jan; Hutter, Marcus",,,Loss Bounds and Time Complexity for Speed Priors,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"This paper establishes for the first time the predictive performance of speed priors and their computational complexity. A speed prior is essentially a probability distribution that puts low probability on strings that are not efficiently computable. We propose a variant to the original speed prior (Schmidhuber, 2002), and show that our prior can predict sequences drawn from probability measures that are estimable in polynomial time. Our speed prior is computable in doubly-exponential time, but not in polynomial time. On a polynomial time computable sequence our speed prior is computable in exponential time. We show better upper complexity bounds for Schmidhuber's speed prior under the same conditions, and that it predicts deterministic sequences that are computable in polynomial time; however, we also show that it is not computable in polynomial time, and the question of its predictive properties for stochastic sequences remains open.",,,,,,"Hutter, Marcus/0000-0002-3263-4097",,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,1394,1402,,,,,,,,,,,,,,,,WOS:000508662100151,0
C,"Jaffe, A; Fetaya, E; Nadler, B; Jiang, TT; Kluger, Y",,"Gretton, A; Robert, CC",,"Jaffe, Ariel; Fetaya, Ethan; Nadler, Boaz; Jiang, Tingting; Kluger, Yuval",,,Unsupervised Ensemble Learning with Dependent Classifiers,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"In unsupervised ensemble learning, one obtains predictions from multiple sources or classifiers, yet without knowing the reliability and expertise of each source, and with no labeled data to assess it. The task is to combine these possibly conflicting predictions into an accurate meta-learner. Most works to date assumed perfect diversity between the different sources, a property known as conditional independence. In realistic scenarios, however, this assumption is often violated, and ensemble learners based on it can be severely sub-optimal. The key challenges we address in this paper are: (i) how to detect, in an unsupervised manner, strong violations of conditional independence; and (ii) construct a suitable meta-learner. To this end we introduce a statistical model that allows for dependencies between classifiers. Based on this model, we develop novel unsupervised methods to detect strongly dependent classifiers, better estimate their accuracies, and construct an improved meta-learner. Using both artificial and real datasets, we showcase the importance of taking classifier dependencies into account and the competitive performance of our approach.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,351,360,,,,,,,,,,,,,,,,WOS:000508662100039,0
C,"Sedghi, H; Janzamin, M; Anandkumar, A",,"Gretton, A; Robert, CC",,"Sedghi, Hanie; Janzamin, Majid; Anandkumar, Anima",,,Provable Tensor Methods for Learning Mixtures of Generalized Linear Models,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"We consider the problem of learning mixtures of generalized linear models (GLM) which arise in classification and regression problems. Typical learning approaches such as expectation maximization (EM) or variational Bayes can get stuck in spurious local optima. In contrast, we present a tensor decomposition method which is guaranteed to correctly recover the parameters. The key insight is to employ certain feature transformations of the input, which depend on the input generative model. Specifically, we employ score function tensors of the input and compute their cross-correlation with the response variable. We establish that the decomposition of this tensor consistently recovers the parameters, under mild non-degeneracy conditions. We demonstrate that the computational and sample complexity of our method is a low order polynomial of the input and the latent dimensions.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,1223,1231,,,,,,,,,,,,,,,,WOS:000508662100133,0
C,"Wang, LX; Ren, X; Gu, QQ",,"Gretton, A; Robert, CC",,"Wang, Lingxiao; Ren, Xiang; Gu, Quanquan",,,Precision Matrix Estimation in High Dimensional Gaussian Graphical Models with Faster Rates,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"We present a new estimator for precision matrix in high dimensional Gaussian graphical models. At the core of the proposed estimator is a collection of node-wise linear regression with nonconvex penalty. In contrast to existing estimators for Gaussian graphical models with O(s root log d/n) estimation error bound in terms of spectral norm, where s is the maximum degree of a graph, the proposed estimator could attain O(s/root n + root log d/n) spectral norm based convergence rate in the best case, and it is no worse than exiting estimators in general. In addition, our proposed estimator enjoys the oracle property under a milder condition than existing estimators. We show through extensive experiments on both synthetic and real datasets that our estimator outperforms the state-of-the-art estimators.",,,,,,"Wang, Lingxiao/0000-0002-8798-692X",,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,177,185,,,,,,,,,,,,,,,,WOS:000508662100020,0
C,"Cohn, R; Singh, S; Durfee, E",,"Kaski, S; Corander, J",,"Cohn, Robert; Singh, Satinder; Durfee, Edmund",,,Characterizing EVOI-Sufficient k-Response Query Sets in Decision Problems,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"In finite decision problems where an agent can query its human user to obtain information about its environment before acting, a query's usefulness is in terms of its Expected Value of Information (EVOI). The usefulness of a query set is similarly measured in terms of the EVOI of the queries it contains. When the only constraint on what queries can be asked is that they have exactly k possible responses (with k >= 2), we show that the set of k-response decision queries (which ask the user to select his/her preferred decision given a choice of k decisions) is EVOI-Sufficient, meaning that no single k-response query can have higher EVOI than the best single k-response decision query for any decision problem. When multiple queries can be asked before acting, we provide a negative result that shows the set of depth-n query trees constructed from k-response decision queries is not EVOI-Sufficient. However, we also provide a positive result that the set of depth-n query trees constructed from k-response decision-set queries, which ask the user to select from among k sets of decisions as to which set contains the best decision, is EVOI-Sufficient. We conclude with a discussion and analysis of algorithms that draws on a connection to other recent work on decision-theoretic knowledge elicitation.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,131,139,,,,,,,,,,,,,,,,WOS:000508355800015,0
C,"Oliva, JB; Poczos, B; Verstynen, T; Singh, A; Schneider, J; Yeh, FC; Tseng, WY",,"Kaski, S; Corander, J",,"Oliva, Junier B.; Poczos, Barnabas; Verstynen, Timothy; Singh, Aarti; Schneider, Jeff; Yeh, Fang-Cheng; Tseng, Wen-Yih",,,FuSSO: Functional Shrinkage and Selection Operator,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"We present the FuSSO, a functional analogue to the LASSO, that efficiently finds a sparse set of functional input covariates to regress a real-valued response against. The FuSSO does so in a semi-parametric fashion, making no parametric assumptions about the nature of input functional covariates and assuming a linear form to the mapping of functional covariates to the response. We provide a statistical backing for use of the FuSSO via proof of asymptotic sparsistency under various conditions. Furthermore, we observe good results on both synthetic and real-world data.",,,,,"Yeh, Fang-Cheng/E-9698-2012","Yeh, Fang-Cheng/0000-0002-7946-2173; Oliva, Junier/0000-0002-2601-5652; Tseng, Wen-Yih Isaac/0000-0002-2314-6868; Verstynen, Timothy/0000-0003-4720-0336",,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,715,723,,,,,,,,,,,,,,,,WOS:000508355800079,0
C,"Yang, E; Baker, Y; Ravikumar, P; Allen, GI; Liu, ZD",,"Kaski, S; Corander, J",,"Yang, Eunho; Baker, Yulia; Ravikumar, Pradeep; Allen, Genevera, I; Liu, Zhandong",,,Mixed Graphical Models via Exponential Families,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"Markov Random Fields, or undirected graphical models are widely used to model high-dimensional multivariate data. Classical instances of these models, such as Gaussian Graphical and Ising Models, as well as recent extensions (Yang et al., 2012) to graphical models specified by univariate exponential families, assume all variables arise from the same distribution. Complex data from high-throughput genomics and social networking for example, often contain discrete, count, and continuous variables measured on the same set of samples. To model such heterogeneous data, we develop a novel class of mixed graphical models by specifying that each node-conditional distribution is a member of a possibly different univariate exponential family. We study several instances of our model, and propose scalable M-estimators for recovering the underlying network structure. Simulations as well as an application to learning mixed genomic networks from next generation sequencing and mutation data demonstrate the versatility of our methods.",,,,,"Yang, Eunho/K-8395-2016","Liu, Zhandong/0000-0002-7608-0831",,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,1042,1050,,,,,,,,,,,,,,,,WOS:000508355800115,0
C,"Benard, C; Biau, G; Da Veiga, S; Scornet, E",,"Banerjee, A; Fukumizu, K",,"Benard, Clement; Biau, Gerard; Da Veiga, Sebastien; Scornet, Erwan",,,Interpretable Random Forests via Rule Extraction,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We introduce SIRUS (Stable and Interpretable RUle Set) for regression, a stable rule learning algorithm, which takes the form of a short and simple list of rules. State-of-the-art learning algorithms are often referred to as black boxes because of the high number of operations involved in their prediction process. Despite their powerful predictivity, this lack of interpretability may be highly restrictive for applications with critical decisions at stake. On the other hand, algorithms with a simple structure-typically decision trees, rule algorithms, or sparse linear models-are well known for their instability. This undesirable feature makes the conclusions of the data analysis unreliable and turns out to be a strong operational limitation. This motivates the design of SIRUS, based on random forests, which combines a simple structure, a remarkable stable behavior when data is perturbed, and an accuracy comparable to its competitors. We demonstrate the efficiency of the method both empirically (through experiments) and theoretically (with the proof of its asymptotic stability). A R/C++ software implementation sirus is available from CRAN.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801018,0
C,"Bennett, A; Kallus, N; Li, LH; Mousavi, A",,"Banerjee, A; Fukumizu, K",,"Bennett, Andrew; Kallus, Nathan; Li, Lihong; Mousavi, Ali",,,Off-policy Evaluation in Infinite-horizon Reinforcement Learning with Latent Confounders,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Off-policy evaluation (OPE) in reinforcement learning is an important problem in settings where experimentation is limited, such as education and healthcare. But, in these very same settings, observed actions are often confounded by unobserved variables making OPE even more difficult. We study an OPE problem in an infinite-horizon, ergodic Markov decision process with unobserved confounders, where states and actions can act as proxies for the unobserved confounders. We show how, given only a latent variable model for states and actions, policy value can be identified from off-policy data. Our method involves two stages. In the first, we show how to use proxies to estimate stationary distribution ratios, extending recent work on breaking the curse of horizon to the confounded setting. In the second, we show optimal balancing can be combined with such learned ratios to obtain policy value while avoiding direct modeling of reward functions. We establish theoretical guarantees of consistency, and benchmark our method empirically.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802042,0
C,"Bhattacharyya, A; Desai, R; Nagarajan, SG; Panageas, I",,"Banerjee, A; Fukumizu, K",,"Bhattacharyya, Arnab; Desai, Rathin; Nagarajan, Sai Ganesh; Panageas, Ioannis",,,Efficient Statistics for Sparse Graphical Models from Truncated Samples,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"In this paper, we study high-dimensional estimation from truncated samples. We focus on two fundamental and classical problems: (i) inference of sparse graphical Gaussian models and (ii) support recovery of sparse linear models. (i) For Gaussian graphical models, suppose d-dimensional samples x are generated from a Gaussian N (mu,Sigma) and observed only if they belong to a subset S subset of R-d. We show that mu and Sigma can be estimated with error in the Frobenius norm, using(1) (O) over tilde (nz(Sigma(-1))/epsilon(2)) samples from a truncated N (mu,Sigma) and having access to a membership oracle for S. The set S is assumed to have non-trivial measure under the unknown distribution but is otherwise arbitrary. (ii) For sparse linear regression, suppose samples (x; y) are generated where y = Omega*(inverted perpendicular) x + N (0, 1) and (x, y) is seen only if y belongs to a truncation set S subset of R. We consider the case that Omega* is sparse with a support set of size k. Our main result is to establish precise conditions on the problem dimension d, the support size k, the number of observations n, and properties of the samples and the truncation that are sufficient to recover the support of Omega*. Specifically, we show that under some natural assumptions, only O(k(2) log d) samples are needed to estimate Omega* in the l(infinity)-norm up to a bounded error. Similar results are also estabilished for estimating Omega* in the Euclidean norm up to arbitrary error. For both problems, the estimator is obtained by minimizing the sum of the empirical negative log-likelihood function and an l(infinity)-regularization term.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801075,0
C,"Du, Q; Biau, G; Peti, F; Porcher, R",,"Banerjee, A; Fukumizu, K",,"Du, Q.; Biau, G.; Peti, F.; Porcher, R.",,,Wasserstein Random Forests and Applications in Heterogeneous Treatment Effects,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We present new insights into causal inference in the context of Heterogeneous Treatment Effects by proposing natural variants of Random Forests to estimate the key conditional distributions. To achieve this, we recast Breiman's original splitting criterion in terms of Wasserstein distances between empirical measures. This reformulation indicates that Random Forests are well adapted to estimate conditional distributions and provides a natural extension of the algorithm to multi-variate outputs. Following the philosophy of Breiman's construction, we propose some variants of the splitting rule that are well-suited to the conditional distribution estimation problem. Some preliminary theoretical connections are established along with various numerical experiments, which show how our approach may help to conduct more transparent causal inference in complex situations. A Python package is also provided.",,,,,"Petit, Francois V/U-2582-2018","Petit, Francois V/0000-0003-2258-170X",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802012,0
C,"Gandikota, V; Kane, D; Maity, RK; Mazumdar, A",,"Banerjee, A; Fukumizu, K",,"Gandikota, Venkata; Kane, Daniel; Maity, Raj Kumar; Mazumdar, Arya",,,vqSGD: Vector Quantized Stochastic Gradient Descent,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"In this work, we present a family of vector quantization schemes vqSGD (Vector-Quantized Stochastic Gradient Descent) that provide an asymptotic reduction in the communication cost with convergence guarantees in first-order distributed optimization. In the process we derive the following fundamental information theoretic fact: Theta(d/R-2) bits are necessary and sufficient (up to an additive O(log d) term) to describe an unbiased estimator (g) over cap (g) for any g in the d-dimensional unit sphere, under the constraint that parallel to(g) over cap (g)parallel to(2) <= R almost surely. In particular, we consider a randomized scheme based on the convex hull of a point set, that returns an unbiased estimator of a d-dimensional gradient vector with almost surely bounded norm. We provide multiple efficient instances of our scheme, that are near optimal, and require only o(d) bits of communication at the expense of tolerable increase in error. The instances of our quantization scheme are obtained using the properties of binary error-correcting codes and provide a smooth tradeo. between the communication and the estimation error of quantization. Furthermore, we show that vqSGD also offers some automatic privacy guarantees.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802064,0
C,"Greenberg, CS; Macaluso, S; Monath, N; Lee, JA; Flaherty, P; Cranmer, K; McGregor, A; McCallum, A",,"Banerjee, A; Fukumizu, K",,"Greenberg, Craig S.; Macaluso, Sebastian; Monath, Nicholas; Lee, Ji Ah; Flaherty, Patrick; Cranmer, Kyle; McGregor, Andrew; McCallum, Andrew",,,Cluster Trellis: Data Structures & Algorithms for Exact Inference in Hierarchical Clustering,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Hierarchical clustering is a fundamental task often used to discover meaningful structures in data. Due to the combinatorial number of possible hierarchical clusterings, approximate algorithms are typically used for inference. In contrast to existing methods, we present novel dynamic-programming algorithms for exact inference in hierarchical clustering based on a novel trellis data structure, and we prove that we can exactly compute the partition function, maximum likelihood hierarchy, and marginal probabilities of sub-hierarchies and clusters. Our algorithms scale in time and space proportional to the powerset of N elements, which is super-exponentially more efficient than explicitly considering each of the (2N - 3)!! possible hierarchies. Also, for larger datasets where our exact algorithms become infeasible, we introduce an approximate algorithm based on a sparse trellis that outperforms greedy and beam search baselines.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803001,0
C,"Kumar, N; Kuzelka, O",,"Banerjee, A; Fukumizu, K",,"Kumar, Nitesh; Kuzelka, Ondrej",,,Context-Specific Likelihood Weighting,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Sampling is a popular method for approximate inference when exact inference is impractical. Generally, sampling algorithms do not exploit contextspecific independence (CSI) properties of probability distributions. We introduce context-specific likelihood weighting (CS-LW), a new sampling methodology, which besides exploiting the classical conditional independence properties, also exploits CSI properties. Unlike the standard likelihood weighting, CS-LW is based on partial assignments of random variables and requires fewer samples for convergence due to the sampling variance reduction. Furthermore, the speed of generating samples increases. Our novel notion of contextual assignments theoretically justifies CS-LW. We empirically show that CS-LW is competitive with state-of-the-art algorithms for approximate inference in the presence of a significant amount of CSIs.",,,,,"Kuzelka, Ondrej/T-3922-2017","Kuzelka, Ondrej/0000-0002-6523-9114",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802056,0
C,"Maronas, J; Hamelijnck, O; Knoblauch, J; Damoulas, T",,"Banerjee, A; Fukumizu, K",,"Maronas, Juan; Hamelijnck, Oliver; Knoblauch, Jeremias; Damoulas, Theodoros",,,Transforming Gaussian Processes With Normalizing Flows,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Gaussian Processes ( GPs) can be used as flexible, non-parametric function priors. Inspired by the growing body of work on Normalizing Flows, we enlarge this class of priors through a parametric invertible transformation that can be made input-dependent. Doing so also allows us to encode interpretable prior knowledge (e.g., boundedness constraints). We derive a variational approximation to the resulting Bayesian inference problem, which is as fast as stochastic variational GP regression (Hensman et al., 2013; Dezfouli and Bonilla, 2015). This makes the model a computationally efficient alternative to other hierarchical extensions of GP priors (Lazaro-Gredilla, 2012; Damianou and Lawrence, 2013). The resulting algorithm's computational and inferential performance is excellent, and we demonstrate this on a range of data sets. For example, even with only 5 inducing points and an input-dependent flow, our method is consistently competitive with a standard sparse GP fitted using 100 inducing points.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801034,0
C,"Mayekar, P; Suresh, AT; Tyagi, H",,"Banerjee, A; Fukumizu, K",,"Mayekar, Prathamesh; Suresh, Ananda Theertha; Tyagi, Himanshu",,,Wyner-Ziv Estimators: Efficient Distributed Mean Estimation with Side Information,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Communication efficient distributed mean estimation is an important primitive that arises in many distributed learning and optimization scenarios such as federated learning. Without any probabilistic assumptions on the underlying data, we study the problem of distributed mean estimation where the server has access to side information. We propose Wyner-Ziv estimators, which are communication and computationally efficient and nearoptimal when an upper bound for the distance between the side information and the data is known. In a different direction, when there is no knowledge assumed about the distance between side information and the data, we present an alternative Wyner-Ziv estimator that uses correlated sampling. This latter setting offers universal recovery guarantees, and perhaps will be of interest in practice when the number of users is large and keeping track of the distances between the data and the side information may not be possible.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804024,0
C,"Phan, M; Arbour, D; Dimmery, D; Rao, AB",,"Banerjee, A; Fukumizu, K",,"My Phan; Arbour, David; Dimmery, Drew; Rao, Anup B.",,,Designing Transportable Experiments Under S-Admissability,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We consider the problem of designing a randomized experiment on a source population to estimate the Average Treatment Effect (ATE) on a target population. We propose a novel approach which explicitly considers the target when designing the experiment on the source. Under the covariate shift assumption, we design an unbiased importance-weighted estimator for the target population's ATE. To reduce the variance of our estimator, we design a covariate balance condition (Target Balance) between the treatment and control groups based on the target population. We show that Target Balance achieves a higher variance reduction asymptotically than methods that do not consider the target population during the design phase. Our experiments illustrate that Target Balance reduces the variance even for small sample sizes.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803009,0
C,"Ozbay, E; Kamble, V",,"Banerjee, A; Fukumizu, K",,"Ozbay, Eren; Kamble, Vijay",,,Training a Single Bandit Arm,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"In several applications of the stochastic multi-armed bandit problem, the traditional objective of maximizing the expected sum of rewards obtained can be inappropriate. Motivated by the problem of optimizing job assignments to train novice workers of unknown quality in labor platforms, we consider a new objective in the classical setup. Instead of maximizing the expected total reward from T pulls, we consider the vector of cumulative rewards earned from the K arms at the end of T pulls, and aim to maximize the expected value of the highest cumulative reward across the K arms. This corresponds to the objective of training a single, highly skilled worker using a limited supply of training jobs. For this new objective, we show that any policy must incur an instance-dependent asymptotic regret of Omega(log T) (with a higher instance-dependent constant compared to the traditional objective) and an instanceindependent regret of Omega((KT2/3)-T-1/3). We then design an explore-then-commit policy, featuring exploration based on appropriately tuned confidence bounds on the mean reward and an adaptive stopping criterion, which adapts to the problem difficulty and achieves these bounds (up to logarithmic factors). Our numerical experiments demonstrate the efficacy of this policy compared to several natural alternatives in practical parameter regimes.",,,,,,"Ozbay, Eren/0000-0001-5634-4003",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803060,0
C,"Rinaldo, A; Wang, DR; Wen, Q; Willett, R; Yu, Y",,"Banerjee, A; Fukumizu, K",,"Rinaldo, Alessandro; Wang, Daren; Wen, Qin; Willett, Rebecca; Yu, Yi",,,Localizing Changes in High-Dimensional Regression Models,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"This paper addresses the problem of localizing change points in high-dimensional linear regression models with piecewise constant regression coefficients. We develop a dynamic programming approach to estimate the locations of the change points whose performance improves upon the current state-ofthe-art, even as the dimensionality, the sparsity of the regression coefficients, the temporal spacing between two consecutive change points, and the magnitude of the difference of two consecutive regression coefficient vectors are allowed to vary with the sample size. Furthermore, we devise a computationallyefficient refinement procedure that provably reduces the localization error of preliminary estimates of the change points. We demonstrate minimax lower bounds on the localization error that nearly match the upper bound on the localization error of our methodology and show that the signal-to-noise condition we impose is essentially the weakest possible based on information-theoretic arguments. Extensive numerical results support our theoretical findings, and experiments on real air quality data reveal change points supported by historical information not used by the algorithm.",,,,,"Wang, Daren/AFQ-9738-2022",,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802052,0
C,"Roy, C; Rahman, T; Dong, HL; Ruozzi, N; Gogate, V",,"Banerjee, A; Fukumizu, K",,"Roy, Chiradeep; Rahman, Tahrima; Dong, Hailiang; Ruozzi, Nicholas; Gogate, Vibhav",,,Dynamic Cutset Networks,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Tractable probabilistic models (TPMs) are appealing because they admit polynomial-time inference for a wide variety of queries. In this work, we extend the cutset network (CN) framework, a powerful sub-class of TPMs that often outperforms probabilistic graphical models in terms of prediction accuracy, to the temporal domain. This extension, dubbed dynamic cutset networks (DCNs), uses a CN to model the prior distribution and a conditional CN to model the transition distribution. We show that although exact inference is intractable when arbitrary conditional CNs are used, particle filtering is efficient. To ensure tractability of exact inference, we introduce a novel conditional model called AND/OR conditional cutset networks and show that under certain restrictions exact inference is linear in the size of the corresponding constrained DCN. Experiments on several sequential datasets demonstrate the efficacy of our framework.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803072,0
C,"Samo, YLK",,"Banerjee, A; Fukumizu, K",,"Samo, Yves-Laurent Kom",,,Inductive Mutual Information Estimation: A Convex Maximum-Entropy Copula Approach,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We propose a novel estimator of the mutual information between two ordinal vectors x and y. Our approach is inductive (as opposed to deductive) in that it depends on the data generating distribution solely through some nonparametric properties revealing associations in the data, and does not require having enough data to fully characterize the true joint distributions P-x,P- y. Specifically, our approach consists of (i) noting that I (y; x) = I (u(y); u(x)) where u(y) and u(x) are the copula-uniform dual representations of y and x (i.e. their images under the probability integral transform), and (ii) estimating the copula entropies h (u(y)), h (u(x)) and h (u(y), u(x)) by solving a maximum-entropy problem over the space of copula densities under a constraint of the type alpha(m) = E [phi(m)(u(y), u(x))]. We prove that, so long as the constraint is feasible, this problem admits a unique solution, it is in the exponential family, and it can be learned by solving a convex optimization problem. The resulting estimator, which we denote MIND, is marginal-invariant, always non-negative, unbounded for any sample size n, consistent, has MSE rate O(1/n), and is more data-efficient than competing approaches.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802069,0
C,"Suresh, AT",,"Banerjee, A; Fukumizu, K",,"Suresh, Ananda Theertha",,,Robust hypothesis testing and distribution estimation in Hellinger distance,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We propose a simple robust hypothesis test that has the same sample complexity as that of the optimal Neyman-Pearson test up to constants, but robust to distribution perturbations under Hellinger distance. We discuss the applicability of such a robust test for estimating distributions in Hellinger distance. We empirically demonstrate the power of the test on canonical distributions.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803056,0
C,"Takemura, K; Ito, S; Hatano, D; Sumita, H; Fukunaga, T; Kakimura, N; Kawarabayashi, KI",,"Banerjee, A; Fukumizu, K",,"Takemura, Kei; Ito, Shinji; Hatano, Daisuke; Sumita, Hanna; Fukunaga, Takuro; Kakimura, Naonori; Kawarabayashi, Ken-ichi",,,A Parameter-Free Algorithm for Misspecified Linear Contextual Bandits,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We investigate the misspecified linear contextual bandit (MLCB) problem, which is a generalization of the linear contextual bandit (LCB) problem. The MLCB problem is a decision-making problem in which a learner observes d-dimensional feature vectors, called arms, chooses an arm from K arms, and then obtains a reward from the chosen arm in each round. The learner aims to maximize the sum of the rewards over T rounds. In contrast to the LCB problem, the rewards in the MLCB problem may not be represented by a linear function in feature vectors; instead, it is approximated by a linear function with additive approximation parameter epsilon >= 0. In this paper, we propose an algorithm that achieves (O) over tilde(root dT log(K) + epsilon root dT) regret, where (O) over tilde(.) ignores polylogarithmic factors in d and T. This is the first algorithm that guarantees a high-probability regret bound for the MLCB problem without knowledge of the approximation parameter epsilon.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804009,0
C,"Wicker, M; Laurenti, L; Patane, A; Chen, ZT; Zhang, Z; Kwiatkowska, M",,"Banerjee, A; Fukumizu, K",,"Wicker, Matthew; Laurenti, Luca; Patane, Andrea; Chen, Zhoutong; Zhang, Zheng; Kwiatkowska, Marta",,,Bayesian Inference with Certifiable Adversarial Robustness,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We consider adversarial training of deep neural networks through the lens of Bayesian learning, and present a principled framework for adversarial training of Bayesian Neural Networks (BNNs) with certifiable guarantees. We rely on techniques from constraint relaxation of non-convex optimisation problems and modify the standard cross-entropy error model to enforce posterior robustness to worst-case perturbations in.-balls around input points. We illustrate how the resulting framework can be combined with methods commonly employed for approximate inference of BNNs. In an empirical investigation, we demonstrate that the presented approach enables training of certifiably robust models on MNIST, FashionMNIST and CIFAR-10 and can also be beneficial for uncertainty calibration. Our method is the first to directly train certifiable BNNs, thus facilitating their deployment in safety-critical applications.",,,,,,"Zhang, Zheng/0000-0002-2292-0030",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802090,0
C,"Wu, CL; Santhanam, N",,"Banerjee, A; Fukumizu, K",,"Wu, Changlong; Santhanam, Narayana",,,Prediction with Finitely many Errors Almost Surely,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Using only samples from a probabilistic model, we predict properties of the model and of future observations. The prediction game continues in an online fashion as the sample size grows with new observations. After each prediction, the predictor incurs a binary (0-1) loss. The probability model underlying a sample is otherwise unknown except that it belongs to a known class of models. The goal is to make finitely many errors (i.e. loss of 1) with probability 1 under the generating model, no matter what it may be in the known model class. Model classes admitting predictors that make only finitely many errors are eventually almost surely (eas) predictable. When the losses incurred are observable (the supervised case), we completely characterize eas predictable classes. We provide analogous results in the unsupervised case. Our results have a natural interpretation in terms of regularization. In eas-predictable classes, we study if it is possible to have a universal stopping rule that identifies (to any given confidence) when no more errors will be made. Classes admitting such a stopping rule are eas learnable. When samples are generated i.i.d., we provide a complete characterization of eas learnability. We also study cases when samples are not generated i.i.d., but a full characterization remains open at this point.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803085,0
C,"Zhang, YX; Cheng, XY; Reeves, G",,"Banerjee, A; Fukumizu, K",,"Zhang, Yixing; Cheng, Xiuyuan; Reeves, Galen",,,Convergence of Gaussian-smoothed optimal transport distance with sub-gamma distributions and dependent samples,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"The Gaussian-smoothed optimal transport (GOT) framework, recently proposed by Gold-feld et al., scales to high dimensions in estimation and provides an alternative to entropy regularization. This paper provides convergence guarantees for estimating the GOT distance under more general settings. For the Gaussian-smoothed p-Wasserstein distance in d dimensions, our results require only the existence of a moment greater than d + 2p. For the special case of sub-gamma distributions, we quantify the dependence on the dimension d and establish a phase transition with respect to the scale parameter. We also prove convergence for dependent samples, only requiring a condition on the pairwise dependence of the samples measured by the covariance of the feature map of a kernel space. A key step in our analysis is to show that the GOT distance is dominated by a family of kernel maximum mean discrepancy (MMD) distances with a kernel that depends on the cost function as well as the amount of Gaussian smoothing. This insight provides further interpretability for the GOT framework and also introduces a class of kernel MMD distances with desirable properties. The theoretical results are supported by numerical experiments.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802089,0
C,"Zhao, G; Dougherty, ER; Yoon, BJ; Alexander, FJ; Qian, XN",,"Banerjee, A; Fukumizu, K",,"Zhao, Guang; Dougherty, Edward R.; Yoon, Byung-Jun; Alexander, Francis J.; Qian, Xiaoning",,,Bayesian Active Learning by Soft Mean Objective Cost of Uncertainty,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"To achieve label efficiency for training supervised learning models, pool-based active learning sequentially selects samples from a set of candidates as queries to label by optimizing an acquisition function. One category of existing methods adopts one-step-look-ahead strategies based on acquisition functions tailored with the learning objectives, for example based on the expected loss reduction (ELR) or the mean objective cost of uncertainty (MOCU) proposed recently. These active learning methods are optimal with the maximum classification error reduction when one considers a single query. However, it is well-known that there is no performance guarantee in the long run for these myopic methods. In this paper, we show that these methods are not guaranteed to converge to the optimal classifier of the true model because MOCU is not strictly concave. Moreover, we suggest a strictly concave approximation of MOCU-Soft MOCU-that can be used to define an acquisition function to guide Bayesian active learning with theoretical convergence guarantee. For training Bayesian classifiers with both synthetic and real-world data, our experiments demonstrate the superior performance of active learning by Soft MOCU compared to other existing methods.",,,,,"Yoon, Byung-Jun/B-1466-2008","Yoon, Byung-Jun/0000-0001-9328-1101",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804076,0
C,"Calderon, D; Juba, B; Li, SR; Li, ZY; Ruan, L",,"Chiappa, S; Calandra, R",,"Calderon, Diego; Juba, Brendan; Li, Sirui; Li, Zongyi; Ruan, Lisa",,,Conditional Linear Regression,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Work in machine learning and statistics commonly focuses on building models that capture the vast majority of data, possibly ignoring a segment of the population as outliers. However, there may not exist a good, simple model for the distribution, so we seek to find a small subset where there exists such a model. We give a computationally efficient algorithm with theoretical analysis for the conditional linear regression task, which is the joint task of identifying a significant portion of the data distribution, described by a k-DNF, along with a linear predictor on that portion with a small loss. In contrast to work in robust statistics on small subsets, our loss bounds do not feature a dependence on the density of the portion we fit, and compared to previous work on conditional linear regression, our algorithm's running time scales polynomially with the sparsity of the linear predictor. We also demonstrate empirically that our algorithm can leverage this advantage to obtain a k-DNF with a better linear predictor in practice.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2164,2172,,,,,,,,,,,,,,,,WOS:000559931300042,0
C,"Gessner, A; Kanjilal, O; Hennig, P",,"Chiappa, S; Calandra, R",,"Gessner, Alexandra; Kanjilal, Oindrila; Hennig, Philipp",,,Integrals over Gaussians under Linear Domain Constraints,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Integrals of linearly constrained multivariate Gaussian densities are a frequent problem in machine learning and statistics, arising in tasks like generalized linear models and Bayesian optimization. Yet they are notoriously hard to compute, and to further complicate matters, the numerical values of such integrals may be very small. We present an efficient black-box algorithm that exploits geometry for the estimation of integrals over a small, truncated Gaussian volume, and to simulate therefrom. Our algorithm uses the Holmes-Diaconis-Ross (HDR) method combined with an analytic version of elliptical slice sampling (ESS). Adapted to the linear setting, ESS allows for rejection-free sampling, because intersections of ellipses and domain boundaries have closed-form solutions. The key idea of HDR is to decompose the integral into easier-to-compute conditional probabilities by using a sequence of nested domains. Remarkably, it allows for direct computation of the logarithm of the integral value and thus enables the computation of extremely small probability masses. We demonstrate the effectiveness of our tailored combination of HDR and ESS on high-dimensional integrals and on entropy search for Bayesian optimization.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2764,2773,,,,,,,,,,,,,,,,WOS:000559931301018,0
C,"Kivaranovic, D; Johnson, KD; Leeb, H",,"Chiappa, S; Calandra, R",,"Kivaranovic, Danijel; Johnson, Kory D.; Leeb, Hannes",,,"Adaptive, Distribution-Free Prediction Intervals for Deep Networks","INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"The machine learning literature contains several constructions for prediction intervals that are intuitively reasonable but ultimately adhoc in that they do not come with provable performance guarantees. We present methods from the statistics literature that can be used efficiently with neural networks under minimal assumptions with guaranteed performance. We propose a neural network that outputs three values instead of a single point estimate and optimizes a loss function motivated by the standard quantile regression loss. We provide two prediction interval methods with finite sample coverage guarantees solely under the assumption that the observations are independent and identically distributed. The first method leverages the conformal inference framework and provides average coverage. The second method provides a new, stronger guarantee by conditioning on the observed data. Lastly, our loss function does not compromise the predictive accuracy of the network like other prediction interval methods. We demonstrate the ease of use of our procedures as well as its improvements over other methods on both simulated and real data. As most deep networks can easily be modified by our method to output predictions with valid prediction intervals, its use should become standard practice, much like reporting standard errors along with mean estimates.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,4346,4355,,,,,,,,,,,,,,,,WOS:000559931301080,0
C,"Kveton, B; Zaheer, M; Szepesvari, C; Li, LH; Ghavamzadeh, M; Boutilier, C",,"Chiappa, S; Calandra, R",,"Kveton, Branislav; Zaheer, Manzil; Szepesvari, Csaba; Li, Lihong; Ghavamzadeh, Mohammad; Boutilier, Craig",,,Randomized Exploration in Generalized Linear Bandits,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We study two randomized algorithms for generalized linear bandits. The first, GLM-TSL, samples a generalized linear model (GLM) from the Laplace approximation to the posterior distribution. The second, GLM-FPL, fits a GLM to a randomly perturbed history of past rewards. We analyze both algorithms and derive (O) over tilde (d root n log K) upper bounds on their n-round regret, where d is the number of features and K is the number of arms. The former improves on prior work while the latter is the first for Gaussian noise perturbations in non-linear models. We empirically evaluate both GLM-TSL and GLM-FPL in logistic bandits, and apply GLM-FPL to neural network bandits. Our work showcases the role of randomization, beyond posterior sampling, in exploration.",,,,,"Zaheer, Manzil/ABG-6249-2021",,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2066,2075,,,,,,,,,,,,,,,,WOS:000559931301091,0
C,"Levine, A; Feizi, S",,"Chiappa, S; Calandra, R",,"Levine, Alexander; Feizi, Soheil",,,Wasserstein Smoothing: Certified Robustness against Wasserstein Adversarial Attacks,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"In the last couple of years, several adversarial attack methods based on different threat models have been proposed for the image classification problem. Most existing defenses consider additive threat models in which sample perturbations have bounded L-p norms. These defenses, however, can be vulnerable against adversarial attacks under non-additive threat models. An example of an attack method based on a non-additive threat model is the Wasserstein adversarial attack proposed by Wong et al. (2019), where the distance between an image and its adversarial example is determined by the Wasserstein metric (earth-mover distance) between their normalized pixel intensities. Until now, there has been no certifiable defense against this type of attack. In this work, we propose the first defense with certified robustness against Wasserstein adversarial attacks using randomized smoothing. We develop this certificate by considering the space of possible flows between images, and representing this space such that Wasserstein distance between images is upper-bounded by L-1 distance in this flow-space. We can then apply existing randomized smoothing certificates for the L-1 metric. In MNIST and CIFAR-10 datasets, we find that our proposed defense is also practically effective, demonstrating significantly improved accuracy under Wasserstein adversarial attack compared to unprotected models.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3938,3946,,,,,,,,,,,,,,,,WOS:000559931301101,0
C,"Li, WZ; Dasarathy, G; Berisha, V",,"Chiappa, S; Calandra, R",,"Li, Weizhi; Dasarathy, Gautam; Berisha, Visar",,,Regularization via Structural Label Smoothing,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Regularization is an effective way to promote the generalization performance of machine learning models. In this paper, we focus on label smoothing, a form of output distribution regularization that prevents overfitting of a neural network by softening the ground-truth labels in the training data in an attempt to penalize overconfident outputs. Existing approaches typically use cross-validation to impose this smoothing, which is uniform across all training data. In this paper, we show that such label smoothing imposes a quantifiable bias in the Bayes error rate of the training data, with regions of the feature space with high overlap and low marginal likelihood having a lower bias and regions of low overlap and high marginal likelihood having a higher bias. These theoretical results motivate a simple objective function for data-dependent smoothing to mitigate the potential negative consequences of the operation while maintaining its desirable properties as a regularizer. We call this approach Structural Label Smoothing (SLS). We implement SLS and empirically validate on several synthetic and benchmark datasets (including the CIFAR-100). The results confirm our theoretical insights and demonstrate the effectiveness of the proposed method in comparison to traditional label smoothing.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1453,1462,,,,,,,,,,,,,,,,WOS:000559931302005,0
C,"Lim, NJS; Durrant, RJ",,"Chiappa, S; Calandra, R",,"Lim, Nick Jin Sean; Durrant, Robert John",,,A Diversity-aware Model for Majority Vote Ensemble Accuracy,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Ensemble classifiers are a successful and popular approach for classification, and are frequently found to have better generalization performance than single models in practice. Although it is widely recognized that 'diversity' between ensemble members is important in achieving these performance gains, for classification ensembles it is not widely understood which diversity measures are most predictive of ensemble performance, nor how large an ensemble should be for a particular application. In this paper, we explore the predictive power of several common diversity measures and show - with extensive experiments - that contrary to earlier work that finds no clear link between these diversity measures (in isolation) and ensemble accuracy instead by using the rho diversity measure of Sneath and Sokal as an estimator for the dispersion parameter of a Polya-Eggenberger distribution we can predict, independently of the choice of base classifier family, the accuracy of a majority vote classifier ensemble ridiculously well. We discuss our model and some implications of our findings - such as diversity-aware (non-greedy) pruning of a majority-voting ensemble.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,4078,4086,,,,,,,,,,,,,,,,WOS:000559931300079,0
C,"Vaswani, S; Mehrabian, A; Durand, A; Kveton, B",,"Chiappa, S; Calandra, R",,"Vaswani, Sharan; Mehrabian, Abbas; Durand, Audrey; Kveton, Branislav",,,Old Dog Learns New Tricks: Randomized UCB for Bandit Problems,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We propose RandUCB, a bandit strategy that builds on theoretically derived confidence intervals similar to upper confidence bound (UCB) algorithms, but akin to Thompson sampling (TS), it uses randomization to trade off exploration and exploitation. In the Karmed bandit setting, we show that there are infinitely many variants of RandUCB, all of which achieve the minimax-optimal 0( \/KT) regret after T rounds. Moreover, for a specific multi-armed bandit setting, we show that both UCB and TS can be recovered as special cases of RandUCB. For structured bandits, where each arm is associated with a d-dimensional feature vector and rewards are distributed according to a linear or generalized linear model, we prove that RandUCB achieves the minimax-optimal O(dVT) regret even in the case of infinitely many arms. Through experiments in both the multi-armed and structured bandit settings, we demonstrate that RandUCB matches or outperforms TS and other randomized exploration strategies. Our theoretical and empirical results together imply that RandUCB achieves the best of both worlds.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303054,0
C,"Wang, YH; Roy, U; Uhler, C",,"Chiappa, S; Calandra, R",,"Wang, Yuhao; Roy, Uma; Uhler, Caroline",,,Learning High-dimensional Gaussian Graphical Models under Total Positivity without Adjustment of Tuning Parameters,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We consider the problem of estimating an undirected Gaussian graphical model when the underlying distribution is multivariate totally positive of order 2 (MTP2), a strong form of positive dependence. Such distributions are relevant for example for portfolio selection, since assets are usually positively dependent. A large body of methods have been proposed for learning undirected graphical models without the MTP2 constraint. A major limitation of these methods is that their structure recovery guarantees in the high-dimensional setting usually require a particular choice of a tuning parameter, which is unknown a priori in real world applications. We here propose a new method to estimate the underlying undirected graphical model under MTP2 and show that it is provably consistent in structure recovery without adjusting the tuning parameters. This is achieved by a constraint-based estimator that infers the structure of the underlying graphical model by testing the signs of the empirical partial correlation coefficients. We evaluate the performance of our estimator in simulations and on financial data.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303065,0
C,"Wu, L; Yu, HF; Rao, N; Sharpnack, J; Hsieh, CJ",,"Chiappa, S; Calandra, R",,"Wu, Liwei; Yu, Hsiang-Fu; Rao, Nikhil; Sharpnack, James; Hsieh, Cho-Jui",,,Graph DNA: Deep Neighborhood Aware Graph Encoding for Collaborative Filtering,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"In this paper, we consider recommender systems with side information in the form of graphs. Existing collaborative filtering algorithms mainly utilize only immediate neighborhood information and do not efficiently take advantage of deeper neighborhoods beyond 1-2 hops. The main issue with exploiting deeper graph information is the rapidly growing time and space complexity when incorporating information from these neighborhoods. In this paper, we propose using Graph DNA, a novel Deep Neighborhood Aware graph encoding algorithm, for exploiting multi-hop neighborhood information. DNA encoding computes approximate deep neighborhood information in linear time using Bloom filters, and results in a per-node encoding whose dimension is logarithmic in the number of nodes in the graph. It can be used in conjunction with both feature-based and graph-regularization-based collaborative filtering algorithms. Graph DNA has the advantages of being memory and time efficient and providing additional regularization when compared to directly using higher order graph information. We provide theoretical performance bounds for graph DNA encoding, and experimentally show that graph DNA can be used with 4 popular collaborative filtering algorithms to consistently boost their performances with little computational and memory overhead.",,,,,"WU, LIWEI/ABG-4582-2021",,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303081,0
C,"Yang, P; Li, P",,"Chiappa, S; Calandra, R",,"Yang, Peng; Li, Ping",,,Adaptive Online Kernel Sampling for Vertex Classification,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"This paper studies online kernel learning (OKL) for graph classification problem, since the large approximation space provided by reproducing kernel Hilbert spaces often contains an accurate function. Nonetheless, optimizing over this space is computationally expensive. To address this issue, approximate OKL is introduced to reduce the complexity either by limiting the support vector (SV) used by the predictor, or by avoiding the kernelization process altogether using embedding. Nonetheless, as long as the size of the approximation space or the number of SV does not grow over time, an adversarial environment can always exploit the approximation process. In this paper, we introduce an online kernel sampling (OKS) technique, a new second-order OKL method that slightly improve the bound from O(d log T) down to O(r log T) where r is the rank of the learned data and is usually much smaller than d. To reduce the computational complexity of second-order methods, we introduce a randomized sampling algorithm for sketching kernel matrix Kt and show that our method is effective to reduce the time and space complexity significantly while maintaining comparable performance. Empirical experimental results demonstrate that the proposed model is highly effective on real-world graph datasets.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303092,0
C,"Ye, ZZ; Mollenhoff, T; Wu, T; Cremers, D",,"Chiappa, S; Calandra, R",,"Ye, Zhenzhang; Moellenhoff, Thomas; Wu, Tao; Cremers, Daniel",,,Optimization of Graph Total Variation via Active-Set-based Combinatorial Reconditioning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Structured convex optimization on weighted graphs finds numerous applications in machine learning and computer vision. In this work, we propose a novel adaptive preconditioning strategy for proximal algorithms on this problem class. Our preconditioner is driven by a sharp analysis of the local linear convergence rate depending on the active set at the current iterate. We show that nested-forest decomposition of the inactive edges yields a guaranteed local linear convergence rate. Further, we propose a practical greedy heuristic which realizes such nested decompositions and show in several numerical experiments that our reconditioning strategy, when applied to proximal gradient or primal-dual hybrid gradient algorithm, achieves competitive performances. Our results suggest that local convergence analysis can serve as a guideline for selecting variable metrics in proximal algorithms.",,,,,,"Mollenhoff, Thomas/0000-0001-7730-0843",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303096,0
C,"Zhao, P; Wang, GH; Zhang, LJ; Zhou, ZH",,"Chiappa, S; Calandra, R",,"Zhao, Peng; Wang, Guanghui; Zhang, Lijun; Zhou, Zhi-Hua",,,Bandit Convex Optimization in Non-stationary Environments,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Bandit Convex Optimization (BCO) is a fundamental framework for modeling sequential decision-making with partial information, where the only feedback available to the player is the one-point or two-point function values. In this paper, we investigate BCO in non-stationary environments and choose the dynamic regret as the performance measure, which is defined as the difference between the cumulative loss incurred by the algorithm and that of any feasible comparator sequence. Let T be the time horizon and P-T be the path-length of the comparator sequence that reflects the non-stationarity of environments. We propose a novel algorithm that achieves O(T-3/4(1 + P-T)(1/2)) and O(T-1/2(1 + P-T)(1/2)) dynamic regret respectively for the one-point and two-point feedback models. The latter result is optimal, matching the Omega(T-1/2 (1 + P-T)(1/2)) lower bound established in this paper. Notably, our algorithm is more adaptive to non-stationary environments since it does not require prior knowledge of the path-length P-T ahead of time, which is generally unknown.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1508,1517,,,,,,,,,,,,,,,,WOS:000559931304017,0
C,"Durrande, N; Adam, V; Bordeaux, L; Eleftheriadis, S; Hensman, J",,"Chaudhuri, K; Sugiyama, M",,"Durrande, Nicolas; Adam, Vincent; Bordeaux, Lucas; Eleftheriadis, Stefanos; Hensman, James",,,Banded Matrix Operators for Gaussian Markov Models in the Automatic Differentiation Era,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Banded matrices can be used as precision matrices in several models including linear state-space models, some Gaussian processes, and Gaussian Markov random fields. The aim of the paper is to make modern inference methods (such as variational inference or gradient-based sampling) available for Gaussian models with banded precision. We show that this can efficiently be achieved by equipping an automatic differentiation framework, such as TensorFlow or PyTorch, with some linear algebra operators dedicated to banded matrices. This paper studies the algorithmic aspects of the required operators, details their reverse-mode derivatives, and show that their complexity is linear in the number of observations.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902085,0
C,"Feng, HJ; Ning, Y",,"Chaudhuri, K; Sugiyama, M",,"Feng, Huijie; Ning, Yang",,,High-dimensional Mixed Graphical Model with Ordinal Data: Parameter Estimation and Statistical Inference,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We consider parameter estimation and statistical inference of high-dimensional undirected graphical models for mixed data comprising both ordinal and continuous variables. We propose a flexible model called Latent Mixed Gaussian Copula Model that simultaneously deals with such mixed data by assuming that the observed ordinal variables are generated by latent variables. For parameter estimation, we introduce a convenient rank-based ensemble approach to estimate the latent correlation matrix, which can be subsequently applied to recover the latent graph structure. In addition, based on the ensemble estimator, we develop test statistics via a pseudolikelihood approach to quantify the uncertainty associated with the low dimensional components of high-dimensional parameters. Our theoretical analysis shows the consistency of the estimator and asymptotic normality of the test statistic. Experiments on simulated and real gene expression data are conducted to validate our approach.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,654,663,,,,,,,,,,,,,,,,WOS:000509687900068,0
C,"Iyer, R; Bilmes, J",,"Chaudhuri, K; Sugiyama, M",,"Iyer, Rishabh; Bilmes, Jeff",,,A Memoization Framework for Scaling Submodular Optimization to Large Scale Problems,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We are motivated by large scale submodular optimization problems, where standard algorithms that treat the submodular functions in the value oracle model do not scale. In this paper, we present a model called the precomputational complexity model, along with a unifying memoization based framework, which looks at the specific form of the given submodular function. A key ingredient in this framework is the notion of a precomputed statistic, which is maintained in the course of the algorithms. We show that we can easily integrate this idea into a large class of submodular optimization problems including constrained and unconstrained submodular maximization, minimization, difference of submodular optimization, optimization with submodular constraints and several other related optimization problems. Moreover, memoization can be integrated in both discrete and continuous relaxation flavors of algorithms for these problems. We demonstrate this idea for several commonly occurring submodular functions, and show how the precomputational model provides significant speedups compared to the value oracle model. Finally, we empirically demonstrate this for large scale machine learning problems of data subset selection and summarization.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902040,0
C,"Kuzelka, O; Kungurtsev, V",,"Chaudhuri, K; Sugiyama, M",,"Kuzelka, Ondrej; Kungurtsev, Vyacheslav",,,Lifted Weight Learning of Markov Logic Networks Revisited,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,We study lifted weight learning of Markov logic networks. We show that there is an algorithm for maximum-likelihood learning of 2-variable Markov logic networks which runs in time polynomial in the domain size. Our results are based on existing lifted-inference algorithms and recent algorithmic results on computing maximum entropy distributions.,,,,,"Kuzelka, Ondrej/T-3922-2017; Kungurtsev, Vyacheslav/CAH-0113-2022; Kungurtsev, Vyacheslav/B-3003-2016; Krejcar, Ondrej/A-8639-2008","Kuzelka, Ondrej/0000-0002-6523-9114; Kungurtsev, Vyacheslav/0000-0003-2229-8824; Krejcar, Ondrej/0000-0002-5992-2574",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901083,0
C,"Madjiheurem, S; Toni, L",,"Chaudhuri, K; Sugiyama, M",,"Madjiheurem, Sephora; Toni, Laura",,,Representation Learning on Graphs: A Reinforcement Learning Application,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"In this work, we study value function approximation in reinforcement learning (RL) problems with high dimensional state or action spaces via a generalized version of representation policy iteration (RPI). We consider the limitations of proto-value functions (PVFs) at accurately approximating the value function in low dimensions and we highlight the importance of features learning for an improved low-dimensional value function approximation. Then, we adopt different representation learning algorithms on graphs to learn the basis functions that best represent the value function. We empirically show that node2vec, an algorithm for scalable feature learning in networks, and Variational Graph Auto-Encoder constantly outperform the commonly used smooth proto-value functions in low-dimensional feature space.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687903046,0
C,"Ho, N; Huynh, V; Phung, D; Jordan, MI",,"Chaudhuri, K; Sugiyama, M",,"Nhat Ho; Viet Huynh; Dinh Phung; Jordan, Michael, I",,,Probabilistic Multilevel Clustering via Composite Transportation Distance,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We propose a novel probabilistic approach to multilevel clustering problems based on composite transportation distance, which is a variant of transportation distance where the underlying metric is Kullback-Leibler divergence. Our method involves solving a joint optimization problem over spaces of probability measures to simultaneously discover grouping structures within groups and among groups. By exploiting the connection of our method to the problem of finding composite transportation barycenters, we develop fast and efficient optimization algorithms even for potentially large-scale multilevel datasets. Finally, we present experimental results with both synthetic and real data to demonstrate the efficiency and scalability of the proposed approach.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687903021,0
C,"Ramachandran, A; Lumetta, SS; Klee, E; Chen, DM",,"Chaudhuri, K; Sugiyama, M",,"Ramachandran, Anand; Lumetta, Steven S.; Klee, Eric; Chen, Deming",,,A recurrent Markov state-space generative model for sequences,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"While the Hidden Markov Model (HMM) is a versatile generative model of sequences capable of performing many exact inferences efficiently, it is not suited for capturing complex long-term structure in the data. Advanced state-space models based on Deep Neural Networks (DNN) overcome this limitation but cannot perform exact inferences. In this article, we present a new generative model for sequences that combines both aspects, the ability to perform exact inferences and the ability to model long-term structure, by augmenting the HMM with a deterministic, continuous state variable modeled through a Recurrent Neural Network. We empirically study the performance of the model on (i) synthetic data comparing it to the HMM, (ii) a supervised learning task in bioinformatics where it outperforms two DNN-based regressors and (iii) in the generative modeling of music where it outperforms many prominent DNN-based generative models.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687903013,0
C,"Yu, M; Gupta, V; Kolar, M",,"Chaudhuri, K; Sugiyama, M",,"Yu, Ming; Gupta, Varun; Kolar, Mladen",,,Learning Influence-Receptivity Network Structure with Guarantee,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Traditional works on community detection from observations of information cascade assume that a single adjacency matrix parametrizes all the observed cascades. However, in reality the connection structure usually does not stay the same across cascades. For example, different people have different topics of interest, therefore the connection structure depends on the information/topic content of the cascade. In this paper we consider the case where we observe a sequence of noisy adjacency matrices triggered by information/event with different topic distributions. We propose a novel latent model using the intuition that a connection is more likely to exist between two nodes if they are interested in similar topics, which are common with the information/event. Specifically, we endow each node with two node-topic vectors: an influence vector that measures how influential/authoritative they are on each topic; and a receptivity vector that measures how receptive/susceptible they are to each topic. We show how these two node-topic structures can be estimated from observed adjacency matrices with theoretical guarantee on estimation error, in cases where the topic distributions of the information/event are known, as well as when they are unknown. Experiments on synthetic and real data demonstrate the effectiveness of our model and superior performance compared to state-of-the-art methods.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901054,0
C,"Zhou, P; Yuan, XT; Feng, JS",,"Chaudhuri, K; Sugiyama, M",,"Zhou, Pan; Yuan, Xiao-Tong; Feng, Jiashi",,,Faster First-Order Methods for Stochastic Non-Convex Optimization on Riemannian Manifolds,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"SPIDER (Stochastic Path Integrated Differential EstimatoR) is an efficient gradient estimation technique developed for non-convex stochastic optimization. Although having been shown to attain nearly optimal computational complexity bounds, the SPIDER-type methods are limited to linear metric spaces. In this paper, we introduce the Riemannian SPIDER (R-SPIDER) method as a novel nonlinear-metric extension of SPIDER for efficient non-convex optimization on Riemannian manifolds. We prove that for finite-sum problems with n components, R-SPIDER converges to an epsilon-accuracy stationary point within O(min + root n/epsilon(2), 1/epsilon(3)) stochastic gradient evaluations, which is sharper in magnitude than the prior Riemannian first-order methods. For online optimization, R-SPIDER is shown to converge with O(1/epsilon(3)) complexity which is, to the best of our knowledge, the first non-asymptotic result for online Riemannian optimization. Especially, for gradient dominated functions, we further develop a variant of R-SPIDER and prove its linear convergence rate. Numerical results testify the computational efficiency of the proposed methods.",,,,,"Feng, Jiashi/AGX-6209-2022",,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,138,147,,,,,,,,,,,,,,,,WOS:000509687900015,0
C,"Ding, HY; Khand, ME; Sato, I; Sugiyama, M",,"Storkey, A; PerezCruz, F",,"Ding, Hongyi; Khand, Mohammad Emtiyaz; Sato, Issei; Sugiyama, Masashi",,,Bayesian Nonparametric Poisson-Process Allocation for Time-Sequence Modeling,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Analyzing the underlying structure of multiple time-sequences provides insights into the understanding of social networks and human activities. In this work, we present the Bayesian nonparametric Poisson process allocation (BaNPPA), a latent-function model for time-sequences, which automatically infers the number of latent functions. We model the intensity of each sequence as an infinite mixture of latent functions, each of which is obtained using a function drawn from a Gaussian process. We show that a technical challenge for the inference of such mixture models is the unidentifiability of the weights of the latent functions. We propose to cope with the issue by regulating the volume of each latent function within a variational inference algorithm. Our algorithm is computationally efficient and scales well to large data sets. We demonstrate the usefulness of our proposed model through experiments on both synthetic and real-world data sets.",,,,,"Sugiyama, Masashi/AEO-1176-2022","Sugiyama, Masashi/0000-0001-6658-6743",,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300116,0
C,"Kandasamy, K; Krishnamurthy, A; Schneider, J; Poczos, B",,"Storkey, A; PerezCruz, F",,"Kandasamy, Kirthevasan; Krishnamurthy, Akshay; Schneider, Jeff; Poczos, Barnabas",,,Parallelised Bayesian Optimisation via Thompson Sampling,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"We design and analyse variations of the classical Thompson sampling (TS) procedure for Bayesian optimisation (BO) in settings where function evaluations are expensive but can be performed in parallel. Our theoretical analysis shows that a direct application of the sequential Thompson sampling algorithm in either synchronous or asynchronous parallel settings yields a surprisingly powerful result: making n evaluations distributed among M workers is essentially equivalent to performing n evaluations in sequence. Further, by modelling the time taken to complete a function evaluation, we show that, under a time constraint, asynchronous parallel TS achieves asymptotically lower regret than both the synchronous and sequential versions. These results are complemented by an experimental analysis, showing that asynchronous TS outperforms a suite of existing parallel BO algorithms in simulations and in an application involving tuning hyper-parameters of a convolutional neural network. In addition to these, the proposed procedure is conceptually much simpler than existing work for parallel BO.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300015,0
C,"Linderman, SW; Mena, GE; Cooper, H; Paninski, L; Cunningham, JP",,"Storkey, A; PerezCruz, F",,"Linderman, Scott W.; Mena, Gonzalo E.; Cooper, Hal; Paninski, Liam; Cunningham, John P.",,,Reparameterizing the Birkhoff Polytope for Variational Permutation Inference,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Many matching, tracking, sorting, and ranking problems require probabilistic reasoning about possible permutations, a set that grows factorially with dimension. Combinatorial optimization algorithms may enable efficient point estimation, but fully Bayesian inference poses a severe challenge in this high-dimensional, discrete space. To surmount this challenge, we start by relaxing the discrete set of permutation matrices to its convex hull the Birkhoff polytope, the set of doubly-stochastic matrices. We then introduce two novel transformations: an invertible and differentiable stick-breaking procedure that maps unconstrained space to the Birkhoff polytope, and a map that rounds points toward the vertices of the polytope. Both transformations include a temperature parameter that, in the limit, concentrates the densities on permutation matrices. We exploit these transformations and reparameterization gradients to introduce variational inference over permutation matrices, and we demonstrate its utility in a series of experiments.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300169,0
C,"Murray, LM; Lunden, D; Kudlicka, J; Broman, D; Schon, TB",,"Storkey, A; PerezCruz, F",,"Murray, Lawrence M.; Lunden, Daniel; Kudlicka, Jan; Broman, David; Schon, Thomas B.",,,Delayed Sampling and Automatic Rao-Blackwellization of Probabilistic Programs,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"We introduce a dynamic mechanism for the solution of analytically-tractable substructure in probabilistic programs, using conjugate priors and affine transformations to reduce variance in Monte Carlo estimators. For inference with Sequential Monte Carlo, this automatically yields improvements such as locallyoptimal proposals and Rao-Blackwellization. The mechanism maintains a directed graph alongside the running program that evolves dynamically as operations are triggered upon it. Nodes of the graph represent random variables, edges the analytically-tractable relationships between them. Random variables remain in the graph for as long as possible, to be sampled only when they are used by the program in a way that cannot be resolved analytically. In the meantime, they are conditioned on as many observations as possible. We demonstrate the mechanism with a few pedagogical examples, as well as a linear-nonlinear state-space model with simulated data, and an epidemiological model with real data of a dengue outbreak in Micronesia. In all cases one or more variables are automatically marginalized out to significantly reduce variance in estimates of the marginal likelihood, in the final case facilitating a random-weight or pseudo-marginal-type importance sampler for parameter estimation. We have implemented the approach in Anglican and a new probabilistic programming language called Birch.",,,,,"Murray, Lawrence/C-9770-2009; Schon, Thomas/D-4169-2009","Murray, Lawrence/0000-0002-6567-6015; Schon, Thomas/0000-0001-5183-234X",,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300109,0
C,"Sen, R; Shanmugam, K; Shakkottai, S",,"Storkey, A; PerezCruz, F",,"Sen, Rajat; Shanmugam, Karthikeyan; Shakkottai, Sanjay",,,Contextual Bandits with Stochastic Experts,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"We consider the problem of contextual bandits with stochastic experts, which is a variation of the traditional stochastic contextual bandit with experts problem. In our problem setting, we assume access to a class of stochastic experts, where each expert is a conditional distribution over the arms given a context. We propose upper-confidence bound (UCB) algorithms for this problem, which employ two different importance sampling based estimators for the mean reward for each expert. Both these estimators leverage information leakage among the experts, thus using samples collected under all the experts to estimate the mean reward of any given expert. This leads to instance dependent regret bounds of O (lambda(mu)M log T/Delta), where lambda(mu) is a term that depends on the mean rewards of the experts, Delta is the smallest gap between the mean reward of the optimal expert and the rest, and M quantifies the information leakage among the experts. We show that under some assumptions lambda(mu) is typically O(log N). We implement our algorithm with stochastic experts generated from cost-sensitive classification oracles and show superior empirical performance on real-world datasets, when compared to other state of the art contextual bandit algorithms.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300090,0
C,"Singh, S; Poczos, B; Ma, J",,"Storkey, A; PerezCruz, F",,"Singh, Shashank; Poczos, Barnabas; Ma, Jian",,,Minimax Reconstruction Risk of Convolutional Sparse Dictionary Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Sparse dictionary learning (SDL) has become a popular method for learning parsimonious representations of data, a fundamental problem in machine learning and signal processing. While most work on SDL assumes a training dataset of independent and identically distributed (IID) samples, a variant known as convolutional sparse dictionary learning (CSDL) relaxes this assumption to allow dependent, non-stationary sequential data sources. Recent work has explored statistical properties of IID SDL; however, the statistical properties of CSDL remain largely unstudied. This paper identifies minimax rates of CSDL in terms of reconstruction risk, providing both lower and upper bounds in a variety of settings. Our results make minimal assumptions, allowing arbitrary dictionaries and showing that CSDL is robust to dependent noise. We compare our results to similar results for IID SDL and verify our theory with synthetic experiments.",,,,,"Ma, Jian/A-9838-2008","Ma, Jian/0000-0002-4202-5834",,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300139,0
C,"Garg, VK; Rudin, C; Jaakkola, T",,"Gretton, A; Robert, CC",,"Garg, Vikas K.; Rudin, Cynthia; Jaakkola, Tommi",,,CRAFT: ClusteR-specific Assorted Feature selecTion,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"We present a hierarchical Bayesian framework for clustering with cluster-specific feature selection. We derive a simplified model, CRAFT, by analyzing the asymptotic behavior of the log posterior formulations in a nonparametric MAP-based clustering setting in this framework. CRAFT handles assorted data, i.e., both numeric and categorical data, and the underlying objective functions are intuitively appealing. The resulting algorithm is simple to implement and scales nicely, requires minimal parameter tuning, obviates the need to specify the number of clusters a priori, and compares favorably with other state-of-the-art methods on several datasets. We provide empirical evidence on carefully designed synthetic data sets to highlight the robustness of the algorithm to recover the underlying feature subspaces, even when the average dimensionality of the features across clusters is misspecified. Besides, the framework seamlessly allows for multiple views of clustering by interpolating between the two extremes of cluster-specific feature selection and global selection, and recovers the DP-means objective [14] under the degenerate setting of clustering without feature selection.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,305,313,,,,,,,,,,,,,,,,WOS:000508662100034,0
C,"Gu, QQ; Wang, ZR; Liu, H",,"Gretton, A; Robert, CC",,"Gu, Quanquan; Wang, Zhaoran; Liu, Han",,,Low-Rank and Sparse Structure Pursuit via Alternating Minimization,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"In this paper, we present a nonconvex alternating minimization optimization algorithm for low-rank and sparse structure pursuit. Compared with convex relaxation based methods, the proposed algorithm is computationally more efficient for large scale problems. In our study, we define a notion of bounded difference of gradients, based on which we rigorously prove that with suitable initialization, the proposed nonconvex optimization algorithm enjoys linear convergence to the global optima and exactly recovers the underlying low rank and sparse matrices under standard conditions such as incoherence and sparsity conditions. For a wide range of statistical models such as multi-task learning and robust principal component analysis (RPCA), our algorithm provides a principled approach to learning the low rank and sparse structures with provable guarantee. Thorough experiments on both synthetic and real datasets backup our theory.",,,,,"Wang, Zhaoran/P-7113-2018",,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,600,609,,,,,,,,,,,,,,,,WOS:000508662100066,0
C,"Lim, CH; Wright, SJ",,"Gretton, A; Robert, CC",,"Lim, Cong Han; Wright, Stephen J.",,,Efficient Bregman Projections onto the Permutahedron and Related Polytopes,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"The problem of projecting onto the permutahedron PH(c)-the convex hull of all permutations of a fixed vector c-under a uniformly separable Bregman divergence is shown to be reducible to the Isotonic Optimization problem. This allows us to employ known fast algorithms to improve on several recent results on Bregman projections onto permutahedra. In addition, we present a new algorithm MergeAndPool that have better complexity when the number of distinct entries d in the vector c is small, the simplex being one such example, with c = (1, 0, 0, ..., 0)T and d = 2. MergeAndPool runs in O(n log d) for certain popular Bregman divergence measures and requires O((n log d) log U/epsilon) to find epsilon-close solutions for general uniformly separable Bregman divergences, where U is a bound on the width of the interval containing the dual solution components. These estimates matches or improves best known bounds for all Bregman projection problems onto various permutahedra, including recent results for projection onto the simplex. The same complexity bounds apply to signed permutahedra, a class that includes the l(1)-ball as a special case. In summary, this work describes a fast unified approach to this well-known class of problems.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,1205,1213,,,,,,,,,,,,,,,,WOS:000508662100131,0
C,"Lloyd, C; Gunter, T; Nickson, T; Osborne, MA; Roberts, SJ",,"Gretton, A; Robert, CC",,"Lloyd, Chris; Gunter, Tom; Nickson, Tom; Osborne, Michael A.; Roberts, Stephen J.",,,Latent Poisson Process Allocation,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"We introduce a probabilistic model for the factorisation of continuous Poisson process rate functions. Our model can be thought of as a topic model for Poisson point processes in which each point is assigned to one of a set of latent rate functions that are shared across multiple outputs. We show that the model brings a means of incorporating structure in point process inference beyond the state-of-the-art. We derive an efficient variational inference scheme for the model based on sparse Gaussian processes that scales linearly in the number of data points. Finally, we demonstrate, using examples from spatial and temporal statistics, how the model can be used for discovering hidden structure with greater precision than standard frequentist approaches.",,,,,,"Osborne, Michael/0000-0003-1959-012X",,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,389,397,,,,,,,,,,,,,,,,WOS:000508662100043,0
C,"Oates, CJ; Girolami, M",,"Gretton, A; Robert, CC",,"Oates, Chris J.; Girolami, Mark",,,Control Functionals for Quasi-Monte Carlo Integration,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Quasi-Monte Carlo (QMC) methods are being adopted in statistical applications due to the increasingly challenging nature of numerical integrals that are now routinely encountered. For integrands with d-dimensions and derivatives of order alpha, an optimal QMC rule converges at a best-possible rate O(N-alpha/d). However, in applications the value of alpha can be unknown and/or a rate-optimal QMC rule can be unavailable. Standard practice is to employ alpha(L)-optimal QMC where the lower bound alpha(L) <= alpha is known, but in general this does not exploit the full power of QMC. One solution is to trade-off numerical integration with functional approximation. This strategy is explored herein and shown to be well-suited to modern statistical computation. A challenging application to robotic arm data demonstrates a substantial variance reduction in predictions for mechanical torques.",,,,,,"Girolami, Mark/0000-0003-3008-253X",,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,56,65,,,,,,,,,,,,,,,,WOS:000508662100007,0
C,"Tyagi, H; Kyrillidis, A; Gartner, B; Krause, A",,"Gretton, A; Robert, CC",,"Tyagi, Hemant; Kyrillidis, Anastasios; Gaertner, Bernd; Krause, Andreas",,,Learning Sparse Additive Models with Interactions in High Dimensions,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"A function f : R-d -> R is referred to as a Sparse Additive Model (SPAM), if it is of the form f(x) = Sigma(l is an element of S) phi(l)(x(l)), where S subset of [d], |S| << d. Assuming phi(l)'s and S to be unknown, the problem of estimating f from its samples has been studied extensively. In this work, we consider a generalized SPAM, allowing for second order interaction terms. For some S-1 subset of [d], S-2 subset of (([d])(2)) , the function f is assumed to be of the form: f(x) = Sigma(p is an element of S1) phi(p)(x(p)) + Sigma((l,l')subset of S2) phi((l,l')) (x(l), x(l')). Assuming phi(p), phi((l,l')), S-1 and, S-2 to be unknown, we provide a randomized algorithm that queries f and exactly recovers S-1, S-2. Consequently, this also enables us to estimate the underlying phi(p), phi((l,l')). We derive sample complexity bounds for our scheme and also extend our analysis to include the situation where the queries are corrupted with noise - either stochastic, or arbitrary but bounded. Lastly, we provide simulation results on synthetic data, that validate our theoretical findings.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,111,120,,,,,,,,,,,,,,,,WOS:000508662100013,0
C,"Begin, L; Germain, P; Laviolette, F; Roy, JF",,"Kaski, S; Corander, J",,"Begin, Luc; Germain, Pascal; Laviolette, Francois; Roy, Jean-Francis",,,PAC-Bayesian Theory for Transductive Learning,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"We propose a PAC-Bayesian analysis of the transductive learning setting, introduced by Vapnik [1998], by proposing a family of new bounds on the generalization error. Some of them are derived from their counterpart in the inductive setting, and others are new. We also compare their behavior.",,,,,,"Germain, Pascal/0000-0003-3998-9533",,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,105,113,,,,,,,,,,,,,,,,WOS:000508355800012,0
C,"Moroshko, E; Crammer, K",,"Kaski, S; Corander, J",,"Moroshko, Edward; Crammer, Koby",,,Selective Sampling with Drift,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"Recently there has been much work on selective sampling, an online active learning setting, in which algorithms work in rounds. On each round an algorithm receives an input and makes a prediction. Then, it can decide whether to query a label, and if so to update its model, otherwise the input is discarded. Most of this work is focused on the stationary case, where it is assumed that there is a fixed target model, and the performance of the algorithm is compared to a fixed model. However, in many real-world applications, such as spam prediction, the best target function may drift over time, or have shifts from time to time. We develop a novel selective sampling algorithm for the drifting setting, analyze it under no assumptions on the mechanism generating the sequence of instances, and derive new mistake bounds that depend on the amount of drift in the problem. Simulations on synthetic and real-world datasets demonstrate the superiority of our algorithms as a selective sampling algorithm in the drifting setting.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,651,659,,,,,,,,,,,,,,,,WOS:000508355800072,0
C,"Pena, JM; Sonntag, D; Nielsen, JD",,"Kaski, S; Corander, J",,"Pena, Jose M.; Sonntag, Dag; Nielsen, Jens D.",,,An inclusion optimal algorithm for chain graph structure learning,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"This paper presents and proves an extension of Meek's conjecture to chain graphs under the Lauritzen-Wermuth-Frydenberg interpretation. The proof of the conjecture leads to the development of a structure learning algorithm that finds an inclusion optimal chain graph for any given probability distribution satisfying the composition property. Finally, the new algorithm is experimentally evaluated.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,778,786,,,,,,,,,,,,,,,,WOS:000508355800086,0
C,"Xue, S; Fern, A; Sheldon, D",,"Kaski, S; Corander, J",,"Xue, Shan; Fern, Alan; Sheldon, Daniel",,,Dynamic Resource Allocation for Optimizing Population Diffusion,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"This paper addresses adaptive conservation planning, where the objective is to maximize the population spread of a species by allocating limited resources over time to conserve land parcels. This problem is characterized by having highly stochastic exogenous events (population spread), a large action branching factor (number of allocation options) and state space, and the need to reason about numeric resources. Together these characteristics render most existing AI planning techniques ineffective. The main contribution of this paper is to design and evaluate an online planner for this problem based on Hindsight Optimization (HOP), a technique that has shown promise in other stochastic planning problems. Unfortunately, standard implementations of HOP scale linearly with the number of actions in a domain, which is not feasible for conservation problems such as ours. Thus, we develop a new approach for computing HOP policies based on mixed-integer programming and dual decomposition. Our experiments on synthetic and real-world scenarios show that this approach is effective and scalable compared to existing alternatives.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,1033,1041,,,,,,,,,,,,,,,,WOS:000508355800114,0
C,"Yang, JM; Han, F; Irizarry, RA; Liu, H",,"Kaski, S; Corander, J",,"Yang, Juemin; Han, Fang; Irizarry, Rafael A.; Liu, Han",,,Context Aware Group Nearest Shrunken Centroids in Large-Scale Genomic Studies,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"Recent genomic studies have identified genes related to specific phenotypes. In addition to marginal association analysis for individual genes, analyzing gene pathways (functionally related sets of genes) may yield additional valuable insights. We have devised an approach to phenotype classification from gene expression profiling. Our method named group Nearest Shrunken Centroids (gNSC) is an enhancement of the Nearest Shrunken Centroids (NSC) (Tibshirani, Hastie, Narasimhan and Chu 2002) which is a popular and scalable method to analyze big data. While fully utilizing the variable structure of gene pathways, gNSC shares comparable computational speed as NSC if the group size is small. Comparing with NSC, gNSC improves the power of classification by utilizing the gene pathway information. In practice, we investigate the performance of gNSC on one of the largest microarray datasets aggregated from the internet. We show the effectiveness of our method by comparing the misclassification rate of gNSC with that of NSC. Additionally, we present a novel application of NSC/gNSC on context analysis of association between pathways and certain medical words. Some newest biological findings are rediscovered.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,1051,1059,,,,,,,,,,,,,,,,WOS:000508355800116,0
C,"Anagnostidis, S; Lucchi, A; Diouane, Y",,"Banerjee, A; Fukumizu, K",,"Anagnostidis, Sotiris; Lucchi, Aurelien; Diouane, Youssef",,,Direct-Search for a Class of Stochastic Min-Max Problems,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Recent applications in machine learning have renewed the interest of the community in min-max optimization problems. While gradient-based optimization methods are widely used to solve such problems, there are however many scenarios where these techniques are not well-suited, or even not applicable when the gradient is not accessible. We investigate the use of direct-search methods that belong to a class of derivativefree techniques that only access the objective function through an oracle. In this work, we design a novel algorithm in the context of min-max saddle point games where one sequentially updates the min and the max player. We prove convergence of this algorithm under mild assumptions, where the objective of the max-player satisfies the Polyak-Lojasiewicz (PL) condition, while the minplayer is characterized by a nonconvex objective. Our method only assumes dynamically adjusted accurate estimates of the oracle with a fixed probability. To the best of our knowledge, our analysis is the first one to address the convergence of a direct-search method for min-max objectives in a stochastic setting.",,,,,"Diouane, Youssef/AAN-4161-2020","Diouane, Youssef/0000-0002-6609-7330; Lucchi, Aurelien/0000-0001-7015-2710",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804054,0
C,"Chowdhury, SR; Gopalan, A; Maillard, OA",,"Banerjee, A; Fukumizu, K",,"Chowdhury, Sayak Ray; Gopalan, Aditya; Maillard, Odalric-Ambrym",,,Reinforcement Learning in Parametric MDPs with Exponential Families,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Extending model-based regret minimization strategies for Markov decision processes (MDPs) beyond discrete state-action spaces requires structural assumptions on the reward and transition models. Existing parametric approaches establish regret guarantees by making strong assumptions about either the state transition distribution or the value function as a function of state-action features, and often do not satisfactorily capture classical problems like linear dynamical systems or factored MDPs. This paper introduces a new MDP transition model defined by a collection of linearly parameterized exponential families with d unknown parameters. For finite-horizon episodic RL with horizon H in this MDP model, we propose a model-based upper confidence RL algorithm (Exp-UCRL) that solves a penalized maximum likelihood estimation problem to learn the d-dimensional representation of the transition distribution, balancing the exploitation-exploration tradeoff using confidence sets in the exponential family space. We demonstrate the efficiency of our algorithm by proving a frequentist (worst-case) regret bound that is of order O (d root(HN)-N-3), sub-linear in total time N, linear in dimension d, and polynomial in the planning horizon H. This is achieved by deriving a novel concentration inequality for conditional exponential families that might be of independent interest. The exponential family MDP model also admits an efficient posterior sampling-style algorithm for which a similar guarantee on the Bayesian regret is shown.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802026,0
C,"Covert, I; Lee, SI",,"Banerjee, A; Fukumizu, K",,"Covert, Ian; Lee, Su-In",,,Improving KernelSHAP: Practical Shapley Value Estimation via Linear Regression,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"The Shapley value concept from cooperative game theory has become a popular technique for interpreting ML models, but efficiently estimating these values remains challenging, particularly in the model-agnostic setting. Here, we revisit the idea of estimating Shapley values via linear regression to understand and improve upon this approach. By analyzing the original KernelSHAP alongside a newly proposed unbiased version, we develop techniques to detect its convergence and calculate uncertainty estimates. We also find that the original version incurs a negligible increase in bias in exchange for significantly lower variance, and we propose a variance reduction technique that further accelerates the convergence of both estimators. Finally, we develop a version of KernelSHAP for stochastic cooperative games that yields fast new estimators for two global explanation methods.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804019,0
C,"Gattami, A; Bai, QB; Aggarwal, V",,"Banerjee, A; Fukumizu, K",,"Gattami, Ather; Bai, Qinbo; Aggarwal, Vaneet",,,Reinforcement Learning for Constrained Markov Decision Processes,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"In this paper, we consider the problem of optimization and learning for constrained and multi-objective Markov decision processes, for both discounted rewards and expected average rewards. We formulate the problems as zero-sum games where one player (the agent) solves a Markov decision problem and its opponent solves a bandit optimization problem, which we here call Markov-Bandit games. We extend Q-learning to solve Markov-Bandit games and show that our new Q-learning algorithms converge to the optimal solutions of the zero-sum Markov-Bandit games, and hence converge to the optimal solutions of the constrained and multi-objective Markov decision problems. We provide numerical examples where we calculate the optimal policies and show by simulations that the algorithm converges to the calculated optimal policies. To the best of our knowledge, this is the first time Q-learning algorithms guarantee convergence to optimal stationary policies for the multi-objective Reinforcement Learning problem with discounted and expected average rewards, respectively.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803022,0
C,"Gundersen, GW; Zhang, MM; Engelhardt, BE",,"Banerjee, A; Fukumizu, K",,"Gundersen, Gregory W.; Zhang, Michael Minyi; Engelhardt, Barbara E.",,,Latent Variable Modeling with Random Features,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Gaussian process-based latent variable models are flexible and theoretically grounded tools for nonlinear dimension reduction, but generalizing to non-Gaussian data likelihoods within this nonlinear framework is statistically challenging. Here, we use random features to develop a family of nonlinear dimension reduction models that are easily extensible to nonGaussian data likelihoods; we call these random feature latent variable models (RFLVMs). By approximating a nonlinear relationship between the latent space and the observations with a function that is linear with respect to random features, we induce closed-form gradients of the posterior distribution with respect to the latent variable. This allows the RFLVM framework to support computationally tractable nonlinear latent variable models for a variety of data likelihoods in the exponential family without specialized derivations. Our generalized RFLVMs produce results comparable with other state-of-the-art dimension reduction methods on diverse types of data, including neural spike train recordings, images, and text data.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801062,0
C,"Hu, LJ; Reingold, O",,"Banerjee, A; Fukumizu, K",,"Hu, Lunjia; Reingold, Omer",,,Robust Mean Estimation on Highly Incomplete Data with Arbitrary Outliers,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We study the problem of robustly estimating the mean of a d-dimensional distribution given N examples, where most coordinates of every example may be missing and epsilon N examples may be arbitrarily corrupted. Assuming each coordinate appears in a constant factor more than epsilon N examples, we show algorithms that estimate the mean of the distribution with information-theoretically optimal dimension-independent error guarantees in nearly-linear time (O) over tilde (Nd). Our results extend recent work on computationally-efficient robust estimation to a more widely applicable incomplete-data setting.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801087,0
C,"Hu, TY; Wang, WJ; Lin, C; Cheng, G",,"Banerjee, A; Fukumizu, K",,"Hu, Tianyang; Wang, Wenjia; Lin, Cong; Cheng, Guang",,,Regularization Matters: A Nonparametric Perspective on Overparametrized Neural Network,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Overparametrized neural networks trained by gradient descent (GD) can provably overfit any training data. However, the generalization guarantee may not hold for noisy data. From a nonparametric perspective, this paper studies how well overparametrized neural networks can recover the true target function in the presence of random noises. We establish a lower bound on the L-2 estimation error with respect to the GD iterations, which is away from zero without a delicate scheme of early stopping. In turn, through a comprehensive analysis of l(2)-regularized GD trajectories, we prove that for overparametrized one-hidden-layer ReLU neural network with the l(2) regularization: (1) the output is close to that of the kernel ridge regression with the corresponding neural tangent kernel; (2) minimax optimal rate of the L-2 estimation error can be achieved. Numerical experiments confirm our theory and further demonstrate that the l(2) regularization approach improves the training robustness and works for a wider range of neural networks.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801006,0
C,"Kuzborskij, I; Vernade, C; Gyorgy, A; Szepesvari, C",,"Banerjee, A; Fukumizu, K",,"Kuzborskij, Ilja; Vernade, Claire; Gyorgy, Andras; Szepesvari, Csaba",,,Confident Off-Policy Evaluation and Selection through Self-Normalized Importance Weighting,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We consider off-policy evaluation in the contextual bandit setting for the purpose of obtaining a robust off-policy selection strategy, where the selection strategy is evaluated based on the value of the chosen policy in a set of proposal (target) policies. We propose a new method to compute a lower bound on the value of an arbitrary target policy given some logged data in contextual bandits for a desired coverage. The lower bound is built around the so-called Self-normalized Importance Weighting (SN) estimator. It combines the use of a semi-empirical Efron-Stein tail inequality to control the concentration and Harris' inequality to control the bias. The new approach is evaluated on a number of synthetic and real datasets and is found to be superior to its main competitors, both in terms of tightness of the confidence intervals and the quality of the policies chosen.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,640,+,,,,,,,,,,,,,,,,WOS:000659893800072,0
C,"Li, HC; Li, LY; Xu, XJ; Zhang, XL; Yang, S; Li, B",,"Banerjee, A; Fukumizu, K",,"Li, Huichen; Li, Linyi; Xu, Xiaojun; Zhang, Xiaolu; Yang, Shuang; Li, Bo",,,Nonlinear Projection Based Gradient Estimation for Query Efficient Blackbox Attacks,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Gradient estimation and vector space projection have been studied as two distinct topics. We aim to bridge the gap between the two by investigating how to efficiently estimate gradient based on a projected low-dimensional space. We first provide lower and upper bounds for gradient estimation under both linear and nonlinear gradient projections, and outline checkable sufficient conditions under which one is better than the other. Moreover, we analyze the query complexity for the projection-based gradient estimation and present a sufficient condition for query-efficient estimators. Built upon our theoretic analysis, we propose a novel query-efficient Nonlinear Gradient Projection-based Boundary Black-box Attack (NonLinear-BA). We conduct extensive experiments on four datasets: ImageNet, CelebA, CIFAR-10, and MNIST, and show the superiority of the proposed methods compared with the stateof-the-art baselines. In particular, we show that the projection-based boundary blackbox attacks are able to achieve much smaller magnitude of perturbations with 100% attack success rate based on efficient queries. Both linear and nonlinear projections demonstrate their advantages under different conditions. We also evaluate NonLinear-BA against the commercial online API MEGVII Face++, and demonstrate the high blackbox attack performance both quantitatively and qualitatively. The code is publicly available at https://github.com/AI-secure/NonLinear-BA.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803076,0
C,"Nalisnick, E; Gordon, J; Hernandez-Lobato, JM",,"Banerjee, A; Fukumizu, K",,"Nalisnick, Eric; Gordon, Jonathan; Hernandez-Lobato, Jose Miguel",,,Predictive Complexity Priors,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Specifying a Bayesian prior is notoriously difficult for complex models such as neural networks. Reasoning about parameters is made challenging by the high-dimensionality and over-parameterization of the space. Priors that seem benign and uninformative can have unintuitive and detrimental effects on a model's predictions. For this reason, we propose predictive complexity priors: a functional prior that is defined by comparing the model's predictions to those of a reference model. Although originally defined on the model outputs, we transfer the prior to the model parameters via a change of variables. The traditional Bayesian workflow can then proceed as usual. We apply our predictive complexity prior to high-dimensional regression, reasoning over neural network depth, and sharing of statistical strength for few-shot learning.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,694,+,,,,,,,,,,,,,,,,WOS:000659893800078,0
C,"Radul, A; Alexeev, B",,"Banerjee, A; Fukumizu, K",,"Radul, Alexey; Alexeev, Boris",,,The Base Measure Problem and its Solution,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Probabilistic programming systems generally compute with probability density functions, leaving the base measure of each such function implicit. This mostly works, but creates problems when densities with respect to different base measures are accidentally combined or compared. Mistakes also happen when computing volume corrections for continuous changes of variables, which in general depend on the support measure. We motivate and clarify the problem in the context of a composable library of probability distributions and bijective transformations. We solve the problem by standardizing on Hausdorff measure as a base, and deriving formulas for comparing and combining mixed-dimension densities, as well as updating densities with respect to Hausdorff measure under diffeomorphic transformations. We also propose a software architecture that implements these formulas efficiently in the common case. We hope that by adopting our solution, probabilistic programming systems can become more robust and general, and make a broader class of models accessible to practitioners.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804033,0
C,"Roderick, M; Nagarajan, V; Kolter, JZ",,"Banerjee, A; Fukumizu, K",,"Roderick, Melrose; Nagarajan, Vaishnavh; Kolter, J. Zico",,,Provably Safe PAC-MDP Exploration Using Analogies,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"A key challenge in applying reinforcement learning to safety-critical domains is understanding how to balance exploration (needed to attain good performance on the task) with safety (needed to avoid catastrophic failure). Although a growing line of work in reinforcement learning has investigated this area of safe exploration, most existing techniques either 1) do not guarantee safety during the actual exploration process; and/or 2) limit the problem to a priori known and/or deterministic transition dynamics with strong smoothness assumptions. Addressing this gap, we propose Analogous Safe-state Exploration (ASE), an algorithm for provably safe exploration in Markov Decision Processes (MDPs) with unknown, stochastic dynamics. Our method exploits analogies between state-action pairs to safely learn a near-optimal policy in a PAC-MDP (Probably Approximately Correct-MDP) sense. Additionally, ASE also guides exploration towards the most task-relevant states, which empirically results in significant improvements in terms of sample efficiency, when compared to existing methods. Source code for the experiments is available at https://github.com/locuslab/ase.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801049,0
C,"Bello, K; Ghoshal, A; Honorio, J",,"Chiappa, S; Calandra, R",,"Bello, Kevin; Ghoshal, Asish; Honorio, Jean",,,Minimax Bounds for Structured Prediction Based on Factor Graphs,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Structured prediction can be considered as a generalization of many standard supervised learning tasks, and is usually thought as a simultaneous prediction of multiple labels. One standard approach is to maximize a score function on the space of labels, which usually decomposes as a sum of unary and pairwise potentials, each depending on one or two specific labels, respectively. For this approach, several learning and inference algorithms have been proposed over the years, ranging from exact to approximate methods while balancing the computational complexity. However, in contrast to binary and multiclass classification, results on the necessary number of samples for achieving learning are still limited, even for a specific family of predictors such as factor graphs. In this work, we provide minimax lower bounds for a class of general factor-graph inference models in the context of structured prediction. That is, we characterize the necessary sample complexity for any conceivable algorithm to achieve learning of general factor-graph predictors.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,213,221,,,,,,,,,,,,,,,,WOS:000559931300029,0
C,"Cisneros-Velarde, P; Oh, SY; Petersen, A",,"Chiappa, S; Calandra, R",,"Cisneros-Velarde, Pedro; Oh, Sang-Yun; Petersen, Alexander",,,Distributionally Robust Formulation and Model Selection for the Graphical Lasso,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Building on a recent framework for distributionally robust optimization, we consider estimation of the inverse covariance matrix for multivariate data. We provide a novel notion of a Wasserstein ambiguity set specifically tailored to this estimation problem, leading to a tractable class of regularized estimators. Special cases include penalized likelihood estimators for Gaussian data, specifically the graphical lasso estimator. As a consequence of this formulation, the radius of the Wasserstein ambiguity set is directly related to the regularization parameter in the estimation problem. Using this relationship, the level of robustness of the estimation procedure can be shown to correspond to the level of confidence with which the ambiguity set contains a distribution with the population covariance. Furthermore, a unique feature of our formulation is that the radius can be expressed in closed-form as a function of the ordinary sample covariance matrix. Taking advantage of this finding, we develop a simple algorithm to determine a regularization parameter for graphical lasso, using only the bootstrapped sample covariance matrices, meaning that computationally expensive repeated evaluation of the graphical lasso algorithm is not necessary. Alternatively, the distributionally robust formulation can also quantify the robustness of the corresponding estimator if one uses an off-the-shelf method such as cross-validation. Finally, we numerically study the obtained regularization criterion and analyze the robustness of other automated tuning procedures used in practice.",,,,,,"Oh, Sang-Yun/0000-0002-0364-5109",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,756,764,,,,,,,,,,,,,,,,WOS:000559931300063,0
C,"Curtin, RR; Moseley, B; Ngo, HQ; Nguyen, X; Olteanu, D; Schleich, M",,"Chiappa, S; Calandra, R",,"Curtin, Ryan R.; Moseley, Benjamin; Ngo, Hung Q.; XuanLong Nguyen; Olteanu, Dan; Schleich, Maximilian",,,Rk-means: Fast Clustering for Relational Data,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Conventional machine learning algorithms cannot be applied until a data matrix is available to process. When the data matrix needs to be obtained from a relational database via a feature extraction query, the computation cost can be prohibitive, as the data matrix may be (much) larger than the total input relation size. This paper introduces Rk-means, or relational k-means algorithm, for clustering relational data tuples without having to access the full data matrix. As such, we avoid having to run the expensive feature extraction query and storing its output. Our algorithm leverages the underlying structures in relational data. It involves construction of a small grid coreset of the data matrix for subsequent cluster construction. This gives a constant approximation for the k-means objective, while having asymptotic runtime improvements over standard approaches of first running the database query and then clustering. Empirical results show orders-of-magnitude speedup, and Rk-means can run faster on the database than even just computing the data matrix.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2742,2751,,,,,,,,,,,,,,,,WOS:000559931300067,0
C,"Garcelon, E; Ghavamzadeh, M; Lazaric, A; Pirotta, M",,"Chiappa, S; Calandra, R",,"Garcelon, Evrard; Ghavamzadeh, Mohammad; Lazaric, Alessandro; Pirotta, Matteo",,,Conservative Exploration in Reinforcement Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"While learning in an unknown Markov Decision Process (MDP), an agent should trade off exploration to discover new information about the MDP, and exploitation of the current knowledge to maximize the reward. Although the agent will eventually learn a good or optimal policy, there is no guarantee on the quality of the intermediate policies. This lack of control is undesired in real-world applications where a minimum requirement is that the executed policies are guaranteed to perform at least as well as an existing baseline. In this paper, we introduce the notion of conservative exploration for average reward and finite horizon problems. We present two optimistic algorithms that guarantee (w.h.p.) that the conservative constraint is never violated during learning. We derive regret bounds showing that being conservative does not hinder the learning ability of these algorithms.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1431,1440,,,,,,,,,,,,,,,,WOS:000559931301014,0
C,"Hendrickx, JM; Olshevsky, A; Saligrama, V",,"Chiappa, S; Calandra, R",,"Hendrickx, Julien M.; Olshevsky, Alex; Saligrama, Venkatesh",,,Minimax Rank-1 Matrix Factorization,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We consider the problem of recovering a rank-one matrix when a perturbed subset of its entries is revealed. We propose a method based on least squares in the log-space and show its performance matches the lower bounds that we derive for this problem in the small-perturbation regime, which are related to the spectral gap of a graph representing the revealed entries. Unfortunately, we show that for larger disturbances, potentially exponentially growing errors are unavoidable for any consistent recovery method. We then propose a second algorithm relying on encoding the matrix factorization in the stationary distribution of a certain Markov chain. We show that, under the stronger assumption of known upper and lower bounds on the entries of the true matrix, this second method does not have exponential error growth for large disturbances. Both algorithms can be implemented in nearly linear time.",,,,,"Hendrickx, Julien/AFL-9332-2022",,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303003,0
C,"Kim, D; Hwang, J; Kim, Y",,"Chiappa, S; Calandra, R",,"Kim, Dongha; Hwang, Jaesung; Kim, Yongdai",,,On casting importance weighted autoencoder to an EM algorithm to learn deep generative models,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We propose a new and general approach to learn deep generative models. Our approach is based on a new observation that the importance weighted autoencoders (IWAE, Burda et al. [2015]) can be understood as a procedure of estimating the MLE with an EM algorithm. Utilizing this interpretation, we develop a new learning algorithm called importance weighted EM algorithm (IWEM). IWEM is an EM algorithm with self-normalized importance sampling (snIS) where the proposal distribution is carefully selected to reduce the variance due to snIS. In addition, we devise an annealing strategy to stabilize the learning algorithm. For missing data problems, we propose a modified IWEM algorithm called miss-IWEM. Using multiple benchmark datasets, we demonstrate empirically that our proposed methods outperform IWAE with significant margins for both fully-observed and missing data cases.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2153,2162,,,,,,,,,,,,,,,,WOS:000559931301073,0
C,"Massias, M; Bertrand, Q; Gramfort, A; Salmon, J",,"Chiappa, S; Calandra, R",,"Massias, Mathurin; Bertrand, Quentin; Gramfort, Alexandre; Salmon, Joseph",,,Support recovery and sup-norm convergence rates for sparse pivotal estimation,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"In high dimensional sparse regression, pivotal estimators are estimators for which the optimal regularization parameter is independent of the noise level. The canonical pivotal estimator is the square-root Lasso, formulated along with its derivatives as a non-smooth + non-smooth optimization problem. Modern techniques to solve these include smoothing the datafitting term, to benefit from fast efficient proximal algorithms. In this work we show minimax sup-norm convergence rates for non smoothed and smoothed, single task and multitask square-root Lasso-type estimators. Thanks to our theoretical analysis, we provide some guidelines on how to set the smoothing hyperparameter, and illustrate on synthetic data the interest of such guidelines.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2655,2664,,,,,,,,,,,,,,,,WOS:000559931302033,0
C,"Ren, J; Kunes, R; Doshi-Velez, F",,"Chiappa, S; Calandra, R",,"Ren, Jason; Kunes, Russell; Doshi-Velez, Finale",,,Prediction Focused Topic Models via Feature Selection,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Supervised topic models are often sought to balance prediction quality and interpretability. However, when models are (inevitably) misspecified, standard approaches rarely deliver on both. We introduce a novel approach, the prediction-focused topic model, that uses the supervisory signal to retain only vocabulary terms that improve, or at least do not hinder, prediction performance. By removing terms with irrelevant signal, the topic model is able to learn task-relevant, coherent topics. We demonstrate on several data sets that compared to existing approaches, prediction-focused topic models learn much more coherent topics while maintaining competitive predictions.",,,,,"Ren, Zhiyong Jason/A-7401-2019","Ren, Zhiyong Jason/0000-0001-7606-0331",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,4420,4428,,,,,,,,,,,,,,,,WOS:000559931302090,0
C,"Shah, V; Wu, XX; Sanghavi, S",,"Chiappa, S; Calandra, R",,"Shah, Vatsal; Wu, Xiaoxia; Sanghavi, Sujay",,,Choosing the Sample with Lowest Loss makes SGD Robust,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"The presence of outliers can potentially significantly skew the parameters of machine learning models trained via stochastic gradient descent (SGD). In this paper we propose a simple variant of the SGD method: in each step, first choose a set of k samples, then from these choose the one with the smallest current loss, and do an SGD-like update with this chosen sample. Vanilla SGD corresponds to k = 1, i.e. no choice; k >= 2 represents a new algorithm that is however effectively minimizing a non-convex surrogate loss. Our main contribution is a theoretical analysis of the robustness properties of this idea for machine learning problems which are sums of convex losses; these are backed up with synthetic and neural network experiments.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303009,0
C,"Berthet, Q; Kanade, V",,"Chaudhuri, K; Sugiyama, M",,"Berthet, Quentin; Kanade, Varun",,,Statistical Windows in Testing for the Initial Distribution of a Reversible Markov Chain,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We study the problem of hypothesis testing between two discrete distributions, where we only have access to samples after the action of a known reversible Markov chain, playing the role of noise. We derive instance-dependent minimax rates for the sample complexity of this problem, and show how its dependence in time is related to the spectral properties of the Markov chain. We show that there exists a wide statistical window, in terms of sample complexity for hypothesis testing between different pairs of initial distributions. We illustrate these results in several concrete examples.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,246,255,,,,,,,,,,,,,,,,WOS:000509687900026,0
C,"Chen, Y; Silva, T; Prudencio, RBC; Diethe, T; Flach, P",,"Chaudhuri, K; Sugiyama, M",,"Chen, Yu; Silva Filho, Telmo; Prudencio, Ricardo B. C.; Diethe, Tom; Flach, Peter",,,beta(3)-IRT: A New Item Response Model and its Applications,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Item Response Theory (IRT) aims to assess latent abilities of respondents based on the correctness of their answers in aptitude test items with different difficulty levels. In this paper, we propose the beta(3)-IRT model, which models continuous responses and can generate a much enriched family of Item Characteristic Curves. In experiments we applied the proposed model to data from an online exam platform, and show our model outperforms a more standard 2PL-ND model on all datasets. Furthermore, we show how to apply beta(3)-IRT to assess the ability of machine learning classifiers. This novel application results in a new metric for evaluating the quality of the classifier's probability estimates, based on the inferred difficulty and discrimination of data instances.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901006,0
C,"Gidel, G; Hemmat, RA; Pezeshki, M; Le Priol, R; Huang, G; Lacoste-Julien, S; Mitliagkas, I",,"Chaudhuri, K; Sugiyama, M",,"Gidel, Gauthier; Hemmat, Reyhane Askari; Pezeshki, Mohammad; Le Priol, Remi; Huang, Gabriel; Lacoste-Julien, Simon; Mitliagkas, Ioannis",,,Negative Momentum for Improved Game Dynamics,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Games generalize the single-objective optimization paradigm by introducing different objective functions for different players. Differentiable games often proceed by simultaneous or alternating gradient updates. In machine learning, games are gaining new importance through formulations like generative adversarial networks (GANs) and actorcritic systems. However, compared to single-objective optimization, game dynamics is more complex and less understood. In this paper, we analyze gradient-based methods with momentum on simple games. We prove that alternating updates are more stable than simultaneous updates. Next, we show both theoretically and empirically that alternating gradient updates with a negative momentum term achieves convergence in a difficult toy adversarial problem, but also on the notoriously difficult to train saturating GANs.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901088,0
C,"Kushnir, D; Jalali, S; Saniee, I",,"Chaudhuri, K; Sugiyama, M",,"Kushnir, Dan; Jalali, Shirin; Saniee, Iraj",,,Towards Clustering High-dimensional Gaussian Mixture Clouds in Linear Running Time,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Clustering mixtures of Gaussian distributions is a fundamental and challenging problem. State-of-the-art theoretical work on learning Gaussian mixture models has mostly focused on estimating the mixture parameters, where clustering is given as a byproduct. These methods have focused mostly on improving separation bounds for different mixture classes, and doing so in polynomial time and sample complexity. Less emphasis has been given to aligning these algorithms to the challenges of big data. In this paper, we focus on clustering n samples from an arbitrary mixture of c-separated Gaussians in I p in time that is linear in p and n, and sample complexity that is independent of p. Our analysis suggests that for sufficiently separated Gaussians after o(logp) random projections a good direction is found that yields a small clustering error. Specifically, for a user-specified error e, the expected number of such projections is small and bounded by o(ln p) when < cln In p and ry= Q -1(e) is the separation of the Gaussians with Q as the tail distribution function of the normal distribution. Consequently, the expected overall running time of the algorithm is linear in n and quasilinear in p at o(ln p)O(np), and the sample complexity is independent of p. Unlike the methods that are based on k-means, our analysis is applicable to any mixture class (spherical or non-spherical). Finally, an extension to k > 2 components is also provided.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901044,0
C,"Nash, C; Kushman, N; Williams, CKI",,"Chaudhuri, K; Sugiyama, M",,"Nash, Charlie; Kushman, Nate; Williams, Christopher K., I",,,Inverting Supervised Representations with Autoregressive Neural Density Models,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We present a method for feature interpretation that makes use of recent advances in autoregressive density estimation models to invert model representations. We train generative inversion models to express a distribution over input features conditioned on intermediate model representations. Insights into the invariances learned by supervised models can be gained by viewing samples from these inversion models. In addition, we can use these inversion models to estimate the mutual information between a model's inputs and its intermediate representations, thus quantifying the amount of information preserved by the network at different stages. Using this method we examine the types of information preserved at different layers of convolutional neural networks, and explore the invariances induced by different architectural choices. Finally we show that the mutual information between inputs and network layers initially increases and then decreases over the course of training, supporting recent work by Shwartz-Ziv and Tishby (2017) on the information bottleneck theory of deep learning.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901069,0
C,"Oliveira, R; Ott, L; Ramos, F",,"Chaudhuri, K; Sugiyama, M",,"Oliveira, Rafael; Ott, Lionel; Ramos, Fabio",,,Sayesian optimisation under uncertain inputs,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Bayesian optimisation (BO) has been a successful approach to optiunse functions which are expensive to evaluate and whose observations are noisy, Classical BO algonthi s, however, do not, account for errors about the location where observations are taken, which is a common issue in problem with physical components. In these cases, the estimation of the actual query location is also subject to uncertainty. In this context, we propose an upper confidence bound (UCB) algorithm for BO problems where both the outcome of a query and the true query location arc uncertain. The algorithm employs a Gaussian process model that takes probability distributions as inputs. Theoretical results are provided for both the proposed algorithm and a conventional UCB approach within the uncertain-inputs setting. Finally, we evaluate each method's performance experimentally, comparing them to other input noise aware BO approaches on simulated scenarios involving synthetic and real data,.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901023,0
C,"Pandit, P; Sahraee, M; Amini, AA; Rangan, S; Fletcher, AK",,"Chaudhuri, K; Sugiyama, M",,"Pandit, Parthe; Sahraee, Mojtaba; Amini, Arash A.; Rangan, Sundeep; Fletcher, Alyson K.",,,Sparse Multivariate Bernoulli Processes in High Dimensions,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We consider the problem of estimating the parameters of a multivariate Bernoulli process with auto-regressive feedback in the high dimensional setting where the number of samples available is much less than the number of parameters. This problem arises in learning interconnections of networks of dynamical systems with spiking or binary valued data. We also allow the process to depend on its past up to a lag p, for a general p >= 1, allowing for more realistic modeling in many applications. We propose and analyze an l(1)-regularized maximum likelihood (ML) estimator under the assumption that the parameter tensor is approximately sparse. Rigorous analysis of such estimators is made challenging by the dependent and non-Gaussian nature of the process as well as the presence of the nonlinearities and multi-level feedback. We derive precise upper bounds on the mean-squared estimation error in terms of the number of samples, dimensions of the process, the lag p and other key statistical properties of the model. The ideas presented can be used in the rigorous high-dimensional analysis of regularized M-estimators for other sparse nonlinear and non-Gaussian processes with long-range dependence.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,457,466,,,,,,,,,,,,,,,,WOS:000509687900048,0
C,"Saad, FA; Freer, CE; Ackerman, NL; Mansinghka, VK",,"Chaudhuri, K; Sugiyama, M",,"Saad, Feras A.; Freer, Cameron E.; Ackerman, Nathanael L.; Mansinghka, Vikash K.",,,A Family of Exact Goodness-of-Fit Tests for High-Dimensional Discrete Distributions,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"The objective of goodness-of-fit testing is to assess whether a dataset of observations is likely to have been drawn from a candidate probability distribution. This paper presents a rank-based family of goodness-of-fit tests that is specialized to discrete distributions on high-dimensional domains. The test is readily implemented using a simulation-based, linear-time procedure. The testing procedure can be customized by the practitioner using knowledge of the underlying data domain. Unlike most existing test statistics, the proposed test statistic is distribution-free and its exact (non-asymptotic) sampling distribution is known in closed form. We establish consistency of the test against all alternatives by showing that the test statistic is distributed as a discrete uniform if and only if the samples were drawn from the candidate distribution. We illustrate its efficacy for assessing the sample quality of approximate sampling algorithms over combinatorially large spaces with intractable probabilities, including random partitions in Dirichlet process mixture models and random lattices in Ising models.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901071,0
C,"Tassarotti, J; Tristan, JB; Wick, M",,"Chaudhuri, K; Sugiyama, M",,"Tassarotti, Joseph; Tristan, Jean-Baptiste; Wick, Michael",,,Sketching for Latent Dirichlet-Categorical Models,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Recent work has explored transforming data sets into smaller, approximate summaries in order to scale Bayesian inference. We examine a related problem in which the parameters of a Bayesian model are very large and expensive to store ill memory, and propose more compact representations of parameter values that can be used during inference. We focus on a class of graphical models that we refer to as latent Dirichlet-Categorical models, and show how a combination of two sketching algorithms known as count-min sketch and approximate counters provide an efficient representation for them. We show that this sketch combination which, despite having been used before in NLP applications, has not been previously analyzed enjoys desirable properties. We prove that for this class of models, when the sketches are used during Markov Chain Monte Carlo inference, the equilibrium of sketched MCMC, converges to that of the exact chain as sketch parameters are tuned to reduce the error rate.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,256,265,,,,,,,,,,,,,,,,WOS:000509687900027,0
C,"Vaicenavicius, J; Widmann, D; Andersson, C; Lindsten, F; Roll, J; Schon, TB",,"Chaudhuri, K; Sugiyama, M",,"Vaicenavicius, Juozas; Widmann, David; Andersson, Carl; Lindsten, Fredrik; Roll, Jacob; Schon, Thomas B.",,,Evaluating model calibration in classification,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Probabilistic classifiers output a probability distribution on target classes rather than just a class prediction. Besides providing a clear separation of prediction and decision making, the main advantage of probabilistic models is their ability to represent uncertainty about predictions. In safety-critical applications, it is pivotal for a model to possess an adequate sense of uncertainty, which for probabilistic classifiers translates into outputting probability distributions that are consistent with the empirical frequencies observed from realized outcomes. A classifier with such a property is called calibrated. In this work, we develop a general theoretical calibration evaluation framework grounded in probability theory, and point out subtleties present in model calibration evaluation that lead to refined interpretations of existing evaluation techniques. Lastly, we propose new ways to quantify and visualize miscalibration in probabilistic classification, including novel multidimensional reliability diagrams.",,,,,"Widmann, David/HCJ-0034-2022","Widmann, David/0000-0001-9282-053X",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687903053,0
C,"Veitch, V; Austern, M; Zhou, WD; Blei, DM; Orbanz, P",,"Chaudhuri, K; Sugiyama, M",,"Veitch, Vicor; Austern, Morgane; Zhou, Wenda; Blei, David M.; Orbanz, Peter",,,Empirical Risk Minimization and Stochastic Gradient Descent for Relational Data,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Empirical risk minimization is the main tool for prediction problems, but its extension to relational data remains unsolved. We solve this problem using recent ideas from graph sampling theory to (i) define an empirical risk for relational data and (ii) obtain stochastic gradients for this empirical risk that are automatically unbiased. This is achieved by considering the method by which data is sampled from a graph as an explicit component of model design. By integrating fast implementations of graph sampling schemes with standard automatic differentiation tools, we provide an efficient turnkey solver for the risk minimization problem. We establish basic theoretical properties of the procedure. Finally, we demonstrate relational ERM with application to two non-standard problems: one-stage training for semi-supervised node classification, and learning embedding vectors for vertex attributes. Experiments confirm that the turnkey inference procedure is effective in practice, and that the sampling scheme used for model specification has a strong effect on model performance. Code is available at github.com/wooden-spoon/relational-ERM.",,,,,"Zhou, Wenda/AFF-2109-2022","Zhou, Wenda/0000-0001-5549-7884",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901081,0
C,"Zhang, J; May, A; Dao, T; Re, C",,"Chaudhuri, K; Sugiyama, M",,"Zhang, Jian; May, Avner; Dao, Tri; Re, Christopher",,,Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We investigate how to train kernel approximation methods that generalize well under a memory budget. Building on recent theoretical work, we define a measure of kernel approximation error which we find to be more predictive of the empirical generalization performance of kernel approximation methods than conventional metrics. An important consequence of this definition is that a kernel approximation matrix must be high rank to attain close approximation. Because storing a high-rank approximation is memory intensive, we propose using a low-precision quantization of random Fourier features (LP-RFFs) to build a high-rank approximation under a memory budget. Theoretically, we show quantization has a negligible effect on generalization performance in important settings. Empirically, we demonstrate across four benchmark datasets that LP-RFFs can match the performance of full-precision RFFs and the Nystrom method, with 3x-10x and 50x-460x less memory, respectively.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901032,0
C,"Awasthi, P; Kalantari, B; Zhang, YK",,"Storkey, A; PerezCruz, F",,"Awasthi, Pranjal; Kalantari, Bahman; Zhang, Yikai",,,Robust Vertex Enumeration for Convex Hulls in High Dimensions,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"We design a fast and robust algorithm named All Vertex Traingle Algorithm (AVTA) for detecting the vertices of the convex hull of a set of points in high dimensions. Our proposed algorithm is very general and works for arbitrary convex hulls. In addition to being a fundamental problem in computational geometry and linear programming, vertex enumeration in high dimensions has numerous applications in machine learning. In particular, we apply AVTA to design new practical algorithms for topic models and non-negative matrix factorization. For topic models, our new algorithm leads to significantly better reconstruction of the topic-word matrix than state of the art approaches [2, 5]. Additionally, we provide a robust analysis of AVTA and empirically demonstrate that it can handle larger amounts of noise than existing methods. For non-negative matrix factorization we show that AVTA is competitive with existing methods that are specialized for this task [3].",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300145,0
C,"Grunewalder, S",,"Storkey, A; PerezCruz, F",,"Grunewalder, Steffen",,,Plug-in Estimators for Conditional Expectations and Probabilities,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"We study plug-in estimators of conditional expectations and probabilities, and we provide a systematic analysis of their rates of convergence. The plug-in approach is particularly useful in this setting since it introduces a natural link to VC- and empirical process theory. We make use of this link to derive rates of convergence that hold uniformly over large classes of functions and sets, and under various conditions. For instance, we demonstrate that elementary conditional probabilities are estimated by these plug-in estimators with a rate of n(alpha-1/2) one conditions with a VC-class of sets and where alpha is an element of [0, 1/2) controls a lower bound on the size of sets we can estimate given n samples. We gain similar results for Kolmogorov's conditional expectation and probability which generalize the elementary forms of conditioning. Due to their simplicity, plug-in estimators can be evaluated in linear time and there is no up-front cost for inference.",,,,,,"Grunewalder, Steffen/0000-0002-4017-2048",,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300158,0
C,"Mitrovic, M; Feldman, M; Krause, A; Karbasi, A",,"Storkey, A; PerezCruz, F",,"Mitrovic, Marko; Feldman, Moran; Krause, Andreas; Karbasi, Amin",,,Submodularity on Hypergraphs: From Sets to Sequences,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"In a nutshell, submodular functions encode an intuitive notion of diminishing returns. As a result, submodularity appears in many important machine learning tasks such as feature selection and data summarization. Although there has been a large volume of work devoted to the study of submodular functions in recent years, the vast majority of this work has been focused on algorithms that output sets, not sequences. However, in many settings, the order in which we output items can be just as important as the items themselves. To extend the notion of submodularity to sequences, we use a directed graph on the items where the edges encode the additional value of selecting items in a particular order. Existing theory is limited to the case where this underlying graph is a directed acyclic graph. In this paper, we introduce two new algorithms that provably give constant factor approximations for general graphs and hypergraphs having bounded in or out degrees. Furthermore, we show the utility of our new algorithms for real-world applications in movie recommendation, online link prediction, and the design of course sequences for MOOCs.",,,,,"Gurfil, Pini/AAA-4072-2020","Krause, Andreas/0000-0001-7260-9673",,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300123,0
C,"Ratsaby, J",,"Storkey, A; PerezCruz, F",,"Ratsaby, Joel",,,On how complexity affects the stability of a predictor,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Given a finite random sample from a Markov chain environment, we select a predictor that minimizes a criterion function and refer to it as being calibrated to its environment. If its prediction error is not bounded by its criterion value, we say that the criterion fails. We define the predictor's complexity to be the amount of uncertainty in detecting that the criterion fails given that it fails. We define a predictor's stability to be the discrepancy between the average number of prediction errors that it makes on two random samples. We show that complexity is inversely proportional to the level of adaptivity of the calibrated predictor to its random environment. The calibrated predictor becomes less stable as its complexity increases or as its level of adaptivity decreases.",,,,,"Ratsaby, Joel/AAV-8979-2021",,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300018,0
C,"Sakaue, S; Ishihata, M; Minato, S",,"Storkey, A; PerezCruz, F",,"Sakaue, Shinsaku; Ishihata, Masakazu; Minato, Shin-ichi",,,Efficient Bandit Combinatorial Optimization Algorithm with Zero-suppressed Binary Decision Diagrams,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"We consider bandit combinatorial optimization (BCO) problems. A BCO instance generally has a huge set of all feasible solutions, which we call the action set. To avoid dealing with such huge action sets directly, we propose an algorithm that takes advantage of zerosuppressed binary decision diagrams, which encode action sets as compact graphs. The proposed algorithm achieves either O(T2/3) regret with high probability or O(root T) expected regret at any T-th round. Typically, our algorithm works efficiently for BCO problems defined on networks. Experiments show that our algorithm is applicable to various large BCO instances including adaptive routing problems on real-world networks.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300062,0
C,"Sutera, A; Chatel, C; Louppe, G; Wehenkel, L; Geurts, P",,"Storkey, A; PerezCruz, F",,"Sutera, Antonio; Chatel, Celia; Louppe, Gilles; Wehenkel, Louis; Geurts, Pierre",,,Random Subspace with Trees for Feature Selection Under Memory Constraints,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Dealing with datasets of very high dimension is a major challenge in machine learning. In this paper, we consider the problem of feature selection in applications where the memory is not large enough to contain all features. In this setting, we propose a novel tree-based feature selection approach that builds a sequence of randomized trees on small sub-samples of variables mixing both variables already identified as relevant by previous models and variables randomly selected among the other variables. As our main contribution, we provide an in-depth theoretical analysis of this method in infinite sample setting. In particular, we study its soundness with respect to common definitions of feature relevance and its convergence speed under various variable dependance scenarios. We also provide some preliminary empirical results highlighting the potential of the approach.",,,,,"Louppe, Gilles/D-1923-2017","Louppe, Gilles/0000-0002-2082-3106",,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300098,0
C,"Xu, HT; Luo, DX; Chen, X; Carin, L",,"Storkey, A; PerezCruz, F",,"Xu, Hongteng; Luo, Dixin; Chen, Xu; Carin, Lawrence",,,Benefits from Superposed Hawkes Processes,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"The superposition of temporal point processes has been studied for many years, although the usefulness of such models for practical applications has not be fully developed. We investigate superposed Hawkes process as an important class of such models, with properties studied in the framework of least squares estimation. The superposition of Hawkes processes is demonstrated to be beneficial for tightening the upper bound of excess risk under certain conditions, and we show the feasibility of the benefit in typical situations. The usefulness of superposed Hawkes processes is verified on synthetic data, and its potential to solve the cold-start problem of recommendation systems is demonstrated on real-world data.",,,,,"Xu, Hongteng/AAB-1636-2021","Carin, Lawrence/0000-0001-6277-7948",,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300066,0
C,"Nguyen, CH; Mamitsuka, H",,"Gretton, A; Robert, CC",,"Canh Hao Nguyen; Mamitsuka, Hiroshi",,,New Resistance Distances with Global Information on Large Graphs,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"We consider the problem that on large random geometric graphs, random walk-based distances between nodes do not carry global information such as cluster structure. Instead, as the graphs become larger, the distances contain mainly the obsolete information of local density of the nodes. Many distances or similarity measures between nodes on a graph have been proposed but none are both proved to overcome this problem or computationally feasible even for small graphs. We propose new distance functions between nodes for this problem. The idea is to use electrical flows with different energy functions. Our proposed distances are proved to be metrics in L-p spaces, to keep global information, avoiding the problem, and can be computed efficiently for large graphs. Our experiments with synthetic and real data confirmed the theoretical properties and practical performances of our proposed distances.",,,,,,"Nguyen, Canh Hao/0000-0002-0274-5693",,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,639,647,,,,,,,,,,,,,,,,WOS:000508662100070,0
C,"Park, M; Jitkrittum, W; Sejdinovic, D",,"Gretton, A; Robert, CC",,"Park, Mijung; Jitkrittum, Wittawat; Sejdinovic, Dino",,,K2-ABC: Approximate Bayesian Computation with Kernel Embeddings,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Complicated generative models often result in a situation where computing the likelihood of observed data is intractable, while simulating from the conditional density given a parameter value is relatively easy. Approximate Bayesian Computation (ABC) is a paradigm that enables simulation-based posterior inference in such cases by measuring the similarity between simulated and observed data in terms of a chosen set of summary statistics. However, there is no general rule to construct sufficient summary statistics for complex models. Insufficient summary statistics will leak information, which leads to ABC algorithms yielding samples from an incorrect posterior. In this paper, we propose a fully nonparametric ABC paradigm which circumvents the need for manually selecting summary statistics. Our approach, K2-ABC, uses maximum mean discrepancy (MMD) to construct a dissimilarity measure between the observed and simulated data. The embedding of an empirical distribution of the data into a reproducing kernel Hilbert space plays a role of the summary statistic and is sufficient whenever the corresponding kernels are characteristic. Experiments on a simulated scenario and a real-world biological problem illustrate the effectiveness of the proposed algorithm.",,,,,,"Sejdinovic, Dino/0000-0001-5547-9213",,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,398,407,,,,,,,,,,,,,,,,WOS:000508662100044,0
C,"Rabusseau, G; Balle, B; Cohen, SB",,"Gretton, A; Robert, CC",,"Rabusseau, Guillaume; Balle, Borja; Cohen, Shay B.",,,Low-Rank Approximation of Weighted Tree Automata,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"We describe a technique to minimize weighted tree automata (WTA), a powerful formalism that subsumes probabilistic context-free grammars (PCFGs) and latent-variable PCFGs. Our method relies on a singular value decomposition of the underlying Hankel matrix defined by the WTA. Our main theoretical result is an efficient algorithm for computing the SVD of an infinite Hankel matrix implicitly represented as a WTA. We evaluate our method on real-world data originating in newswire treebank. We show that our approach achieves lower perplexity than previous methods for PCFG minimization, and also is much more stable due to the absence of local optima.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,839,847,,,,,,,,,,,,,,,,WOS:000508662100091,0
C,"Chen, EYJ; Pearl, J",,"Kaski, S; Corander, J",,"Chen, Eunice Yuh-Jie; Pearl, Judea",,,Random Bayesian networks with bounded indegree,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"Bayesian networks (BN) are an extensively used graphical model for representing a probability distribution in artificial intelligence, data mining, and machine learning. In this paper, we propose a simple model for large random BNs with bounded indegree, that is, large directed acyclic graphs (DAG) where the edges appear at random and each node has at most a given number of parents. Using this model, we can study useful asymptotic properties of large BNs and BN algorithms with basic combinatorics tools. We estimate the expected size of a BN, the expected size increase of moralization, the expected size of the Markov blanket, and the maximum size of a minimal d-separator. We also provide an upper bound on the average time complexity of an algorithm for finding a minimal d-separator. In addition, the estimates are evaluated against BNs learned from real world data.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,114,121,,,,,,,,,,,,,,,,WOS:000508355800013,0
C,"Sun, SQ; Zhu, YC; Xu, JB",,"Kaski, S; Corander, J",,"Sun, Siqi; Zhu, Yuancheng; Xu, Jinbo",,,Adaptive Variable Clustering in Gaussian Graphical Models,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"Gaussian graphical models (GGMs) are widely-used to describe the relationship between random variables. In many real-world applications, GGMs have a block structure in the sense that the variables can be clustered into groups so that inter-group correlation is much weaker than intra-group correlation. We present a novel nonparametric Bayesian generative model for such a block-structured GGM and an efficient inference algorithm to find the clustering of variables in this GGM by combining a Gibbs sampler and a split-merge Metropolis-Hastings algorithm. Experimental results show that our method performs well on both synthetic and real data. In particular, our method outperforms generic clustering algorithms and can automatically identify the true number of clusters.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,931,939,,,,,,,,,,,,,,,,WOS:000508355800103,0
C,"Vladymyrov, M; Carreira-Perpinan, MA",,"Kaski, S; Corander, J",,"Vladymyrov, Max; Carreira-Perpinan, Miguel A.",,,Linear-time Training of Nonlinear Low-Dimensional Embeddings,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"Nonlinear embeddings such as stochastic neighbor embedding or the elastic embedding achieve better results than spectral methods but require an expensive, nonconvex optimization, where the objective function and gradient are quadratic on the sample size. We address this bottleneck by formulating the optimization as an N-body problem and using fast multipole methods (FMMs) to approximate the gradient in linear time. We study the effect, in theory and experiment, of approximating gradients in the optimization and show that the expected error is related to the mean curvature of the objective function, and that gradually increasing the accuracy level in the FMM over iterations leads to a faster training. When combined with standard optimizers, such as gradient descent or L-BFGS, the resulting algorithm beats the O(N log N) Barnes-Hut method and achieves reasonable embeddings for one million points in around three hours' runtime.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,968,977,,,,,,,,,,,,,,,,WOS:000508355800107,0
C,"Yang, LF; Hanrahan, P; Goodman, ND",,"Kaski, S; Corander, J",,"Yang, Lingfeng; Hanrahan, Pat; Goodman, Noah D.",,,Generating Efficient MCMC Kernels from Probabilistic Programs,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"Universal probabilistic programming languages (such as CHURCH [6]) trade performance for abstraction: any model can be represented compactly as an arbitrary stochastic computation, but costly online analyses are required for inference. We present a technique that recovers hand-coded levels of performance from a universal probabilistic language, for the Metropolis-Hastings (MH) MCMC inference algorithm. It takes a CHURCH program as input and traces its execution to remove computation overhead. It then analyzes the trace for each proposal, using slicing, to identify the minimal computation needed to evaluate the MH acceptance probability. Generated incremental code is much faster than a baseline implementation (up to 600x) and usually as fast as hand-coded MH kernels.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,1068,1076,,,,,,,,,,,,,,,,WOS:000508355800118,0
C,"Bertrand, Q; Massias, M",,"Banerjee, A; Fukumizu, K",,"Bertrand, Quentin; Massias, Mathurin",,,Anderson acceleration of coordinate descent,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Acceleration of first order methods is mainly obtained via inertia a la Nesterov, or via nonlinear extrapolation. The latter has known a recent surge of interest, with successful applications to gradient and proximal gradient techniques. On multiple Machine Learning problems, coordinate descent achieves performance significantly superior to full-gradient methods. Speeding up coordinate descent in practice is not easy: inertially accelerated versions of coordinate descent are theoretically accelerated, but might not always lead to practical speed-ups. We propose an accelerated version of coordinate descent using extrapolation, showing considerable speed up in practice, compared to inertial accelerated coordinate descent and extrapolated (proximal) gradient descent. Experiments on least squares, Lasso, elastic net and logistic regression validate the approach.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801057,0
C,"Budhathoki, K; Janzing, D; Blobaum, P; Ng, H",,"Banerjee, A; Fukumizu, K",,"Budhathoki, Kailash; Janzing, Dominik; Bloebaum, Patrick; Ng, Hoiyi",,,Why did the distribution change?,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We describe a formal approach based on graphical causal models to identify the root causes of the change in the probability distribution of variables. After factorizing the joint distribution into conditional distributions of each variable, given its parents (the causal mechanisms), we attribute the change to changes of these causal mechanisms. This attribution analysis accounts for the fact that mechanisms often change independently and sometimes only some of them change. Through simulations, we study the performance of our distribution change attribution proposal. We then present a real-world case study identifying the drivers of the difference in the income distribution between men and women.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802005,0
C,"Chatziafratis, V; Mahdian, M; Ahmadian, S",,"Banerjee, A; Fukumizu, K",,"Chatziafratis, Vaggos; Mahdian, Mohammad; Ahmadian, Sara",,,"Maximizing Agreements for Ranking, Clustering and Hierarchical Clustering via MAX-CUT",24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"In this paper, we study a number of well-known combinatorial optimization problems that fit in the following paradigm: the input is a collection of (potentially inconsistent) local relationships between the elements of a ground set (e.g., pairwise comparisons, similar/dissimilar pairs, or ancestry structure of triples of points), and the goal is to aggregate this information into a global structure (e.g., a ranking, a clustering, or a hierarchical clustering) in a way that maximizes agreement with the input. Well-studied problems such as rank aggregation, correlation clustering, and hierarchical clustering with triplet constraints fall in this class of problems. We study these problems on stochastic instances with a hidden embedded ground truth solution. Our main algorithmic contribution is a unified technique that uses the maximum cut problem in graphs to approximately solve these problems. Using this technique, we can often get approximation guarantees in the stochastic setting that are better than the known worst case inapproximability bounds for the corresponding problem. On the negative side, we improve the worst case inapproximability bound on several hierarchical clustering formulations through a reduction to related ranking problems.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802004,0
C,"Deng, YY; Mandavi, M",,"Banerjee, A; Fukumizu, K",,"Deng, Yuyang; Mandavi, Mehrdad",,,Local Stochastic Gradient Descent Ascent: Convergence Analysis and Communication Efficiency,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Local SGD is a promising approach to overcome the communication overhead in distributed learning by reducing the synchronization frequency among worker nodes. Despite the recent theoretical advances of local SGD in empirical risk minimization, the efficiency of its counterpart in minimax optimization remains unexplored. Motivated by large scale minimax learning problems, such as adversarial robust learning and training generative adversarial networks (GANs), we propose local Stochastic Gradient Descent Ascent (local SGDA), where the primal and dual variables can be trained locally and averaged periodically to significantly reduce the number of communications. We show that local SGDA can provably optimize distributed minimax problems in both homogeneous and heterogeneous data with reduced number of communications and establish convergence rates under strongly-convex-strongly-concave and nonconvex-strongly-concave settings. In addition, we propose a novel variant local SGDA+, to solve nonconvex-nonconcave problems. We give corroborating empirical evidence on different distributed minimax problems.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801068,0
C,"Gorbunov, E; Hanzely, F; Richtarik, P",,"Banerjee, A; Fukumizu, K",,"Gorbunov, Eduard; Hanzely, Filip; Richtarik, Peter",,,Local SGD: Unified Theory and New Efficient Methods,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We present a unified framework for analyzing local SGD methods in the convex and strongly convex regimes for distributed/federated training of supervised machine learning models. We recover several known methods as a special case of our general framework, including Local-SGD/FedAvg, SCAFFOLD, and several variants of SGD not originally designed for federated learning. Our framework covers both the identical and heterogeneous data settings, supports both random and deterministic number of local steps, and can work with a wide array of local stochastic gradient estimators, including shifted estimators which are able to adjust the fixed points of local iterations for faster convergence. As an application of our framework, we develop multiple novel FL optimizers which are superior to existing methods. In particular, we develop the first linearly converging local SGD method which does not require any data homogeneity or other strong assumptions.",,,,,"Gorbunov, Eduard/U-1740-2019","Gorbunov, Eduard/0000-0002-3370-4130",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804030,0
C,"Hayase, T; Karakida, R",,"Banerjee, A; Fukumizu, K",,"Hayase, Tomohiro; Karakida, Ryo",,,The Spectrum of Fisher Information of Deep Networks Achieving Dynamical Isometry,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"The Fisher information matrix (FIM) is fundamental to understanding the trainability of deep neural nets (DNN), since it describes the parameter space's local metric. We investigate the spectral distribution of the conditional FIM, which is the FIM given a single sample, by focusing on fully-connected networks achieving dynamical isometry. Then, while dynamical isometry is known to keep specific backpropagated signals independent of the depth, we find that the parameter space's local metric linearly depends on the depth even under the dynamical isometry. More precisely, we reveal that the conditional FIM's spectrum concentrates around the maximum and the value grows linearly as the depth increases. To examine the spectrum, considering random initialization and the wide limit, we construct an algebraic methodology based on the free probability theory. As a byproduct, we provide an analysis of the solvable spectral distribution in two-hidden-layer cases. Lastly, experimental results verify that the appropriate learning rate for the online training of DNNs is in inverse proportional to depth, which is determined by the conditional FIM's spectrum.",,,,,,"Hayase, Tomohiro/0000-0001-6453-4317",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,334,+,,,,,,,,,,,,,,,,WOS:000659893800038,0
C,"Kuhnle, A",,"Banerjee, A; Fukumizu, K",,"Kuhnle, Alan",,,Quick Streaming Algorithms for Maximization of Monotone Submodular Functions in Linear Time,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We consider the problem of monotone, submodular maximization over a ground set of size n subject to cardinality constraint k. For this problem, we introduce the first deterministic algorithms with linear time complexity; these algorithms are streaming algorithms. Our single-pass algorithm obtains a constant ratio in inverted right perpendicularn/cinverted left perpendicular + c, for any c >= 1. In addition, we propose a deterministic, multi-pass streaming algorithm with a constant number of passes that achieves nearly the optimal ratio with linear query and time complexities. We prove a lower bound that implies no constant-factor approximation exists using o(n) queries, even if queries to infeasible sets are allowed. An empirical analysis demonstrates that our algorithms require fewer queries (often substantially less than n) yet still achieve better objective value than the current state-of-the-art algorithms, including single-pass, multi-pass, and non-streaming algorithms.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801065,0
C,"Li, CH; Wu, QY; Wang, HN",,"Banerjee, A; Fukumizu, K",,"Li, Chuanhao; Wu, Qingyun; Wang, Hongning",,,Unifying Clustered and Non-stationary Bandits,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Non-stationary bandits and clustered bandits lift the restrictive assumptions in contextual bandits and provide solutions to many important real-world scenarios. Though they have been studied independently so far, we point out the essence in solving these two problems overlaps considerably. In this work, we connect these two strands of bandit research under the notion of test of homogeneity, which seamlessly addresses change detection for non-stationary bandit and cluster identification for clustered bandit in a unified solution framework. Rigorous regret analysis and extensive empirical evaluations demonstrate the value of our proposed solution, especially its flexibility in handling various environment assumptions, e.g., a clustered non-stationary environment.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801032,0
C,"Liley, J; Emerson, SR; Mateen, BA; Vallejos, CA; Aslett, LJM; Vollmer, SJ",,"Banerjee, A; Fukumizu, K",,"Liley, James; Emerson, Samuel R.; Mateen, Bilal A.; Vallejos, Catalina A.; Aslett, Louis J. M.; Vollmer, Sebastian J.",,,Model updating after interventions paradoxically introduces bias,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Machine learning is increasingly being used to generate prediction models for use in a number of real-world settings, from credit risk assessment to clinical decision support. Recent discussions have highlighted potential problems in the updating of a predictive score for a binary outcome when an existing predictive score forms part of the standard workflow, driving interventions. In this setting, the existing score induces an additional causative pathway which leads to miscalibration when the original score is replaced. We propose a general causal framework to describe and address this problem, and demonstrate an equivalent formulation as a partially observed Markov decision process. We use this model to demonstrate the impact of such `naive updating' when performed repeatedly. Namely, we show that successive predictive scores may converge to a point where they predict their own effect, or may eventually tend toward a stable oscillation between two values, and we argue that neither outcome is desirable. Furthermore, we demonstrate that even if model-fitting procedures improve, actual performance may worsen. We complement these findings with a discussion of several potential routes to overcome these issues.",,,,,"Liley, James/GSD-4767-2022",,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804070,0
C,"Liu, YS; Wang, YN; Singh, A",,"Banerjee, A; Fukumizu, K",,"Liu, Yusha; Wang, Yining; Singh, Aarti",,,Smooth Bandit Optimization: Generalization to Holder Space,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We consider bandit optimization of a smooth reward function, where the goal is cumulative regret minimization. This problem has been studied for alpha-Holder continuous (including Lipschitz) functions with 0 < alpha <= 1. Our main result is in generalization of the reward function to Holder space with exponent alpha > 1 to bridge the gap between Lipschitz bandits and infinitely-differentiable models such as linear bandits. For Holder continuous functions, approaches based on random sampling in bins of a discretized domain suffices as optimal. In contrast, we propose a class of two-layer algorithms that deploy misspecified linear/polynomial bandit algorithms in bins. We demonstrate that the proposed algorithm can exploit higher-order smoothness of the function by deriving a regret upper bound of (O) over tilde (Td+alpha/d+2 alpha) for when alpha > 1, which matches existing lower bound. We also study adaptation to unknown function smoothness over a continuous scale of Holder spaces indexed by alpha, with a bandit model selection approach applied with our proposed two-layer algorithms. We show that it achieves regret rate that matches the existing lower bound for adaptation within the alpha <= 1 subset.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802065,0
C,"Molinari, C; Massias, M; Rosasco, L; Villa, S",,"Banerjee, A; Fukumizu, K",,"Molinari, Cesare; Massias, Mathurin; Rosasco, Lorenzo; Villa, Silvia",,,Iterative regularization with convex regularizers,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We study iterative/implicit regularization for linear models, when the bias is convex but not necessarily strongly convex. We characterize the stability properties of a primaldual gradient based approach, analyzing its convergence in the presence of worst case deterministic noise. As a main example, we specialize and illustrate the results for the problem of robust sparse recovery. Key to our analysis is a combination of ideas from regularization theory and optimization in the presence of errors. Theoretical results are complemented by experiments showing that state-of-the-art performances can be achieved with considerable computational speed-ups.",,,,,"Villa, Silvia/J-1474-2012",,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802007,0
C,"Raj, A; Bach, F",,"Banerjee, A; Fukumizu, K",,"Raj, Anant; Bach, Francis",,,Explicit Regularization of Stochastic Gradient Methods through Duality,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We consider stochastic gradient methods under the interpolation regime where a perfect fit can be obtained (minimum loss at each observation). While previous work highlighted the implicit regularization of such algorithms, we consider an explicit regularization framework as a minimum Bregman divergence convex feasibility problem. Using convex duality, we propose randomized Dykstra-style algorithms based on randomized dual coordinate ascent. For non-accelerated coordinate descent, we obtain an algorithm which bears strong similarities with (non-averaged) stochastic mirror descent on specific functions, as it is equivalent for quadratic objectives, and equivalent in the early iterations for more general objectives. It comes with the benefit of an explicit convergence theorem to a minimum norm solution. For accelerated coordinate descent, we obtain a new algorithm that has better convergence properties than existing stochastic gradient methods in the interpolating regime. This leads to accelerated versions of the perceptron for generic l(p)-norm regularizers, which we illustrate in experiments.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802029,0
C,"Reing, K; Ver Steeg, G; Galstyan, A",,"Banerjee, A; Fukumizu, K",,"Reing, Kyle; Ver Steeg, Greg; Galstyan, Aram",,,Influence Decompositions For Neural Network Attribution,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Methods of neural network attribution have emerged out of a necessity for explanation and accountability in the predictions of black-box neural models. Most approaches use a variation of sensitivity analysis, where individual input variables are perturbed and the downstream effects on some output metric are measured. We demonstrate that a number of critical functional properties are not revealed when only considering lower-order perturbations. Motivated by these shortcomings, we propose a general framework for decomposing the orders of influence that a collection of input variables has on an output classification. These orders are based on the cardinality of input subsets which are perturbed to yield a change in classification. This decomposition can be naturally applied to attribute which input variables rely on higher-order coordination to impact the classification decision. We demonstrate that our approach correctly identifies higher-order attribution on a number of synthetic examples. Additionally, we showcase the differences between attribution in our approach and existing approaches on benchmark networks for MNIST and ImageNet.",,,,,,"Galstyan, Aram/0000-0003-4215-0886",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803028,0
C,"Shi, CS; Shen, C; Yang, J",,"Banerjee, A; Fukumizu, K",,"Shi, Chengshuai; Shen, Cong; Yang, Jing",,,Federated Multi-armed Bandits with Personalization,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"A general framework of personalized federated multi-armed bandits (PF-MAB) is proposed, which is a new bandit paradigm analogous to the federated learning (FL) framework in supervised learning and enjoys the features of FL with personalization. Under the PF-MAB framework, a mixed bandit learning problem that flexibly balances generalization and personalization is studied. A lower bound analysis for the mixed model is presented. We then propose the Personalized Federated Upper Confidence Bound (PF-UCB) algorithm, where the exploration length is chosen carefully to achieve the desired balance of learning the local model and supplying global information for the mixed learning objective. Theoretical analysis proves that PF-UCB achieves an O(log(T)) regret regardless of the degree of personalization, and has a similar instance dependency as the lower bound. Experiments using both synthetic and real-world datasets corroborate the theoretical analysis and demonstrate the effectiveness of the proposed algorithm.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803051,0
C,"Solomon, E; Wagner, A; Bendich, P",,"Banerjee, A; Fukumizu, K",,"Solomon, Elchanan; Wagner, Alexander; Bendich, Paul",,,A Fast and Robust Method for Global Topological Functional Optimization,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Topological statistics, in the form of persistence diagrams, are a class of shape descriptors that capture global structural information in data. The mapping from data structures to persistence diagrams is almost everywhere differentiable, allowing for topological gradients to be backpropagated to ordinary gradients. However, as a method for optimizing a topological functional, this backpropagation method is expensive, unstable, and produces very fragile optima. Our contribution is to introduce a novel backpropagation scheme that is significantly faster, more stable, and produces more robust optima. Moreover, this scheme can also be used to produce a stable visualization of dots in a persistence diagram as a distribution over critical, and near-critical, simplices in the data structure.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,109,+,,,,,,,,,,,,,,,,WOS:000659893800013,0
C,"Xing, Y; Zhang, RZ; Cheng, G",,"Banerjee, A; Fukumizu, K",,"Xing, Yue; Zhang, Ruizhi; Cheng, Guang",,,Adversarially Robust Estimate and Risk Analysis in Linear Regression,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Adversarially robust learning aims to design algorithms that are robust to small adversarial perturbations on input variables. Beyond the existing studies on the predictive performance to adversarial samples, our goal is to understand the statistical properties of adversarially robust estimates and analyze adversarial risk in the setup of linear regression models. By discovering the statistical minimax rate of convergence of adversarially robust estimators, we emphasize incorporating model information, e.g., sparsity, in adversarially robust learning. Further, we reveal an explicit connection between adversarial and standard estimates and propose a straightforward two-stage adversarial learning framework that facilitates utilizing model structure information to improve adversarial robustness. In theory, the consistency of the adversarially robust estimator is proven and its Bahadur representation is also developed for the statistical inference purpose. The proposed estimator converges in a sharp rate under either a low-dimensional or a sparse scenario. Moreover, our theory confirms two phenomena in adversarially robust learning: adversarial robustness hurts generalization, and unlabeled data improves generalization. In the end, we conduct numerical simulations to verify our theory.",,,,,,"Xing, Yue/0000-0001-7723-0048",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,514,+,,,,,,,,,,,,,,,,WOS:000659893800058,0
C,"Zhang, ZY; Paschalidis, IC",,"Banerjee, A; Fukumizu, K",,"Zhang, Zhiyu; Paschalidis, Ioannis Ch",,,Provable Hierarchical Imitation Learning via EM,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Due to recent empirical successes, the options framework for hierarchical reinforcement learning is gaining increasing popularity. Rather than learning from rewards, we consider learning an options-type hierarchical policy from expert demonstrations. Such a problem is referred to as hierarchical imitation learning. Converting this problem to parameter inference in a latent variable model, we develop convergence guarantees for the EM approach proposed by Daniel et al. (2016b). The population level algorithm is analyzed as an intermediate step, which is nontrivial due to the samples being correlated. If the expert policy can be parameterized by a variant of the options framework, then, under regularity conditions, we prove that the proposed algorithm converges with high probability to a norm ball around the true parameter. To our knowledge, this is the first performance guarantee for an hierarchical imitation learning algorithm that only observes primitive state-action pairs.(1)",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801012,0
C,"Zhao, E; Liu, AQ; Anandkumar, A; Yue, YS",,"Banerjee, A; Fukumizu, K",,"Zhao, Eric; Liu, Anqi; Anandkumar, Anima; Yue, Yisong",,,Active Learning under Label Shift,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,We address the problem of active learning under label shift : when the class proportions of source and target domains differ. We introduce a \medial distribution to incorporate a tradeoff between importance weighting and class-balanced sampling and propose their combined usage in active learning. Our method is known as Mediated Active Learning under Label Shift (MALLS). It balances the bias from class-balanced sampling and the variance from importance weighting. We prove sample complexity and generalization guarantees for MALLS which show active learning reduces asymptotic sample complexity even under arbitrary label shift. We empirically demonstrate MALLS scales to high-dimensional datasets and can reduce the sample complexity of active learning by 60% in deep active learning tasks.,,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804014,0
C,"Awan, MU; Roy, S; Morucci, M; Rudin, C; Orlandi, V; Volfovsky, A",,"Chiappa, S; Calandra, R",,"Awan, M. Usaid; Roy, Sudeepa; Morucci, Marco; Rudin, Cynthia; Orlandi, Vittorio; Volfovsky, Alexander",,,Almost-Matching-Exactly for Treatment Effect Estimation under Network Interference,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We propose a matching method that recovers direct treatment effects from randomized experiments where units are connected in an observed network, and units that share edges can potentially influence each others' outcomes. Traditional treatment effect estimators for randomized experiments are biased and error prone in this setting. Our method matches units almost exactly on counts of unique subgraphs within their neighborhood graphs. The matches that we construct are interpretable and high-quality. Our method can be extended easily to accommodate additional unit-level covariate information. We show empirically that our method performs better than other existing methodologies for this problem, while producing meaningful, interpretable results.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3252,3261,,,,,,,,,,,,,,,,WOS:000559931300018,0
C,"Hueihel, W; Shayevitz, O",,"Chiappa, S; Calandra, R",,"Hueihel, Wasim; Shayevitz, Ofer",,,Sharp Thresholds of the Information Cascade Fragility Under a Mismatched Model,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We analyze a sequential decision making model in which decision makers (or, players) take their decisions based on their own private information as well as the actions of previous decision makers. Such decision making processes often lead to what is known as the information cascade or herding phenomenon. Specifically, a cascade develops when it seems rational for some players to abandon their own private information and imitate the actions of earlier players. The risk, however, is that if the initial decisions were wrong, then the whole cascade will be wrong. Nonetheless, information cascade are known to be fragile: there exists a sequence of revealing probabilities {p(l)}(l >= 1), such that if with probability p(l) player l ignores the decisions of previous players, and rely on his private information only, then wrong cascades can be avoided. Previous related papers which study the fragility of information cascades always assume that the revealing probabilities are known to all players perfectly, which might be unrealistic in practice. Accordingly, in this paper we study a mismatch model where players believe that the revealing probabilities are {q(l)}(l is an element of N) when they truly are {p(l)}(l is an element of N), and study the effect of this mismatch on information cascades. We consider both adversarial and probabilistic sequential decision making models, and derive closed-form expressions for the optimal learning rates at which the error probability associated with a certain decision maker goes to zero. We prove several novel phase transitions in the behaviour of the asymptotic learning rate.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,548,556,,,,,,,,,,,,,,,,WOS:000559931301046,0
C,"Kawaguchi, K; Lu, HH",,"Chiappa, S; Calandra, R",,"Kawaguchi, Kenji; Lu, Haihao",,,Ordered SGD: A New Stochastic Optimization Framework for Empirical Risk Minimization,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We propose a new stochastic optimization framework for empirical risk minimization problems such as those that arise in machine learning. The traditional approaches, such as (mini-batch) stochastic gradient descent (SGD), utilize an unbiased gradient estimator of the empirical average loss. In contrast, we develop a computationally efficient method to construct a gradient estimator that is purposely biased toward those observations with higher current losses. On the theory side, we show that the proposed method minimizes a new ordered modification of the empirical average loss, and is guaranteed to converge at a sublinear rate to a global optimum for convex loss and to a critical point for weakly convex (non-convex) loss. Furthermore, we prove a new generalization bound for the proposed algorithm. On the empirical side, the numerical experiments show that our proposed method consistently improves the test errors compared with the standard mini-batch SGD in various models including SVM, logistic regression, and deep learning problems.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,669,678,,,,,,,,,,,,,,,,WOS:000559931301067,0
C,"Li, AH; Bradic, J",,"Chiappa, S; Calandra, R",,"Li, Alexander Hanbo; Bradic, Jelena",,,Censored Quantile Regression Forest,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Random forests are powerful non-parametric regression method but are severely limited in their usage in the presence of randomly censored observations, and naively applied can exhibit poor predictive performance due to the incurred biases. Based on a local adaptive representation of random forests, we develop its regression adjustment for randomly censored regression quantile models. Regression adjustment is based on a new estimating equation that adapts to censoring and leads to quantile score whenever the data do not exhibit censoring. The proposed procedure named censored quantile regression forest, allows us to estimate quantiles of time-to-event without any parametric modeling assumption. We establish its consistency under mild model specifications. Numerical studies showcase a clear advantage of the proposed procedure.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2109,2118,,,,,,,,,,,,,,,,WOS:000559931302007,0
C,"Lucena, B",,"Chiappa, S; Calandra, R",,"Lucena, Brian",,,Exploiting Categorical Structure Using Tree-Based Methods,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Standard methods of using categorical variables as predictors either endow them with an ordinal structure or assume they have no structure at all. However, categorical variables often possess structure that is more complicated than a linear ordering can capture. We develop a mathematical framework for representing the structure of categorical variables and show how to generalize decision trees to make use of this structure. This approach is applicable to methods such as Gradient Boosted Trees which use a decision tree as the underlying learner. We show results on weather data to demonstrate the improvement yielded by this approach.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2949,2957,,,,,,,,,,,,,,,,WOS:000559931302023,0
C,"Sakaue, S",,"Chiappa, S; Calandra, R",,"Sakaue, Shinsaku",,,"On Maximization of Weakly Modular Functions: Guarantees of Multi-stage Algorithms, Tractability, and Hardness","INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Maximization of non-submodular functions appears in various scenarios, and many previous works studied it based on some measures that quantify the closeness to being submodular. On the other hand, some practical non-submodular functions are actually close to being modular, which has been utilized in few studies. In this paper, we study cardinality-constrained maximization of weakly modular functions, whose closeness to being modular is measured by submodularity and supermodularity ratios, and reveal what we can and cannot do by using the weak modularity. We first show that guarantees of multi-stage algorithms can be proved with the weak modularity, which generalize and improve some existing results, and experiments confirm their effectiveness. We then show that weakly modular maximization is fixed-parameter tractable under certain conditions; as a byproduct, we provide a new time accuracy trade-off for to constrained minimization. We finally prove that, even if objective functions are weakly modular, no polynomial-time algorithms can improve the existing approximation guarantee achieved by the greedy algorithm in general.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303002,0
C,"Valavi, H; Liu, SL; Ramadge, PJ",,"Chiappa, S; Calandra, R",,"Valavi, Hossein; Liu, Sulin; Ramadge, Peter J.",,,Revisiting the Landscape of Matrix Factorization,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Prior work has shown that low-rank matrix factorization has infinitely many critical points, each of which is either a global minimum or a (strict) saddle point. We revisit this problem and provide simple, intuitive proofs of a set of extended results for low-rank and general-rank problems. We couple our investigation with a known invariant manifold M-0 of gradient flow. This restriction admits a uniform negative upper bound on the least eigenvalue of the Hessian map at all strict saddles in M-0. The bound depends on the size of the nonzero singular values and the separation between distinct singular values of the matrix to be factorized.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303051,0
C,"Xu, JM; Tian, Y; Sun, Y; Scutari, G",,"Chiappa, S; Calandra, R",,"Xu, Jinming; Tian, Ye; Sun, Ying; Scutari, Gesualdo",,,Accelerated Primal-Dual Algorithms for Distributed Smooth Convex Optimization over Networks,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"This paper proposes a novel family of primal-dual-based distributed algorithms for smooth, convex, multi-agent optimization over networks that uses only gradient information and gossip communications. The algorithms can also employ acceleration on the computation and communications. We provide a unified analysis of their convergence rate, measured in terms of the Bregman distance associated to the saddle point reformation of the distributed optimization problem. When acceleration is employed, the rate is shown to be optimal, in the sense that it matches (under the proposed metric) existing complexity lower bounds of distributed algorithms applicable to such a class of problem and using only gradient information and gossip communications. Preliminary numerical results on distributed least-square regression problems show that the proposed algorithm compares favorably on existing distributed schemes.",,,,,"Sun, Yingyuan/HHS-7124-2022",,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303087,0
C,"Zhang, X; Chen, JH; Gu, QQ; Evans, D",,"Chiappa, S; Calandra, R",,"Zhang, Xiao; Chen, Jinghui; Gu, Quanquan; Evans, David",,,Understanding the Intrinsic Robustness of Image Distributions using Conditional Generative Models,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Starting with Gilmer et al. (2018), several works have demonstrated the inevitability of adversarial examples based on different assumptions about the underlying input probability space. It remains unclear, however, whether these results apply to natural image distributions. In this work, we assume the underlying data distribution is captured by some conditional generative model, and prove intrinsic robustness bounds for a general class of classifiers, which solves an open problem in Fawzi et al. (2018). Building upon the state-of-the-art conditional generative models, we study the intrinsic robustness of two common image benchmarks under l(2) perturbations, and show the existence of a large gap between the robustness limits implied by our theory and the adversarial robustness achieved by current state-of-the-art robust models.",,,,,,"Chen, Jinghui/0000-0002-1486-4526",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3883,3892,,,,,,,,,,,,,,,,WOS:000559931304014,0
C,"Zheng, X; Dan, C; Aragam, B; Ravikumar, P; Xing, EP",,"Chiappa, S; Calandra, R",,"Zheng, Xun; Dan, Chen; Aragam, Bryon; Ravikumar, Pradeep; Xing, Eric P.",,,Learning Sparse Nonparametric DAGs,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We develop a framework for learning sparse nonparametric directed acyclic graphs (DAGs) from data. Our approach is based on a recent algebraic characterization of DAGs that led to a fully continuous program for score-based learning of DAG models parametrized by a linear structural equation model (SEM). We extend this algebraic characterization to nonparametric SEM by leveraging nonparametric sparsity based on partial derivatives, resulting in a continuous optimization problem that can be applied to a variety of nonparametric and semiparametric models including GLMs, additive noise models, and index models as special cases. Unlike existing approaches that require specific modeling choices, loss functions, or algorithms, we present a completely general framework that can be applied to general nonlinear models (e.g. without additive noise), general differentiable loss functions, and generic black-box optimization routines. The code is available at https://github.com/xunzhenenotears.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3414,3424,,,,,,,,,,,,,,,,WOS:000559931304021,0
C,"Cao, Y; Wen, Z; Kveton, B; Xie, Y",,"Chaudhuri, K; Sugiyama, M",,"Cao, Yang; Wen, Zheng; Kveton, Branislav; Xie, Yao",,,Nearly Optimal Adaptive Procedure with Change Detection for Piecewise-Stationary Bandit,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Multi-armed bandit (MAB) is a class of online learning problems where a learning agent aims to maximize its expected cumulative reward while repeatedly selecting to pull arms with unknown reward distributions. We consider a scenario where the reward distributions may change in a piecewise-stationary fashion at unknown time steps. We show that by incorporating a simple change-detection component with classic UCB algorithms to detect and adapt to changes, our so-called M-UCB algorithm can achieve nearly optimal regret bound on the order of O(root MKT log T), where T is the number of time steps, K is the number of arms, and M is the number of stationary segments. Comparison with the best available lower bound shows that our M-UCB is nearly optimal in T up to a logarithmic factor. We also compare M-UCB with the state-ofthe-art algorithms in numerical experiments using a public Yahoo! dataset and a real-world digital marketing dataset to demonstrate its superior performance.",,,,,"wen, zheng/HII-3705-2022",,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,418,427,,,,,,,,,,,,,,,,WOS:000509687900044,0
C,"D'Amour, A",,"Chaudhuri, K; Sugiyama, M",,"D'Amour, Alexander",,,"On Multi-Cause Causal Inference with Unobserved Confounding: Counterexamples, Impossibility, and Alternatives","22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Unobserved confounding is a central barrier to drawing causal inferences from observational data. Several authors have recently proposed that this barrier can be overcome in the case where one attempts to infer the effects of several variables simultaneously. In this paper, we present two simple, analytical counterexamples that challenge the general claims that are central to these approaches. In addition, we show that nonparametric identification is impossible in this setting. We discuss practical implications, and suggest alternatives to the methods that have been proposed so far in this line of work: using proxy variables and shifting focus to sensitivity analysis.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687903055,0
C,"Esmaeili, B; Huang, HY; Wallace, BC; van de Meent, JW",,"Chaudhuri, K; Sugiyama, M",,"Esmaeili, Babak; Huang, Hongyi; Wallace, Byron C.; van de Meent, Jan-Willem",,,Structured Neural Topic Models for Reviews,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We present Variational Aspect-based Latent Topic Allocation (VALTA), a family of autoencoding topic models that learn aspect-based representations of reviews. VALTA defines a user-item encoder that maps bag-of-words vectors for combined reviews associated with each paired user and item onto structured embeddings, which in turn define per-aspect topic weights. We model individual reviews in a structured manner by inferring an aspect assignment for each sentence in a given review, where the per-aspect topic weights obtained by the user-item encoder serve to define a mixture over topics, conditioned on the aspect. The result is an autoencoding neural topic model for reviews, which can be trained in a fully unsupervised manner to learn topics that are structured into aspects. Experimental evaluation on large number of datasets demonstrates that aspects are interpretable, yield higher coherence scores than non-structured autoencoding topic model variants, and can be utilized to perform aspect-based comparison and genre discovery.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687903050,0
C,"Forrow, A; Hutter, JC; Nitzan, M; Rigollet, P; Schiebinger, G; Weed, J",,"Chaudhuri, K; Sugiyama, M",,"Forrow, Aden; Huetter, Jan-Christian; Nitzan, Mor; Rigollet, Philippe; Schiebinger, Geoffrey; Weed, Jonathan",,,Statistical Optimal Transport via Factored Couplings,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We propose a new method to estimate Wasserstein distances and optimal transport plans between two probability distributions from samples in high dimension. Unlike plug-in rules that simply replace the true distributions by their empirical counterparts, our method promotes couplings with low transport rank, a new structural assumption that is similar to the nonnegative rank of a matrix. Regularizing based on this assumption leads to drastic improvements on high-dimensional data for various tasks, including domain adaptation in single-cell RNA sequencing data. These findings are supported by a theoretical analysis that indicates that the transport rank is key in overcoming the curse of dimensionality inherent to data-driven optimal transport.",,,,,"Huetter, Jan-Christian/AAU-9020-2020","Huetter, Jan-Christian/0000-0002-1219-4821",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902052,0
C,"Hirt, M; Dellaportas, P",,"Chaudhuri, K; Sugiyama, M",,"Hirt, Marcel; Dellaportas, Petros",,,Scalable Bayesian Learning for State Space Models using Variational Inference with SMC Samplers,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,We present a scalable approach to performing approximate fully Bayesian inference in generic state space models. The proposed method is an alternative to particle MCMC that provides fully Bayesian inference of both the dynamic latent states and the static parameters of the model. We build up on recent advances in computational statistics that combine variational methods with sequential Monte Carlo sampling and we demonstrate the advantages of performing full Bayesian inference over the static parameters rather than just performing variational EM approximations. We illustrate how our approach enables scalable inference in multivariate stochastic volatility models and self-exciting point process models that allow for flexible dynamics in the latent intensity function.,,,,,"Dellaportas, Petros/AAI-7042-2021",,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,76,86,,,,,,,,,,,,,,,,WOS:000509687900009,0
C,"Li, YX; Ma, C; Chen, YX; Chi, YJ",,"Chaudhuri, K; Sugiyama, M",,"Li, Yuanxin; Ma, Cong; Chen, Yuxin; Chi, Yuejie",,,Nonconvex Matrix Factorization from Rank-One Measurements,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We consider the problem of recovering low rank matrices from random rank-one measurements, which spans numerous applications including phase retrieval, quantum state tomography, and learning shallow neural networks with quadratic activations, among others. Our approach is to directly estimate the low-rank factor by minimizing a nonconvex least-squares loss function via vanilla gradient descent, following a tailored spectral initialization. When the true rank is small, this algorithm is guaranteed to converge to the ground truth (up to global ambiguity) with near-optimal sample and computational complexities with respect to the problem size. To the best of our knowledge, this is the first theoretical guarantee that achieves near optimality in both metrics. In particular, the key enabler of near-optimal computational guarantees is an implicit regularization phenomenon: without explicit regularization, both spectral initialization and the gradient descent iterates automatically stay within a region incoherent with the measurement vectors. This feature allows one to employ much more aggressive step sizes compared with the ones suggested in prior literature, without the need of sample splitting.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901056,0
C,"Podosinnikova, A; Perry, A; Wein, A; Bach, F; d'Aspremont, A; Sontag, D",,"Chaudhuri, K; Sugiyama, M",,"Podosinnikova, Anastasia; Perry, Amelia; Wein, Alexander; Bach, Francis; d'Aspremont, Alexandre; Sontag, David",,,Overcomplete Independent Component Analysis via SDP,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We present a novel algorithm for over-complete independent components analysis (ICA), where the number of latent sources k exceeds the dimension p of observed variables. Previous algorithms either suffer from high computational complexity or make strong assumptions about the form of the mixing matrix. Our algorithm does not make any sparsity assumption yet enjoys favorable computational and theoretical properties. Our algorithm consists of two main steps: (a) estimation of the Hessians of the cumulant generating function (as opposed to the fourth and higher order cumulants used by most algorithms) and (b) a novel semi-definite programming (SDP) relaxation for recovering a mixing component. We show that this relaxation can be efficiently solved with a projected accelerated gradient descent method, which makes the whole algorithm computationally practical. Moreover, we conjecture that the proposed program recovers a mixing component at the rate k < p(2)/4 and prove that a mixing component can be recovered with high probability when k < (2 - epsilon)p log p when the original components are sampled uniformly at random on the hypersphere. Experiments are provided on synthetic data and the CIFAR-10 dataset of real images.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902065,0
C,"Tran, GL; Bonilla, EV; Cunningham, JP; Michiardi, P; Filippone, M",,"Chaudhuri, K; Sugiyama, M",,"Tran, G-L; Bonilla, E., V; Cunningham, J. P.; Michiardi, P.; Filippone, M.",,,Calibrating Deep Convolutional Gaussian Processes,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"The wide adoption of Convolutional Neural Networks (CNNS) in applications where decision-making under uncertainty is fundamental, has brought a great deal of attention to the ability of these models to accurately quantify the uncertainty in their predictions. Previous work on combining CNNS with Gaussian processes (GPs) has been developed under the assumption that the predictive probabilities of these models are well-calibrated. In this paper we show that, in fact, current combinations of CNNS and GPs are miscalibrated. We propose a novel combination that considerably outperforms previous approaches on this aspect, while achieving state-of-the-art performance on image classification tasks.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901062,0
C,"Wu, YF; Poczos, B; Singh, A",,"Chaudhuri, K; Sugiyama, M",,"Wu, Yifan; Poczos, Barnabas; Singh, Aarti",,,Towards Understanding the Generalization Bias of Two Layer Convolutional Linear Classifiers with Gradient Descent,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"A major challenge in understanding the generalization of deep learning is to explain why (stochastic) gradient descent can exploit the network architecture to find solutions that have good generalization performance when using high capacity models. We find simple but realistic examples showing that this phenomenon exists even when learning linear classifiers between two linear networks with the same capacity, the one with a convolutional layer can generalize better than the other when the data distribution has some underlying spatial structure. We argue that this difference results from a combination of the convolution architecture, data distribution and gradient descent, all of which are necessary to be included in a meaningful analysis. We analyze of the generalization performance as a function of data distribution and convolutional filter size, given gradient descent as the optimization algorithm, then interpret the results using concrete examples. Experimental results show that our analysis is able to explain what happens in our introduced examples.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901012,0
C,"Yang, QM; Li, CR; Guo, J",,"Chaudhuri, K; Sugiyama, M",,"Yang, Qimao; Li, Changrong; Guo, Jun",,,Multi-Order Information for Working Set Selection of Sequential Minimal Optimization,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"A new working set selection method for sequential minimal optimization (SMO) is proposed in this paper. Instead of the method adopted in the current version of LIBSVM, which uses the second order information of the objective function to choose the violating pairs, we suggest a new method where a higher order information is considered. It includes the descent degree of the objective function and the stride of variables update. Many experimental results show, in contrast to LIBSVM, the number of iterations obtained by the proposed method is less in the vast majority of cases and the training of support vector machines (SVMs) is sped up. Meanwhile, the convergence of the proposed approach can be guaranteed and its accuracy is at the same level as LIBSVM's.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687903033,0
C,"Yu, HF; Hsieh, CJ; Dhillon, IS",,"Chaudhuri, K; Sugiyama, M",,"Yu, Hsiang-Fu; Hsieh, Cho-Jui; Dhillon, Inderjit S.",,,Parallel Asynchronous Stochastic Coordinate Descent with Auxiliary Variables,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"The key to the recent success of coordinate descent (CD) in many applications is to maintain a set of auxiliary variables to facilitate efficient single variable updates. For example, the vector of residual/primal variables has to be maintained when CD is applied for Lasso/linear SVM, respectively. An implementation without maintenance is O(n) times slower than the one with maintenance, where n is the number of variables. In serial implementations, maintaining auxiliary variables is only a computing trick without changing the behavior of coordinate descent. However, maintenance of auxiliary variables is non-trivial when there are multiple threads/workers which read/write the auxiliary variables concurrently. Thus, most existing theoretical analysis of parallel CD either assumes vanilla CD without auxiliary variables (which ends up being extremely slow in practice) or limits to a small class of problems. In this paper, we consider a rich family of objective functions where AUX-PCD can be applied. We also establish global linear convergence for AUX-PCD with atomic operations for a general family of functions and perform a complete backward error analysis of AUX-PCD with wild updates, where some updates are not just delayed but lost because of memory conflicts. Our results enable us to provide theoretical guarantees for many practical parallel coordinate descent implementations, which currently lack guarantees (such as the implementation of Shotgun proposed by Bradley et al. 2011, which uses auxiliary variables).",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902071,0
C,"Zhe, SD; Xing, W; Kirby, RM",,"Chaudhuri, K; Sugiyama, M",,"Zhe, Shandian; Xing, Wei; Kirby, Robert M.",,,Scalable High-Order Gaussian Process Regression,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"While most Gaussian processes (GP) work focus on learning single-output functions, many applications, such as physical simulations and gene expressions prediction, require estimations of functions with many outputs. The number of outputs can be much larger than or comparable to the size of training samples. Existing multi-output GP models either are limited to low-dimensional outputs and restricted kernel choices, or assume oversimplified low-rank structures within the outputs. To address these issues, we propose HOGPR, a High-Order Gaussian Process Regression model, which can flexibly capture complex correlations among the outputs and scale up to a large number of outputs. Specifically, we tensorize the high dimensional outputs, introducing latent coordinate features to index each tensor element (i.e., output) and to capture their correlations. We then generalize a multilinear model to a hybrid of a GP and latent GP model. The model is endowed with a Kronecker product structure over the inputs and the latent features. Using the Kronecker product properties and tensor algebra, we are able to perform exact inference over millions of outputs. We show the advantage of the proposed model on several real-world applications.",,,,,,"XING, WEI/0000-0002-3177-8478",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902068,0
C,"Bachem, O; Lucic, M; Lattanzi, S",,"Storkey, A; PerezCruz, F",,"Bachem, Olivier; Lucic, Mario; Lattanzi, Silvio",,,One-Shot Coresets: The Case of k-Clustering,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Scaling clustering algorithms to massive data sets is a challenging task. Recently, several successful approaches based on data summarization methods, such as coresets and sketches, were proposed. While these techniques provide provably good and small summaries, they are inherently problem dependent - the practitioner has to commit to a fixed clustering objective before even exploring the data. However, can one construct small data summaries for a wide range of clustering problems simultaneously? We affirmatively answer this question by proposing an efficient algorithm that constructs such one-shot summaries for a large family of k-clustering problems while retaining strong theoretical guarantees.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300083,0
C,"Bellet, A; Guerraoui, R; Taziki, M; Tommasi, M",,"Storkey, A; PerezCruz, F",,"Bellet, Aurelien; Guerraoui, Rachid; Taziki, Mahsa; Tommasi, Marc",,,Personalized and Private Peer-to-Peer Machine Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"The rise of connected personal devices together with privacy concerns call for machine learning algorithms capable of leveraging the data of a large number of agents to learn personalized models under strong privacy requirements. In this paper, we introduce an efficient algorithm to address the above problem in a fully decentralized (peer-to-peer) and asynchronous fashion, with provable convergence rate. We show how to make the algorithm differentially private to protect against the disclosure of information about the personal datasets, and formally analyze the trade-off between utility and privacy. Our experiments show that our approach dramatically outperforms previous work in the non-private case, and that under privacy constraints, we can significantly improve over models learned in isolation.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300050,0
C,"Cheng, X; Roosta-Khorasani, F; Palombo, S; Bartlett, PL; Mahoney, MW",,"Storkey, A; PerezCruz, F",,"Cheng, Xiang; Roosta-Khorasani, Farbod; Palombo, Stefan; Bartlett, Peter L.; Mahoney, Michael W.",,,FLAG n' FLARE: Fast Linearly-Coupled Adaptive Gradient Methods,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"We consider first order gradient methods for effectively optimizing a composite objective in the form of a sum of smooth and, potentially, non-smooth functions. We present accelerated and adaptive gradient methods, called FLAG and FLARE, which can offer the best of both worlds. They can achieve the optimal convergence rate by attaining the optimal first-order oracle complexity for smooth convex optimization. Additionally, they can adaptively and non-uniformly re-scale the gradient direction to adapt to the limited curvature available and conform to the geometry of the domain. We show theoretically and empirically that, through the compounding effects of acceleration and adaptivity, FLAG and FLARE can be highly effective for many data fitting and machine learning applications.",,,,,"Roosta, Fred/U-2926-2018","Roosta, Fred/0000-0002-6920-7072",,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300043,0
C,"Hara, S; Hayashi, K",,"Storkey, A; PerezCruz, F",,"Hara, Satoshi; Hayashi, Kohei",,,Making Tree Ensembles Interpretable: A Bayesian Model Selection Approach,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Tree ensembles, such as random forests, are renowned for their high prediction performance. However, their interpretability is critically limited due to the enormous complexity. In this study, we propose a method to make a complex tree ensemble interpretable by simplifying the model. Specifically, we formalize the simplification of tree ensembles as a model selection problem. Given a complex tree ensemble, we aim at obtaining the simplest representation that is essentially equivalent to the original one. To this end, we derive a Bayesian model selection algorithm that optimizes the simplified model while maintaining the prediction performance. Our numerical experiments on several datasets showed that complicated tree ensembles were approximated interpretably.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300009,0
C,"Karasuyama, M; Mamitsuka, H",,"Storkey, A; PerezCruz, F",,"Karasuyama, Masayuki; Mamitsuka, Hiroshi",,,Factor Analysis on a Graph,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Graph is a common way to represent relationships among a set of objects in a variety of data analysis fields. We consider the case that the input data is not only a graph but also numerical features, each one of which corresponds to a node in the graph. In practice, the primary importance is often in understanding interactions on the graph nodes which effect on covariance structure of the numerical features. We propose a Gaussian based analysis which is a combination of graph constrained covariance matrix estimation and factor analysis (FA). We show that this approach, called graph FA, has desirable interpretability. In particular, we prove the connection between graph FA and a graph node clustering based on a perspective of kernel method. This connection indicates that graph FA is effective not only on the conventional noise-reduction explanation of the observation by FA but also on identifying important subgraphs. The experiments on synthetic and real-world datasets demonstrate the effectiveness of the approach.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300117,0
C,"Karimireddy, SP; Stich, SU; Jaggi, M",,"Storkey, A; PerezCruz, F",,"Karimireddy, Sai Praneeth; Stich, Sebastian U.; Jaggi, Martin",,,Adaptive Balancing of Gradient and Update Computation Times using Global Geometry and Approximate Subproblems,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"First-order optimization methods comprise two important primitives: i) the computation of gradient information and ii) the computation of the update that leads to the next iterate. In practice there is often a wide mismatch between the time required for the two steps, leading to underutilization of resources. In this work, we propose a new framework, Approx Composite Minimization (ACM) that uses approximate update steps to ensure balance between the two operations. The accuracy is adaptively chosen in an online fashion to take advantage of changing conditions. Our unified analysis for approximate composite minimization generalizes and extends previous work to new settings. Numerical experiments on Lasso regression and SVMs demonstrate the effectiveness of the novel scheme.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300126,0
C,"Mohapatra, P; Jawahar, CV; Kumar, MP",,"Storkey, A; PerezCruz, F",,"Mohapatra, Pritish; Jawahar, C. V.; Kumar, M. Pawan",,,Learning to Round for Discrete Labeling Problems,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Discrete labeling problems are often solved by formulating them as an integer program, and relaxing the integrality constraint to a continuous domain. While the continuous relaxation is closely related to the original integer program, its optimal solution is often fractional. Thus, the success of a relaxation depends crucially on the availability of an accurate rounding procedure. The problem of identifying an accurate rounding procedure has mainly been tackled in the theoretical computer science community through mathematical analysis of the worst-case. However, this approach is both onerous and ignores the distribution of the data encountered in practice. We present a novel interpretation of rounding procedures as sampling from a latent variable model, which opens the door to the use of powerful machine learning formulations in their design. Inspired by the recent success of deep latent variable models we parameterize rounding procedures as a neural network, which lends itself to efficient optimization via back-propagation. By minimizing the expected value of the objective of the discrete labeling problem over training samples, we learn a rounding procedure that is more suited to the task at hand. Using both synthetic and real world data sets, we demonstrate that our approach can outperform the state-of-the-art hand-designed rounding procedures.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300110,0
C,"Pati, D; Bhattacharya, A; Yang, Y",,"Storkey, A; PerezCruz, F",,"Pati, Debdeep; Bhattacharya, Anirban; Yang, Yun",,,On Statistical Optimality of Variational Bayes,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"The article addresses a long-standing open problem on the justification of using variational Bayes methods for parameter estimation. We provide general conditions for obtaining optimal risk bounds for point estimates acquired from mean-field variational Bayesian inference. The conditions pertain to the existence of certain test functions for the distance metric on the parameter space and minimal assumptions on the prior. A general recipe for verification of the conditions is outlined which is broadly applicable to existing Bayesian models with or without latent variables. As illustrations, specific applications to Latent Dirichlet Allocation and Gaussian mixture models are discussed.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300165,0
C,"Pierson, E; Corbett-Davies, S; Goel, S",,"Storkey, A; PerezCruz, F",,"Pierson, Emma; Corbett-Davies, Sam; Goel, Sharad",,,Fast Threshold Tests for Detecting Discrimination,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Threshold tests have recently been proposed as a useful method for detecting bias in lending, hiring, and policing decisions. For example, in the case of credit extensions, these tests aim to estimate the bar for granting loans to white and minority applicants, with a higher inferred threshold for minorities indicative of discrimination. This technique, however, requires fitting a complex Bayesian latent variable model for which inference is often computationally challenging. Here we develop a method for fitting threshold tests that is two orders of magnitude faster than the existing approach, reducing computation from hours to minutes. To achieve these performance gains, we introduce and analyze a flexible family of probability distributions on the interval [0, 1]|which we call discriminant distributions-that is computationally efficient to work with. We demonstrate our technique by analyzing 2.7 million police stops of pedestrians in New York City.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300011,0
C,"Sinn, M; Rawat, A",,"Storkey, A; PerezCruz, F",,"Sinn, Mathieu; Rawat, Ambrish",,,Non-parametric estimation of Jensen-Shannon Divergence in Generative Adversarial Network training,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Generative Adversarial Networks (GANs) have become a widely popular framework for generative modelling of high-dimensional datasets. However their training is known to be difficult. This work presents a rigorous statistical analysis of GANs providing straight-forward explanations for common training pathologies such as vanishing gradients. Furthermore, it proposes a new training objective, Kernel GANs, and demonstrates its practical effectiveness on real-world data sets. A key element in the analysis is the distinction between training with respect to the (unknown) data distribution, and its empirical counterpart. To overcome issues in GAN training, we pursue the idea of smoothing the Jensen-Shannon Divergence (JSD) by incorporating noise in the input distributions of the discriminator. As we show, this effectively leads to an empirical version of the JSD in which the true and the generator densities are replaced by kernel density estimates, which leads to Kernel GANs.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300068,0
C,"Tomczak, JM; Welling, M",,"Storkey, A; PerezCruz, F",,"Tomczak, Jakub M.; Welling, Max",,,VAE with a VampPrior,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Many different methods to train deep generative models have been introduced in the past. In this paper, we propose to extend the variational auto-encoder (VAE) framework with a new type of prior which we call Variational Mixture of Posteriors prior, or VampPrior for short. The VampPrior consists of a mixture distribution (e.g., a mixture of Gaussians) with components given by variational posteriors conditioned on learnable pseudo-inputs. We further extend this prior to a two layer hierarchical model and show that this architecture with a coupled prior and posterior, learns significantly better models. The model also avoids the usual local optima issues related to useless latent dimensions that plague VAEs. We provide empirical studies on six datasets, namely, static and binary MNIST, OMNIGLOT, Caltech 101 Silhouettes, Frey Faces and Histopathology patches, and show that applying the hierarchical VampPrior delivers state-of-the-art results on all datasets in the unsupervised permutation invariant setting and the best results or comparable to SOTA methods for the approach with convolutional networks.",,,,,"Tomczak, Jakub/ABB-7948-2020","Tomczak, Jakub/0000-0001-8634-694X",,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300127,0
C,"Xu, P; He, B; De Sa, C; Mitliagkas, I; Re, C",,"Storkey, A; PerezCruz, F",,"Xu, Peng; He, Bryan; De Sa, Christopher; Mitliagkas, Ioannis; Re, Christopher",,,Accelerated Stochastic Power Iteration,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Principal component analysis (PCA) is one of the most powerful tools for analyzing matrices in machine learning. In this paper, we study methods to accelerate power iteration in the stochastic setting by adding a momentum term. While in the deterministic setting, power iteration with momentum has optimal iteration complexity, we show that naively adding momentum to a stochastic method does not always result in acceleration. We perform a novel, tight variance analysis that reveals a breaking-point variance beyond which this acceleration does not occur. Combining this insight with modern variance reduction techniques yields a simple version of power iteration with momentum that achieves the optimal iteration complexities in both the online and o.ine setting. Our methods are embarrassingly parallel and can produce wall-clock-time speedups. Our approach is very general and applies to many non-convex optimization problems that can now be accelerated using the same technique.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300007,0
C,"Zhang, KQ; Yang, ZR; Wang, ZR",,"Storkey, A; PerezCruz, F",,"Zhang, Kaiqing; Yang, Zhuoran; Wang, Zhaoran",,,Nonlinear Structured Signal Estimation in High Dimensions via Iterative Hard Thresholding,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"We study the high-dimensional signal estimation problem with nonlinear measurements, where the signal of interest is either sparse or low-rank. In both settings, our estimator is formulated as the minimizer of the nonlinear least-squares loss function under a combinatorial constraint, which is obtained efficiently by the iterative hard thresholding (IHT) algorithm. Although the loss function is non-convex due to the nonlinearity of the statistical model, the IHT algorithm is shown to converge linearly to a point with optimal statistical accuracy using arbitrary initialization. Moreover, our analysis only hinges on conditions similar to those required in the linear case. Detailed numerical experiments are included to corroborate the theoretical results.",,,,,"Wang, Zhaoran/P-7113-2018",,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300028,0
C,"Han, SB; Liao, XJ; Dunson, DB; Carin, L",,"Gretton, A; Robert, CC",,"Han, Shaobo; Liao, Xuejun; Dunson, David B.; Carin, Lawrence",,,Variational Gaussian Copula Inference,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"We utilize copulas to constitute a unified framework for constructing and optimizing variational proposals in hierarchical Bayesian models. For models with continuous and non-Gaussian hidden variables, we propose a semiparametric and automated variational Gaussian copula approach, in which the parametric Gaussian copula family is able to preserve multivariate posterior dependence, and the nonparametric transformations based on Bernstein polynomials provide ample flexibility in characterizing the univariate marginal posteriors.",,,,,,"Carin, Lawrence/0000-0001-6277-7948; Han, Shaobo/0000-0003-2545-7114",,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,829,838,,,,,,,,,,,,,,,,WOS:000508662100090,0
C,"Rooshenas, A; Lowd, D",,"Gretton, A; Robert, CC",,"Rooshenas, Amirmohammad; Lowd, Daniel",,,Discriminative Structure Learning of Arithmetic Circuits,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"The biggest limitation of probabilistic graphical models is the complexity of inference, which is often intractable. An appealing alternative is to use tractable probabilistic models, such as arithmetic circuits (ACs) and sum-product networks (SPNs), in which marginal and conditional queries can be answered efficiently. In this paper, we present the first discriminative structure learning algorithm for ACs, DACLearn (Discriminative AC Learner). Like previous work on generative structure learning, DACLearn finds a log-linear model with conjunctive features, using the size of an equivalent AC representation as a learning bias. Unlike previous work, DACLearn optimizes conditional likelihood, resulting in a more accurate conditional distribution. DACLearn also learns much more compact ACs than generative methods, since it does not need to represent a consistent distribution over the evidence variables. To ensure efficiency, DACLearn uses novel initialization and search heuristics to drastically reduce the number of feature evaluations required to learn an accurate model. In experiments on 20 benchmark domains, we find that our DACLearn learns models that are more accurate and compact than other tractable generative and discriminative methods.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,1506,1514,,,,,,,,,,,,,,,,WOS:000508662100163,0
C,"He, P; Zhang, CS",,"Kaski, S; Corander, J",,"He, Peng; Zhang, Changshui",,,Non-Asymptotic Analysis for Relational Learning with One Network,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"This theoretical paper is concerned with a rigorous non-asymptotic analysis of relational learning applied to a single network. Under suitable and intuitive conditions on features and clique dependencies over the network, we present the first probably approximately correct (PAC) bound for maximum likelihood estimation (MLE). To our best knowledge, this is the first sample complexity result of this problem. We propose a novel combinational approach to analyze complex dependencies of relational data, which is crucial to our non-asymptotic analysis. The consistency of MLE under our conditions is also proved as the consequence of our sample complexity bound. Finally, our combinational method for analyzing dependent data can be easily generalized to treat other generalized maximum likelihood estimators for relational learning.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,320,327,,,,,,,,,,,,,,,,WOS:000508355800036,0
C,"Honda, J; Takemura, A",,"Kaski, S; Corander, J",,"Honda, Junya; Takemura, Akimichi",,,Optimality of Thompson Sampling for Gaussian Bandits Depends on Priors,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"In stochastic bandit problems, a Bayesian policy called Thompson sampling (TS) has recently attracted much attention for its excellent empirical performance. However, the theoretical analysis of this policy is difficult and its asymptotic optimality is only proved for one-parameter models. In this paper we discuss the optimality of TS for the model of normal distributions with unknown means and variances as one of the most fundamental examples of multiparameter models. First we prove that the expected regret of TS with the uniform prior achieves the theoretical bound, which is the first result to show that the asymptotic bound is achievable for the normal distribution model. Next we prove that TS with Jeffreys prior and reference prior cannot achieve the theoretical bound. Therefore choice of priors is important for TS and non-informative priors are sometimes risky in cases of multiparameter models.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,375,383,,,,,,,,,,,,,,,,WOS:000508355800042,0
C,"Arora, R; Marinov, TV; Mohri, M",,"Banerjee, A; Fukumizu, K",,"Arora, Raman; Marinov, Teodor, V; Mohri, Mehryar",,,Corralling Stochastic Bandit Algorithms,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We study the problem of corralling stochastic bandit algorithms, that is combining multiple bandit algorithms designed for a stochastic environment, with the goal of devising a corralling algorithm that performs almost as well as the best base algorithm. We give two general algorithms for this setting, which we show benefit from favorable regret guarantees. We show that the regret of the corralling algorithms is no worse than that of the best algorithm containing the arm with the highest reward, and depends on the gap between the highest reward and other rewards.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802055,0
C,"Cai, YF; Li, P",,"Banerjee, A; Fukumizu, K",,"Cai, Yunfeng; Li, Ping",,,Identification of Matrix Joint Block Diagonalization,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Given a set C = {C-i}(i=1)(m) of square matrices, the matrix blind joint block diagonalization problem (BJBDP) is to find a full column rank matrix A such that C-i = A Sigma(i)A(inverted perpendicular) for all i, where Sigma(i)'s are all block diagonal matrices with as many diagonal blocks as possible. The bjbdp plays an important role in independent subspace analysis (ISA). This paper considers the identification problem for bjbdp, that is, under what conditions and by what means, we can identify the diagonalizer A and the block diagonal structure of Sigma(i), especially when there is noise in C-i's. In this paper, we propose a bi-block diagonalization method to solve bjbdp, and establish sufficient conditions under which the method is able to accomplish the task. Numerical simulations validate our theoretical results. To the best of the authors' knowledge, existing numerical methods for bjbdp have no theoretical guarantees for the identification of the exact solution, whereas our method does.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801080,0
C,"Canal, G; Bloch, M; Rozell, C",,"Banerjee, A; Fukumizu, K",,"Canal, Gregory; Bloch, Matthieu; Rozell, Christopher",,,Feedback Coding for Active Learning,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"The iterative selection of examples for labeling in active machine learning is conceptually similar to feedback channel coding in information theory: in both tasks, the objective is to seek a minimal sequence of actions to encode information in the presence of noise. While this high-level overlap has been previously noted, there remain open questions on how to best formulate active learning as a communications system to leverage existing analysis and algorithms in feedback coding. In this work, we formally identify and leverage the structural commonalities between the two problems, including the characterization of encoder and noisy channel components, to design a new algorithm. Specifically, we develop an optimal transport-based feedback coding scheme called Approximate Posterior Matching (APM) for the task of active example selection and explore its application to Bayesian logistic regression, a popular model in active learning. We evaluate APM on a variety of datasets and demonstrate learning performance comparable to existing active learning methods, at a reduced computational cost. These results demonstrate the potential of directly deploying concepts from feedback channel coding to design efficient active learning strategies.",,,,,,"Rozell, Christopher/0000-0001-5173-1661",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801077,0
C,"Chen, YF; Yang, Y",,"Banerjee, A; Fukumizu, K",,"Chen, Yifan; Yang, Yun",,,Accumulations of Projections-A Unified Framework for Random Sketches in Kernel Ridge Regression,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Building a sketch of an n-by-n empirical kernel matrix is a common approach to accelerate the computation of many kernel methods. In this paper, we propose a unified framework of constructing sketching methods in kernel ridge regression (KRR), which views the sketching matrix S as an accumulation of m rescaled sub-sampling matrices with independent columns. Our framework incorporates two commonly used sketching methods, subsampling sketches (known as the Nystrom method) and sub-Gaussian sketches, as special cases with m = 1 and m = 1 respectively. Under the new framework, we provide a unified error analysis of sketching approximation and show that our accumulation scheme improves the low accuracy of sub-sampling sketches when certain incoherence characteristic is high, and accelerates the more accurate but computationally heavier sub-Gaussian sketches. By optimally choosing the number m of accumulations, we show that a best trade-off between computational efficiency and statistical accuracy can be achieved. In practice, the sketching method can be as efficiently implemented as the sub-sampling sketches, as only minor extra matrix additions are needed. Our empirical evaluations also demonstrate that the proposed method may attain the accuracy close to sub-Gaussian sketches, while is as efficient as sub-sampling-based sketches.",,,,,"Chen, Yi/HIR-2608-2022",,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803055,0
C,"Connor, M; Canal, G; Rozell, C",,"Banerjee, A; Fukumizu, K",,"Connor, Marissa; Canal, Gregory; Rozell, Christopher",,,Variational Autoencoder with Learned Latent Structure,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"The manifold hypothesis states that high-dimensional data can be modeled as lying on or near a low-dimensional, nonlinear manifold. Variational Autoencoders (VAEs) approximate this manifold by learning mappings from low-dimensional latent vectors to high-dimensional data while encouraging a global structure in the latent space through the use of a specified prior distribution. When this prior does not match the structure of the true data manifold, it can lead to a less accurate model of the data. To resolve this mismatch, we introduce the Variational Autoencoder with Learned Latent Structure (VAELLS) which incorporates a learnable manifold model into the latent space of a VAE. This enables us to learn the nonlinear manifold structure from the data and use that structure to define a prior in the latent space. The integration of a latent manifold model not only ensures that our prior is well-matched to the data, but also allows us to define generative transformation paths in the latent space and describe class manifolds with transformations stemming from examples of each class. We validate our model on examples with known latent structure and also demonstrate its capabilities on a real-world dataset.",,,,,,"Rozell, Christopher/0000-0001-5173-1661",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802082,0
C,"Ding, Q; Hsieh, CJ; Sharpnack, J",,"Banerjee, A; Fukumizu, K",,"Ding, Qin; Hsieh, Cho-Jui; Sharpnack, James",,,An Efficient Algorithm For Generalized Linear Bandit: Online Stochastic Gradient Descent and Thompson Sampling,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We consider the contextual bandit problem, where a player sequentially makes decisions based on past observations to maximize the cumulative reward. Although many algorithms have been proposed for contextual bandit, most of them rely on finding the maximum likelihood estimator at each iteration, which requires O(t) time at the t-th iteration and are memory inefficient. A natural way to resolve this problem is to apply online stochastic gradient descent (SGD) so that the per-step time and memory complexity can be reduced to constant with respect to t, but a contextual bandit policy based on online SGD updates that balances exploration and exploitation has remained elusive. In this work, we show that online SGD can be applied to the generalized linear bandit problem. The proposed SGD-TS algorithm, which uses a single-step SGD update to exploit past information and uses Thompson Sampling for exploration, achieves (O) over tilde(root T) regret with the total time complexity that scales linearly in T and d, where T is the total number of rounds and d is the number of features. Experimental results show that SGD-TS consistently outperforms existing algorithms on both synthetic and real datasets.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801090,0
C,"Gong, Y; Hajimirsadeghi, H; He, JW; Durand, T; Mori, G",,"Banerjee, A; Fukumizu, K",,"Gong, Yu; Hajimirsadeghi, Hossein; He, Jiawei; Durand, Thibaut; Mori, Greg",,,Variational Selective Autoencoder: Learning from Partially-Observed Heterogeneous Data,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Learning from heterogeneous data poses challenges such as combining data from various sources and of different types. Meanwhile, heterogeneous data are often associated with missingness in real-world applications due to heterogeneity and noise of input sources. In this work, we propose the variational selective autoencoder (VSAE), a general framework to learn representations from partially-observed heterogeneous data. VSAE learns the latent dependencies in heterogeneous data by modeling the joint distribution of observed data, unobserved data, and the imputation mask which represents how the data are missing. It results in a unified model for various downstream tasks including data generation and imputation. Evaluation on both low-dimensional and high-dimensional heterogeneous datasets for these two tasks shows improvement over state-of-the-art models.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802084,0
C,"Gupta, V; Bedathur, S; Bhattacharya, S; De, A",,"Banerjee, A; Fukumizu, K",,"Gupta, Vinayak; Bedathur, Srikanta; Bhattacharya, Sourangshu; De, Abir",,,Learning Temporal Point Processes with Intermittent Observations,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Marked temporal point processes (MTPP) have emerged as a powerful framework to model the underlying generative mechanism of asynchronous events localized in continuous time. Most existing models and inference methods in MTPP framework consider only the complete observation scenario i.e. the event sequence being modeled is completely observed with no missing events an ideal setting barely encountered in practice. A recent line of work which considers missing events uses supervised learning techniques which require a missing or observed label for each event. In this work, we provide a novel unsupervised model and inference method for MTPPs in presence of missing events. We first model the generative processes of observed events and missing events using two MTPPs, where the missing events are represented as latent random variables. Then we devise an unsupervised training method that jointly learns both the MTPPs by means of variational inference. Experiments with real datasets show that our modeling and inference frameworks can effectively impute the missing data among the observed events, which in turn enhances its predictive prowess.",,,,,"Gupta, Vinayak/GVR-7633-2022",,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804056,0
C,"Jasinska-Kobus, K; Wydmuch, M; Thiruvenkatachari, D; Dembczynski, K",,"Banerjee, A; Fukumizu, K",,"Jasinska-Kobus, Kalina; Wydmuch, Marek; Thiruvenkatachari, Devanathan; Dembczynski, Krzysztof",,,Online probabilistic label trees,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We introduce online probabilistic label trees (OPLTs), an algorithm that trains a label tree classifier in a fully online manner without any prior knowledge about the number of training instances, their features and labels. OPLTs are characterized by low time and space complexity as well as strong theoretical guarantees. They can be used for online multi-label and multi-class classification, including the very challenging scenarios of one- or few-shot learning. We demonstrate the attractiveness of OPLTs in a wide empirical study on several instances of the tasks mentioned above.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802020,0
C,"Lee, HS; Shen, C; Zame, W; Lee, JW; van der Schaar, M",,"Banerjee, A; Fukumizu, K",,"Lee, Hyun-Suk; Shen, Cong; Zame, William; Lee, Jang-Won; van der Schaar, Mihaela",,,SDF-Bayes: Cautious Optimism in Safe Dose-Finding Clinical Trials with Drug Combinations and Heterogeneous Patient Groups,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Phase I clinical trials are designed to test the safety (non-toxicity) of drugs and find the maximum tolerated dose (MTD). This task becomes significantly more challenging when multiple-drug dose-combinations (DC) are involved, due to the inherent conict between the exponentially increasing DC candidates and the limited patient budget. This paper proposes a novel Bayesian design, SDF-Bayes, for finding the MTD for drug combinations in the presence of safety constraints. Rather than the conventional principle of escalating or de-escalating the current dose of one drug (perhaps alternating between drugs), SDF-Bayes proceeds by cautious optimism: it chooses the next DC that, on the basis of current information, is most likely to be the MTD (optimism), subject to the constraint that it only chooses DCs that have a high probability of being safe (caution). We also propose an extension, SDF-Bayes-AR, that accounts for patient heterogeneity and enables heterogeneous patient recruitment. Extensive experiments based on both synthetic and realworld datasets demonstrate the advantages of SDF-Bayes over state of the art DC trial designs in terms of accuracy and safety.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803058,0
C,"Lee, JN; Pacchiano, A; Muthukumar, V; Kong, WH; Brunskill, E",,"Banerjee, A; Fukumizu, K",,"Lee, Jonathan N.; Pacchiano, Aldo; Muthukumar, Vidya; Kong, Weihao; Brunskill, Emma",,,Online Model Selection for Reinforcement Learning with Function Approximation,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Deep reinforcement learning has achieved impressive successes yet often requires a very large amount of interaction data. This result is perhaps unsurprising, as using complicated function approximation often requires more data to fit, and early theoretical results on linear Markov decision processes provide regret bounds that scale with the dimension of the linear approximation. Ideally, we would like to automatically identify the minimal dimension of the approximation that is sufficient to encode an optimal policy. Towards this end, we consider the problem of model selection in RL with function approximation, given a set of candidate RL algorithms with known regret guarantees. The learner's goal is to adapt to the complexity of the optimal algorithm without knowing it a priori. We present a meta-algorithm that successively rejects increasingly complex models using a simple statistical test. Given at least one candidate that satisfies realizability, we prove the meta-algorithm adapts to the optimal complexity with regret that is only marginally suboptimal in the number of episodes and number of candidate algorithms. The dimension and horizon dependencies remain optimal with respect to the best candidate, and our meta-algorithmic approach is flexible to incorporate multiple candidate algorithms and models. Finally, we show that the meta-algorithm automatically admits significantly improved instance-dependent regret bounds that depend on the gaps between the maximal values attainable by the candidates.",,,,,,"Muthukumar, Vidya/0000-0003-2786-7360",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804006,0
C,"Li, ZC; Chen, PY; Liu, SJ; Lu, ST; Xu, YY",,"Banerjee, A; Fukumizu, K",,"Li, Zichong; Chen, Pin-Yu; Liu, Sijia; Lu, Songtao; Xu, Yangyang",,,Rate-improved Inexact Augmented Lagrangian Method for Constrained Nonconvex Optimization,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"First-order methods have been studied for nonlinear constrained optimization within the framework of the augmented Lagrangian method (ALM) or penalty method. We propose an improved inexact ALM (iALM) and conduct a unified analysis for nonconvex problems with either affine equality or nonconvex constraints. Under certain regularity conditions (that are also assumed by existing works), we show an (O) over tilde (epsilon(-5/2)) complexity result for a problem with a nonconvex objective and affine equality constraints and an (O) over tilde (epsilon(-3)) complexity result for a problem with a nonconvex objective and nonconvex constraints, where the complexity is measured by the number of first-order oracles to yield an epsilon-KKT solution. Both results are the best known. The same-order complexity results have been achieved by penalty methods. However, two different analysis techniques are used to obtain the results, and more importantly, the penalty methods generally perform significantly worse than iALM in practice. Our improved iALM and analysis close the gap between theory and practice. Numerical experiments on nonconvex problems with affine equality or nonconvex constraints are provided to demonstrate the effectiveness of our proposed method.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802061,0
C,"Ohsaka, N",,"Banerjee, A; Fukumizu, K",,"Ohsaka, Naoto",,,"Unconstrained MAP Inference, Exponentiated Determinantal Point Processes, and Exponential Inapproximability",24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We study the computational complexity of two hard problems on determinantal point processes (DPPs). One is maximum a posteriori (MAP) inference, i.e., to find a principal submatrix having the maximum determinant. The other is probabilistic inference on exponentiated DPPs (E-DPPs), which can sharpen or weaken the diversity preference of DPPs with an exponent parameter p. We prove the following complexity-theoretic hardness results that explain the difficulty in approximating unconstrained MAP inference and the normalizing constant for E-DPPs. Unconstrained MAP inference for an n x n matrix is NP-hard to approximate within a 2(beta n)-factor, where beta = 10(-1013). This result improves upon a (9/8 - epsilon)-factor inapproximability given by Kulesza and Taskar (2012). The normalizing constant for E-DPPs of any (fixed) constant exponent p >= beta(-1) = 10(1013) is NP-hard to approximate within a 2(beta pn)-factor. This gives a(nother) negative answer to open questions posed by Kulesza and Taskar (2012); Ohsaka and Matsuoka (2020).",,,,,,"Ohsaka, Naoto/0000-0001-9584-4764",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,154,+,,,,,,,,,,,,,,,,WOS:000659893800018,0
C,"Sapkota, H; Ying, YM; Chen, F; Yu, Q",,"Banerjee, A; Fukumizu, K",,"Sapkota, Hitesh; Ying, Yiming; Chen, Feng; Yu, Qi",,,Distributionally Robust Optimization for Deep Kernel Multiple Instance Learning,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Multiple Instance Learning (MIL) provides a promising solution to many real-world problems, where labels are only available at the bag level but missing for instances due to a high labeling cost. As a powerful Bayesian non-parametric model, Gaussian Processes (GP) have been extended from classical supervised learning to MIL settings, aiming to identify the most likely positive (or least negative) instance from a positive (or negative) bag using only the bag-level labels. However, solely focusing on a single instance in a bag makes the model less robust to outliers or multi-modal scenarios, where a single bag contains a diverse set of positive instances. We propose a general GP mixture framework that simultaneously considers multiple instances through a latent mixture model. By adding a top-k constraint, the framework is equivalent to choosing the top-k most positive instances, making it more robust to outliers and multimodal scenarios. We further introduce a Distributionally Robust Optimization (DRO) constraint that removes the limitation of specifying a fix k value. To ensure the prediction power over high-dimensional data (e.g., videos and images) that are common in MIL, we augment the GP kernel with fixed basis functions by using a deep neural network to learn adaptive basis functions so that the covariance structure of high-dimensional data can be accurately captured. Experiments are conducted on highly challenging real-world video anomaly detection tasks to demonstrate the effectiveness of the proposed model.",,,,,"Ying, Yiming/AGD-7246-2022","Ying, Yiming/0000-0001-7345-6672",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802063,0
C,"Shekhar, S; Javidi, T",,"Banerjee, A; Fukumizu, K",,"Shekhar, Shubhanshu; Javidi, Tara",,,Significance of Gradient Information in Bayesian Optimization,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We consider the problem of Bayesian Optimization (BO) where the goal is to design an adaptive querying strategy to optimize a function f : [0, 1]d 7! R. The function is assumed to be drawn from a Gaussian Process GP (0, K), and can only be accessed through noisy oracle queries. The most commonly used oracle in BO literature is the noisy Zeroth-Order-Oracle (ZOO) which returns noise-corrupted function value y = f(x)+eta at any point x 2 X queried by the agent. A less studied oracle in BO is the First-Order-Oracle (FOO) which also returns noisy gradient value at the queried point. In this paper we consider the fundamental question of quantifying the possible improvement in regret that can be achieved under FOO access as compared to the case in which only ZOO access is available. Under some regularity assumptions on K, we first show that the expected cumulative regret Rn with ZOO of any algorithm must satisfy a lower bound of Omega(root 2(d) n), where n is the query budget. This lower bound captures the appropriate scaling of the regret on both dimension d and budget n, and relies on a novel reduction from BO to a multi-armed bandit (MAB) problem. We then propose a two-phase algorithm which, with some additional prior knowledge, achieves a vastly improved O d(log n)2 regret when given access to a FOO. Together, these two results highlight the significant value of incorporating gradient information in BO algorithms.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803042,0
C,"van der Heide, C; Hodgkinson, L; Roosta, F; Kroese, DP",,"Banerjee, A; Fukumizu, K",,"van der Heide, Chris; Hodgkinson, Liam; Roosta, Fred; Kroese, Dirk P.",,,Shadow Manifold Hamiltonian Monte Carlo,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Hamiltonian Monte Carlo and its descendants have found success in machine learning and computational statistics due to their ability to draw samples in high dimensions with greater efficiency than classical MCMC. One of these derivatives, Riemannian manifold Hamiltonian Monte Carlo (RMHMC), better adapts the sampler to the geometry of the target density, allowing for improved performances in sampling problems with complex geometric features. Other approaches have boosted acceptance rates by sampling from an integrator-dependent shadow density and compensating for the induced bias via importance sampling. We combine the benefits of RMHMC with those attained by sampling from the shadow density, by deriving the shadow Hamiltonian corresponding to the generalized leapfrog integrator used in RMHMC. This leads to a new algorithm, shadow manifold Hamiltonian Monte Carlo, that shows improved performance over RMHMC, and leaves the target density invariant.",,,,,,"Hodgkinson, Liam/0000-0002-4595-0347",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801078,0
C,"Xie, SR; Hu, SK; Wang, XJ; Liu, CX; Shi, JP; Liu, XY; Lin, DH",,"Banerjee, A; Fukumizu, K",,"Xie, Sirui; Hu, Shoukang; Wang, Xinjiang; Liu, Chunxiao; Shi, Jianping; Liu, Xunying; Lin, Dahua",,,Understanding the wiring evolution in differentiable neural architecture search,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Controversy exists on whether differentiable neural architecture search methods discover wiring topology effectively. To understand how wiring topology evolves, we study the underlying mechanism of several leading differentiable NAS frameworks. Our investigation is motivated by three observed searching patterns of differentiable NAS: 1) they search by growing instead of pruning; 2) wider networks are more preferred than deeper ones; 3) no edges are selected in bi-level optimization. To anatomize these phenomena, we propose a unified view on searching algorithms of existing frameworks, transferring the global optimization to local cost minimization. Based on this reformulation, we conduct empirical and theoretical analyses, revealing implicit biases in the cost's assignment mechanism and evolution dynamics that cause the observed phenomena. These biases indicate strong discrimination towards certain topologies. To this end, we pose questions that future differentiable methods for neural wiring discovery need to confront, hoping to evoke a discussion and rethinking on how much bias has been enforced implicitly in existing NAS methods.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801011,0
C,"Boursier, E; Kaufmann, E; Mehrabian, A; Perchet, V",,"Chiappa, S; Calandra, R",,"Boursier, Etienne; Kaufmann, Emilie; Mehrabian, Abbas; Perchet, Vianney",,,A Practical Algorithm for Multiplayer Bandits when Arm Means Vary Among Players,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We study a multiplayer stochastic multi-armed bandit problem in which players cannot communicate, and if two or more players pull the same arm, a collision occurs and the involved players receive zero reward. We consider the challenging heterogeneous setting, in which different arms may have different means for different players, and propose a new and efficient algorithm that combines the idea of leveraging forced collisions for implicit communication and that of performing matching eliminations. We present a finite-time analysis of our algorithm, giving the first sublinear minimax regret bound for this problem, and prove that if the optimal assignment of players to arms is unique, our algorithm attains the optimal O(ln(T)) regret, solving an open question raised at NeurIPS 2018 by Bistritz and Leshem (2018).",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1211,1220,,,,,,,,,,,,,,,,WOS:000559931302038,0
C,"Calauzenes, C; Nedelec, T; Perchet, V; El Karoui, N",,"Chiappa, S; Calandra, R",,"Calauzenes, Clement; Nedelec, Thomas; Perchet, Vianney; El Karoui, Noureddine",,,Robust Stackelberg buyers in repeated auctions,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We consider the practical and classical setting where the seller is using an exploration stage to learn the value distributions of the bidders before running a revenue-maximizing auction in a exploitation phase. In this two-stage process, we exhibit practical, simple and robust strategies with large utility uplifts for the bidders. We quantify precisely the seller revenue against non-discounted buyers, complementing recent studies that had focused on impatient/heavily discounted buyers. We also prove the robustness of these shading strategies to sample approximation error of the seller, to bidder's approximation error of the competition and to possible change of the mechanisms.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1342,1350,,,,,,,,,,,,,,,,WOS:000559931302053,0
C,"Cheng, LF; Dumitrascu, B; Zhang, M; Chivers, C; Draugelis, M; Li, K; Engelhardt, BE",,"Chiappa, S; Calandra, R",,"Cheng, Li-Fang; Dumitrascu, Bianca; Zhang, Michael; Chivers, Corey; Draugelis, Michael; Li, Kai; Engelhardt, Barbara E.",,,Patient-Specific Effects of Medication Using Latent Force Models with Gaussian Processes,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"A multi-output Gaussian process (GP) is a flexible Bayesian nonparametric framework that has proven useful in jointly modeling the physiological states of patients in medical time series data. However, capturing the short-term effects of drugs and therapeutic interventions on patient physiological state remains challenging. We propose a novel approach that models the effect of interventions as a hybrid Gaussian process composed of a GP capturing patient baseline physiology convolved with a latent force model capturing effects of treatments on specific physiological features. The combination of a multi-output GP with a time-marked kernel GP leads to a well-characterized model of patients' physiological state across a hospital stay, including response to interventions. Our model leads to analytically tractable cross-covariance functions that allow for scalable inference. Our hierarchical model includes estimates of patient-specific effects but allows sharing of support across patients. Our approach achieves competitive predictive performance on challenging hospital data, where we recover patient-specific response to the administration of three common drugs: one antihypertensive drug and two anticoagulants.",,,,,,"Chivers, Corey/0000-0001-7290-2183",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,4045,4054,,,,,,,,,,,,,,,,WOS:000559931300061,0
C,"Correia, AHC; Cussens, J; de Campos, CP",,"Chiappa, S; Calandra, R",,"Correia, Alvaro H. C.; Cussens, James; de Campos, Cassio P.",,,On Pruning for Score-Based Bayesian Network Structure Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Many algorithms for score-based Bayesian network structure learning (BNSL), in particular exact ones, take as input a collection of potentially optimal parent sets for each variable in the data. Constructing such collections naively is computationally intensive since the number of parent sets grows exponentially with the number of variables. Thus, pruning techniques are not only desirable but essential. While good pruning rules exist for the Bayesian Information Criterion (BIC), current results for the Bayesian Dirichlet equivalent uniform (BDeu) score reduce the search space very modestly, hampering the use of the (often preferred) BDeu. We derive new non-trivial theoretical upper bounds for the BDeu score that considerably improve on the state-of-the-art. Since the new bounds are mathematically proven to be tighter than previous ones and at little extra computational cost, they are a promising addition to BNSL methods.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2709,2717,,,,,,,,,,,,,,,,WOS:000559931300065,0
C,"Curtin, RR; Im, SJ; Moseley, B; Pruhs, K; Samadian, A",,"Chiappa, S; Calandra, R",,"Curtin, Ryan R.; Im, Sungjin; Moseley, Benjamin; Pruhs, Kirk; Samadian, Alireza",,,Unconditional Coresets for Regularized Loss Minimization,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We design and mathematically analyze sampling-based algorithms for regularized loss minimization problems that are implementable in popular computational models for large data, in which the access to the data is restricted in some way. Our main result is that if the regularizer's effect does not become negligible as the norm of the hypothesis scales, and as the data scales, then a uniform sample of modest size is with high probability a coreset. In the case that the loss function is either logistic regression or soft-margin support vector machines, and the regularizer is one of the common recommended choices, this result implies that a uniform sample of size O(d root n) is with high probability a coreset of n points in R-d. We contrast this upper bound with two lower bounds. The first lower bound shows that our analysis of uniform sampling is tight; that is, a smaller uniform sample will likely not be a core set. The second lower bound shows that in some sense uniform sampling is close to optimal, as significantly smaller core sets do not generally exist.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303004,0
C,"Galy-Fajou, T; Wenzel, F; Opper, M",,"Chiappa, S; Calandra, R",,"Galy-Fajou, Theo; Wenzel, Florian; Opper, Manfred",,,Automated Augmented Conjugate Inference for Non-conjugate Gaussian Process Models,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We propose automated augmented conjugate inference, a new inference method for non-conjugate Gaussian processes (GP) models. Our method automatically constructs an auxiliary variable augmentation that renders the GP model conditionally conjugate. Building on the conjugate structure of the augmented model, we develop two inference methods. First, a fast and scalable stochastic variational inference method that uses efficient block coordinate ascent updates, which are computed in closed form. Second, an asymptotically correct Gibbs sampler that is useful for small datasets. Our experiments show that our method are up two orders of magnitude faster and more robust than existing state-of-the-art black-box methods.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3025,3034,,,,,,,,,,,,,,,,WOS:000559931301012,0
C,"Gangrade, A; Acar, DAE; Saligrama, V",,"Chiappa, S; Calandra, R",,"Gangrade, Aditya; Acar, Durmus Alp Emre; Saligrama, Venkatesh",,,Budget Learning via Bracketing,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Conventional machine learning applications in the mobile/IoT setting transmit data to a cloud-server for predictions. Due to cost considerations (power, latency, monetary), it is desirable to minimise device-to-server transmissions. The budget learning (BL) problem poses the learner's goal as minimising use of the cloud while suffering no discernible loss in accuracy, under the constraint that the methods employed be edge-implementable. We propose a new formulation for the BL problem via the concept of bracketings. Concretely, we propose to sandwich the cloud's prediction, g, via functions h(-), h(+) from a `simple' class so that h(-) <= g <= h(+) nearly always. On an instance x, if h(+) (x) = h(-) (x), we leverage local processing, and bypass the cloud. We explore theoretical aspects of this formulation, providing PAC-style learnability definitions; associating the notion of budget learnability to approximability via brackets; and giving VC-theoretic analyses of their properties. We empirically validate our theory on real-world datasets, demonstrating improved performance over prior gating based methods.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,4109,4118,,,,,,,,,,,,,,,,WOS:000559931300003,0
C,"Geyer, K; Kyrillidis, A; Kalev, A",,"Chiappa, S; Calandra, R",,"Geyer, Kelly; Kyrillidis, Anastasios; Kalev, Amir",,,Low-rank regularization and solution uniqueness in over-parameterized matrix sensing,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We consider the question whether algorithmic choices in over-parameterized linear matrix factorization introduce implicit low-rank regularization. We focus on the noiseless matrix sensing scenario over low-rank positive semi-definite (PSD) matrices over the reals, with a sensing mechanism that satisfies restricted isometry properties. Surprisingly, it was recently argued that for recovery of PSD matrices, gradient descent over a squared, full-rank factorized space introduces implicit low-rank regularization. Thus, a clever choice of the recovery algorithm avoids the need for explicit low-rank regularization. In this contribution, we prove that in fact, under certain conditions, the PSD constraint by itself is sufficient to lead to a unique low-rank matrix recovery, without explicit or implicit regularization. Therefore, under these conditions, the set of PSD matrices that are consistent with the observed data, is a singleton, regardless of the algorithm used. Our numerical study indicates that this result is general and extends to cases beyond the those covered by the proof.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,930,939,,,,,,,,,,,,,,,,WOS:000559931301019,0
C,"Makkuva, AV; Kannan, S; Oh, S; Viswanath, P",,"Chiappa, S; Calandra, R",,"Makkuva, Ashok Vardhan; Kannan, Sreeram; Oh, Sewoong; Viswanath, Pramod",,,Learning in Gated Neural Networks,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Gating is a key feature in modern neural networks including LSTMs, GRUs and sparsely-gated deep neural networks. The backbone of such gated networks is a mixture-of-experts layer, where several experts make regression decisions and gating controls how to weigh the decisions in an input-dependent manner. Despite having such a prominent role in both modern and classical machine learning, very little is understood about parameter recovery of mixture-of-experts since gradient descent and EM algorithms are known to be stuck in local optima in such models. In this paper, we perform a careful analysis of the optimization landscape and show that with appropriately designed loss functions, gradient descent can indeed learn the parameters of a MoE accurately. A key idea underpinning our results is the design of two distinct loss functions, one for recovering the expert parameters and another for recovering the gating parameters. We demonstrate the first sample complexity results for parameter recovery in this model for any algorithm and demonstrate significant performance gains over standard loss functions in numerical experiments.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3338,3347,,,,,,,,,,,,,,,,WOS:000559931302029,0
C,"Marconi, GM; Rosasco, L; Ciliberto, C",,"Chiappa, S; Calandra, R",,"Marconi, Gian Maria; Rosasco, Lorenzo; Ciliberto, Carlo",,,Hyperbolic Manifold Regression,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Geometric representation learning has recently shown great promise in several machine learning settings, ranging from relational learning to language processing and generative models. In this work, we consider the problem of performing manifold-valued regression onto an hyperbolic space as an intermediate component for a number of relevant machine learning applications. In particular, by formulating the problem of predicting nodes of a tree as a manifold regression task in the hyperbolic space, we propose a novel perspective on two challenging tasks: 1) hierarchical classification via label embeddings and 2) taxonomy extension of hyperbolic representations. To address the regression problem we consider previous methods as well as proposing two novel approaches that are computationally more advantageous: a parametric deep learning model that is informed by the geodesics of the target space and a non-parametric kernel-method for which we also prove excess risk bounds. Our experiments show that the strategy of leveraging the hyperbolic geometry is promising. In particular, in the taxonomy expansion setting, we find that the hyperbolic-based estimators significantly outperform methods performing regression in the ambient Euclidean space.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2570,2579,,,,,,,,,,,,,,,,WOS:000559931302030,0
C,"Mutny, M; Derezinski, M; Krause, A",,"Chiappa, S; Calandra, R",,"Mutny, Mojmir; Derezinski, Michal; Krause, Andreas",,,Convergence Analysis of Block Coordinate Algorithms with Determinantal Sampling,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We analyze the convergence rate of the randomized Newton-like method introduced by Chu et al. (2016) for smooth and convex objectives, which uses random coordinate blocks of a Hessian-over-approximation matrix M instead of the true Hessian. The convergence analysis of the algorithm is challenging because of its complex dependence on the structure of M. However, we show that when the coordinate blocks are sampled with probability proportional to their determinant, the convergence rate depends solely on the eigenvalue distribution of matrix M, and has an analytically tractable form. To do so, we derive a fundamental new expectation formula for determinantal point processes. We show that determinantal sampling allows us to reason about the optimal subset size of blocks in terms of the spectrum of M. Additionally, we provide a numerical evaluation of our analysis, demonstrating cases where determinantal sampling is superior or on par with uniform sampling.",,,,,,"Krause, Andreas/0000-0001-7260-9673",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3110,3119,,,,,,,,,,,,,,,,WOS:000559931302052,0
C,"Niu, CH; Song, Y; Song, JM; Zhao, SJ; Grover, A; Ermon, S",,"Chiappa, S; Calandra, R",,"Niu, Chenhao; Song, Yang; Song, Jiaming; Zhao, Shengjia; Grover, Aditya; Ermon, Stefano",,,Permutation Invariant Graph Generation via Score-Based Generative Modeling,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Learning generative models for graph-structured data is challenging because graphs are discrete, combinatorial, and the underlying data distribution is invariant to the ordering of nodes. However, most of the existing generative models for graphs are not invariant to the chosen ordering, which might lead to an undesirable bias in the learned distribution. To address this difficulty, we propose a permutation invariant approach to modeling graphs, using the recent framework of score-based generative modeling. In particular, we design a permutation equivariant, multi-channel graph neural network to model the gradient of the data distribution at the input graph (a.k.a., the score function). This permutation equivariant model of gradients implicitly defines a permutation invariant distribution for graphs. We train this graph neural network with score matching and sample from it with annealed Langevin dynamics. In our experiments, we first demonstrate the capacity of this new architecture in learning discrete graph algorithms. For graph generation, we find that our learning approach achieves better or comparable results to existing models on benchmark datasets.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,4474,4483,,,,,,,,,,,,,,,,WOS:000559931302058,0
C,"Pillaud-Vivien, L; Bach, F; Lelievre, T; Rudi, A; Stoltz, G",,"Chiappa, S; Calandra, R",,"Pillaud-Vivien, Loucas; Bach, Francis; Lelievre, Tony; Rudi, Alessandro; Stoltz, Gabriel",,,Statistical Estimation of the Poincare constant and Application to Sampling Multimodal Distributions,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Poincare inequalities are ubiquitous in probability and analysis and have various applications in statistics (concentration of measure, rate of convergence of Markov chains). The Poincare constant, for which the inequality is tight, is related to the typical convergence rate of diffusions to their equilibrium measure. In this paper, we show both theoretically and experimentally that, given sufficiently many samples of a measure, we can estimate its Poincare constant. As a by-product of the estimation of the Poincare constant, we derive an algorithm that captures a low dimensional representation of the data by finding directions which are difficult to sample. These directions are of crucial importance for sampling or in fields like molecular dynamics, where they are called reaction coordinates. Their knowledge can leverage, with a simple conditioning step, computational bottlenecks by using importance sampling techniques.",,,,,"Lelivre, Tony/AFS-1655-2022","Lelivre, Tony/0000-0002-3412-113X",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2753,2762,,,,,,,,,,,,,,,,WOS:000559931302073,0
C,"Rush, C",,"Chiappa, S; Calandra, R",,"Rush, Cynthia",,,An Asymptotic Rate for the LASSO Loss,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"The LASSO is a well-studied method for use in high-dimensional linear regression where one wishes to recover a sparse vector beta is an element of R-p from noisy observations y is an element of R-n measured through a matrix X is an element of R-nxp with the model y = X beta + w where w is a vector of independent, mean-zero noise. We study the linear asymptotic regime where n/p -> delta for a constant delta is an element of(0, infinity). Using a carefully constructed approximate message passing (AMP) algorithm that converges to the LASSO estimator and recent finite sample theoretical performance guarantees for AMP, we provide large deviations bounds between various measures of LASSO loss and their concentrating values predicted by the AMP state evolution that shows exponentially fast convergence (in n) when the measurement matrix X is i.i.d. Gaussian. This work refines previous asymptotic analysis of LASSO loss in [Bayati and Montanari, 2012].",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3664,3672,,,,,,,,,,,,,,,,WOS:000559931302097,0
C,"Shechner, M; Sheffet, O; Stemmer, U",,"Chiappa, S; Calandra, R",,"Shechner, Moshe; Sheffet, Or; Stemmer, Uri",,,Private k-Means Clustering with Stability Assumptions,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We study the problem of differentially private clustering under input-stability assumptions. Despite the ever-growing volume of works on differential privacy in general and differentially private clustering in particular, only three works (Nissim et al., 2007; Wang et al., 2015; Huang and Liu, 2018) looked at the problem of privately clustering nice k-means instances, all three relying on the sample-and-aggregate framework and all three measuring utility in terms of Wasserstein distance between the true cluster centers and the centers returned by the private algorithm. In this work we improve upon this line of works on multiple axes. We present a simpler algorithm for clustering stable inputs (not relying on the sample-and-aggregate framework), and analyze its utility in both the Wasserstein distance and the k-means cost. Moreover, our algorithm has straight-forward analogues for nice k-median instances and for the local-model of differential privacy.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303013,0
C,"Trapp, M; Peharz, R; Pernkopf, F; Rasmussen, CE",,"Chiappa, S; Calandra, R",,"Trapp, Martin; Peharz, Robert; Pernkopf, Franz; Rasmussen, Carl Edward",,,Deep Structured Mixtures of Gaussian Processes,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Gaussian Processes (GPs) are powerful non-parametric Bayesian regression models that allow exact posterior inference, but exhibit high computational and memory costs. In order to improve scalability of GPs, approximate posterior inference is frequently employed, where a prominent class of approximation techniques is based on local GP experts. However, local-expert techniques proposed so far are either not well-principled, come with limited approximation guarantees, or lead to intractable models. In this paper, we introduce deep structured mixtures of GP experts, a stochastic process model which i) allows exact posterior inference, ii) has attractive computational and memory costs, and iii) when used as GP approximation, captures predictive uncertainties consistently better than previous expert-based approximations. In a variety of experiments, we show that deep structured mixtures have a low approximation error and often perform competitive or outperform prior work.",,,,,,"Trapp, Martin/0000-0003-1725-3381; Peharz, Robert/0000-0002-8644-9655",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303045,0
C,"Xu, WK; Matsuda, T",,"Chiappa, S; Calandra, R",,"Xu, Wenkai; Matsuda, Takeru",,,A Stein Goodness-of-fit Test for Directional Distributions,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"In many fields, data appears in the form of direction (unit vector) and usual statistical procedures are not applicable to such directional data. In this study, we propose nonparametric goodness-of-fit testing procedures for general directional distributions based on kernel Stein discrepancy. Our method is based on Stein's operator on spheres, which is derived by using Stokes' theorem. Notably, the proposed method is applicable to distributions with an intractable normalization constant, which commonly appear in directional statistics. Experimental results demonstrate that the proposed methods control type-I error well and have larger power than existing tests, including the test based on the maximum mean discrepancy.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303086,0
C,"Ambrogioni, L; Ebel, P; Hinne, M; Guclu, U; van Gerven, M; Maris, E",,"Chaudhuri, K; Sugiyama, M",,"Ambrogioni, L.; Ebel, P.; Hinne, M.; Guclu, U.; van Gerven, M.; Maris, E.",,,SpikeCaKe: Semi-Analytic Nonparametric Bayesian Inference for Spike-Spike Neuronal Connectivity,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"In this paper we introduce a semi-analytic variational framework for approximating the posterior of a Gaussian processes coupled through non-linear emission models. While the semi-analytic method can be applied to a large class of models, the present paper is devoted to the analysis of effective connectivity between biological spiking neurons. Estimating effective connectivity between spiking neurons from measured spike sequences is one of the main challenges of systems neuroscience. This semi-analytic method exploits the tractability of GP regression when the membrane potential is observed. The resulting posterior is then marginalized analytically in order to obtain the posterior of the response functions given the spike sequences alone. We validate our methods on both simulated data and real neuronal recordings.",,,,,"Gl, Umut/AAX-6105-2020; Hinne, Max/ABD-1486-2021","Gl, Umut/0000-0003-4753-159X; Hinne, Max/0000-0002-9279-6725",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,787,795,,,,,,,,,,,,,,,,WOS:000509687900082,0
C,"Chen, L; Zhang, MR; Karbasi, A",,"Chaudhuri, K; Sugiyama, M",,"Chen, Lin; Zhang, Mingrui; Karbasi, Amin",,,Projection-Free Bandit Convex Optimization,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"In this paper, we propose the first computationally efficient projection-free algorithm for bandit convex optimization (BCO) with a general convex constraint. We show that our algorithm achieves a sublinear regret of O(nT(4/5)) (where T is the horizon and n is the dimension) for any bounded convex functions with uniformly bounded gradients. We also evaluate the performance of our algorithm against baselines on both synthetic and real data sets for quadratic programming, portfolio selection and matrix completion problems.",,,,,"Chen, Lin/CAH-1961-2022","Chen, Lin/0000-0003-0349-6577",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902010,0
C,"Croce, F; Andriushchenko, M; Hein, M",,"Chaudhuri, K; Sugiyama, M",,"Croce, Francesco; Andriushchenko, Maksym; Hein, Matthias",,,Provable Robustness of ReLU networks via Maximization of Linear Regions,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,It has been shown that neural network classifiers are not robust. This raises concerns about their usage in safety-critical systems. We propose in this paper a regularization scheme for ReLU networks which provably improves the robustness of the classifier by maximizing the linear region of the classifier as well as the distance to the decision boundary. Our techniques allow even to find the minimal adversarial perturbation for a fraction of test points for large networks. In the experiments we show that our approach improves upon adversarial training both in terms of lower and upper bounds on the robustness and is comparable or better than the state of the art in terms of test error and robustness.,,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902011,0
C,"Dev, S; Phillips, JM",,"Chaudhuri, K; Sugiyama, M",,"Dev, Sunipa; Phillips, Jeff M.",,,Attenuating Bias in Word Vectors,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Word vector representations are well developed tools for various NLP and Machine Learning tasks and are known to retain significant semantic and syntactic structure of languages. But they are prone to carrying and amplifying bias which can perpetrate discrimination in various applications. In this work, we explore new simple ways to detect the most stereotypically gendered words in an embedding and remove the bias from them. We verify how names are masked carriers of gender bias and then use that as a tool to attenuate bias in embeddings. Further, we extend this property of names to show how names can be used to detect other types of bias in the embeddings such as bias based on race, ethnicity, and age.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,879,887,,,,,,,,,,,,,,,,WOS:000509687900091,0
C,"Genevay, A; Chizat, L; Bach, F; Cuturi, M; Peyre, G",,"Chaudhuri, K; Sugiyama, M",,"Genevay, Aude; Chizat, Lenaic; Bach, Francis; Cuturi, Marco; Peyre, Gabriel",,,Sample Complexity of Sinkhorn Divergences,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Optimal transport (OT) distances and maximum mean discrepancies (MMD) are now routinely used in machine learning to compare probability measures. Our focus in this paper is on Sinhhorn divergences (SDs), a regularized variant of OT distances that can interpolate, depending on the regularization strength s, between OT (s = 0) and MMD (s = oc). Although the tradeoff induced by that regularization is by now well understood computationally (OT, SDs and MMD require respectively 0(n3logn), 0(n2) and nz operations to compare two samples of size n), much less is known in terms of the sample complexity of SDs, namely bounding the gap between the evaluation of SDs on two densities vs. samples from these densities. That complexity for OT and MMD stand at two extremes: 0(1/nl/d) for OT in dimension d and 0(1//) for MMD. that for SDs has only been studied empirically. In this paper, we (i) derive a bound on the approximation error made with SDs when approximating OT as a function of the regularizer s, (ii) prove that the optimizers of regularized OT are bounded in a Sobolev (RKHS) ball independent of the two measures and (iii) reformulate SDs as a maximization problem in a RKHS to obtain a sample complexity in 1// (as in MMD), with a constant that depends however on s, making the bridge between OT and MMD complete.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901064,0
C,"Hanzely, F; Richtarik, P",,"Chaudhuri, K; Sugiyama, M",,"Hanzely, Filip; Richtarik, Peter",,,Accelerated Coordinate Descent with Arbitrary Sampling and Best Rates for Minibatches,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Accelerated coordinate descent is a widely popular optimization algorithm due to its efficiency on large-dimensional problems. It achieves state-of-the-art complexity on an important class of empirical risk minimization problems. In this paper we design and analyze an accelerated coordinate descent (ACD) method which in each iteration updates a random subset of coordinates according to an arbitrary but fixed probability law, which is a parameter of the method. While minibatch variants of ACD are more popular and relevant in practice, there is no importance sampling for ACD that outperforms the standard uniform minibatch sampling. Through insights enabled by our general analysis, we design new importance sampling for minibatch ACD which significantly outperforms previous state-of-the-art minibatch ACD in practice. We prove a rate that is at most O(root tau) times worse than the rate of minibatch ACD with uniform sampling, but can be O(n/T) times better, where tau is the minibatch size. Since in modern supervised learning training systems it is standard practice to choose tau << n, and often tau = O(1), our method can lead to dramatic speedups. Lastly, we obtain similar results for minibatch nonaccelerated CD as well, achieving improvements on previous best rates.",,,,,"Richtarik, Peter/O-5797-2018","Hanzely, Filip/0000-0003-0203-4004",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,304,312,,,,,,,,,,,,,,,,WOS:000509687900032,0
C,"Khanna, R; Kim, B; Ghosh, J; Koyejo, O",,"Chaudhuri, K; Sugiyama, M",,"Khanna, Rajiv; Kim, Been; Ghosh, Joydeep; Koyejo, Oluwasanmi",,,Interpreting Black Box Predictions using Fisher Kernels,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Research in both machine learning and psychology suggests that salient examples can help humans to interpret learning models. To this end, we take a novel look at black box interpretation of test predictions in terms of training examples. Our goal is to ask which training examples are most responsible for a given set of predictions? To answer this question, we make use of Fisher kernels as the defining feature embedding of each data point, combined with Sequential Bayesian Quadrature (SBQ) for efficient selection of examples. In contrast to prior work, our method is able to seamlessly handle any sized subset of test predictions in a principled way. We theoretically analyze our approach, providing novel convergence bounds for SBQ over discrete candidate atoms. Our approach recovers the application of influence functions for interpretability as a special case yielding novel insights from this connection. We also present applications of the proposed approach to three use cases: cleaning training data, fixing mislabeled examples and data summarization.",,,,,"Khanna, Rajiv/GPK-2566-2022","Khanna, Rajiv/0000-0003-1314-3126",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687903045,0
C,"Ko, S; Won, JH",,"Chaudhuri, K; Sugiyama, M",,"Ko, Seyoon; Won, Joong-Ho",,,Optimal Minimization of the Sum of Three Convex Functions with a Linear Operator,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We propose a class of optimal-rate primal-dual algorithms for minimization of the sum of three convex functions with a linear operator. We first establish the optimal convergence rates for solving the saddle-point reformulation of the problem only using first-order information under deterministic and stochastic settings, respectively. We then proceed to show that the proposed algorithm class achieves these rates. The studied algorithm class does not require matrix inversion and is simple to implement. To our knowledge, this is the first work to establish and attain the optimal rates for this class of problems with minimal assumptions. Numerical experiments show that our method outperforms state-of-the-art methods.",,,,,"Won, Joong-Ho/GPF-9286-2022","Ko, Seyoon/0000-0002-9897-1993",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901024,0
C,"Muthukumar, V; Ray, M; Sahai, A; Bartlett, PL",,"Chaudhuri, K; Sugiyama, M",,"Muthukumar, Vidya; Ray, Mitas; Sahai, Anant; Bartlett, Peter L.",,,Best of many worlds: Robust model selection for online supervised learning,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We introduce algorithms for online, full-information prediction that are computationally efficient and competitive with contextual tree experts of unknown complexity, in both probabilistic and adversarial settings. We incorporate a novel probabilistic framework of structural risk minimization into existing adaptive algorithms and show that we can robustly learn not only the presence of stochastic structure when it exists, but also the correct model order. When the stochastic data is actually realized from a predictor in the model class considered, we obtain regret bounds that are competitive with the regret of an optimal algorithm that possesses strong side information about both the true model order and whether the process generating the data is stochastic or adversarial. In cases where the data does not arise from any of the models, our algorithm selects models of higher order as we play more rounds. We display empirically improved overall prediction error over other adversarially robust approaches.",,,,,,"Muthukumar, Vidya/0000-0003-2786-7360",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687903024,0
C,"Shen, J; Awasthi, P; Li, P",,"Chaudhuri, K; Sugiyama, M",,"Shen, Jie; Awasthi, Pranjal; Li, Ping",,,Robust Matrix Completion from Quantized Observations,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"1-bit matrix completion refers to the problem of recovering a real-valued low-rank matrix from a small fraction of its sign patterns. In many real-world applications, however, the observations are not only highly quantized, but also grossly corrupted. In this work, we consider the noisy statistical model where each observed entry can be flipped with some probability after quantization. We propose a simple maximum likelihood estimator which is shown to be robust to the sign flipping noise. In particular, we prove an upper bound on the statistical error, showing that with overwhelming probability n = (poly(1 - 2 E[tau]) 2rd log d) samples are sufficient for accurate recovery, where r and d are the rank and dimension of the underlying matrix respectively, and tau is an element of [0, 1/2) is a random variable that parameterizes the sign flipping noise. Furthermore, a lower bound is established showing that the obtained sample complexity is near-optimal for prevalent statistical models. Finally, we substantiate our theoretical findings with a comprehensive study on synthetic and realistic data sets, and demonstrate the state-of-theart performance.",,,,,,"Shen, Jie/0000-0002-4440-7756",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,397,407,,,,,,,,,,,,,,,,WOS:000509687900042,0
C,"Stojanov, P; Gong, MM; Carbonell, JG; Zhang, K",,"Chaudhuri, K; Sugiyama, M",,"Stojanov, Petar; Gong, Mingming; Carbonell, Jaime G.; Zhang, Kun",,,Low-Dimensional Density Ratio Estimation for Covariate Shift Correction,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Covariate shift is a prevalent setting for supervised learning in the wild when the training and test data are drawn from different time periods, different but related domains, or via different sampling strategies. This paper addresses a transfer learning setting, with covariate shift between source and target domains. Most existing methods for correcting covariate shift exploit density ratios of the features to reweight the source-domain data, and when the features are high-dimensional, the estimated density ratios may suffer large estimation variances, leading to poor prediction performance. In this work, we investigate the dependence of covariate shift correction performance on the dimensionality of the features, and propose a correction method that finds a low-dimensional representation of the features, which takes into account feature relevant to the target Y, and exploits the density ratio of this representation for importance reweighting. We discuss the factors affecting the performance of our method and demonstrate its capabilities on both pseudo-real and real-world data.",,,,,"Stojanov, Petar/GQI-4462-2022","Stojanov, Petar/0000-0001-7815-776X",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687903052,0
C,"Huusari, R; Kadri, H; Capponi, C",,"Storkey, A; PerezCruz, F",,"Huusari, Riikka; Kadri, Hachem; Capponi, Cecile",,,Multi-view Metric Learning in Vector-valued Kernel Spaces,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"We consider the problem of metric learning for multi-view data and present a novel method for learning within-view as well as between-view metrics in vector-valued kernel spaces, as a way to capture multi-modal structure of the data. We formulate two convex optimization problems to jointly learn the metric and the classifier or regressor in kernel feature spaces. An iterative three-step multi-view metric learning algorithm is derived from the optimization problems. In order to scale the computation to large training sets, a blockwise Nystrom approximation of the multi-view kernel matrix is introduced. We justify our approach theoretically and experimentally, and show its performance on real-world datasets against relevant state-of-the-art methods.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300044,0
C,"Ito, S; Hatano, D; Sumita, H; Yabe, A; Fukunaga, T; Kakimura, N; Kawarabayashi, K",,"Storkey, A; PerezCruz, F",,"Ito, Shinji; Hatano, Daisuke; Sumita, Hanna; Yabe, Akihiro; Fukunaga, Takuro; Kakimura, Naonori; Kawarabayashi, Ken-ichi",,,Online Regression with Partial Information: Generalization and Linear Projection,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"We investigate an online regression problem in which the learner makes predictions sequentially while only the limited information on features is observable. In this paper, we propose a general setting for the limitation of the available information, where the observed information is determined by a function chosen from a given set of observation functions. Our problem setting is a generalization of the online sparse linear regression problem, which has been actively studied. For our general problem, we present an algorithm by combining multi-armed bandit algorithms and online learning methods. This algorithm admits a sublinear regret bound when the number of observation functions is constant. We also show that the dependency on the number of observation functions is inevitable unless additional assumptions are adopted. To mitigate this inefficiency, we focus on a special case of practical importance, in which the observed information is expressed through linear combinations of the original features. We propose efficient algorithms for this special case. Finally, we also demonstrate the efficiency of the proposed algorithms by simulation studies using both artificial and real data.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300167,0
C,"Viinikka, J; Eggeling, R; Koivisto, M",,"Storkey, A; PerezCruz, F",,"Viinikka, Jussi; Eggeling, Ralf; Koivisto, Mikko",,,Intersection-Validation: A Method for Evaluating Structure Learning without Ground Truth,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"To compare learning algorithms that differ by the adopted statistical paradigm, model class, or search heuristic, it is common to evaluate the performance on training data of varying size. Measuring the performance is straightforward if the data are generated from a known model, the ground truth. However, when the study concerns real-world data, the current methodology is limited to estimating predictive performance, typically by cross-validation. This work introduces a method to compare algorithms' ability to learn the model structure, assuming no ground truth is given. The idea is to identify a partial structure on which the algorithms agree, and measure the performance in relation to that structure on subsamples of the data. The method is instantiated to structure learning in Bayesian networks, measuring the performance by the structural Hamming distance. It is tested using benchmark ground truth networks and algorithms that maximize various scoring functions. The results show that the method can produce evaluation outcomes that are close to those one would obtain if the ground truth was available.",,,,,"Eggeling, Ralf/P-3403-2019","Eggeling, Ralf/0000-0002-3583-1029",,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300164,0
C,"Wu, C; Ioannidis, S; Sznaier, M; Li, XY; Kaeli, D; Dy, JG",,"Storkey, A; PerezCruz, F",,"Wu, Chieh; Ioannidis, Stratis; Sznaier, Mario; Li, Xiangyu; Kaeli, David; Dy, Jennifer G.",,,Iterative Spectral Method for Alternative Clustering,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Given a dataset and an existing clustering as input, alternative clustering aims to find an alternative partition. One of the state-of-the-art approaches is Kernel Dimension Alternative Clustering (KDAC). We propose a novel Iterative Spectral Method (ISM) that greatly improves the scalability of KDAC. Our algorithm is intuitive, relies on easily implementable spectral decompositions, and comes with theoretical guarantees. Its computation time improves upon existing implementations of KDAC by as much as 5 orders of magnitude.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300013,0
C,"Zhang, X; Wang, LX; Gu, QQ",,"Storkey, A; PerezCruz, F",,"Zhang, Xiao; Wang, Lingxiao; Gu, Quanquan",,,A Unified Framework for Nonconvex Low-Rank plus Sparse Matrix Recovery,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"We propose a unified framework to solve general low-rank plus sparse matrix recovery problems based on matrix factorization, which covers a broad family of objective functions satisfying the restricted strong convexity and smoothness conditions. Based on projected gradient descent and the double thresholding operator, our proposed generic algorithm is guaranteed to converge to the unknown low-rank and sparse matrices at a locally linear rate, while matching the best-known robustness guarantee (i.e., tolerance for sparsity). At the core of our theory is a novel structural Lipschitz gradient condition for low-rank plus sparse matrices, which is essential for proving the linear convergence rate of our algorithm, and we believe is of independent interest to prove fast rates for general superposition-structured models. We illustrate the application of our framework through two concrete examples: robust matrix sensing and robust PCA. Empirical experiments corroborate our theory.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300115,0
C,"Hernandez-Lobato, D; Hernandez-Lobato, JM",,"Gretton, A; Robert, CC",,"Hernandez-Lobato, Daniel; Hernandez-Lobato, Jose Miguel",,,Scalable Gaussian Process Classification via Expectation Propagation,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Variational methods have been recently considered for scaling the training process of Gaussian process classifiers to large datasets. As an alternative, we describe here how to train these classifiers efficiently using expectation propagation (EP). The proposed EP method allows to train Gaussian process classifiers on very large datasets, with millions of instances, that were out of the reach of previous implementations of EP. More precisely, it can be used for (i) training in a distributed fashion where the data instances are sent to different nodes in which the required computations are carried out, and for (ii) maximizing an estimate of the marginal likelihood using a stochastic approximation of the gradient. Several experiments involving large datasets show that the method described is competitive with the variational approach.",,,,,"Hernandez-Lobato, Jose Miguel/F-2056-2016","Hernandez-Lobato, Jose Miguel/0000-0001-7610-949X",,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,168,176,,,,,,,,,,,,,,,,WOS:000508662100019,0
C,"Oliva, JB; Dubey, A; Wilson, AG; Poczos, B; Schneider, J; Xing, EP",,"Gretton, A; Robert, CC",,"Oliva, Junier B.; Dubey, Avinava; Wilson, Andrew G.; Poczos, Barnabas; Schneider, Jeff; Xing, Eric P.",,,Bayesian Nonparametric Kernel-Learning,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Kernel methods are ubiquitous tools in machine learning. However, there is often little reason for the common practice of selecting a kernel a priori. Even if a universal approximating kernel is selected, the quality of the finite sample estimator may be greatly affected by the choice of kernel. Furthermore, when directly applying kernel methods, one typically needs to compute a NxN Gram matrix of pairwise kernel evaluations to work with a dataset of N instances. The computation of this Gram matrix precludes the direct application of kernel methods on large datasets, and makes kernel learning especially difficult. In this paper we introduce Bayesian nonparmetric kernel-learning (BaNK), a generic, data-driven framework for scalable learning of kernels. BaNK places a nonparametric prior on the spectral distribution of random frequencies allowing it to both learn kernels and scale to large datasets. We show that this framework can be used for large scale regression and classification tasks. Furthermore, we show that BaNK outperforms several other scalable approaches for kernel learning on a variety of real world datasets.",,,,,,"Oliva, Junier/0000-0002-2601-5652",,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,1078,1086,,,,,,,,,,,,,,,,WOS:000508662100117,0
C,"Sasaki, H; Niu, G; Sugiyama, M",,"Gretton, A; Robert, CC",,"Sasaki, Hiroaki; Niu, Gang; Sugiyama, Masashi",,,Non-Gaussian Component Analysis with Log-Density Gradient Estimation,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Non-Gaussian component analysis (NGCA) is aimed at identifying a linear subspace such that the projected data follows a non-Gaussian distribution. In this paper, we propose a novel NGCA algorithm based on log-density gradient estimation. Unlike existing methods, the proposed NGCA algorithm identifies the linear subspace by using the eigenvalue decomposition without any iterative procedures, and thus is computationally reasonable. Furthermore, through theoretical analysis, we prove that the identified subspace converges to the true subspace at the optimal parametric rate. Finally, the practical performance of the proposed algorithm is demonstrated on both artificial and benchmark datasets.",,,,,"Sugiyama, Masashi/AEO-1176-2022; Sasaki, Hiroaki/GXG-5024-2022","Sugiyama, Masashi/0000-0001-6658-6743; ",,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,1177,1185,,,,,,,,,,,,,,,,WOS:000508662100128,0
C,"Weller, A; Domke, J",,"Gretton, A; Robert, CC",,"Weller, Adrian; Domke, Justin",,,Clamping Improves TRW and Mean Field Approximations,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"We examine the effect of clamping variables for approximate inference in undirected graphical models with pairwise relationships and discrete variables. For any number of variable labels, we demonstrate that clamping and summing approximate sub-partition functions can lead only to a decrease in the partition function estimate for TRW, and an increase for the naive mean field method, in each case guaranteeing an improvement in the approximation and bound. We next focus on binary variables, add the Bethe approximation to consideration and examine ways to choose good variables to clamp, introducing new methods. We show the importance of identifying highly frustrated cycles, and of checking the singleton entropy of a variable. We explore the value of our methods by empirical analysis and draw lessons to guide practitioners.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,38,46,,,,,,,,,,,,,,,,WOS:000508662100005,0
C,"Zhang, DJ; Balzano, L",,"Gretton, A; Robert, CC",,"Zhang, Dejiao; Balzano, Laura",,,Global Convergence of a Grassmannian Gradient Descent Algorithm for Subspace Estimation,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"It has been observed in a variety of contexts that gradient descent methods have great success in solving low-rank matrix factorization problems, despite the relevant problem formulation being non-convex. We tackle a particular instance of this scenario, where we seek the d-dimensional subspace spanned by a streaming data matrix. We apply the natural first order incremental gradient descent method, constraining the gradient method to the Grassmannian. In this paper, we propose an adaptive step size scheme that is greedy for the noiseless case, that maximizes the improvement of our metric of convergence at each data index t, and yields an expected improvement for the noisy case. We show that, with noise-free data, this method converges from any random initialization to the global minimum of the problem. For noisy data, we provide the expected convergence rate of the proposed algorithm per iteration.",,,,,", lantong/AAR-7206-2020","Balzano, Laura/0000-0003-2914-123X",,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,1460,1468,,,,,,,,,,,,,,,,WOS:000508662100158,0
C,"Kim, YD; Choi, S",,"Kaski, S; Corander, J",,"Kim, Yong-Deok; Choi, Seungjin",,,Scalable Variational Bayesian Matrix Factorization with Side Information,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"Bayesian matrix factorization (BMF) is a popular method for collaborative prediction, because of its robustness to overfitting as well as of being free from cross-validation for fine tuning of regularization parameters. In practice, however, due to its cubic time complexity with respect to the rank of factor matrices, existing variational inference algorithms for BMF are not well suited to web-scale datasets where billions of ratings provided by millions of users are available. The time complexity even increases when the side information, such as user binary implicit feedback or item content information, is incorporated into variational Bayesian matrix factorization (VBMF). For instance, a state of the arts in VBMF with side information, is to place Gaussian priors on user and item factor matrices, where mean of each prior is regressed on the corresponding side information. Since this approach introduces additional cubic time complexity with respect to the size of feature vectors, the use of rich side information in a form of high-dimensional feature vector is prohibited. In this paper, we present a scalable inference for VBMF with side information, the complexity of which is linear in the rank K of factor matrices. Moreover, the algorithm can be easily parallelized on multi-core systems. Experiments on large-scale datasets demonstrate the useful behavior of our algorithm such as scalability, fast learning, and prediction accuracy.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,493,502,,,,,,,,,,,,,,,,WOS:000508355800055,0
C,"Min, MR; Ning, X; Cheng, C; Gerstein, M",,"Kaski, S; Corander, J",,"Min, Martin Renqiang; Ning, Xia; Cheng, Chao; Gerstein, Mark",,,Interpretable Sparse High-Order Boltzmann Machines,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"Fully-observable high-order Boltzmann Machines are capable of identifying explicit high-order feature interactions theoretically. However, they have never been used in practice due to their prohibitively high computational cost for inference and learning. In this paper, we propose an efficient approach for learning a fully-observable high-order Boltzmann Machine based on sparse learning and contrastive divergence, resulting in an interpretable Sparse High-order Boltzmann Machine, denoted as SHBM. Experimental results on synthetic datasets and a real dataset demonstrate that SHBM can produce higher pseudo-log-likelihood and better reconstructions on test data than the state-of-the-art methods. In addition, we apply SHBM to a challenging bioinformatics problem of discovering complex Transcription Factor interactions. Compared to conventional Boltzmann Machine and directed Bayesian Network, SHBM can identify much more biologically meaningful interactions that are supported by recent biological studies. To the best of our knowledge, SHBM is the first working Boltzmann Machine with explicit high-order feature interactions applied to real-world problems.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,614,622,,,,,,,,,,,,,,,,WOS:000508355800068,0
C,"Zhong, LW; Kwok, JT",,"Kaski, S; Corander, J",,"Zhong, Leon Wenliang; Kwok, James T.",,,Accelerated Stochastic Gradient Method for Composite Regularization,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"Regularized risk minimization often involves nonsmooth optimization. This can be particularly challenging when the regularizer is a sum of simpler regularizers, as in the overlapping group lasso. Very recently, this is alleviated by using the proximal average, in which an implicitly nonsmooth function is employed to approximate the composite regularizer. In this paper, we propose a novel extension with accelerated gradient method for stochastic optimization. On both general convex and strongly convex problems, the resultant approximation errors reduce at a faster rate than methods based on stochastic smoothing and ADMM. This is also verified experimentally on a number of synthetic and real-world data sets.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,1086,1094,,,,,,,,,,,,,,,,WOS:000508355800120,0
C,"Al-Tahan, H; Mohsenzadeh, Y",,"Banerjee, A; Fukumizu, K",,"Al-Tahan, Haider; Mohsenzadeh, Yalda",,,CLAR: Contrastive Learning of Auditory Representations,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Learning rich visual representations using contrastive self-supervised learning has been extremely successful. However, it is still a major question whether we could use a similar approach to learn superior auditory representations. In this paper, we expand on prior work (SimCLR) to learn better auditory representations. We (1) introduce various data augmentations suitable for auditory data and evaluate their impact on predictive performance, (2) show that training with time-frequency audio features substantially improves the quality of the learned representations compared to raw signals, and (3) demonstrate that training with both supervised and contrastive losses simultaneously improves the learned representations compared to self-supervised pre-training followed by supervised fine-tuning. We illustrate that by combining all these methods and with substantially less labeled data, our framework (CLAR) achieves significant improvement on prediction performance compared to supervised approach. Moreover, compared to self-supervised approach, our framework converges faster with significantly better representations(1).",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803008,0
C,"Arief, M; Huang, ZY; Kumar, GKS; Bai, YL; He, SY; Ding, WH; Lam, HR; Zhao, D",,"Banerjee, A; Fukumizu, K",,"Arief, Mansur; Huang, Zhiyuan; Kumar, Guru K. S.; Bai, Yuanlu; He, Shengyi; Ding, Wenhao; Lam, Henry; Zhao, Ding",,,Deep Probabilistic Accelerated Evaluation: A Robust Certifiable Rare-Event Simulation Methodology for Black-Box Safety-Critical Systems,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Evaluating the reliability of intelligent physical systems against rare safety-critical events poses a huge testing burden for real-world applications. Simulation provides a useful platform to evaluate the extremal risks of these systems before their deployments. Importance Sampling (IS), while proven to be powerful for rare-event simulation, faces challenges in handling these learning-based systems due to their black-box nature that fundamentally undermines its efficiency guarantee, which can lead to under-estimation without diagnostically detected. We propose a framework called Deep Probabilistic Accelerated Evaluation (Deep-PrAE) to design statistically guaranteed IS, by converting black-box samplers that are versatile but could lack guarantees, into one with what we call a relaxed efficiency certificate that allows accurate estimation of bounds on the safety-critical event probability. We present the theory of Deep-PrAE that combines the dominating point concept with rare-event set learning via deep neural network classifiers, and demonstrate its effectiveness in numerical examples including the safety-testing of an intelligent driving algorithm.",,,,,"Arief, Mansur Maturidi/GNP-5662-2022","Arief, Mansur Maturidi/0000-0002-4636-3451",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,595,+,,,,,,,,,,,,,,,,WOS:000659893800067,0
C,"Arnold, SMR; Iqbal, S; Sha, F",,"Banerjee, A; Fukumizu, K",,"Arnold, Sebastien M. R.; Iqbal, Shariq; Sha, Fei",,,When MAML Can Adapt Fast and How to Assist When It Cannot,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Model-Agnostic Meta-Learning (MAML) and its variants have achieved success in meta-learning tasks on many datasets and settings. Nonetheless, we have just started to understand and analyze how they are able to adapt fast to new tasks. In this work, we contribute by conducting a series of empirical and theoretical studies, and discover several interesting, previously unknown properties of the algorithm. First, we find MAML adapts better with a deep architecture even if the tasks need only a shallow one. Secondly, linear layers can be added to the output layers of a shallower model to increase the depth without altering the modelling capacity, leading to improved performance in adaptation. Alternatively, an external and separate neural network meta-optimizer can also be used to transform the gradient updates of a smaller model so as to obtain improved performances in adaptation. Drawing from these evidences, we theorize that for a deep neural network to meta-learn well, the upper layers must transform the gradients of the bottom layers as if the upper layers were an external meta-optimizer, operating on a smaller network that is composed of the bottom layers.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,244,+,,,,,,,,,,,,,,,,WOS:000659893800028,0
C,"Ausset, G; Clemencon, S; Portier, F",,"Banerjee, A; Fukumizu, K",,"Ausset, Guillaume; Clemencon, Stephan; Portier, Francois",,,Nearest Neighbour Based Estimates of Gradients: Sharp Nonasymptotic Bounds and Applications,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Motivated by a wide variety of applications, ranging from stochastic optimization to dimension reduction through variable selection, the problem of estimating gradients accurately is of crucial importance in statistics and learning theory. We consider here the classical regression setup, where a real valued square integrable r.v. Y is to be predicted upon observing a (possibly high dimensional) random vector X by means of a predictive function f (X) as accurately as possible in the mean-squared sense and study a nearest-neighbourbased pointwise estimate of the gradient of the optimal predictive function, the regression function m(x) = E[Y vertical bar X = x]. Under classical smoothness conditions combined with the assumption that the tails of Y - m(X) are sub-Gaussian, we prove nonasymptotic bounds improving upon those obtained for alternative estimation methods. Beyond the novel theoretical results established, several illustrative numerical experiments have been carried out. The latter provide strong empirical evidence that the estimation method proposed here performs very well for various statistical problems involving gradient estimation, namely dimensionality reduction, stochastic gradient descent optimization and disentanglement quantification.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,532,+,,,,,,,,,,,,,,,,WOS:000659893800060,0
C,"Baratin, A; George, T; Laurent, C; Hjelm, RD; Lajoie, G; Vincent, P; Lacoste-Julien, S",,"Banerjee, A; Fukumizu, K",,"Baratin, Aristide; George, Thomas; Laurent, Cesar; Hjelm, R. Devon; Lajoie, Guillaume; Vincent, Pascal; Lacoste-Julien, Simon",,,Implicit Regularization via Neural Feature Alignment,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We approach the problem of implicit regularization in deep learning from a geometrical viewpoint. We highlight a regularization effect induced by a dynamical alignment of the neural tangent features introduced by Jacot et al. (2018), along a small number of task-relevant directions. This can be interpreted as a combined mechanism of feature selection and compression. By extrapolating a new analysis of Rademacher complexity bounds for linear models, we motivate and study a heuristic complexity measure that captures this phenomenon, in terms of sequences of tangent kernel classes along optimization paths. The code for our experiments is available as https://github.com/tfjgeorge/ntk_alignment.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802072,0
C,"Bhandari, J; Russo, D",,"Banerjee, A; Fukumizu, K",,"Bhandari, Jalaj; Russo, Daniel",,,On the Linear Convergence of Policy Gradient Methods for Finite MDPs,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We revisit the finite time analysis of policy gradient methods in the one of the simplest settings: finite state and action MDPs with a policy class consisting of all stochastic policies and with exact gradient evaluations. There has been some recent work viewing this setting as an instance of smooth non-linear optimization problems and showing sub-linear convergence rates with small step-sizes. Here, we take a different perspective based on connections with policy iteration and show that many variants of policy gradient methods succeed with large step-sizes and attain a linear rate of convergence.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802085,0
C,"Bhattacharya, R; Nagarajan, T; Malinsky, D; Shpitser, I",,"Banerjee, A; Fukumizu, K",,"Bhattacharya, Rohit; Nagarajan, Tushar; Malinsky, Daniel; Shpitser, Ilya",,,Differentiable Causal Discovery Under Unmeasured Confounding,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"The data drawn from biological, economic, and social systems are often confounded due to the presence of unmeasured variables. Prior work in causal discovery has focused on discrete search procedures for selecting acyclic directed mixed graphs (ADMGs), specifically ancestral ADMGs, that encode ordinary conditional independence constraints among the observed variables of the system. However, confounded systems also exhibit more general equality restrictions that cannot be represented via these graphs, placing a limit on the kinds of structures that can be learned using ancestral ADMGs. In this work, we derive differentiable algebraic constraints that fully characterize the space of ancestral ADMGs, as well as more general classes of ADMGs, arid ADMGs and bow-free ADMGs, that capture all equality restrictions on the observed variables. We use these constraints to cast causal discovery as a continuous optimization problem and design differentiable procedures to find the best fitting ADMG when the data comes from a confounded linear system of equations with correlated errors. We demonstrate the efficacy of our method through simulations and application to a protein expression dataset. Code implementing our methods is open-source and publicly available at https: //gitlab.com/rbhatta8/dcd and will be incorporated into the Ananke package.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802077,0
C,"Bouche, D; Clausel, M; Roueff, F; d'Alche-Buc, F",,"Banerjee, A; Fukumizu, K",,"Bouche, Dimitri; Clausel, Marianne; Roueff, Francois; d'Alche-Buc, Florence",,,Nonlinear Functional Output Regression: A Dictionary Approach,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"To address functional-output regression, we introduce projection learning (PL), a novel dictionary-based approach that learns to predict a function that is expanded on a dictionary while minimizing an empirical risk based on a functional loss. PL makes it possible to use non orthogonal dictionaries and can then be combined with dictionary learning; it is thus much more flexible than expansion-based approaches relying on vectorial losses. This general method is instantiated with reproducing kernel Hilbert spaces of vector-valued functions as kernel-based projection learning (KPL). For the functional square loss, two closed-form estimators are proposed, one for fully observed output functions and the other for partially observed ones. Both are backed theoretically by an excess risk analysis. Then, in the more general setting of integral losses based on differentiable ground losses, KPL is implemented using first-order optimization for both fully and partially observed output functions. Eventually, several robustness aspects of the proposed algorithms are highlighted on a toy dataset; and a study on two real datasets shows that they are competitive compared to other nonlinear approaches. Notably, using the square loss and a learnt dictionary, KPL enjoys a particularily attractive trade-off between computational cost and performances.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,235,+,,,,,,,,,,,,,,,,WOS:000659893800027,0
C,"Chen, TY; Guo, ZY; Sun, YJ; Yin, WT",,"Banerjee, A; Fukumizu, K",,"Chen, Tianyi; Guo, Ziye; Sun, Yuejiao; Yin, Wotao",,,CADA: Communication-Adaptive Distributed Adam,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Stochastic gradient descent (SGD) has taken the stage as the primary workhorse for large-scale machine learning. It is often used with its adaptive variants such as AdaGrad, Adam, and AMSGrad. This paper proposes an adaptive stochastic gradient descent method for distributed machine learning, which can be viewed as the communication-adaptive counterpart of the celebrated Adam method - justifying its name CADA. The key components of CADA are a set of new rules tailored for adaptive stochastic gradients that can be implemented to save communication upload. The new algorithms adaptively reuse the stale Adam gradients, thus saving communication, and still have convergence rates comparable to original Adam. In numerical experiments, CADA achieves impressive empirical performance in terms of total communication round reduction.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,613,+,,,,,,,,,,,,,,,,WOS:000659893800069,0
C,"Dubey, A",,"Banerjee, A; Fukumizu, K",,"Dubey, Abhimanyu",,,No-Regret Algorithms for Private Gaussian Process Bandit Optimization,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"The widespread proliferation of data-driven decision-making has ushered in a recent interest in the design of privacy-preserving algorithms. In this paper, we consider the ubiquitous problem of gaussian process (GP) bandit optimization from the lens of privacy-preserving statistics. We propose a solution for differentially private GP bandit optimization that combines a uniform kernel approximator with random perturbations, providing a generic framework to create differentially-private (DP) Gaussian process bandit algorithms. For two specific DP settings joint and local differential privacy, we provide algorithms based on efficient quadrature Fourier feature approximators, that are computationally efficient and provably no-regret for popular stationary kernel functions. Our algorithms maintain differential privacy throughout the optimization procedure and critically do not rely explicitly on the sample path for prediction, making the parameters straightforward to release as well.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802049,0
C,"Jethani, N; Sudarshan, M; Aphinyanaphongs, Y; Ranganath, R",,"Banerjee, A; Fukumizu, K",,"Jethani, Neil; Sudarshan, Mukund; Aphinyanaphongs, Yindalon; Ranganath, Rajesh",,,Have We Learned to Explain?: How Interpretability Methods Can Learn to Encode Predictions in their Interpretations,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"While the need for interpretable machine learning has been established, many common approaches are slow, lack fidelity, or hard to evaluate. Amortized explanation methods reduce the cost of providing interpretations by learning a global selector model that returns feature importances for a single instance of data. The selector model is trained to optimize the fidelity of the interpretations, as evaluated by a predictor model for the target. Popular methods learn the selector and predictor model in concert, which we show allows predictions to be encoded within interpretations. We introduce EVAL-X as a method to quantitatively evaluate interpretations and REAL-X as an amortized explanation method, which learn a predictor model that approximates the true data generating distribution given any subset of the input. We show EVAL-X can detect when predictions are encoded in interpretations and show the advantages of REAL-X through quantitative and radiologist evaluation.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801076,0
C,"Lemhadri, I; Ruan, F; Tibshirani, R",,"Banerjee, A; Fukumizu, K",,"Lemhadri, Ismael; Ruan, Feng; Tibshirani, Robert",,,LassoNet: Neural Networks with Feature Sparsity,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Much work has been done recently to make neural networks more interpretable, and one approach is to arrange for the network to use only a subset of the available features. In linear models, Lasso (or l(1)-regularized) regression assigns zero weights to the most irrelevant or redundant features, and is widely used in data science. However the Lasso only applies to linear models. Here we introduce LassoNet, a neural network framework with global feature selection. Our approach achieves feature sparsity by allowing a feature to participate in a hidden unit only if its linear representative is active. Unlike other approaches to feature selection for neural nets, our method uses a modified objective function with constraints, and so integrates feature selection with the parameter learning directly. As a result, it delivers an entire regularization path of solutions with a range of feature sparsity. In experiments with real and simulated data, LassoNet significantly outperforms state-of-the-art methods for feature selection and regression. The LassoNet method uses projected proximal gradient descent, and generalizes directly to deep networks. It can be implemented by adding just a few lines of code to a standard neural network.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,10,+,,,,,,,,,,,36092461,,,,,WOS:000659893800002,0
C,"Ohnishi, Y; Honorio, J",,"Banerjee, A; Fukumizu, K",,"Ohnishi, Yuki; Honorio, Jean",,,Novel Change of Measure Inequalities with Applications to PAC-Bayesian Bounds and Monte Carlo Estimation,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We discuss several novel change of measure inequalities for two families of divergences: f-divergences and alpha-divergences. We show how the variational representation for f-divergences leads to novel change of measure inequalities. We also present a multiplicative change of measure inequality for alpha-divergences and a generalized version of Hammersley-Chapman-Robbins inequality. Finally, we present several applications of our change of measure inequalities, including PAC-Bayesian bounds for various classes of losses and non-asymptotic intervals for Monte Carlo estimates.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802010,0
C,"Peng, SD; Ning, Y",,"Banerjee, A; Fukumizu, K",,"Peng, Sida; Ning, Yang",,,Regression Discontinuity Design under Self-selection,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Regression Discontinuity (RD) design is commonly used to estimate the causal effect of a policy. Existing RD relies on the continuity assumption of potential outcomes. However, self selection leads to different distributions of covariates on two sides of the policy intervention, which violates this assumption. The standard RD estimators are no longer applicable in such setting. We show that the direct causal effect can still be recovered under a class of weighted average treatment effects. We propose a set of estimators through a weighted local linear regression framework and prove the consistency and asymptotic normality of the estimators. We apply our method to a novel data set from Microsoft Bing on Generalized Second Price (GSP) auction and show that by placing the advertisement on the second ranked position can increase the click-ability by 1.91%.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,118,126,,,,,,,,,,,,,,,,WOS:000659893800014,0
C,"Tsaknakis, I; Hong, MY",,"Banerjee, A; Fukumizu, K",,"Tsaknakis, Ioannis; Hong, Mingyi",,,Finding First-Order Nash Equilibria of Zero-Sum Games with the Regularized Nikaido-Isoda Function,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Efficiently finding First-order Nash Equilibria (FNE) in zero-sum games can be challenging, even in a two-player setting. This work proposes an algorithm for finding the FNEs of a two-player zero-sum game, in which the local cost functions can be non-convex, and the players only have access to local stochastic gradients. The proposed approach is based on reformulating the problem of interest as minimizing the Regularized Nikaido-Isoda (RNI) function. We show that the global minima of the RNI correspond to the set of FNEs, and that for certain classes of non-convex games the RNI minimization problem becomes convex. Moreover, we introduce a first-order (stochastic) optimization method, and establish its convergence to a neighborhood of a stationary solution of the RNI objective. The key in the analysis is to properly control the bias between the local stochastic gradient and the true one. Although the RNI function has been used in analyzing convex games, to our knowledge, this is the first time that the properties of the RNI formulation have been exploited to find FNEs for non-convex games in a stochastic setting.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801046,0
C,"Wen, M; Bastani, O; Topcu, U",,"Banerjee, A; Fukumizu, K",,"Wen, Min; Bastani, Osbert; Topcu, Ufuk",,,Algorithms for Fairness in Sequential Decision Making,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"It has recently been shown that if feedback effects of decisions are ignored, then imposing fairness constraints such as demographic parity or equality of opportunity can actually exacerbate unfairness. We propose to address this challenge by modeling feedback effects as Markov decision processes (MDPs). First, we propose analogs of fairness properties for the MDP setting. Second, we propose algorithms for learning fair decision-making policies for MDPs. Finally, we demonstrate the need to account for dynamical effects using simulations on a loan applicant MDP.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801041,0
C,"Xu, RT; Chen, L; Karbasi, A",,"Banerjee, A; Fukumizu, K",,"Xu, Ruitu; Chen, Lin; Karbasi, Amin",,,Meta Learning in the Continuous Time Limit,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"In this paper, we establish the ordinary differential equation (ode) that underlies the training dynamics of Model-Agnostic Meta-Learning (maml). Our continuous-time limit view of the process eliminates the influence of the manually chosen step size of gradient descent and includes the existing gradient descent training algorithm as a special case that results from a specific discretization. We show that the maml ode enjoys a linear convergence rate to an approximate stationary point of the maml loss function for strongly convex task losses, even when the corresponding maml loss is non-convex. Moreover, through the analysis of the maml ode, we propose a new bi-maml training algorithm that reduces the computational burden associated with existing maml training methods, and empirical experiments are performed to showcase the superiority of our proposed methods in the rate of convergence with respect to the vanilla maml algorithm.",,,,,"Chen, Lin/CAH-1961-2022","Chen, Lin/0000-0003-0349-6577",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803066,0
C,"Adam, V; Eleftheriadis, S; Durrande, N; Artemev, A; Hensman, J",,"Chiappa, S; Calandra, R",,"Adam, Vincent; Eleftheriadis, Stefanos; Durrande, Nicolas; Artemev, Artem; Hensman, James",,,Doubly Sparse Variational Gaussian Processes,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"The use of Gaussian process models is typically limited to datasets with a few tens of thousands of observations due to their complexity and memory footprint. The two most commonly used methods to overcome this limitation are 1) the variational sparse approximation which relies on inducing points and 2) the state-space equivalent formulation of Gaussian processes which can be seen as exploiting sonic sparsity in the precision matrix. We propose to take the best of both worlds: we show that the inducing point framework is still valid for state space models and that, it can bring further computational and memory savings. Furthermore, we provide the natural gradient formulation for the proposed variational parameierisation. Finally, this work makes it possible to use the state-space formulation inside deep Gaussian process models as illustrated in one of the experiments.",,,,,,"Adam, Vincent/0000-0002-9953-3434",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2874,2883,,,,,,,,,,,,,,,,WOS:000559931300004,0
C,"Balle, B; Barthe, G; Gaboardi, M; Hsu, J; Sato, T",,"Chiappa, S; Calandra, R",,"Balle, Borja; Barthe, Gilles; Gaboardi, Marco; Hsu, Justin; Sato, Tetsuya",,,Hypothesis Testing Interpretations and Renyi Differential Privacy,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Differential privacy is a de facto standard in data privacy, with applications in the public and private sectors. One way of explaining differential privacy that is particularly appealing to statistician and social scientists is through its statistical hypothesis testing interpretation. Informally, one cannot effectively test whether a specific individual has contributed her data by observing the output of a private mechanism-no test can have both high significance and high power. In this paper, we identify some conditions under which a privacy definition given in terms of a statistical divergence satisfies a similar interpretation. These conditions are useful to analyze the distinguishing power of divergences and we use them to study the hypothesis testing interpretation of relaxations of differential privacy based on Renyi divergence. Our analysis also results in an improved conversion rule between these definitions and differential privacy.",,,,,,"Sato, Tetsuya/0000-0001-9895-9209",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2496,2505,,,,,,,,,,,,,,,,WOS:000559931300023,0
C,"Di Benedetto, G; Caron, F; Teh, YW",,"Chiappa, S; Calandra, R",,"Di Benedetto, Giuseppe; Caron, Francois; Teh, Yee Whye",,,Non-exchangeable feature allocation models with sublinear growth of the feature sizes,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Feature allocation models are popular models used in different applications such as unsupervised learning or network modeling. In particular, the Indian buffet process is a flexible and simple one-parameter feature allocation model where the number of features grows unboundedly with the number of objects. The Indian buffet process, like most feature allocation models, satisfies a symmetry property of exchangeability: the distribution is invariant under permutation of the objects. While this property is desirable in some cases, it has some strong implications. Importantly, the number of objects sharing a particular feature grows linearly with the number of objects. In this article, we describe a class of non-exchangeable feature allocation models where the number of objects sharing a given feature grows sublinearly, where the rate can be controlled by a tuning parameter. We derive the asymptotic properties of the model, and show that such model provides a better fit and better predictive performances on various datasets.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3208,3217,,,,,,,,,,,,,,,,WOS:000559931300030,0
C,"Diakonikolas, J; Carderera, A; Pokutta, S",,"Chiappa, S; Calandra, R",,"Diakonikolas, Jelena; Carderera, Alejandro; Pokutta, Sebastian",,,Locally Accelerated Conditional Gradients,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Conditional gradients constitute a class of projection-free first-order algorithms for smooth convex optimization. As such, they are frequently used in solving smooth convex optimization problems over polytopes, for which the computational cost of projections is prohibitive. However, they do not enjoy the optimal convergence rates achieved by projection-based accelerated methods; moreover, achieving such globally-accelerated rates is information-theoretically impossible. To address this issue, we present Locally Accelerated Conditional Gradients - an algorithmic framework that couples accelerated steps with conditional gradient steps to achieve local acceleration on smooth strongly convex problems. Our approach does not require projections onto the feasible set, but only on (typically low-dimensional) simplices, thus keeping the computational cost of projections at bay. Further, it achieves optimal accelerated local convergence. Our theoretical results are supported by numerical experiments, which demonstrate significant speedups over state of the art methods in both per-iteration progress and wall-clock time.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1737,1746,,,,,,,,,,,,,,,,WOS:000559931300074,0
C,"Ergen, T; Pilanci, M",,"Chiappa, S; Calandra, R",,"Ergen, Tolga; Pilanci, Mert",,,Convex Geometry of Two-Layer ReLU Networks: Implicit Autoencoding and Interpretable Models,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We develop a convex analytic framework for ReLU neural networks which elucidates the inner workings of hidden neurons and their function space characteristics. We show that rectified linear units in neural networks act as convex regularizers, where simple solutions are encouraged via extreme points of a certain convex set. For one dimensional regression and classification, we prove that finite two-layer ReLU networks with norm regularization yield linear spline interpolation. In the more general higher dimensional case, we show that the training problem for two-layer networks can be cast as a convex optimization problem with infinitely many constraints. We then provide a family of convex relaxations to approximate the solution, and a cutting-plane algorithm to improve the relaxations. We derive conditions for the exactness of the relaxations and provide simple closed form formulas for the optimal neural network weights in certain cases. Our results show that the hidden neurons of a ReLU network can be interpreted as convex autoencoders of the input layer. We also establish a connection to l(0)-l(1) equivalence for neural networks analogous to the minimal cardinality solutions in compressed sensing. Extensive experimental results show that the proposed approach yields interpretable and accurate models.",,,,,,"Ergen, Tolga/0000-0003-4806-0224",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,4024,4032,,,,,,,,,,,,,,,,WOS:000559931300084,0
C,"Farajtabar, M; Azizan, N; Mott, A; Li, A",,"Chiappa, S; Calandra, R",,"Farajtabar, Mehrdad; Azizan, Navid; Mott, Alex; Li, Ang",,,Orthogonal Gradient Descent for Continual Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Neural networks are achieving state of the art and sometimes super-human performance on learning tasks across a variety of domains. Whenever these problems require learning in a continual or sequential manner, however, neural networks suffer from the problem of catastrophic forgetting; they forget how to solve previous tasks after being trained on a new task, despite having the essential capacity to solve both tasks if they were trained on both simultaneously. In this paper, we propose to address this issue from a parameter space perspective and study an approach to restrict the direction of the gradient updates to avoid forgetting previously-learned data. We present the Orthogonal Gradient Descent (OGD) method, which accomplishes this goal by projecting the gradients from new tasks onto a subspace in which the neural network output on previous task does not change and the projected gradient is still in a useful direction for learning the new task. Our approach utilizes the high capacity of a neural network more efficiently and does not require storing the previously learned data that might raise privacy concerns. Experiments on common benchmarks reveal the effectiveness of the proposed OGD method.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3762,3772,,,,,,,,,,,,,,,,WOS:000559931300090,0
C,"Guo, WS; Ho, N; Jordan, MI",,"Chiappa, S; Calandra, R",,"Guo, Wenshuo; Ho, Nhat; Jordan, Michael I.",,,Fast Algorithms for Computational Optimal Transport and Wasserstein Barycenter,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We provide theoretical complexity analysis for new algorithms to compute the optimal transport (OT) distance between two discrete probability distributions, and demonstrate their favorable practical performance compared to state-of-art primal-dual algorithms. First, we introduce the accelerated primal-dual randomized coordinate descent (APDRCD) algorithm for computing the OT distance. We show that its complexity is (O) over tildeO(n(5/2)/epsilon), where n stands for the number of atoms of these probability measures and epsilon > 0 is the desired accuracy. This complexity bound matches the best known complexities of primal-dual algorithms for the OT problems, including the adaptive primal-dual accelerated gradient descent (APDAGD) and the adaptive primal-dual accelerated mirror descent (APDAMD) algorithms. Then, we demonstrate the improved practical efficiency of the APDRCD algorithm through comparative experimental studies. We also propose a greedy version of APDRCD, which we refer to as accelerated primal-dual greedy coordinate descent (APDGCD), to further enhance practical performance. Finally, we generalize the APDRCD and APDGCD algorithms to distributed algorithms for computing the Wasserstein barycenter for multiple probability distributions.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2088,2096,,,,,,,,,,,,,,,,WOS:000559931301029,0
C,"Hsu, H; Asoodeh, S; Calmon, FP",,"Chiappa, S; Calandra, R",,"Hsu, Hsiang; Asoodeh, Shahab; Calmon, Flavio P.",,,Obfuscation via Information Density Estimation,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Identifying features that leak information about sensitive attributes is a key challenge in the design of information obfuscation mechanisms. In this paper, we propose a framework to identify information-leaking features via information density estimation. Here, features whose information densities exceed a pre-defined threshold are deemed information-leaking features. Once these features are identified, we sequentially pass them through a targeted obfuscation mechanism with a provable leakage guarantee in terms of E-gamma-divergence. The core of this mechanism relies on a data-driven estimate of the trimmed information density for which we propose a novel estimator, named the trimmed information density estimator (TIDE). We then use TIDE to implement our mechanism on three real-world datasets. Our approach can be used as a data-driven pipeline for designing obfuscation mechanisms targeting specific features.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,906,916,,,,,,,,,,,,,,,,WOS:000559931301042,0
C,"Khemakhem, I; Kingma, DP; Monti, RP; Hyvarinen, A",,"Chiappa, S; Calandra, R",,"Khemakhem, Ilyes; Kingma, Diederik P.; Monti, Ricardo Pio; Hyvarinen, Aapo",,,Variational Autoencoders and Nonlinear ICA: A Unifying Framework,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"The framework of variational autoencoders allows us to efficiently learn deep latent-variable models, such that the model's marginal distribution over observed variables fits the data. Often, we're interested in going a step further, and want to approximate the true joint distribution over observed and latent variables, including the true prior and posterior distributions over latent variables. This is known to be generally impossible due to unidentifiability of the model. We address this issue by showing that for a broad family of deep latent-variable models, identification of the true joint distribution over observed and latent variables is actually possible up to very simple transformations, thus achieving a principled and powerful form of disentanglement. Our result requires a factorized prior distribution over the latent variables that is conditioned on an additionally observed variable, such as a class label or almost any other observation. We build on recent developments in nonlinear ICA, which we extend to the case with noisy, undercomplete or discrete observations, integrated in a maximum likelihood framework. The result also trivially contains identifiable flow-based generative models as a special case.",,,,,,"Hyvarinen, Aapo/0000-0002-5806-4432",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2207,2216,,,,,,,,,,,,,,,,WOS:000559931301070,0
C,"Kirchler, M; Khorasani, S; Kloft, M; Lippert, C",,"Chiappa, S; Calandra, R",,"Kirchler, Matthias; Khorasani, Shahryar; Kloft, Marius; Lippert, Christoph",,,Two-sample Testing Using Deep Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We propose a two-sample testing procedure based on learned deep neural network representations. To this end, we define two test statistics that perform an asymptotic location test on data samples mapped onto a hidden layer. The tests are consistent and asymptotically control the type-1 error rate. Their test statistics can be evaluated in linear time (in the sample size). Suitable data representations are obtained in a data-driven way, by solving a supervised or unsupervised transfer-learning task on an auxiliary (potentially distinct) data set. If no auxiliary data is available, we split the data into two chunks: one for learning representations and one for computing the test statistic. In experiments on audio samples, natural images and three-dimensional neuroimaging data our tests yield significant decreases in type-2 error rate (up to 35 percentage points) compared to state-of-the-art two-sample tests such as kernel-methods and classifier two-sample tests.*",,,,,,"Kirchler, Matthias/0000-0003-0616-2740",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1387,1397,,,,,,,,,,,,,,,,WOS:000559931301077,0
C,"Le Morvan, M; Prost, N; Josse, J; Scornet, E; Varoquaux, G",,"Chiappa, S; Calandra, R",,"Le Morvan, Marine; Prost, Nicolas; Josse, Julie; Scornet, Erwan; Varoquaux, Gael",,,Linear predictor on linearly-generated data with missing values: non consistency and solutions,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We consider building predictors when the data have missing values. We study the seemingly-simple case where the target to predict is a linear function of the fully-observed data and we show that, in the presence of missing values, the optimal predictor is not linear in general. In the particular Gaussian case, it can be written as a linear function of multiway interactions between the observed data and the various missing-value indicators. Due to its intrinsic complexity, we study a simple approximation and prove generalization bounds with finite samples, highlighting regimes for which each method performs best. We then show that multilayer perceptrons with ReLU activation functions can be consistent, and can explore good trade-offs between the true model and approximations. Our study highlights the interesting family of models that are beneficial to fit with missing values depending on the amount of data available.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3165,3173,,,,,,,,,,,,,,,,WOS:000559931302050,0
C,"Li, BY; Cen, SC; Chen, YX; Chi, YJ",,"Chiappa, S; Calandra, R",,"Li, Boyue; Cen, Shicong; Chen, Yuxin; Chi, Yuejie",,,Communication-Efficient Distributed Optimization in Networks with Gradient Tracking and Variance Reduction,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Due to the imminent need to alleviate the communication burden in multi-agent and federated learning, the investigation of communication-efficient distributed optimization algorithms for empirical risk minimization has flourished recently. A large fraction of existing algorithms are developed for the master/slave setting, relying on the presence of a central parameter server. This paper focuses on distributed optimization in the network setting (also known as the decentralized setting), where each agent is only allowed to aggregate information from its neighbors over a graph. By properly adjusting the global gradient estimate via a tracking term, we first develop a communication-efficient approximate Newton-type method, called Network-DANE, which generalizes the attractive DANE algorithm to decentralized networks. Our key algorithmic ideas can be applied, in a systematic manner, to obtain decentralized versions of other master/slave distributed algorithms. Notably, we develop Network-SVRG/SARAH, which employ stochastic variance reduction at each agent to accelerate local computations. We establish linear convergence of Network-DANE and Network-SVRG for strongly convex losses, and Network-SARAH for quadratic losses, which shed light on the impact of data homogeneity, network connectivity, and local averaging upon the rate of convergence. Numerical evidence is provided to demonstrate the appealing performance of our algorithms over competitive baselines, in terms of both communication and computation efficiency.",,,,,"Li, Boyue/ABG-1227-2021",,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1464,1473,,,,,,,,,,,,,,,,WOS:000559931302006,0
C,"Lu, N; Zhang, TY; Niu, G; Sugiyama, M",,"Chiappa, S; Calandra, R",,"Lu, Nan; Zhang, Tianyi; Niu, Gang; Sugiyama, Masashi",,,Mitigating Overfitting in Supervised Classification from Two Unlabeled Datasets: A Consistent Risk Correction Approach,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"The recently proposed unlabeled-unlabeled (UU) classification method allows us to train a binary classifier only from two unlabeled datasets with different class priors. Since this method is based on the empirical risk minimization, it works as if it is a supervised classification method, compatible with any model and optimizer. However, this method sometimes suffers from severe overfitting, which we would like to prevent in this paper. Our empirical finding in applying the original UU method is that overfitting often co-occurs with the empirical risk going negative, which is not legitimate. Therefore, we propose to wrap the terms that cause a negative empirical risk by certain correction functions. Then, we prove the consistency of the corrected risk estimator and derive an estimation error bound for the corrected risk minimizer. Experiments show that our proposal can successfully mitigate overfitting of the UU method and significantly improve the classification accuracy.",,,,,"Sugiyama, Masashi/AEO-1176-2022","Sugiyama, Masashi/0000-0001-6658-6743",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1115,1124,,,,,,,,,,,,,,,,WOS:000559931302021,0
C,"Mokhtari, A; Ozdaglar, A; Pattathil, S",,"Chiappa, S; Calandra, R",,"Mokhtari, Aryan; Ozdaglar, Asuman; Pattathil, Sarath",,,A Unified Analysis of Extra-gradient and Optimistic Gradient Methods for Saddle Point Problems: Proximal Point Approach,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"In this paper we consider solving saddle point problems using two variants of Gradient Descent-Ascent algorithms, Extra-gradient (EG) and Optimistic Gradient Descent Ascent (OGDA) methods. We show that both of these algorithms admit a unified analysis as approximations of the classical proximal point method for solving saddle point problems. This viewpoint enables us to develop a new framework for analyzing EG and OGDA for bilinear and strongly convex-strongly concave settings. Moreover, we use the proximal point approximation interpretation to generalize the results for OGDA for a wide range of parameters.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1497,1506,,,,,,,,,,,,,,,,WOS:000559931302047,0
C,"Tourani, S; Shekhovtsov, A; Rother, C; Savchynskyy, B",,"Chiappa, S; Calandra, R",,"Tourani, Siddharth; Shekhovtsov, Alexander; Rother, Carsten; Savchynskyy, Bogdan",,,Taxonomy of Dual Block-Coordinate Ascent Methods for Discrete Energy Minimization,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We consider the maximum-a-posteriori inference problem in discrete graphical models and study solvers based on the dual block-coordinate ascent rule. We map all existing solvers in a single framework, allowing for a better understanding of their design principles. We theoretically show that some block-optimizing updates are sub-optimal and how to strictly improve them. On a wide range of problem instances of varying graph connectivity, we study the performance of existing solvers as well as new variants that can be obtained within the framework. As a result of this exploration we build a new state-of-the art solver, performing uniformly better on the whole range of test instances.",,,,,,"Shekhovtsov, Alexander/0000-0003-0678-8954",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303044,0
C,"Yildiz, I; Dy, J; Erdogmus, D; Kalpathy-Cramer, J; Ostmo, S; Campbell, JP; Chiang, MF; Ioannidis, S",,"Chiappa, S; Calandra, R",,"Yildiz, Ilkay; Dy, Jennifer; Erdogmus, Deniz; Kalpathy-Cramer, Jayashree; Ostmo, Susan; Campbell, J. Peter; Chiang, Michael F.; Ioannidis, Stratis",,,Fast and Accurate Ranking Regression,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We consider a ranking regression problem in which we use a dataset of ranked choices to learn Plackett-Luce scores as functions of sample features. We solve the maximum likelihood estimation problem by using the Alternating Directions Method of Multipliers (ADMM), effectively separating the learning of scores and model parameters. This separation allows us to express scores as the stationary distribution of a continuous-time Markov Chain. Using this equivalence, we propose two spectral algorithms for ranking regression that learn model parameters up to 579 times faster than the Newton's method.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303097,0
C,"Abbasi-Yadkori, Y; Lazic, N; Szepesvari, C",,"Chaudhuri, K; Sugiyama, M",,"Abbasi-Yadkori, Yasin; Lazic, Nevena; Szepesvari, Csaba",,,Model-Free Linear Quadratic Control via Reduction to Expert Prediction,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Model-free approaches for reinforcement learning (RL) and continuous control find policies based only on past states and rewards, without fitting a model of the system dynamics. They are appealing as they are general purpose and easy to implement; however, they also come with fewer theoretical guarantees than model-based RE In this work, we present a new model-free algorithm for controlling linear quadratic (LQ) systems, and show that its regret scales as O(T xi+2/3) for any small xi > 0 if time horizon satisfies T > C-1/xi for a constant C. The algorithm is based on a reduction of control of Markov decision processes to an expert prediction problem. In practice, it corresponds to a variant of policy iteration with forced exploration, where the policy in each phase is greedy with respect to the average of all previous value functions. This is the first model-free algorithm for adaptive control of LQ systems that provably achieves sublinear regret and has a polynomial computation cost. Empirically, our algorithm dramatically outperforms standard policy iteration, but performs worse than a model-based approach.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687903017,0
C,"Carter, B; Mueller, J; Jain, S; Gifford, D",,"Chaudhuri, K; Sugiyama, M",,"Carter, Brandon; Mueller, Jonas; Jain, Siddhartha; Gifford, David",,,What made you do this? Understanding black-box decisions with sufficient input subsets,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Local explanation frameworks aim to rationalize particular decisions made by a black-box prediction model. Existing techniques are often restricted to a specific type of predictor or based on input saliency, which may be undesirably sensitive to factors unrelated to the model's decision making process. We instead propose sufficient input subsets that identify minimal subsets of features whose observed values alone suffice for the same decision to be reached, even if all other input feature values are missing. General principles that globally govern a model's decision-making can also be revealed by searching for clusters of such input patterns across many data points. Our approach is conceptually straightforward, entirely model-agnostic, simply implemented using instance-wise backward selection, and able to produce more concise rationales than existing techniques. We demonstrate the utility of our interpretation method on various neural network models trained on text, image, and genomic data.",,,,,"Mueller, Jonas/AAY-6891-2020",,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,567,576,,,,,,,,,,,,,,,,WOS:000509687900059,0
C,"Chien, I; Zhou, HZ; Li, P",,"Chaudhuri, K; Sugiyama, M",,"Chien, I. (Eli); Zhou, Huozhi; Li, Pan",,,HS2: Active learning over hypergraphs with pointwise and pairwise queries,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We propose a hypergraph-based active learning scheme which we term HS2; HS2 generalizes the previously reported algorithm S-2 originally proposed for graph-based active learning with pointwise queries [1]. Our HS2 method can accommodate hypergraph structures and allows one to ask both pointwise queries and pairwise queries. Based on a novel parametric system particularly designed for hypergraphs, we derive theoretical results on the query complexity of HS2 for the above described generalized settings. Both the theoretical and empirical results show that HS2 requires a significantly fewer number of queries than S-2 when one uses S-2 over a graph obtained from the corresponding hypergraph via clique expansion.",,,,,"Chien, Eli/AHA-3502-2022",,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902053,0
C,"Garber, D; Kaplan, A",,"Chaudhuri, K; Sugiyama, M",,"Garber, Dan; Kaplan, Atara",,,Fast Stochastic Algorithms for Low-rank and Nonsmooth Matrix Problems,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Composite convex optimization problems which include both a nonsmooth term and a low-rank promoting term have important applications in machine learning and signal processing, such as when one wishes to recover an unknown matrix that is simultaneously low-rank and sparse. However, such problems are highly challenging to solve in large-scale: the low-rank promoting term prohibits efficient implementations of proximal methods for composite optimization and even simple subgradient methods. On the other hand, methods which are tailored for low-rank optimization, such as conditional gradient-type methods, which are often applied to a smooth approximation of the nonsmooth objective, are slow since their runtime scales with both the large Lipchitz parameter of the smoothed gradient vector and with 1/epsilon, where epsilon is the target accuracy. In this paper we develop efficient algorithms for stochastic optimization of a strongly-convex objective which includes both a nonsmooth term and a low-rank promoting term. In particular, to the best of our knowledge, we present the first algorithm that enjoys all following critical properties for large-scale problems: i) (nearly) optimal sample complexity, ii) each iteration requires only a single low-rank SVD computation, and iii) overall number of thin-SVD computations scales only with log 1/epsilon (as opposed to poly(1/epsilon) in previous methods). We also give an algorithm for the closely-related finite-sum setting. We empirically demonstrate our results on the problem of recovering a simultaneously low-rank and sparse matrix.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,286,294,,,,,,,,,,,,,,,,WOS:000509687900030,0
C,"Janati, H; Cuturi, M; Gramfort, A",,"Chaudhuri, K; Sugiyama, M",,"Janati, Hicham; Cuturi, Marco; Gramfort, Alexandre",,,Wasserstein regularization for sparse multi-task regression,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We focus in this paper on high-dimensional regression problems where each regressor can be associated to a location in a physical space, or more generally a generic geometric space. Such problems often employ sparse priors, which promote models using a small subset of regressors. To increase statistical power, the so-called multi-task techniques were proposed, which consist in the simultaneous estimation of several related models. Combined with sparsity assumptions, it lead to models enforcing the active regressors to be shared across models, thanks to, for instance l(1)/l(q) norms. We argue in this paper that these techniques fail to leverage the spatial information associated to regressors. Indeed, while sparse priors enforce that only a small subset of variables is used, the assumption that these regressors overlap across all tasks is overly simplistic given the spatial variability observed in real data. In this paper, we propose a convex regularizer for multi-task regression that encodes a more flexible geometry. Our regularizer is based on unbalanced optimal transport (OT) theory, and can take into account a prior geometric knowledge on the regressor variables, without necessarily requiring overlapping supports. We derive an efficient algorithm based on a regularized formulation of OT, which iterates through applications of Sinkhorn's algorithm along with coordinate descent iterations. The performance of our model is demonstrated on regular grids with both synthetic and real datasets as well as complex triangulated geometries of the cortex with an application in neuroimaging.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901047,0
C,"Katz-Samuels, J; Scott, C",,"Chaudhuri, K; Sugiyama, M",,"Katz-Samuels, Julian; Scott, Clayton",,,Top Feasible Arm Identification,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We propose a new variant of the top arm identification problem, top feasible arm identification, where there are K arms associated with D-dimensional distributions and the goal is to find m arms that maximize some known linear function of their means subject to the constraint that their means belong to a given set P subset of R-D. This problem has many applications since in many settings, feedback is multi-dimensional and it is of interest to perform constrained maximization. We present problem-dependent lower bounds for top feasible arm identification and upper bounds for several algorithms. Our most broadly applicable algorithm, TF-LUCB-B, has an upper bound that is loose by a factor of O(D log(K)). Many problems of practical interest are two-dimensional and, for these, it is loose by a factor of O(log(K)). Finally, we conduct experiments on synthetic and real-world datasets that demonstrate the effectiveness of our algorithms. Our algorithms are superior both in theory and in practice to a naive two-stage algorithm that first identifies the feasible arms and then applies a best arm identification algorithm to the feasible arms.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901066,0
C,"Lin, A; Zhang, YZ; Heng, J; Allsop, SA; Tye, KM; Jacob, PE; Ba, D",,"Chaudhuri, K; Sugiyama, M",,"Lin, Alexander; Zhang, Yingzhuo; Heng, Jeremy; Allsop, Stephen A.; Tye, Kay M.; Jacob, Pierre E.; Ba, Demba",,,Clustering Time Series with Nonlinear Dynamics: A Bayesian Non-Parametric and Particle-Based Approach,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We propose a general statistical framework for clustering multiple time series that exhibit nonlinear dynamics into an a-priori-unknown number of sub-groups. Our motivation comes from neuroscience, where an important problem is to identify, within a large assembly of neurons, subsets that respond similarly to a stimulus or contingency. Upon modeling the multiple time series as the output of a Dirichlet process mixture of nonlinear state-space models, we derive a Metropolis-within-Gibbs algorithm for full Bayesian inference that alternates between sampling cluster assignments and sampling parameter values that form the basis of the clustering. The Metropolis step employs recent innovations in particle-based methods. We apply the framework to clustering time series acquired from the prefrontal cortex of mice in an experiment designed to characterize the neural underpinnings of fear.",,,,,"Heng, Jeremy/F-3914-2019",,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902054,0
C,"Perrault, P; Perchet, V; Valko, M",,"Chaudhuri, K; Sugiyama, M",,"Perrault, Pierre; Perchet, Vianney; Valko, Michal",,,Finding the bandit in a graph: Sequential search-and-stop,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We consider the problem where an agent wants to find a hidden object that is randomly located in sonic vertex of a directed acyclic graph ((DAG) according- to a fixed but possibly unknown distribution. The agent can only examine vertices whose in-neighbors have already been examined. In this paper, we address a. Icarning setting where we allow the agent to stop before having_ found the object and restart searching on a new iudepenclent instance of the same problem. Our goal is to rnaxirnize the total number of hidden objects found given a time budget. The agent can thus skip an instance after realizing that it would spend too much time on it. Our contributions are both to the search. theory and rrculti-oared hero/its. If the distribution is known, we provide a quasi-optimal and efficient stationary strategy. If the distribution is unknown, we additionally show how to sequerntially approximate it and, at the same time, act near optimally in order to collect as many hidden objects as possible.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901074,0
C,"Poon, C; Keriven, N; Peyre, G",,"Chaudhuri, K; Sugiyama, M",,"Poon, Clarice; Keriven, Nicolas; Peyre, Gabriel",,,Support Localization and the Fisher Metric for off-the-grid Sparse Regularization,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Sparse regularization is a central technique for both machine learning (to achieve supervised features selection or unsupervised mixture learning) and imaging sciences (to achieve super-resolution). Existing performance guaranties assume a separation of the spikes based on an ad-hoc (usually Euclidean) minimum distance condition, which ignores the geometry of the problem. In this article, we study the BLASSO (i.e. the off-the-grid version of l(1) LASSO regularization) and show that the Fisher-Rao distance is the natural way to ensure and quantify support recovery, since it preserves the invariance of the problem under reparameterization. We prove that under mild regularity and curvature conditions, stable support identification is achieved even in the presence of randomized sub-sampled observations (which is the case in compressed sensing or learning scenario). On deconvolution problems, which are translation invariant, this generalizes to the multi-dimensional setting existing results of the literature. For more complex translation-varying problems, such as Laplace transform inversion, this gives the first geometry-aware guarantees for sparse recovery.",,,,,"Keriven, Nicolas/AAR-4705-2020; Keriven, Nicolas/CAF-8143-2022","Keriven, Nicolas/0000-0002-3846-8763",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901040,0
C,"Solin, A; Kok, M",,"Chaudhuri, K; Sugiyama, M",,"Solin, Arno; Kok, Manon",,,Know Your Boundaries: Constraining Gaussian Processes by Variational Harmonic Features,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Gaussian processes (GPs) provide a powerful framework for extrapolation,interpolation, and noise removal in regression and classification. This paper considers constraining GPs to arbitrarily-shaped domains with boundary conditions. We solve a Fourier-like generalised harmonic feature representation of the GP prior in the domain of interest, which both constrains the GP and attains a low-rank representation that is used for speeding up inference. The method scales as O(nm(2)) in prediction and O(m(3)) in hyperparameter learning for regression, where is the number of data points and m the number of features. Furthermore, we make use of the variational approach to allow the method to deal with non-Gaussian likelihoods. The experiments cover both simulated and empirical data in which the boundary conditions allow for inclusion of additional physical information.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902025,0
C,"Thrampoulidis, C; Rawat, AS",,"Chaudhuri, K; Sugiyama, M",,"Thrampoulidis, Christos; Rawat, Ankit Singh",,,Lifting high-dimensional non-linear models with Gaussian regressors,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We study the problem of recovering a structured signal x(0) from high-dimensional data y(i) = f (a(i)(T)x(0)) for some nonlinear (and potentially unknown) link function f, when the regressors a(i) are iid Gaussian. Brillinger (1982) showed that ordinary least-squares estimates x(0) up to a constant of proportionality mu(l), which depends on f. Recently, Plan & Vershynin (2015) extended this result to the high-dimensional setting deriving sharp error bounds for the generalized Lasso. Unfortunately, both least-squares and the Lasso fail to recover x(0) when mu(l) = 0. For example, this includes all even link functions. We resolve this issue by proposing and analyzing an alternative convex recovery method. In a nutshell, our method treats such link functions as if they were linear in a lifted space of higher-dimension. Interestingly, our error analysis captures the effect of both the nonlinearity and the problem's geometry in a few simple summary parameters.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687903027,0
C,"Zhu, PK; Acar, DAE; Nan, F; Jain, P; Saligrama, V",,"Chaudhuri, K; Sugiyama, M",,"Zhu, Pengkai; Acar, Durmus Alp Emre; Nan, Feng; Jain, Prateek; Saligrama, Venkatesh",,,Cost aware Inference for IoT Devices,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Networked embedded devices (IoTs) of limited CPU, memory and power resources are revolutionizing data gathering, remote monitoring and planning in many consumer and business applications. Nevertheless, resource limitations place a significant burden on their service life and operation, warranting cost-aware methods that are capable of distributively screening redundancies in device information and transmitting informative data. We propose to train a decentralized gated network that, given an observed instance at test-time, allows for activation of select devices to transmit information to a central node, which then performs inference. We analyze our proposed gradient descent algorithm for Gaussian features and establish convergence guarantees under good initialization. We conduct experiments on a number of real-world datasets arising in IoT applications and show that our model results in over 1.5X service life with negligible accuracy degradation relative to a performance achievable by a neural network.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902084,0
C,"Chee, J; Toulis, P",,"Storkey, A; PerezCruz, F",,"Chee, Jerry; Toulis, Panos",,,Convergence diagnostics for stochastic gradient descent with constant learning rate,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Many iterative procedures in stochastic optimization exhibit a transient phase followed by a stationary phase. During the transient phase the procedure converges towards a region of interest, and during the stationary phase the procedure oscillates in that region, commonly around a single point. In this paper, we develop a statistical diagnostic test to detect such phase transition in the context of stochastic gradient descent with constant learning rate. We present theory and experiments suggesting that the region where the proposed diagnostic is activated coincides with the convergence region. For a class of loss functions, we derive a closed-form solution describing such region. Finally, we suggest an application to speed up convergence of stochastic gradient descent by halving the learning rate each time stationarity is detected. This leads to a new variant of stochastic gradient descent, which in many settings is comparable to state-of-art.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300154,0
C,"Dupuy, C; Bach, F",,"Storkey, A; PerezCruz, F",,"Dupuy, Christophe; Bach, Francis",,,Learning Determinantal Point Processes in Sublinear Time,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"We propose a new class of determinantal point processes (DPPs) which can be manipulated for inference and parameter learning in potentially sublinear time in the number of items. This class, based on a specific low-rank factorization of the marginal kernel, is particularly suited to a subclass of continuous DPPs and DPPs defined on exponentially many items. We apply this new class to modelling text documents as sampling a DPP of sentences, and propose a conditional maximum likelihood formulation to model topic proportions, which is made possible with no approximation for our class of DPPs. We present an application to document summarization with a DPP on 2(500) items.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300027,0
C,"Forster, D; Lucke, J",,"Storkey, A; PerezCruz, F",,"Forster, Dennis; Luecke, Joerg",,,Can clustering scale sublinearly with its clusters? A variational EM acceleration of GMMs and k-means,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"One iteration of standard k-means (i.e., Lloyd's algorithm) or standard EM for Gaussian mixture models (GMMs) scales linearly with the number of clusters C, data points N, and data dimensionality D. In this study, we explore whether one iteration of k-means or EM for GMMs can scale sublinearly with C at run-time, while improving the clustering objective remains effective. The tool we apply for complexity reduction is variational EM, which is typically used to make training of generative models with exponentially many hidden states tractable. Here, we apply novel theoretical results on truncated variational EM to make tractable clustering algorithms more efficient. The basic idea is to use a partial variational E-step which reduces the linear complexity of O (NCD) required for a full E-step to a sublinear complexity. Our main observation is that the linear dependency on C can be reduced to a dependency on a much smaller parameter G which relates to cluster neighborhood relations. We focus on two versions of partial variational EM for clustering: variational GMM, scaling with O (NG(2)D), and variational k-means, scaling with O(NGD) per iteration. Empirical results show that these algorithms still require comparable numbers of iterations to improve the clustering objective to same values as k-means. For data with many clusters, we consequently observe reductions of net computational demands between two and three orders of magnitude. More generally, our results provide substantial empirical evidence in favor of clustering to scale sublinearly with C.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300014,0
C,"Franceschi, JY; Fawzi, A; Fawzi, O",,"Storkey, A; PerezCruz, F",,"Franceschi, Jean-Yves; Fawzi, Alhussein; Fawzi, Omar",,,Robustness of classifiers to uniform l(p) and Gaussian noise,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"We study the robustness of classifiers to various kinds of random noise models. In particular, we consider noise drawn uniformly from the l(p) ball for p is an element of [1, infinity] and Gaussian noise with an arbitrary covariance matrix. We characterize this robustness to random noise in terms of the distance to the decision boundary of the classifier. This analysis applies to linear classifiers as well as classifiers with locally approximately flat decision boundaries, a condition which is satisfied by state-of-the-art deep neural networks. The predicted robustness is verified experimentally.",,,,,,"Fawzi, Omar/0000-0001-8491-0359",,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300134,0
C,"Gao, X; Li, XB; Zhang, SZ",,"Storkey, A; PerezCruz, F",,"Gao, Xiang; Li, Xiaobo; Zhang, Shuzhong",,,Online Learning with Non-Convex Losses and Non-Stationary Regret,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"In this paper, we consider online learning with non-convex loss functions. Similar to Besbes et al. [2015] we apply non-stationary regret as the performance metric. In particular, we study the regret bounds under different assumptions on the information available regarding the loss functions. When the gradient of the loss function at the decision point is available, we propose an online normalized gradient descent algorithm (ONGD) to solve the online learning problem. In another situation, when only the value of the loss function is available, we propose a bandit online normalized gradient descent algorithm (BONGD). Under a condition to be called weak pseudo-convexity (WPC), we show that both algorithms achieve a cumulative regret bound of O(root T + VTT), where VT is the total temporal variations of the loss functions, thus establishing a sublinear regret bound for online learning with non-convex loss functions and non-stationary regret measure.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300026,0
C,"Ge, H; Xu, K; Ghahramani, Z",,"Storkey, A; PerezCruz, F",,"Ge, Hong; Xu, Kai; Ghahramani, Zoubin",,,Turing: a language for flexible probabilistic inference,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Probabilistic programming is becoming an attractive approach to probabilistic machine learning. Through relieving researchers from the tedious burden of hand-deriving inference algorithms, not only does it enable development of more accurate and interpretable models but it also encourages reproducible research. However, successful probabilistic programming systems require flexible, generic and efficient inference engines. In this work, we present a system called Turing for flexible composable probabilistic programming inference. Turing has a intuitive modeling syntax and supports a wide range of sampling based inference algorithms. Most importantly, Turing inference is composable: it combines Markov chain sampling operations on subsets of model variables, e.g. using a combination of a Hamiltonian Monte Carlo (HMC) engine and a particle Gibbs (PG) engine. This composable inference engine allows the user to easily switch between black-box style inference methods such as HMC, and customized inference methods. Our aim is to present Turing and its composable inference engines to the community and encourage other researchers to build on this system to help advance the field of probabilistic machine learning.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300176,0
C,"Grover, A; Gummadi, R; Lazaro-Gredilla, M; Schuurmans, D; Ermon, S",,"Storkey, A; PerezCruz, F",,"Grover, Aditya; Gummadi, Ramki; Lazaro-Gredilla, Miguel; Schuurmans, Dale; Ermon, Stefano",,,Variational Rejection Sampling,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Learning latent variable models with stochastic variational inference is challenging when the approximate posterior is far from the true posterior, due to high variance in the gradient estimates. We propose a novel rejection sampling step that discards samples from the variational posterior which are assigned low likelihoods by the model. Our approach provides an arbitrarily accurate approximation of the true posterior at the expense of extra computation. Using a new gradient estimator for the resulting unnormalized proposal distribution, we achieve average improvements of 3:71 nats and 0:21 nats over state-of-the-art single-sample and multi-sample alternatives respectively for estimating marginal log-likelihoods using sigmoid belief networks on the MNIST dataset.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300087,0
C,"Heckel, R; Simchowitz, M; Ramchandran, K; Wainwright, MJ",,"Storkey, A; PerezCruz, F",,"Heckel, Reinhard; Simchowitz, Max; Ramchandran, Kannan; Wainwright, Martin J.",,,Approximate ranking from pairwise comparisons,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"A common problem in machine learning is to rank a set of n items based on pairwise comparisons. Here ranking refers to partitioning the items into sets of pre-specified sizes according to their scores, which includes identification of the top-k items as the most prominent special case. The score of a given item is defined as the probability that it beats a randomly chosen other item. Finding an exact ranking typically requires a prohibitively large number of comparisons, but in practice, approximate rankings are often adequate. Accordingly, we study the problem of finding approximate rankings from pairwise comparisons. We analyze an active ranking algorithm that counts the number of comparisons won, and decides whether to stop or which pair of items to compare next, based on confidence intervals computed from the data collected in previous steps. We show that this algorithm succeeds in recovering approximate rankings using a number of comparisons that is close to optimal up to logarithmic factors. We also present numerical results, showing that in practice, approximation can drastically reduce the number of comparisons required to estimate a ranking.",,,,,"Heckel, Reinhard/AAV-7904-2020",,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300111,0
C,"Lang, H; Sontag, D; Vijayaraghavan, A",,"Storkey, A; PerezCruz, F",,"Lang, Hunter; Sontag, David; Vijayaraghavan, Aravindan",,,Optimality of Approximate Inference Algorithms on Stable Instances,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,Approximate algorithms for structured prediction problems-such as LP relaxations and the popular alpha-expansion algorithm (Boykov et al. 2001)-typically far exceed their theoretical performance guarantees on real-world instances. These algorithms often find solutions that are very close to optimal. The goal of this paper is to partially explain the performance of alpha-expansion and an LP relaxation algorithm on MAP inference in Ferromagnetic Potts models (FPMs). Our main results give stability conditions under which these two algorithms provably recover the optimal MAP solution. These theoretical results complement numerous empirical observations of good performance.,,,,,"Vijayaraghavan, Aravindan/I-2257-2015",,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300121,0
C,"Chen, CY; Carlson, D; Gan, Z; Li, CY; Carin, L",,"Gretton, A; Robert, CC",,"Chen, Changyou; Carlson, David; Gan, Zhe; Li, Chunyuan; Carin, Lawrence",,,Bridging the Gap between Stochastic Gradient MCMC and Stochastic Optimization,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Stochastic gradient Markov chain Monte Carlo (SG-MCMC) methods are Bayesian analogs to popular stochastic optimization methods; however, this connection is not well studied. We explore this relationship by applying simulated annealing to an SG-MCMC algorithm. Furthermore, we extend recent SG-MCMC methods with two key components: i) adaptive preconditioners (as in ADAgrad or RMSprop), and ii) adaptive element-wise momentum weights. The zero-temperature limit gives a novel stochastic optimization method with adaptive element-wise momentum weights, while conventional optimization methods only have a shared, static momentum weight. Under certain assumptions, our theoretical analysis suggests the proposed simulated annealing approach converges close to the global optima. Experiments on several deep neural network models show state-of-the-art results compared to related stochastic optimization algorithms.",,,,,,"Carlson, David/0000-0003-1005-6385; Carin, Lawrence/0000-0001-6277-7948",,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,1051,1060,,,,,,,,,,,,,,,,WOS:000508662100114,0
C,"Figueiredo, MAT; Nowak, RD",,"Gretton, A; Robert, CC",,"Figueiredo, Mario A. T.; Nowak, Robert D.",,,Ordered Weighted l(1) Regularized Regression with Strongly Correlated Covariates: Theoretical Aspects,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"This paper studies the ordered weighted l(1) (OWL) family of regularizers for sparse linear regression with strongly correlated covariates. We prove sufficient conditions for clustering correlated covariates, extending and qualitatively strengthening previous results for a particular member of the OWL family: OSCAR (octagonal shrinkage and clustering algorithm for regression). We derive error bounds for OWL with correlated Gaussian covariates: for cases in which clusters of covariates are strongly (even perfectly) correlated, but covariates in different clusters are uncorrelated, we show that if the true p-dimensional signal involves only s clusters, thenO(s log p) samples suffice to accurately estimate it, regardless of the number of coefficients within the clusters. Since the estimation of s-sparse signals with completely independent covariates also requires O(s log p) measurements, this shows that by using OWL regularization, we pay no price (in the number of measurements) for the presence of strongly correlated covariates.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,930,938,,,,,,,,,,,,,,,,WOS:000508662100101,0
C,"Herman, M; Gindele, T; Wagner, J; Schmitt, F; Burgard, W",,"Gretton, A; Robert, CC",,"Herman, Michael; Gindele, Tobias; Wagner, Joerg; Schmitt, Felix; Burgard, Wolfram",,,Inverse Reinforcement Learning with Simultaneous Estimation of Rewards and Dynamics,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Inverse Reinforcement Learning (IRL) describes the problem of learning an unknown reward function of a Markov Decision Process (MDP) from observed behavior of an agent. Since the agent's behavior originates in its policy and MDP policies depend on both the stochastic system dynamics as well as the reward function, the solution of the inverse problem is significantly influenced by both. Current IRL approaches assume that if the transition model is unknown, additional samples from the system's dynamics are accessible, or the observed behavior provides enough samples of the system's dynamics to solve the inverse problem accurately. These assumptions are often not satisfied. To overcome this, we present a gradient-based IRL approach that simultaneously estimates the system's dynamics. By solving the combined optimization problem, our approach takes into account the bias of the demonstrations, which stems from the generating policy. The evaluation on a synthetic MDP and a transfer learning task shows improvements regarding the sample efficiency as well as the accuracy of the estimated reward functions and transition models.",,,,,,"Burgard, Wolfram/0000-0002-5680-6500",,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,102,110,,,,,,,,,,,,,,,,WOS:000508662100012,0
C,"Kaganovsky, Y; Odinaka, I; Carlson, D; Carin, L",,"Gretton, A; Robert, CC",,"Kaganovsky, Yan; Odinaka, Ikenna; Carlson, David; Carin, Lawrence",,,Parallel Majorization Minimization with Dynamically Restricted Domains for Nonconvex Optimization,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"We propose an optimization framework for nonconvex problems based on majorization-minimization that is particularity well-suited for parallel computing. It reduces the optimization of a high dimensional nonconvex objective function to successive optimizations of locally tight and convex upper bounds which are additively separable into low dimensional objectives. The original problem is then broken into simpler parallel sub-problems while still guaranteeing the monotonic reduction of the original objective function and convergence to a local minimum. Due to the low dimensionality of each sub-problem, second-order optimization methods become computationally feasible and can be used to accelerate convergence. In addition, the upper bound can be restricted to a local dynamic convex domain, so that it is better matched to the local curvature of the objective function, resulting in accelerated convergence.",,,,,,"Carin, Lawrence/0000-0001-6277-7948; Carlson, David/0000-0003-1005-6385",,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,1497,1505,,,,,,,,,,,,,,,,WOS:000508662100162,0
C,"Kaufmann, D; Parbhoo, S; Wieczorek, A; Keller, S; Adametz, D; Roth, V",,"Gretton, A; Robert, CC",,"Kaufmann, Dinu; Parbhoo, Sonali; Wieczorek, Aleksander; Keller, Sebastian; Adametz, David; Roth, Volker",,,Bayesian Markov Blanket Estimation,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"This paper considers a Bayesian view for estimating the Markov blanket of a set of query variables, where the set of potential neighbours is big. We factorise the posterior such that the Markov blanket is conditionally independent of the network of the potential neighbours. By exploiting this blockwise decoupling, we derive analytic expressions for posterior conditionals. Subsequently, we develop an inference scheme, which makes use of the factorisation. As a result, estimation of a sub-network is possible without inferring an entire network. Since the resulting Gibbs sampler scales linearly with the number of variables, it can handle relatively large neighbourhoods. The proposed scheme results in faster convergence and superior mixing of the Markov chain than existing Bayesian network estimation techniques.",,,,,"Roth, Volker/Q-4025-2017","Roth, Volker/0000-0003-0991-0273; Kaufmann, Dinu/0000-0003-4446-0386; Parbhoo, Sonali/0000-0001-8400-3732",,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,333,341,,,,,,,,,,,,,,,,WOS:000508662100037,0
C,"Wang, Y; Canale, A; Dunson, D",,"Gretton, A; Robert, CC",,"Wang, Ye; Canale, Antonio; Dunson, David",,,Scalable geometric density estimation,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"It is standard to assume a low-dimensional structure in estimating a high-dimensional density. However, popular methods, such as probabilistic principal component analysis, scale poorly computationally. We introduce a novel empirical Bayes method that we term geometric density estimation (GEODE) and show that, with mild conditions and among all d-dimensional linear subspaces, the span of the d leading principal axes of the data maximizes the model posterior. With these axes pre-computed using fast singular value decomposition, GEODE easily scales to high dimensional problems while providing uncertainty characterization. The model is also capable of imputing missing data and dynamically deleting redundant dimensions. Finally, we generalize GEODE by mixing it across a dyadic clustering tree. Both simulation studies and real world data applications show superior performance of GEODE in terms of robustness and computational efficiency.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,857,865,,,,,,,,,,,,,,,,WOS:000508662100093,0
C,"Wang, HH; Fazayeli, F; Chatterjee, S; Banerjee, A",,"Kaski, S; Corander, J",,"Wang, Huahua; Fazayeli, Farideh; Chatterjee, Soumyadeep; Banerjee, Arindam",,,Gaussian Copula Precision Estimation with Missing Values,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"We consider the problem of estimating sparse precision matrix of Gaussian copula distributions using samples with missing values in high dimensions. Existing approaches, primarily designed for Gaussian distributions, suggest using plugin estimators by disregarding the missing values. In this paper, we propose double plugin Gaussian (DoPinG) copula estimators to estimate the sparse precision matrix corresponding to non-paranormal distributions. DoPinG uses two plugin procedures and consists of three steps: (1) estimate nonparametric correlations based on observed values, including Kendall's tau and Spearman's rho; (2) estimate the non paranormal correlation matrix; (3) plug into existing sparse precision estimators. We prove that DoPinG copula estimators consistently estimate the non-paranormal correlation matrix at a rate of O((1/(1-delta)root log p/n) where S is the probability of missing values. We provide experimental results to illustrate the effect of sample size and percentage of missing data on the model performance. Experimental results show that DoPinG is significantly better than estimators like mGlasso, which are primarily designed for Gaussian data.",,,,,"Chatterjee, Soumyadeep/AAV-1995-2020",,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,978,986,,,,,,,,,,,,,,,,WOS:000508355800108,0
J,"Li, YX; Ma, C; Chen, YX; Chi, YJ",,,,"Li, Yuanxin; Ma, Cong; Chen, Yuxin; Chi, Yuejie",,,Nonconvex Matrix Factorization From Rank-One Measurements,IEEE TRANSACTIONS ON INFORMATION THEORY,,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We consider the problem of recovering low-rank matrices from random rank-one measurements, which spans numerous applications including covariance sketching, phase retrieval, quantum state tomography, and learning shallow polynomial neural networks, among others. Our approach is to directly estimate the low-rank factor by minimizing a nonconvex least-squares loss function via vanilla gradient descent, following a tailored spectral initialization. When the true rank is bounded by a constant, this algorithm is guaranteed to converge to the ground truth (up to global ambiguity) with near-optimal sample complexity and computational complexity. To the best of our knowledge, this is the first guarantee that achieves near-optimality in both metrics. In particular, the key enabler of near-optimal computational guarantees is an implicit regularization phenomenon: without explicit regularization, both spectral initialization and the gradient descent iterates automatically stay within a region incoherent with the measurement vectors. This feature allows one to employ much more aggressive step sizes compared with the ones suggested in prior literature, without the need of sample splitting.",,,,,"Chi, Yuejie/G-6033-2012","Chi, Yuejie/0000-0002-6766-5459",,,,,,,,,,,,,0018-9448,1557-9654,,,,MAR,2021,67,3,,,,,1928,1950,,10.1109/TIT.2021.3050427,0,,,,,,,,,,,,,WOS:000619322300033,0
C,"Fotakis, D; Kalavasis, A; Stavropoulos, K",,"Banerjee, A; Fukumizu, K",,"Fotakis, Dimitris; Kalavasis, Alkis; Stavropoulos, Konstantinos",,,Aggregating Incomplete and Noisy Rankings,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We consider the problem of learning the true ordering of a set of alternatives from largely incomplete and noisy rankings. We introduce a natural generalization of both the classical Mallows model of ranking distributions and the extensively studied model of noisy pairwise comparisons. Our selective Mallows model outputs a noisy ranking on any given subset of alternatives, based on an underlying Mallows distribution. Assuming a sequence of subsets where each pair of alternatives appears frequently enough, we obtain strong asymptotically tight upper and lower bounds on the sample complexity of learning the underlying complete ranking and the (identities and the) ranking of the top-k alternatives from selective Mallows rankings. Moreover, building on the work of (Braverman and Mossel, 2009), we show how to efficiently compute the maximum likelihood complete ranking from selective Mallows rankings.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802073,0
C,"Gidel, G; Balduzzi, D; Czarnecki, WM; Garnelo, M; Bachrach, Y",,"Banerjee, A; Fukumizu, K",,"Gidel, Gauthier; Balduzzi, David; Czarnecki, Wojciech Marian; Garnelo, Marta; Bachrach, Yoram",,,A Limited-Capacity Minimax Theorem for Non-Convex Games or: How I Learned to Stop Worrying about Mixed-Nash and Love Neural Nets,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Adversarial training, a special case of multi-objective optimization, is an increasingly prevalent machine learning technique: some of its most notable applications include GAN-based generative modeling and self-play techniques in reinforcement learning which have been applied to complex games such as Go or Poker. In practice, a single pair of networks is typically trained to find an approximate equilibrium of a highly nonconcave-nonconvex adversarial problem. However, while a classic result in game theory states such an equilibrium exists in concave-convex games, there is no analogous guarantee if the payoff is nonconcave-nonconvex. Our main contribution is to provide an approximate minimax theorem for a large class of games where the players pick neural networks including WGAN, StarCraft II and Blotto Game. Our findings rely on the fact that despite being nonconcave-nonconvex with respect to the neural networks parameters, these games are concave-convex with respect to the actual models (e.g., functions or distributions) represented by these neural networks.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803010,0
C,"Izzo, Z; Smart, MA; Chaudhuri, K; Zou, J",,"Banerjee, A; Fukumizu, K",,"Izzo, Zachary; Smart, Mary Anne; Chaudhuri, Kamalika; Zou, James",,,Approximate Data Deletion from Machine Learning Models,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Deleting data from a trained machine learning (ML) model is a critical task in many applications. For example, we may want to remove the influence of training points that might be out of date or outliers. Regulations such as EU's General Data Protection Regulation also stipulate that individuals can request to have their data deleted. The naive approach to data deletion is to retrain the ML model on the remaining data, but this is too time consuming. In this work, we propose a new approximate deletion method for linear and logistic models whose computational cost is linear in the the feature dimension d and in-dependent of the number of training data n. This is a significant gain over all existing methods, which all have superlinear time dependence on the dimension. We also develop a new feature-injection test to evaluate the thoroughness of data deletion from ML models.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802043,0
C,"Lemercier, M; Salvi, C; Damonlas, T; Bonilla, EV; Lyons, T",,"Banerjee, A; Fukumizu, K",,"Lemercier, Maud; Salvi, Cristopher; Damonlas, Theodoros; Bonilla, Edwin, V; Lyons, Terry",,,Distribution Regression for Sequential Data,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Distribution regression refers to the supervised learning problem where labels are only available for groups of inputs instead of individual inputs. In this paper, we develop a rigorous mathematical framework for distribution regression where inputs are complex data streams. Leveraging properties of the expected signature and a recent signature kernel trick for sequential data from stochastic analysis, we introduce two new learning techniques, one feature-based and the other kernel-based. Each is suited to a different data regime in terms of the number of data streams and the dimensionality of the individual streams. We provide theoretical results on the universality of both approaches and demonstrate empirically their robustness to irregularly sampled multivariate time-series, achieving state-of-the-art performance on both synthetic and real-world examples from thermodynamics, mathematical finance and agricultural science.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804052,0
C,"Liu, TY; Li, Y; Wei, S; Zhou, EL; Zhao, T",,"Banerjee, A; Fukumizu, K",,"Liu, Tianyi; Li, Yan; Wei, Song; Zhou, Enlu; Zhao, Tuo",,,Noisy Gradient Descent Converges to Flat Minima for Nonconvex Matrix Factorization,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Numerous empirical evidences have corroborated the importance of noise in nonconvex optimization problems. The theory behind such empirical observations, however, is still largely unknown. This paper studies this fundamental problem through investigating the nonconvex rectangular matrix factorization problem, which has infinitely many global minima due to rotation and scaling invariance. Hence, gradient descent (GD) can converge to any optimum, depending on the initialization. In contrast, we show that a perturbed form of GD with an arbitrary initialization converges to a global optimum that is uniquely determined by the injected noise. Our result implies that the noise imposes implicit bias towards certain optima. Numerical experiments are provided to support our theory.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802030,0
C,"Murata, T; Suzuki, T",,"Banerjee, A; Fukumizu, K",,"Murata, Tomoya; Suzuki, Taiji",,,Gradient Descent in RKHS with Importance Labeling,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Labeling cost is often expensive and is a fundamental limitation of supervised learning. In this paper, we study importance labeling problem, in which we are given many unlabeled data and select a limited number of data to be labeled from the unlabeled data, and then a learning algorithm is executed on the selected one. We propose a new importance labeling scheme that can effectively select an informative subset of unlabeled data in least squares regression in Reproducing Kernel Hilbert Spaces (RKHS). We analyze the generalization error of gradient descent combined with our labeling scheme and show that the proposed algorithm achieves the optimal rate of convergence in much wider settings and especially gives much better generalization ability in a small label noise setting than the usual uniform sampling scheme. Numerical experiments verify our theoretical findings.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802040,0
C,"Sabag, O; Hassibi, B",,"Banerjee, A; Fukumizu, K",,"Sabag, Oron; Hassibi, Babak",,,Regret-Optimal Filtering,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We consider the problem of filtering in linear state-space models (e.g., the Kalman filter setting) through the lens of regret optimization. Specifically, we study the problem of causally estimating a desired signal, generated by a linear state-space model driven by process noise, based on noisy observations of a related observation process. We define a novel regret criterion for estimator design as the difference of the estimation error energies between a clairvoyant estimator that has access to all future observations (a so-called smoother) and a causal one that only has access to current and past observations. The regret-optimal estimator is the causal estimator that minimizes the worst-case regret across all bounded-energy noise sequences. We provide a solution for the regret filtering problem at two levels. First, an horizon-independent solution at the operator level is obtained by reducing the regret to the well-known Nehari problem. Secondly, our main result for state-space models is an explicit estimator that achieves the optimal regret. The regret-optimal estimator is represented as a finite-dimensional state-space whose parameters can be computed by solving three Riccati equations and a single Lyapunov equation. We demonstrate the applicability and efficacy of the estimator in a variety of problems and observe that the estimator has average and worst-case performances that are simultaneously close to their optimal values.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803019,0
C,"Satorras, VG; Welling, M",,"Banerjee, A; Fukumizu, K",,"Satorras, Victor Garcia; Welling, Max",,,Neural Enhanced Belief Propagation on Factor Graphs,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"A graphical model is a structured representation of locally dependent random variables. A traditional method to reason over these random variables is to perform inference using belief propagation. When provided with the true data generating process, belief propagation can infer the optimal posterior probability estimates in tree structured factor graphs. However, in many cases we may only have access to a poor approximation of the data generating process, or we may face loops in the factor graph, leading to suboptimal estimates. In this work we first extend graph neural networks to factor graphs (FG-GNN). We then propose a new hybrid model that runs conjointly a FG-GNN with belief propagation. The FG-GNN receives as input messages from belief propagation at every inference iteration and outputs a corrected version of them. As a result, we obtain a more accurate algorithm that combines the benefits of both belief propagation and graph neural networks. We apply our ideas to error correction decoding tasks, and we show that our algorithm can outperform belief propagation for LDPC codes on bursty channels.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,685,+,,,,,,,,,,,,,,,,WOS:000659893800077,0
C,"Sonoda, S; Ishikawa, I; Ikeda, M",,"Banerjee, A; Fukumizu, K",,"Sonoda, Sho; Ishikawa, Isao; Ikeda, Masahiro",,,Ridge Regression with Over-Parametrized Two-Layer Networks Converge to Ridgelet Spectrum,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Characterization of local minima draws much attention in theoretical studies of deep learning. In this study, we investigate the distribution of parameters in an over-parametrized finite neural network trained by ridge regularized empirical square risk minimization (RERM).We develop a new theory of ridgelet transform, a wavelet-like integral transform that provides a powerful and general framework for the theoretical study of neural networks involving not only the ReLU but general activation functions. We show that the distribution of the parameters converges to a spectrum of the ridgelet transform. This result provides a new insight into the characterization of the local minima of neural networks, and the theoretical background of an inductive bias theory based on lazy regimes. We confirm the visual resemblance between the parameter distribution trained by SGD, and the ridgelet spectrum calculated by numerical integration through numerical experiments with finite models.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803024,0
C,"Tanimoto, A; Sakai, T; Takenouchi, T; Kashima, H",,"Banerjee, A; Fukumizu, K",,"Tanimoto, Akira; Sakai, Tomoya; Takenouchi, Takashi; Kashima, Hisashi",,,Regret Minimization for Causal Inference on Large Treatment Space,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Predicting which action (treatment) will lead to a better outcome is a central task in decision support systems. To build a prediction model in real situations, learning from observational data with a sampling bias is a critical issue due to the lack of randomized controlled trial (RCT) data. To handle such biased observational data, recent efforts in causal inference and counterfactual machine learning have focused on debiased estimation of the potential outcomes on a binary action space and the difference between them, namely, the conditional average treatment effect. When it comes to a large action space (e.g., selecting an appropriate combination of medicines for a patient), however, the regression accuracy of the potential outcomes is no longer sufficient in practical terms to achieve a good decision-making performance. This is because a high mean accuracy on the large action space does not guarantee the nonexistence of a single potential outcome misestimation that misleads the whole decision. Our proposed loss minimizes the classification error of whether or not the action is relatively good for the individual target among all feasible actions, which further improves the decision-making performance, as we demonstrate. We also propose a network architecture and a regularizer that extracts a debiased representation not only from the feature but also from the biased action for better generalization on large action spaces. Extensive experiments on synthetic and semi-synthetic datasets demonstrate the superiority of our method for large combinatorial action spaces.",,,,,"Tanimoto, Akira/AAX-8864-2021",,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801019,0
C,"Vesseron, N; Redko, I; Laclau, C",,"Banerjee, A; Fukumizu, K",,"Vesseron, Nina; Redko, Ievgen; Laclau, Charlotte",,,Deep Neural Networks Are Congestion Games: From Loss Landscape to Wardrop Equilibrium and Beyond,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"The theoretical analysis of deep neural networks (DNN) is arguably among the most challenging research directions in machine learning (ML) right now, as it requires from scientists to lay novel statistical learning foundations to explain their behaviour in practice. While some success has been achieved recently in this endeavour, the question on whether DNNs can be analyzed using the tools from other scientific fields outside the ML community has not received the attention it may well have deserved. In this paper, we explore the interplay between DNNs and game theory (GT), and show how one can benefit from the classic readily available results from the latter when analyzing the former. In particular, we consider the widely studied class of congestion games, and illustrate their intrinsic relatedness to both linear and non-linear DNNs and to the properties of their loss surface. Beyond retrieving the state-of-the-art results from the literature, we argue that our work provides a very promising novel tool for analyzing the DNNs and support this claim by proposing concrete open problems that can advance significantly our understanding of DNNs when solved.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802016,0
C,"Wang, JX; Wiens, J; Lundberg, S",,"Banerjee, A; Fukumizu, K",,"Wang, Jiaxuan; Wiens, Jenna; Lundberg, Scott",,,Shapley Flow: A Graph-based Approach to Interpreting Model Predictions,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Many existing approaches for estimating feature importance are problematic because they ignore or hide dependencies among features. A causal graph, which encodes the relationships among input variables, can aid in assigning feature importance. However, current approaches that assign credit to nodes in the causal graph fail to explain the entire graph. In light of these limitations, we propose Shapley Flow, a novel approach to interpreting machine learning models. It considers the entire causal graph, and assigns credit to edges instead of treating nodes as the fundamental unit of credit assignment. Shapley Flow is the unique solution to a generalization of the Shapley value axioms for directed acyclic graphs. We demonstrate the benefit of using Shapley Flow to reason about the impact of a model's input on its output. In addition to maintaining insights from existing approaches, Shapley Flow extends the flat, set-based, view prevalent in game theory based explanation methods to a deeper, graph-based, view. This graph-based view enables users to understand the flow of importance through a system, and reason about potential interventions.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,721,+,,,,,,,,,,,,,,,,WOS:000659893800081,0
C,"Wu, YH; Bojchevski, A; Kuvshinov, A; Guennemann, S",,"Banerjee, A; Fukumizu, K",,"Wu, Yihan; Bojchevski, Aleksandar; Kuvshinov, Aleksei; Gunnemann, Stephan",,,Completing the Picture: Randomized Smoothing Suffers from the Curse of Dimensionality for a Large Family of Distributions,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Randomized smoothing is currently the most competitive technique for providing provable robustness guarantees. Since this approach is model-agnostic and inherently scalable we can certify arbitrary classifiers. Despite its success, recent works show that for a small class of i.i.d. distributions, the largest l(p) radius that can be certified using randomized smoothing decreases as O(1/d(1/2/p)) with dimension d for p > 2. We complete the picture and show that similar no-go results hold for the l(2) norm for a much more general family of distributions which are continuous and symmetric about the origin. Specifically, we calculate two different upper bounds of the l(2) certified radius which have a constant multiplier of order circle minus(1/d(1/2)). Moreover, we extend our results to l(p) (p > 2) certification with spherical symmetric distributions solidifying the limitations of randomized smoothing. We discuss the implications of our results for how accuracy and robustness are related, and why robust training with noise augmentation can alleviate some of the limitations in practice. We also show that on real-world data the gap between the certified radius and our upper bounds is small.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804053,0
C,"Xing, Y; Song, QF; Cheng, G",,"Banerjee, A; Fukumizu, K",,"Xing, Yue; Song, Qifan; Cheng, Guang",,,On the Generalization Properties of Adversarial Training,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Modern machine learning and deep learning models are shown to be vulnerable when testing data are slightly perturbed. Existing theoretical studies of adversarial training algorithms mostly focus on either adversarial training losses or local convergence properties. In contrast, this paper studies the generalization performance of a generic adversarial training algorithm. Specifically, we consider linear regression models and two-layer neural networks (with lazy training) using squared loss under low-dimensional and high-dimensional regimes. In the former regime, after overcoming the non-smoothness of adversarial training, the adversarial risk of the trained models can converge to the minimal adversarial risk. In the latter regime, we discover that data interpolation prevents the adversarially robust estimator from being consistent. Therefore, inspired by successes of the least absolute shrinkage and selection operator (LASSO), we incorporate the L-1 penalty in the high dimensional adversarial learning and show that it leads to consistent adversarially robust estimation. A series of numerical studies are conducted to demonstrate how the smoothness and L-1 penalization help improve the adversarial robustness of DNN models.",,,,,,"Xing, Yue/0000-0001-7723-0048",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,505,+,,,,,,,,,,,,,,,,WOS:000659893800057,0
C,"Zhang, JY; Khanna, R; Kyrillidis, A; Koyejo, O",,"Banerjee, A; Fukumizu, K",,"Zhang, Jacky Y.; Khanna, Rajiv; Kyrillidis, Anastasios; Koyejo, Oluwasanmi",,,Bayesian Coresets: Revisiting the Nonconvex Optimization Perspective,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Bayesian coresets have emerged as a promising approach for implementing scalable Bayesian inference. The Bayesian coreset problem involves selecting a (weighted) subset of the data samples, such that the posterior inference using the selected subset closely approximates the posterior inference using the full dataset. This manuscript revisits Bayesian coresets through the lens of sparsity constrained optimization. Leveraging recent advances in accelerated optimization methods, we propose and analyze a novel algorithm for coreset selection. We provide explicit convergence rate guarantees and present an empirical evaluation on a variety of benchmark datasets to highlight our proposed algorithm's superior performance compared to state-of-the-art on speed and accuracy.",,,,,"Khanna, Rajiv/GPK-2566-2022; Zhang, Jacky/HHS-9302-2022","Khanna, Rajiv/0000-0003-1314-3126; ",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803036,0
C,"Zhang, PY; Orvieto, A; Daneshmand, H; Hofmann, T; Smith, R",,"Banerjee, A; Fukumizu, K",,"Zhang, Peiyuan; Orvieto, Antonio; Daneshmand, Hadi; Hofmann, Thomas; Smith, Roy",,,Revisiting the Role of Euler Numerical Integration on Acceleration and Stability in Convex Optimization,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Viewing optimization methods as numerical integrators for ordinary differential equations (ODEs) provides a thought-provoking modern framework for studying accelerated first-order optimizers. In this literature, acceleration is often supposed to be linked to the quality of the integrator (accuracy, energy preservation, symplecticity). In this work, we propose a novel ordinary differential equation that questions this connection: both the explicit and the semi-implicit (a.k.a symplectic) Euler discretizations on this ODE lead to an accelerated algorithm for convex programming. Although semi-implicit methods are well-known in numerical analysis to enjoy many desirable features for the integration of physical systems, our findings show that these properties do not necessarily relate to acceleration.",,,,,"Smith, Roy S/C-7340-2009","Smith, Roy S/0000-0002-8139-4683",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804077,0
C,"Amortila, P; Precup, D; Panangaden, P; Bellemare, MG",,"Chiappa, S; Calandra, R",,"Amortila, Philip; Precup, Doina; Panangaden, Prakash; Bellemare, Marc G.",,,A Distributional Analysis of Sampling-Based Reinforcement Learning Algorithms,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We present a distributional approach to theoretical analyses of reinforcement learning algorithms for constant step-sizes. We demonstrate its effectiveness by presenting simple and unified proofs of convergence for a variety of commonly-used methods. We show that value-based methods such as TD(lambda) and Q-Learning have update rules which are contractive in the space of distributions of functions, thus establishing their exponentially fast convergence to a stationary distribution. We demonstrate that the stationary distribution obtained by any algorithm whose target is an expected Bellman update has a mean which is equal to the true value function. Furthermore, we establish that the distributions concentrate around their mean as the step-size shrinks. We further analyse the optimistic policy iteration algorithm, for which the contraction property does not hold, and formulate a probabilistic policy improvement property which entails the convergence of the algorithm.",,,,,"Panangaden, Prakash/GOH-1547-2022",,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,4357,4365,,,,,,,,,,,,,,,,WOS:000559931300013,0
C,"Ao, ZQ; Li, JL",,"Chiappa, S; Calandra, R",,"Ao, Ziqiao; Li, Jinglai",,,An approximate KLD based experimental design for models with intractable likelihoods,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Data collection is a critical step in statistical inference and data science, and the goal of statistical experimental design (ED) is to find the data collection setup that can provide most information for the inference. In this work we consider a special type of ED problems where the likelihoods are not available in a closed form. In this case, the popular information-theoretic Kullback-Leibler divergence (KLD) based design criterion can not be used directly, as it requires to evaluate the likelihood function. To address the issue, we derive a new utility function, which is a lower bound of the original KLD utility. This lower bound is expressed in terms of the summation of two or more entropies in the data space, and thus can be evaluated efficiently via entropy estimation methods. We provide several numerical examples to demonstrate the performance of the proposed method.",,,,,,"Li, Jinglai/0000-0001-7980-6901",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3241,3250,,,,,,,,,,,,,,,,WOS:000559931300015,0
C,"Barshan, E; Brunet, ME; Dziugaite, GK",,"Chiappa, S; Calandra, R",,"Barshan, Elnaz; Brunet, Marc-Etienne; Dziugaite, Gintare Karolina",,,RelatIF: Identifying Explanatory Training Examples via Relative Influence,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"In this work, we focus on the use of influence functions to identify relevant training examples that one might hope explain the predictions of a machine learning model. One shortcoming of influence functions is that the training examples deemed most influential are often outliers or mislabelled, making them poor choices for explanation. In order to address this shortcoming, we separate the role of global versus local influence. We introduce RelatIF, a new class of criteria for choosing relevant training examples by way of an optimization objective that places a constraint on global influence. RelatIF considers the local influence that an explanatory example has on a prediction relative to its global effects on the model. In empirical evaluations, we find that the examples returned by RelatIF are more intuitive when compared to those found using influence functions.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1899,1908,,,,,,,,,,,,,,,,WOS:000559931300025,0
C,"Carmona, CU; Nicholls, GK",,"Chiappa, S; Calandra, R",,"Carmona, Chris U.; Nicholls, Geoff K.",,,Semi-Modular Inference: enhanced learning in multi-modular models by tempering the influence of components,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Bayesian statistical inference loses predictive optimality when generative models are mis-specified. Working within an existing coherent loss-based generalisation of Bayesian inference, we show existing Modular/Cut-model inference is coherent, and write down a new family of Semi-Modular Inference (SMI) schemes, indexed by an influence parameter, with Bayesian inference and Cut-models as special cases. We give a meta-learning criterion and estimation procedure to choose the inference scheme. This returns Bayesian inference when there is no misspecification. The framework applies naturally to Multimodular models. Cut-model inference allows directed information flow from well-specified modules to misspecified modules, but not vice versa. An existing alternative power posterior method gives tunable but undirected control of information flow, improving prediction in some settings. In contrast, SMI allows tunable and directed information flow between modules. We illustrate our methods on two standard test cases from the literature and a motivating archaeological data set.",,,,,"Carmona, Chris/AAD-9556-2020; Carmona, Chris/L-9901-2018","Carmona, Chris/0000-0003-0224-4968; Nicholls, Geoff/0000-0002-1595-9041",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,4226,4234,,,,,,,,,,,,,,,,WOS:000559931300044,0
C,"Chen, SH; Devraj, AM; Busic, A; Meyn, S",,"Chiappa, S; Calandra, R",,"Chen, Shuhang; Devraj, Adithya M.; Busic, Ana; Meyn, Sean",,,Explicit Mean-Square Error Bounds for Monte-Carlo and Linear Stochastic Approximation,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"This paper concerns error bounds for recursive equations subject to Markovian disturbances. Motivating examples abound within the fields of Markov chain Monte Carlo (MCMC) and Reinforcement Learning (RL), and many of these algorithms can be interpreted as special cases of stochastic approximation (SA). It is argued that it is not possible in general to obtain a Hoeffding bound on the error sequence, even when the underlying Markov chain is reversible and geometrically ergodic, such as the M/M/1 queue. This is motivation for the focus on mean square error bounds for parameter estimates. It is shown that mean square error achieves the optimal rate of O(1/n), subject to conditions on the step-size sequence. Moreover, the exact constants in the rate are obtained, which is of great value in algorithm design.",,,,,,"Busic, Ana/0000-0002-4133-3739",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,4173,4182,,,,,,,,,,,,,,,,WOS:000559931300058,0
C,"Choromanski, K; Pacchiano, A; Parker-Holder, J; Tang, Y",,"Chiappa, S; Calandra, R",,"Choromanski, Krzysztof; Pacchiano, Aldo; Parker-Holder, Jack; Tang, Yunhao",,,Practical Nonisotropic Monte Carlo Sampling in High Dimensions via Determinantal Point Processes,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We propose a new class of practical structured methods for nonisotropic Monte Carlo (MC) sampling, called DPPMC, designed for high-dimensional nonisotropic distributions where samples are correlated to reduce the variance of the estimator via determinantal point processes. We successfully apply DPPMCs to high-dimensional problems involving nonisotropic distributions arising in guided evolution strategy (GES) methods for reinforcement learning (RL), CMA-ES techniques and trust region algorithms for blackbox optimization, improving state-of-the-art in all these settings. In particular, we show that DPPMCs drastically improve exploration profiles of the existing evolution strategy algorithms. We further confirm our results, analyzing random feature map estimators for Gaussian mixture kernels. We provide theoretical justification of our empirical results, showing a connection between DPPMCs and recently introduced structured orthogonal MC methods for isotropic distributions.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1363,1373,,,,,,,,,,,,,,,,WOS:000559931300062,0
C,"Eduardo, S; Nazabal, A; Williams, CKI; Sutton, C",,"Chiappa, S; Calandra, R",,"Eduardo, Simao; Nazabal, Alfredo; Williams, Christopher K. I.; Sutton, Charles",,,Robust Variational Autoencoders for Outlier Detection and Repair of Mixed-Type Data,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We focus on the problem of unsupervised cell outlier detection and repair in mixed-type tabular data. Traditional methods are concerned only with detecting which rows in the dataset are outliers. However, identifying which cells are corrupted in a specific row is an important problem in practice, and the very first step towards repairing them. We introduce the Robust Variational Autoencoder (RVAE), a deep generative model that learns the joint distribution of the clean data while identifying the outlier cells, allowing their imputation (repair). RVAE explicitly learns the probability of each cell being an outlier, balancing different likelihood models in the row outlier score, making the method suitable for outlier detection in mixed-type datasets. We show experimentally that not only RVAE performs better than several state-of-the-art methods in cell outlier detection and repair for tabular data, but also that is robust against the initial hyper-parameter selection.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,4056,4065,,,,,,,,,,,,,,,,WOS:000559931300082,0
C,"Fathony, R; Kolter, JZ",,"Chiappa, S; Calandra, R",,"Fathony, Rizal; Kolter, J. Zico",,,AP-Perf: Incorporating Generic Performance Metrics in Differentiable Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We propose a method that enables practitioners to conveniently incorporate custom non-decomposable performance metrics into differentiable learning pipelines, notably those based upon neural network architectures. Our approach is based on the recently developed adversarial prediction framework, a distributionally robust approach that optimizes a metric in the worst case given the statistical summary of the empirical distribution. We formulate a marginal distribution technique to reduce the complexity of optimizing the adversarial prediction formulation over a vast range of non-decomposable metrics. We demonstrate how easy it is to write and incorporate complex custom metrics using our provided tool. Finally, we show the effectiveness of our approach various classification tasks on tabular datasets from the UCI repository and benchmark datasets, as well as image classification tasks. The code for our proposed method is available at https://github.com/rizalzaf/AdversarialPrediction.jl.",,,,,"Fathony, Rizal/AAB-5588-2022",,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,4130,4139,,,,,,,,,,,,,,,,WOS:000559931300092,0
C,"Geffner, T; Domke, J",,"Chiappa, S; Calandra, R",,"Geffner, Tomas; Domke, Justin",,,"A Rule for Gradient Estimator Selection, with an Application to Variational Inference","INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Stochastic gradient descent (SGD) is the workhorse of modern machine learning. Sometimes, there are many different potential gradient estimators that can be used. When so, choosing the one with the best tradeoff between cost and variance is important. This paper analyzes the convergence rates of SGD as a function of time, rather than iterations. This results in a simple rule to select the estimator that leads to the best optimization convergence guarantee. This choice is the same for different variants of SGD, and with different assumptions about the objective (e.g. convexity or smoothness). Inspired by this principle, we propose a technique to automatically select an estimator when a finite pool of estimators is given. Then, we extend to infinite pools of estimators, where each one is indexed by control variate weights. Empirically, automatically choosing an estimator performs comparably to the best estimator chosen with hindsight.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1803,1811,,,,,,,,,,,,,,,,WOS:000559931301016,0
C,"Grover, D; Basu, D; Dimitrakakis, C",,"Chiappa, S; Calandra, R",,"Grover, Divya; Basu, Debabrota; Dimitrakakis, Christos",,,"Bayesian Reinforcement Learning via Deep, Sparse Sampling","INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We address the problem of Bayesian reinforcement learning using efficient model-based online planning. We propose an optimism-free Bayes-adaptive algorithm to induce deeper and sparser exploration with a theoretical bound on its performance relative to the Bayes optimal as well as lower computational complexity. The main novelty is the use of a candidate policy generator, to generate long-term options in the planning tree (over beliefs), which allows us to create much sparser and deeper trees. Experimental results on different environments show that in comparison to the state-of-the-art, our algorithm is both computationally more efficient, and obtains significantly higher reward over time in discrete environments.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3036,3044,,,,,,,,,,,,,,,,WOS:000559931301026,0
C,"Han, CZ; Rao, N; Sorokina, D; Subbian, K",,"Chiappa, S; Calandra, R",,"Han, Cuize; Rao, Nikhil; Sorokina, Daria; Subbian, Karthik",,,Scalable Feature Selection for (Multitask) Gradient Boosted Trees,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Gradient Boosted Decision Trees (GBDTs) are widely used for building ranking and relevance models in search and recommendation. Considerations such as latency and interpretability dictate the use of as few features as possible to train these models. Feature selection in GBDT models typically involves heuristically ranking the features by importance and selecting the top few, or by performing a full backward feature elimination routine. On-the-fly feature selection methods proposed previously scale suboptimally with the number of features, which can be daunting in high dimensional settings. We develop a scalable forward feature selection variant for GBDT, via a novel group testing procedure that works well in high dimensions, and enjoys favorable theoretical performance and computational guarantees. We show via extensive experiments on both public and proprietary datasets that the proposed method offers significant speedups in training time, while being as competitive as existing GBDT methods in terms of model performance metrics. We also extend the method to the multitask setting, allowing the practitioner to select common features across tasks, as well as selecting task-specific features.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,885,893,,,,,,,,,,,,,,,,WOS:000559931301032,0
C,"Lee, HS; Shen, C; Jordon, J; van der Schaar, M",,"Chiappa, S; Calandra, R",,"Lee, Hyun-Suk; Shen, Cong; Jordon, James; van der Schaar, Mihaela",,,Contextual Constrained Learning for Dose-Finding Clinical Trials,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Clinical trials in the medical domain are constrained by budgets. The number of patients that can be recruited is therefore limited. When a patient population is heterogeneous, this creates difficulties in learning subgroup specific responses to a particular drug and especially for a variety of dosages. In addition, patient recruitment can be difficult by the fact that clinical trials do not aim to provide a benefit to any given patient in the trial. In this paper, we propose C3T-Budget, a contextual constrained clinical trial algorithm for dose-finding under both budget and safety constraints. The algorithm aims to maximize drug efficacy within the clinical trial while also learning about the drug being tested. C3T-Budget recruits patients with consideration of the remaining budget, the remaining time, and the characteristics of each group, such as the population distribution, estimated expected efficacy, and estimation credibility. In addition, the algorithm aims to avoid unsafe dosages. These characteristics are further illustrated in a simulated clinical trial study, which corroborates the theoretical analysis and demonstrates an efficient budget usage as well as a balanced learning-treatment trade-off.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2645,2653,,,,,,,,,,,,,,,,WOS:000559931301095,0
C,"Lee, M; Bindel, D; Mimno, D",,"Chiappa, S; Calandra, R",,"Lee, Moontae; Bindel, David; Mimno, David",,,Prior-aware Composition Inference for Spectral Topic Models,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Spectral algorithms operate on matrices or tensors of word co-occurrence to learn latent topics. These approaches remove the dependence on the original documents and produce substantial gains in efficiency with provable inference, but at a cost: the models can no longer infer any information about individual documents. Thresholded Linear Inverse is developed to learn document-specific topic compositions, but its linear characteristics limit the inference quality without considering any prior information on topic distributions. We propose two novel estimation methods that respect previously unclear prior structures of spectral topic models. Experiments on a variety of synthetic to real collections demonstrate that our Prior-Aware Dual Decomposition outperforms the baseline method, whereas our Prior-Aware Manifold Iteration performs even better on short realistic data.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,4258,4267,,,,,,,,,,,,,,,,WOS:000559931301097,0
C,"Maeda, TN; Shimizu, SH",,"Chiappa, S; Calandra, R",,"Maeda, Takashi Nicholas; Shimizu, Shohei",,,RCD: Repetitive causal discovery of linear non-Gaussian acyclic models with latent confounders,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Causal discovery from data affected by latent confounders is an important and difficult challenge. Causal functional model-based approaches have not been used to present variables whose relationships are affected by latent based paper based confounders, while some constraint methods can present them. This proposes a causal functional model-method called repetitive causal discovery (RCD) to discover the causal structure of observed variables affected by latent confounders. RCD repeats inferring the causal directions between a small number of observed variables and determines whether the relationships are affected by latent confounders. RCD finally produces a causal graph where a bi-directed arrow indicates the pair of variables that have the same latent confounders, and a directed arrow indicates the causal direction of a pair of variables that are not affected by the same latent confounder. The results of experimental validation using simulated data and real-world data confirmed that RCD is effective in identifying latent confounders and causal directions between observed variables.",,,,,"Shimizu, Shohei/B-4425-2010; Maeda, Takashi Nicholas/ABE-2733-2020","Shimizu, Shohei/0000-0002-1931-0733; Maeda, Takashi/0000-0003-2419-9280",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,735,744,,,,,,,,,,,,,,,,WOS:000559931302026,0
C,"Yue, YG; Tang, YH; Yin, MZ; Zhou, MY",,"Chiappa, S; Calandra, R",,"Yue, Yuguang; Tang, Yunhao; Yin, Mingzhang; Zhou, Mingyuan",,,Discrete Action On-Policy Learning with Action-Value Critic,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Reinforcement learning (RL) in discrete action space is ubiquitous in real-world applications, but its complexity grows exponentially with the action-space dimension, making it challenging to apply existing on-policy gradient based deep RL algorithms efficiently. To effectively operate in multidimensional discrete action spaces, we construct a critic to estimate action-value functions, apply it on correlated actions, and combine these critic estimated action values to control the variance of gradient estimation. We follow rigorous statistical analysis to design how to generate and combine these correlated actions, and how to sparsify the gradients by shutting down the contributions from certain dimensions. These efforts result in a new discrete action on-policy RL algorithm that empirically outperforms related on-policy algorithms relying on variance control techniques. We demonstrate these properties on OpenAI Gym benchmark tasks, and illustrate how discretizing the action space could benefit the exploration phase and hence facilitate convergence to a better local optimal solution thanks to the flexibility of discrete policy.",,,,,"Yin, Mingzhang/R-5702-2018; Zhou, Mingyuan/AAE-8717-2021",,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1977,1986,,,,,,,,,,,,,,,,WOS:000559931304001,0
C,"Zantedeschi, V; Bellet, A; Tommasi, M",,"Chiappa, S; Calandra, R",,"Zantedeschi, Valentina; Bellet, Aurelien; Tommasi, Marc",,,Fully Decentralized Joint Learning of Personalized Models and Collaboration Graphs,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We consider the fully decentralized machine learning scenario where many users with personal datasets collaborate to learn models through local peer-to-peer exchanges, without a central coordinator. We propose to train personalized models that leverage a collaboration graph describing the relationships between user personal tasks, which we learn jointly with the models. Our fully decentralized optimization procedure alternates between training nonlinear models given the graph in a greedy boosting manner, and updating the collaboration graph (with controlled sparsity) given the models. Throughout the process, users exchange messages only with a small number of peers (their direct neighbors when updating the models, and a few random users when updating the graph), ensuring that the procedure naturally scales with the number of users. Overall, our approach is communication-efficient and avoids exchanging personal data. We provide an extensive analysis of the convergence rate, memory and communication complexity of our approach, and demonstrate its benefits compared to competing techniques on synthetic and real datasets.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,864,873,,,,,,,,,,,,,,,,WOS:000559931304005,0
C,"Zhu, WN; Kairouz, P; McMahan, B; Sun, HC; Li, W",,"Chiappa, S; Calandra, R",,"Zhu, Wennan; Kairouz, Peter; McMahan, Brendan; Sun, Haicheng; Li, Wei",,,Federated Heavy Hitters Discovery with Differential Privacy,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"The discovery of heavy hitters (most frequent items) in user-generated data streams drives improvements in the app and web ecosystems, but can incur substantial privacy risks if not done with care. To address these risks, we propose a distributed and privacy-preserving algorithm for discovering the heavy hitters in a population of user-generated data streams. We leverage the sampling and thresholding properties of our distributed algorithm to prove that it is inherently differentially private, without requiring additional noise. We also examine the trade-off between privacy and utility, and show that our algorithm provides excellent utility while also achieving strong privacy guarantees. A significant advantage of this approach is that it eliminates the need to centralize raw data while also avoiding the significant loss in utility incurred by local differential privacy. We validate our findings both theoretically, using worst-case analyses, and practically, using a Twitter dataset with 1.6M tweets and over 650k users. Finally, we carefully compare our approach to Apple's local differential privacy method for discovering heavy hitters.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3837,3846,,,,,,,,,,,,,,,,WOS:000559931304024,0
C,"Dedieu, A",,"Chaudhuri, K; Sugiyama, M",,"Dedieu, Antoine",,,Error bounds for sparse classifiers in high-dimensions,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We prove an L2 recovery bound for a family of sparse estimators defined as minimizers of some empirical loss functions which include hinge loss and logistic loss. More precisely, we achieve an upper-bound for coefficients estimation scaling as (k*/n) log(p/k*): n x p is the size of the design matrix and k* the dimension of the theoretical loss minimizer. This is done under standard assumptions, for which we derive stronger versions of a cone condition and a restricted strong convexity. Our bound holds with high probability and in expectation and applies to an L1-regularized estimator and to a recently introduced Slope estimator, which we generalize for classification problems. Slope presents the advantage of adapting to unknown sparsity. Thus, we propose a tractable proximal algorithm to compute it and assess its empirical performance. Our results match the best existing bounds for classification and regression problems. (1)",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,48,56,,,,,,,,,,,,,,,,WOS:000509687900006,0
C,"Falorsi, L; de Haan, P; Davidson, TR; Forre, P",,"Chaudhuri, K; Sugiyama, M",,"Falorsi, Luca; de Haan, Pim; Davidson, Tim R.; Forre, Patrick",,,Reparameterizing Distributions on Lie Groups,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Reparameterizable densities are an important way to learn probability distributions in a deep learning setting. For many distributions it is possible to create low-variance gradient estimators by utilizing a 'reparameterization trick'. Due to the absence of a general reparameterization trick, much research has recently been devoted to extend the number of reparameterizable distributional families. Unfortunately, this research has primarily focused on distributions defined in Euclidean space, ruling out the usage of one of the most influential class of spaces with non-trivial topologies: Lie groups. In this work we define a general framework to create reparameterizable densities on arbitrary Lie groups, and provide a detailed practitioners guide to further the ease of usage. We demonstrate how to create complex and multimodal distributions on the well known oriented group of 3D rotations, SO(3), using normalizing flows. Our experiments on applying such distributions in a Bayesian setting for pose estimation on objects with discrete and continuous symmetries, showcase their necessity in achieving realistic uncertainty estimates.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687903031,0
C,"Frongillo, R; Mehta, NA; Morgan, T; Waggoner, B",,"Chaudhuri, K; Sugiyama, M",,"Frongillo, Rafael; Mehta, Nishant A.; Morgan, Tom; Waggoner, Bo",,,Multi-Observation Regression,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Given a data set of (x, y) pairs, a common learning task is to fit a model predicting y (a label or dependent variable) conditioned on x. This paper considers the similar but much less-understood problem of modeling higher-order statistics of y's distribution conditioned on x. Such statistics are often challenging to estimate using traditional empirical risk minimization (ERM) approaches. We develop and theoretically analyze an ERM-like approach with multi-observation loss functions. We propose four algorithms formalizing the concept of ERM for this problem, two of which have statistical guarantees in settings allowing both slow and fast convergence rates, but which are out-performed empirically by the other two. Empirical results illustrate potential practicality of these algorithms in low dimensions and significant improvement over standard approaches in some settings.",,,,,,"Frongillo, Rafael/0000-0002-0170-7572; Waggoner, Bo/0000-0002-1366-1065",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902076,0
C,"Gimenez, JR; Ghorbani, A; Zou, J",,"Chaudhuri, K; Sugiyama, M",,"Gimenez, Jaime Roquero; Ghorbani, Amirata; Zou, James",,,Knockoffs for the Mass: New Feature Importance Statistics with False Discovery Guarantees,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"An important problem in machine learning and statistics is to identify features that causally affect the outcome. This is often impossible to do from purely observational data, and a natural relaxation is to identify features that are correlated with the outcome even conditioned on all other observed features. For example, we want to identify that smoking really is correlated with cancer conditioned on demographics. The knockoff procedure is a recent breakthrough in statistics that, in theory, can identify truly correlated features while guaranteeing that false discovery rate is controlled. The idea is to create synthetic data-knockoffs-that capture correlations among the features. However, there are substantial computational and practical challenges to generating and using knockoffs. This paper makes several key advances that enable knockoff application to be more efficient and powerful. We develop an efficient algorithm to generate valid knockoffs from Bayesian Networks. Then we systematically evaluate knockoff test statistics and develop new statistics with improved power. The paper combines new mathematical guarantees with systematic experiments on real and synthetic data.",,,,,"Zhang, James/HHS-8616-2022; Ghorbani, Amirata/AFU-2782-2022",,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902018,0
C,"Grover, A; Ermon, S",,"Chaudhuri, K; Sugiyama, M",,"Grover, Aditya; Ermon, Stefano",,,Uncertainty Autoencoders: Learning Compressed Representations via Variational Information Maximization,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Compressed sensing techniques enable efficient acquisition and recovery of sparse, high-dimensional data signals via low-dimensional projections. In this work, we propose Uncertainty Autoencoders, a learning framework for unsupervised representation learning inspired by compressed sensing. We treat the low-dimensional projections as noisy latent representations of an autoencoder and directly learn both the acquisition (i.e., encoding) and amortized recovery (i.e., decoding) procedures. Our learning objective optimizes for a tractable variational lower bound to the mutual information between the datapoints and the latent representations. We show how our framework provides a unified treatment to several lines of research in dimensionality reduction, compressed sensing, and generative modeling. Empirically, we demonstrate a 32% improvement on average over competing approaches for the task of statistical compressed sensing of high-dimensional datasets.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902058,0
C,"Jia, RX; Dao, D; Wang, BX; Hubis, FA; Hynes, N; Gurel, NM; Li, B; Zhang, C; Song, D; Spanos, C",,"Chaudhuri, K; Sugiyama, M",,"Jia, Ruoxi; Dao, David; Wang, Boxin; Hubis, Frances Ann; Hynes, Nick; Gurel, Nezihe Merve; Li, Bo; Zhang, Ce; Song, Dawn; Spanos, Costas",,,Towards Efficient Data Valuation Based on the Shapley Value,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"'`Hoer anteh is my (Jul creasiirgly cornrrion question p zations and individuals alike. :? is are riled by organin answer to this question could allow, for Inst distributing profits among multipl trihutors and deterunnurg prospective compensation when data breaches happen. In this paper, we study the problem of du,tu, valuation by utilizing the Shapley value, a popular notion of value which originated in coopoera tive game theory. The Shapley value defines a unique payoff scheme that satisfies many idea'ata for the notion of data, value. IlowtimC to ('011 puce. 7)Onenrs challenge, wee propose a repertoire of efficient algorithms for approximating the Shipley value. We also demonstrate the value of each training instance for various benchmark datasets.",,,,,,"Zhang, Ce/0000-0002-8105-7505; Jia, Ruoxi/0000-0001-9662-9556",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901022,0
C,"Kelly, C; Sarkhel, S; Venugopal, D",,"Chaudhuri, K; Sugiyama, M",,"Kelly, Craig; Sarkhel, Somdeb; Venugopal, Deepak",,,Adaptive Rao-Blackwellisation in Gibbs Sampling for Probabilistic Graphical Models,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Rao-Blackwellisation is a technique that provably improves the performance of Gibbs sampling by summing-out variables from the PGM. However, collapsing variables is computationally expensive, since it changes the PGM structure introducing factors whose size is dependent upon the Markov blanket of the variable. Therefore, collapsing out several variables jointly is typically intractable in arbitrary PGM structures. In this paper, we propose an adaptive approach for Rao-Blackwellisation, where we add parallel Markov chains defined over different collapsed PGM structures. The collapsed variables are chosen based on their convergence diagnostics. However, adding a new chain requires burn-in, thus wasting samples. To address this, we initialize the new chains from a mean field approximation for the distribution, that improves over time, thus reducing the burn-in period. Our experiments on several UAI benchmarks shows that our approach is more accurate than state-of-the-art inference systems such as Merlin that implements algorithms that have previously won the UAI inference challenge.",,,,,,"Kelly, Craig/0000-0002-9356-9444",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902098,0
C,"Kozuno, T; Uchibe, E; Doya, K",,"Chaudhuri, K; Sugiyama, M",,"Kozuno, Tadashi; Uchibe, Eiji; Doya, Kenji",,,Theoretical Analysis of Efficiency and Robustness of Softmax and Gap-Increasing Operators in Reinforcement Learning,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"In this paper, we propose and analyze conservative value iteration, which unifies value iteration, soft value iteration, advantage learning, and dynamic policy programming. Our analysis shows that algorithms using a combination of gap-increasing and max operators are resilient to stochastic errors, but not to non-stochastic errors. In contrast, algorithms using a softmax operator without a gap-increasing operator are less susceptible to all types of errors, but may display poor asymptotic performance. Algorithms using a combination of gap-increasing and softmax operators are much more effective and may asymptotically outperform algorithms with the max operator. Not only do these theoretical results provide a deep understanding of various reinforcement learning algorithms, but they also highlight the effectiveness of gap-increasing operators, as well as the limitations of traditional greedy value updates by the max operator.",,,,,"Kozuno, Tadashi/C-6482-2015",,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687903005,0
C,"Ghoshal, A; Honorio, J",,"Storkey, A; PerezCruz, F",,"Ghoshal, Asish; Honorio, Jean",,,Learning Sparse Polymatrix Games in Polynomial Time and Sample Complexity,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"We consider the problem of learning sparse polymatrix games from observations of strategic interactions. We show that a polynomial time method based on '1,2-group regularized logistic regression recovers a game, whose Nash equilibria are the epsilon-Nash equilibria of the game from which the data was generated (true game), in O(m(4)d(4) log(pd)) samples of strategy profiles - where m is the maximum number of pure strategies of a player, p is the number of players, and d is the maximum degree of the game graph. Under slightly more stringent separability conditions on the payoff matrices of the true game, we show that our method learns a game with the exact same Nash equilibria as the true game. We also show that Omega(d log(pm)) samples are necessary for any method to consistently recover a game, with the same Nash-equilibria as the true game, from observations of strategic interactions. We verify our theoretical results through simulation experiments.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300155,0
C,"Liu, SJ; Chen, J; Chen, PY; Hero, AO",,"Storkey, A; PerezCruz, F",,"Liu, Sijia; Chen, Jie; Chen, Pin-Yu; Hero, Alfred O.",,,Zeroth-Order Online Alternating Direction Method of Multipliers: Convergence Analysis and Applications,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"In this paper, we design and analyze a new zeroth-order online algorithm, namely, the zeroth-order online alternating direction method of multipliers (ZOO-ADMM), which enjoys dual advantages of being gradientfree operation and employing the ADMM to accommodate complex structured regularizers. Compared to the first-order gradientbased online algorithm, we show that ZOOADMM requires A/m times more iterations, leading to a convergence rate of 0(\m/ \j), where m is the number of optimization variables, and T is the number of iterations. To accelerate ZOO-ADMM, we propose two minibatch strategies: gradient sample averaging and observation averaging, resulting in an improved convergence rate of O(/1 q-lmNT), where q is the minibatch size. In addition to convergence analysis, we also demonstrate ZOO-ADMM to applications in signal processing, statistics, and machine learning.",,,,,"cai, jie/HHS-0606-2022","Hero, Alfred/0000-0002-2531-9670",,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300031,0
C,"Urteaga, I; Wiggins, CH",,"Storkey, A; PerezCruz, F",,"Urteaga, Inigo; Wiggins, Chris H.",,,Variational inference for the multi-armed contextual bandit,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"In many biomedical, science, and engineering problems, one must sequentially decide which action to take next so as to maximize rewards. One general class of algorithms for optimizing interactions with the world, while simultaneously learning how the world operates, is the multi-armed bandit setting and, in particular, the contextual bandit case. In this setting, for each executed action, one observes rewards that are dependent on a given 'context', available at each interaction with the world. The Thompson sampling algorithm has recently been shown to enjoy provable optimality properties for this set of problems, and to perform well in real-world settings. It facilitates generative and interpretable modeling of the problem at hand. Nevertheless, the design and complexity of the model limit its application, since one must both sample from the distributions modeled and calculate their expected rewards. We here show how these limitations can be overcome using variational inference to approximate complex models, applying to the reinforcement learning case advances developed for the inference case in the machine learning community over the past two decades. We consider contextual multi-armed bandit applications where the true reward distribution is unknown and complex, which we approximate with a mixture model whose parameters are inferred via variational inference. We show how the proposed variational Thompson sampling approach is accurate in approximating the true distribution, and attains reduced regrets even with complex reward distributions. The proposed algorithm is valuable for practical scenarios where restrictive modeling assumptions are undesirable.",,,,,"Urteaga, Iigo/AAW-5939-2020","Urteaga, Iigo/0000-0003-3656-0037",,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300074,0
C,"Xu, LY; Honda, J; Sugiyama, M",,"Storkey, A; PerezCruz, F",,"Xu, Liyuan; Honda, Junya; Sugiyama, Masashi",,,A fully adaptive algorithm for pure exploration in linear bandits,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"We propose the first fully-adaptive algorithm for pure exploration in linear bandits-the task to find the arm with the largest expected reward, which depends on an unknown parameter linearly. While existing methods partially or entirely fix sequences of arm selections before observing rewards, our method adaptively changes the arm selection strategy based on past observations at each round. We show our sample complexity matches the achievable lower bound up to a constant factor in an extreme case. Furthermore, we evaluate the performance of the methods by simulations based on both synthetic setting and real-world data, in which our method shows vast improvement over existing ones.",,,,,"Sugiyama, Masashi/AEO-1176-2022","Sugiyama, Masashi/0000-0001-6658-6743",,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300089,0
C,"Xu, P; Wang, TH; Gu, QQ",,"Storkey, A; PerezCruz, F",,"Xu, Pan; Wang, Tianhao; Gu, Quanquan",,,Accelerated Stochastic Mirror Descent: From Continuous-time Dynamics to Discrete-time Algorithms,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"We present a new framework to analyze accelerated stochastic mirror descent through the lens of continuous-time stochastic dynamic systems. It enables us to design new algorithms, and perform a unified and simple analysis of the convergence rates of these algorithms. More specifically, under this framework, we provide a Lyapunov function based analysis for the continuous-time stochastic dynamics, as well as several new discrete-time algorithms derived from the continuous-time dynamics. We show that for general convex objective functions, the derived discrete-time algorithms attain the optimal convergence rate. Empirical experiments corroborate our theory.",,,,,"Xu, Pan/AAH-3620-2019; X, Pan/GVS-4402-2022","Xu, Pan/0000-0002-2559-8622; ",,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300114,0
C,"Yang, YZ",,"Storkey, A; PerezCruz, F",,"Yang, Yingzhen",,,Dimensionality Reduced l(0) -Sparse Subspace Clustering,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Subspace clustering partitions the data that lie on a union of subspaces. l(0)-Sparse Subspace Clustering (l(0)-SSC), which belongs to the subspace clustering methods with sparsity prior, guarantees the correctness of subspace clustering under less restrictive assumptions compared to its l(1) counterpart such as Sparse Subspace Clustering (SSC) [1] with demonstrated effectiveness in practice. In this paper, we present Dimensionality Reduced l(0)-Sparse Subspace Clustering (DR-l(0)-SSC). DR-l(0)-SSC first projects the data onto a lower dimensional space by linear transformation, then performs l(0)- SSC on the dimensionality reduced data. The correctness of DR-l(0)-SSC in terms of the subspace detection property is proved, therefore DR-l(0)-SSC recovers the underlying subspace structure in the original data from the dimensionality reduced data. Experimental results demonstrate the effectiveness of DR-l(0)-SSC.",,,,,"Yang, Yingzhen/AAU-6048-2020",,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300216,0
C,"Zhao, H; Raiy, P; Du, L; Buntine, W",,"Storkey, A; PerezCruz, F",,"Zhao, He; Raiy, Piyush; Du, Lan; Buntine, Wray",,,"Bayesian Multi-label Learning with Sparse Features and Labels, and Label Co-occurrences","INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"We present a probabilistic, fully Bayesian framework for multi-label learning. Our framework is based on the idea of learning a joint low-rank embedding of the label matrix and the label co-occurrence matrix. The proposed framework has the following appealing aspects: (1) It leverages the sparsity in the label matrix and the feature matrix, which results in very efficient inference, especially for sparse datasets, commonly encountered in multi-label learning problems, and (2) By effectively utilizing the label co-occurrence information, the model yields improved prediction accuracies, especially in the case where the amount of training data is low and/or the label matrix has a significant fraction of missing labels. Our framework enjoys full local conjugacy and admits a simple inference procedure via a scalable Gibbs sampler. We report experimental results on a number of benchmark datasets, on which it outperforms several state-of-the-art multi-label learning models.",,,,,"Du, Lan/AAY-1249-2021; Zhao, He/GON-4192-2022","Zhao, He/0000-0003-0894-2265",,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300203,0
C,"Bogunovic, I; Scarlett, J; Cevher, V",,"Gretton, A; Robert, CC",,"Bogunovic, Ilija; Scarlett, Jonathan; Cevher, Volkan",,,Time-Varying Gaussian Process Bandit Optimization,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"We consider the sequential Bayesian optimization problem with bandit feedback, adopting a formulation that allows for the reward function to vary with time. We model the reward function using a Gaussian process whose evolution obeys a simple Markov model. We introduce two natural extensions of the classical Gaussian process upper confidence bound (GP-UCB) algorithm. The first, R-GP-UCB, resets GP-UCB at regular intervals. The second, TV-GP-UCB, instead forgets about old data in a smooth fashion. Our main contribution comprises of novel regret bounds for these algorithms, providing an explicit characterization of the trade-o. between the time horizon and the rate at which the function varies. We illustrate the performance of the algorithms on both synthetic and real data, and we find the gradual forgetting of TV-GP-UCB to perform favorably compared to the sharp resetting of R-GP-UCB. Moreover, both algorithms significantly outperform classical GP-UCB, since it treats stale and fresh data equally.",,,,,"Scarlett, Jonathan/AGK-0892-2022",,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,314,323,,,,,,,,,,,,,,,,WOS:000508662100035,0
C,"Cucuringu, M; Koutis, I; Chawla, S; Miller, G; Peng, R",,"Gretton, A; Robert, CC",,"Cucuringu, Mihai; Koutis, Ioannis; Chawla, Sanjay; Miller, Gary; Peng, Richard",,,Simple and Scalable Constrained Clustering: A Generalized Spectral Method,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"We present a simple spectral approach to the well-studied constrained clustering problem. It captures constrained clustering as a generalized eigenvalue problem in which both matrices are graph Laplacians. The algorithm works in nearly-linear time and provides concrete guarantees for the quality of the clusters, at least for the case of 2-way partitioning. In practice this translates to a very fast implementation that consistently outperforms existing spectral approaches both in speed and quality.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,445,454,,,,,,,,,,,,,,,,WOS:000508662100049,0
C,"Dzunic, Z; Fisher, JW",,"Kaski, S; Corander, J",,"Dzunic, Zoran; Fisher, John W., III",,,Bayesian Switching Interaction Analysis Under Uncertainty,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"We introduce a Bayesian discrete-time framework for switching-interaction analysis under uncertainty, in which latent interactions, switching pattern and signal states and dynamics are inferred from noisy (and possibly missing) observations of these signals. We propose reasoning over full posterior distribution of these latent variables as a means of combating and characterizing uncertainty. This approach also allows for answering a variety of questions probabilistically, which is suitable for exploratory pattern discovery and post-analysis by human experts. This framework is based on a fully-Bayesian learning of the structure of a switching dynamic Bayesian network (DBN) and utilizes a state-space approach to allow for noisy observations and missing data. It generalizes the autoregressive switching interaction model of Siracusa et al. [1], which does not allow observation noise, and the switching linear dynamic system model of Fox et al. [2], which does not infer interactions among signals. Posterior samples are obtained via a Gibbs sampling procedure, which is particularly efficient in the case of linear Gaussian dynamics and observation models. We demonstrate the utility of our framework on a controlled human-generated data, and climate data.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,220,228,,,,,,,,,,,,,,,,WOS:000508355800025,0
C,"Lang, D; Hogg, DW; Scholkopf, B",,"Kaski, S; Corander, J",,"Lang, Dustin; Hogg, David W.; Schoelkopf, Bernhard",,,Towards building a Crowd-Sourced Sky Map,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"We describe a system that builds a high dynamic-range and wide-angle image of the night sky by combining a large set of input images. The method makes use of pixel-rank information in the individual input images to improve a consensus pixel rank in the combined image. Because it only makes use of ranks and the complexity of the algorithm is linear in the number of images, the method is useful for large sets of uncalibrated images that might have undergone unknown non-linear tone mapping transformations for visualization or aesthetic reasons. We apply the method to images of the night sky (of unknown provenance) discovered on the Web. The method permits discovery of astronomical objects or features that are not visible in any of the input images taken individually. More importantly, however, it permits scientific exploitation of a huge source of astronomical images that would not be available to astronomical research without our automatic system.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,549,557,,,,,,,,,,,,,,,,WOS:000508355800061,0
C,"Picheny, V",,"Kaski, S; Corander, J",,"Picheny, Victor",,,A Stepwise uncertainty reduction approach to constrained global optimization,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"Using statistical emulators to guide sequential evaluations of complex computer experiments is now a well-established practice. When a model provides multiple outputs, a typical objective is to optimize one of the outputs with constraints (for instance, a threshold not to exceed) on the values of the other outputs. We propose here a new optimization strategy based on the stepwise uncertainty reduction paradigm, which offers an efficient trade-off between exploration and local search near the boundaries. The strategy is illustrated on numerical examples.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,787,795,,,,,,,,,,,,,,,,WOS:000508355800087,0
C,"Wilkinson, RD",,"Kaski, S; Corander, J",,"Wilkinson, Richard D.",,,Accelerating ABC methods using Gaussian processes,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"Approximate Bayesian computation (ABC) methods are used to approximate posterior distributions using simulation rather than likelihood calculations. We introduce Gaussian process (GP) accelerated ABC, which we show can significantly reduce the number of simulations required. As computational resource is usually the main determinant of accuracy in ABC, GP-accelerated methods can thus enable more accurate inference in some models. GP models of the unknown log-likelihood function are used to exploit continuity and smoothness, reducing the required computation. We use a sequence of models that increase in accuracy, using intermediate models to rule out regions of the parameter space as implausible. The methods will not be suitable for all problems, but when they can be used, can result in significant computational savings. For the Ricker model, we are able to achieve accurate approximations to the posterior distribution using a factor of 100 fewer simulator evaluations than comparable Monte Carlo approaches, and for a population genetics model we are able to approximate the exact posterior for the first time.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,1015,1023,,,,,,,,,,,,,,,,WOS:000508355800112,0
C,"Afshar, HM; Oliveira, R; Cripps, S",,"Banerjee, A; Fukumizu, K",,"Afshar, Hadi Mohasel; Oliveira, Rafael; Cripps, Sally",,,Non-Volume Preserving Hamiltonian Monte Carlo and No-U-Turn Samplers,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Volume preservation is usually regarded as a necessary property for the leapfrog transition functions that are used in Hamiltonian Monte Carlo (HMC) and No-U-Turn (NUTS) samplers to guarantee convergence to the target distribution. In this work we rigorously prove that with minimal algorithmic modifications, both HMC and NUTS can be combined with transition functions that are not necessarily volume preserving. In light of these results, we propose a non-volume preserving transition function that conserves the Hamiltonian better than the baseline leapfrog mechanism, on piecewise-continuous distributions. The resulting samplers do not require any assumptions on the geometry of the discontinuity boundaries, and our experimental results show a significant improvement upon traditional HMC and NUTS.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802006,0
C,"Blondel, M; Mensch, A; Vert, JP",,"Banerjee, A; Fukumizu, K",,"Blondel, Mathieu; Mensch, Arthur; Vert, Jean-Philippe",,,Differentiable Divergences Between Time Series,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Computing the discrepancy between time series of variable sizes is notoriously challenging. While dynamic time warping (DTW) is popularly used for this purpose, it is not differentiable everywhere and is known to lead to bad local optima when used as a loss. Soft-DTW addresses these issues, but it is not a positive definite divergence: due to the bias introduced by entropic regularization, it can be negative and it is not minimized when the time series are equal. We propose in this paper a new divergence, dubbed soft-DTW divergence, which aims to correct these issues. We study its properties; in particular, under conditions on the ground cost, we show that it is a valid divergence: it is non-negative and minimized if and only if the two time series are equal. We also propose a new \sharp variant by further removing entropic bias. We showcase our divergences on time series averaging and demonstrate significant accuracy improvements compared to both DTW and soft-DTW on 84 time series classification datasets.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804063,0
C,"Chen, MF; Cohen-Wang, B; Mussmann, S; Sala, F; Re, C",,"Banerjee, A; Fukumizu, K",,"Chen, Mayee F.; Cohen-Wang, Benjamin; Mussmann, Stephen; Sala, Frederic; Re, Christopher",,,Comparing the Value of Labeled and Unlabeled Data in Method-of-Moments Latent Variable Estimation,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Labeling data for modern machine learning is expensive and time-consuming. Latent variable models can be used to infer labels from weaker, easier-to-acquire sources operating on unlabeled data. Such models can also be trained using labeled data, presenting a key question: should a user invest in few labeled or many unlabeled points? We answer this via a framework centered on model misspecification in method-of-moments latent variable estimation. Our core result is a biasvariance decomposition of the generalization error, which shows that the unlabeled-only approach incurs additional bias under misspecification. We then introduce a correction that provably removes this bias in certain cases. We apply our decomposition framework to three scenarios-well-specified, misspecified, and corrected models-to 1) choose between labeled and unlabeled data and 2) learn from their combination. We observe theoretically and with synthetic experiments that for well-specified models, labeled points are worth a constant factor more than unlabeled points. With misspecification, however, their relative value is higher due to the additional bias but can be reduced with correction. We also apply our approach to study real-world weak supervision techniques for dataset construction.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803092,0
C,"Chowdhury, S; Needham, T",,"Banerjee, A; Fukumizu, K",,"Chowdhury, Samir; Needham, Tom",,,Generalized Spectral Clustering via Gromov-Wasserstein Learning,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We establish a bridge between spectral clustering and Gromov-Wasserstein Learning (GWL), a recent optimal transport-based approach to graph partitioning. This connection both explains and improves upon the state-of-the-art performance of GWL. The Gromov-Wasserstein framework provides probabilistic correspondences between nodes of source and target graphs via a quadratic programming relaxation of the node matching problem. Our results utilize and connect the observations that the GW geometric structure remains valid for any rank-2 tensor, in particular the adjacency, distance, and various kernel matrices on graphs, and that the heat kernel outperforms the adjacency matrix in producing stable and informative node correspondences. Using the heat kernel in the GWL framework provides new multiscale graph comparisons without compromising theoretical guarantees, while immediately yielding improved empirical results. A key insight of the GWL framework toward graph partitioning was to compute GW correspondences from a source graph to a template graph with isolated, self-connected nodes. We show that when comparing against a two-node template graph using the heat kernel at the infinite time limit, the resulting partition agrees with the partition produced by the Fiedler vector. This in turn yields a new insight into the k-cut graph partitioning problem through the lens of optimal transport. Our experiments on a range of real-world networks achieve comparable results to, and in many cases outperform, the state-of-the-art achieved by GWL.",,,,,,"Needham, Thomas/0000-0001-6165-3433",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,712,+,,,,,,,,,,,,,,,,WOS:000659893800080,0
C,"Huang, TJ; Singhania, P; Sanjabi, M; Mitra, P; Razaviyayn, M",,"Banerjee, A; Fukumizu, K",,"Huang, Tianjian; Singhania, Prajwal; Sanjabi, Maziar; Mitra, Pabitra; Razaviyayn, Meisam",,,Alternating Direction Method of Multipliers for Quantization,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Quantization of the parameters of machine learning models, such as deep neural networks, requires solving constrained optimization problems, where the constraint set is formed by the Cartesian product of many simple discrete sets. For such optimization problems, we study the performance of the Alternating Direction Method of Multipliers for Quantization (ADMM-Q) algorithm, which is a variant of the widely-used ADMM method applied to our discrete optimization problem. We establish the convergence of the iterates of ADMM-Q to certain stationary points. In addition, our results shows that the Lagrangian function of ADMM converges monotonically. To the best of our knowledge, this is the first analysis of an ADMM-type method for problems with discrete variables/constraints. Based on our theoretical insights, we develop a few variants of ADMM-Q that can handle inexact update rules, and have improved performance via the use of soft projection and injecting randomness to the algorithm. We empirically evaluate the efficacy of our proposed approaches on two problem: 1) solving quantized quadratic optimization problems and 2) training neural networks. Our numerical experiments shows that ADMM-Q outperforms other competing algorithms.",,,,,,"Singhania, Prajwal/0000-0003-4277-1287",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,208,+,,,,,,,,,,,,,,,,WOS:000659893800024,0
C,"Huleihel, W; Pal, S; Shayevitz, O",,"Banerjee, A; Fukumizu, K",,"Huleihel, Wasim; Pal, Soumyabrata; Shayevitz, Ofer",,,Learning User Preferences in Non-Stationary Environments,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Recommendation systems often use online collaborative filtering (CF) algorithms to identify items a given user likes over time, based on ratings that this user and a large number of other users have provided in the past. This problem has been studied extensively when users' preferences do not change over time (static case); an assumption that is often violated in practical settings. In this paper, we introduce a novel model for online non-stationary recommendation systems which allows for temporal uncertainties in the users' preferences. For this model, we propose a user-based CF algorithm, and provide a theoretical analysis of its achievable reward. Compared to related non-stationary multi-armed bandit literature, the main fundamental difficulty in our model lies in the fact that variations in the preferences of a certain user may affect the recommendations for other users severely. We also test our algorithm over real-world datasets, showing its effectiveness in real-world applications. One of the main surprising observations in our experiments is the fact our algorithm outperforms other static algorithms even when preferences do not change over time. This hints toward the general conclusion that in practice, dynamic algorithms, such as the one we propose, might be beneficial even in stationary environments.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801073,0
C,"Klusowski, JM; Tian, PM",,"Banerjee, A; Fukumizu, K",,"Klusowski, Jason M.; Tian, Peter M.",,,Nonparametric Variable Screening with Optimal Decision Stumps,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Decision trees and their ensembles are endowed with a rich set of diagnostic tools for ranking and screening variables in a predictive model. Despite the widespread use of tree based variable importance measures, pinning down their theoretical properties has been challenging and therefore largely unexplored. To address this gap between theory and practice, we derive finite sample performance guarantees for variable selection in nonparametric models using a single-level CART decision tree (a decision stump). Under standard operating assumptions in variable screening literature, we find that the marginal signal strength of each variable and ambient dimensionality can be considerably weaker and higher, respectively, than state-of-the-art nonparametric variable selection methods. Furthermore, unlike previous marginal screening methods that estimate each marginal projection via a truncated basis expansion, the fitted model used here is a simple, parsimonious decision stump, thereby eliminating the need for tuning the number of basis terms. Thus, surprisingly, even though decision stumps are highly inaccurate for estimation purposes, they can still be used to perform consistent model selection.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,748,+,,,,,,,,,,,,,,,,WOS:000659893800084,0
C,"Kumar, A; Zhang, HQ; Singla, A; Chen, YX",,"Banerjee, A; Fukumizu, K",,"Kumar, Akash; Zhang, Hanqi; Singla, Adish; Chen, Yuxin",,,The Teaching Dimension of Kernel Perceptron,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Algorithmic machine teaching has been studied under the linear setting where exact teaching is possible. However, little is known for teaching nonlinear learners. Here, we establish the sample complexity of teaching, aka teaching dimension, for kernelized perceptrons for different families of feature maps. As a warm-up, we show that the teaching complexity is Theta(d) for the exact teaching of linear perceptrons in R-d, and Theta(d(k)) for kernel perceptron with a polynomial kernel of order k. Furthermore, under certain smooth assumptions on the data distribution, we establish a rigorous bound on the complexity for approximately teaching a Gaussian kernel perceptron. We provide numerical examples of the optimal (approximate) teaching set under several canonical settings for linear, polynomial and Gaussian kernel perceptrons.",,,,,"Singla, Adish/ABG-8960-2021","Chen, Yuxin/0000-0003-2133-140X; Singla, Adish/0000-0001-9922-0668",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802050,0
C,"Mardaoui, D; Garreau, D",,"Banerjee, A; Fukumizu, K",,"Mardaoui, Dina; Garreau, Damien",,,An Analysis of LIME for Text Data,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Text data are increasingly handled in an automated fashion by machine learning algorithms. But the models handling these data are not always well-understood due to their complexity and are more and more often referred to as black-boxes. Interpretability methods aim to explain how these models operate. Among them, LIME has become one of the most popular in recent years. However, it comes without theoretical guarantees: even for simple models, we are not sure that LIME behaves accurately. In this paper, we provide a first theoretical analysis of LIME for text data. As a consequence of our theoretical findings, we show that LIME indeed provides meaningful explanations for simple models, namely decision trees and linear models.",,,,,,"Garreau, Damien/0000-0002-7855-2847",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804023,0
C,"Nair, V; Patil, V; Sinha, G",,"Banerjee, A; Fukumizu, K",,"Nair, Vineet; Patil, Vishakha; Sinha, Gaurav",,,Budgeted and Non-Budgeted Causal Bandits,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Learning good interventions in a causal graph can be modeled as a stochastic multi-armed bandit problem with side-information. First, we study this problem when interventions are more expensive than observations, and a budget is specified. If there are no backdoor paths from the intervenable nodes to the reward node, then we propose an algorithm to minimize simple regret that optimally trades-off observations and interventions based on the cost of interventions. We also propose an algorithm that accounts for the cost of interventions, utilizes causal side-information and minimizes the expected cumulative regret without exceeding the budget. Our algorithm performs better than standard algorithms that do not take side-information into account. Finally, we study the problem of learning best interventions without budget constraint in general graphs and give an algorithm that achieves constant expected cumulative regret in terms of the instance parameters when the parent distribution of the reward variable for each intervention is known. Our results are experimentally validated and compared to the best-known bounds in the current literature.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802044,0
C,"Taheri, H; Pedarsani, R; Thrampoulidis, C",,"Banerjee, A; Fukumizu, K",,"Taheri, Hossein; Pedarsani, Ramtin; Thrampoulidis, Christos",,,Fundamental Limits of Ridge-Regularized Empirical Risk Minimization in High Dimensions,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Despite the popularity of Empirical Risk Minimization (ERM) algorithms, a theory that explains their statistical properties in modern high-dimensional regimes is only recently emerging. We characterize for the first time the fundamental limits on the statistical accuracy of convex ridge-regularized ERM for inference in high-dimensional generalized linear models. For a stylized setting with Gaussian features and problem dimensions that grow large at a proportional rate, we start with sharp performance characterizations and then derive tight lower bounds on the estimation and prediction error. Our bounds provably hold over a wide class of loss functions, and, for any value of the regularization parameter and of the sampling ratio. Our precise analysis has several attributes. First, it leads to a recipe for optimally tuning the loss function and the regularization parameter. Second, it allows to precisely quantify the sub-optimality of popular heuristic choices, such as optimally-tuned least-squares. Third, we use the bounds to precisely assess the merits of ridge-regularization as a function of the sampling ratio. Our bounds are expressed in terms of the Fisher Information of random variables that are simple functions of the data distribution, thus making ties to corresponding bounds in classical statistics.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803035,0
C,"Ton, JF; Chan, L; Teh, YW; Sejdinovic, D",,"Banerjee, A; Fukumizu, K",,"Ton, Jean-Francois; Chan, Lucian; Teh, Yee Whye; Sejdinovic, Dino",,,Noise Contrastive Meta-Learning for Conditional Density Estimation using Kernel Mean Embeddings,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Current meta-learning approaches focus on learning functional representations of relationships between variables, i.e. estimating conditional expectations in regression. In many applications, however, the conditional distributions cannot be meaningfully summarized solely by expectation (due to e.g. multimodality). We introduce a novel technique for meta-learning conditional densities, which combines neural representation and noise contrastive estimation together with well-established literature in conditional mean embeddings into reproducing kernel Hilbert spaces. The method shows significant improvements over standard density estimation methods on synthetic and real-world data, by leveraging shared representations across multiple conditional density estimation tasks.",,,,,,"Sejdinovic, Dino/0000-0001-5547-9213",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801036,0
C,"Wei, S; Zhu, SX; Zhang, MH; Xie, Y",,"Banerjee, A; Fukumizu, K",,"Wei, Song; Zhu, Shixiang; Zhang, Minghe; Xie, Yao",,,Goodness-of-Fit Test for Mismatched Self-Exciting Processes,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Recently there have been many research efforts in developing generative models for self-exciting point processes, partly due to their broad applicability for real-world applications. However, rarely can we quantify how well the generative model captures the nature or ground-truth since it is usually unknown. The challenge typically lies in the fact that the generative models typically provide, at most, good approximations to the ground-truth (e.g., through the rich representative power of neural networks), but they cannot be precisely the ground-truth. We thus cannot use the classic goodness-of-fit (GOF) test framework to evaluate their performance. In this paper, we develop a GOF test for generative models of self-exciting processes by making a new connection to this problem with the classical statistical theory of Quasi-maximum-likelihood estimator (QMLE). We present a non-parametric self-normalizing statistic for the GOF test: the Generalized Score (GS) statistics, and explicitly capture the model misspecification when establishing the asymptotic distribution of the GS statistic. Numerical simulation and real-data experiments validate our theory and demonstrate the proposed GS test's good performance.",,,,,"ZHANG, MINGHE/GWN-0933-2022","Zhu, Shixiang/0000-0002-2241-6096",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801052,0
C,"Whiteley, N; Rimella, L",,"Banerjee, A; Fukumizu, K",,"Whiteley, Nick; Rimella, Lorenzo",,,Inference in Stochastic Epidemic Models via Multinomial Approximations,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We introduce a new method for inference in stochastic epidemic models which uses recursive multinomial approximations to integrate over unobserved variables and thus circumvent likelihood intractability. The method is applicable to a class of discretetime, finite-population compartmental models with partial, randomly under-reported or missing count observations. In contrast to state-of-the-art alternatives such as Approximate Bayesian Computation techniques, no forward simulation of the model is required and there are no tuning parameters. Evaluating the approximate marginal likelihood of model parameters is achieved through a computationally simple filtering recursion. The accuracy of the approximation is demonstrated through analysis of real and simulated data using a model of the 1995 Ebola outbreak in the Democratic Republic of Congo. We show how the method can be embedded within a Sequential Monte Carlo approach to estimating the time-varying reproduction number of COVID-19 in Wuhan, China, recently published by Kucharski et al. (2020).",,,,,,"Rimella, Lorenzo/0000-0001-8846-1482",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801058,0
C,"Zhou, TY; Wang, SJ; Bilmes, JA",,"Banerjee, A; Fukumizu, K",,"Zhou, Tianyi; Wang, Shengjie; Bilmes, Jeff A.",,,Curriculum Learning by Optimizing Learning Dynamics,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We study a novel curriculum learning scheme where in each round, samples are selected to achieve the greatest progress and fastest learning speed towards the ground-truth on all available samples. Inspired by an analysis of optimization dynamics under gradient flow for both regression and classification, the problem reduces to selecting training samples by a score computed from samples' residual and linear temporal dynamics. It encourages the model to focus on the samples at the learning frontier, i.e., those with large loss but fast learning speed. The scores in discrete time can be estimated via already-available byproducts of training, and thus require negligible extra compute. We discuss the properties and potential advantages of the proposed dynamics optimization via current deep learning theory and empirical studies. By integrating it with cyclical training of neural networks, we introduce dynamics-optimized curriculum learning (DoCL), which selects the training set at each step by a weighted sampling based on the scores. On nine different datasets, DoCL significantly outperforms random mini-batch SGD and recent curriculum learning methods both in terms of efficiency and final performance.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,433,+,,,,,,,,,,,,,,,,WOS:000659893800049,0
C,"Cheng, CA; des Combes, RT; Boots, B; Gordon, G",,"Chiappa, S; Calandra, R",,"Cheng, Ching-An; des Combes, Remi Tachet; Boots, Byron; Gordon, Geoff",,,A Reduction from Reinforcement Learning to No-Regret Online Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We present a reduction from reinforcement learning (RL) to no-regret online learning based on the saddle-point formulation of RL, by which any online algorithm with sublinear regret can generate policies with provable performance guarantees. This new perspective decouples the RL problem into two parts: regret minimization and function approximation. The first part admits a standard online-learning analysis, and the second part can be quantified independently of the learning algorithm. Therefore, the proposed reduction can be used as a tool to systematically design new RL algorithms. We demonstrate this idea by devising a simple RL algorithm based on mirror descent and the generative-model oracle. For any gamma-discounted tabular RL problem, with probability at least 1 - delta, it learns an epsilon-optimal policy using at most O(vertical bar S parallel to A vertical bar log(1/delta)/(1-gamma)(4)epsilon(2)) samples. Furthermore, this algorithm admits a direct extension to linearly parameterized function approximators for large-scale applications, with computation and sample complexities independent of vertical bar S vertical bar,vertical bar A vertical bar, though at the cost of potential approximation bias.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3514,3523,,,,,,,,,,,,,,,,WOS:000559931300060,0
C,"Clymo, J; Manukian, H; Fijalkow, NS; Gascon, A; Paige, B",,"Chiappa, S; Calandra, R",,"Clymo, Judith; Manukian, Haik; Fijalkow, Nathana Spacingel; Gascon, Adria; Paige, Brooks",,,Data Generation for Neural Programming by Example,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Programming by example is the problem of synthesizing a program from a small set of input / output pairs. Recent works applying machine learning methods to this task show promise, but are typically reliant on generating synthetic examples for training. A particular challenge lies in generating meaningful sets of inputs and outputs, which well-characterize a given program and accurately demonstrate its behavior. Where examples used for testing are generated by the same method as training data then the performance of a model may be partly reliant on this similarity. In this paper we introduce a novel approach using an SMT solver to synthesize inputs which cover a diverse set of behaviors for a given program. We carry out a case study comparing this method to existing synthetic data generation procedures in the literature, and find that data generated using our approach improves both the discriminatory power of example sets and the ability of trained machine learning models to generalize to unfamiliar data.",,,,,,"FIJALKOW, Nathanael/0000-0002-6576-4680",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3450,3458,,,,,,,,,,,,,,,,WOS:000559931300064,0
C,"Dangel, F; Harmeling, S; Hennig, P",,"Chiappa, S; Calandra, R",,"Dangel, Felix; Harmeling, Stefan; Hennig, Philipp",,,Modular Block-diagonal Curvature Approximations for Feedforward Architectures,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We propose a modular extension of back-propagation for the computation of block-diagonal approximations to various curvature matrices of the training objective (in particular, the Hessian, generalized Gauss-Newton, and positive-curvature Hessian). The approach reduces the otherwise tedious manual derivation of these matrices into local modules, and is easy to integrate into existing machine learning libraries. Moreover, we develop a compact notation derived from matrix differential calculus. We outline different strategies applicable to our method. They subsume recently-proposed block-diagonal approximations as special cases, and are extended to convolutional neural networks in this work.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,799,807,,,,,,,,,,,,,,,,WOS:000559931300070,0
C,"Dolatabadi, HM; Erfani, S; Leckie, C",,"Chiappa, S; Calandra, R",,"Dolatabadi, Hadi M.; Erfani, Sarah; Leckie, Christopher",,,Invertible Generative Modeling using Linear Rational Splines,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Normalizing flows attempt to model an arbitrary probability distribution through a set of invertible mappings. These transformations are required to achieve a tractable Jacobian determinant that can be used in high-dimensional scenarios. The first normalizing flow designs used coupling layer mappings built upon affine transformations. The significant advantage of such models is their easy-to-compute inverse. Nevertheless, making use of affine transformations may limit the expressiveness of such models. Recently, invertible piecewise polynomial functions as a replacement for affine transformations have attracted attention. However, these methods require solving a polynomial equation to calculate their inverse. In this paper, we explore using linear rational splines as a replacement for affine transformations used in coupling layers. Besides having a straight-forward inverse, inference and generation have similar cost and architecture in this method. Moreover, simulation results demonstrate the competitiveness of this approach's performance compared to existing methods.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,4236,4245,,,,,,,,,,,,,,,,WOS:000559931300077,0
C,"Dutordoir, V; van der Wilk, M; Artemev, A; Hensman, J",,"Chiappa, S; Calandra, R",,"Dutordoir, Vincent; van der Wilk, Mark; Artemev, Artem; Hensman, James",,,Bayesian Image Classification with Deep Convolutional Gaussian Processes,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"In decision-making systems, it is important to have classifiers that have calibrated uncertainties, with an optimisation objective that can be used for automated model selection and training. Gaussian processes (GPs) provide uncertainty estimates and a marginal likelihood objective, but their weak inductive biases lead to inferior accuracy. This has limited their applicability in certain tasks (e.g. image classification). We propose a translation-insensitive convolutional kernel, which relaxes the translation invariance constraint imposed by previous convolutional GPs. We show how we can use the marginal likelihood to learn the degree of insensitivity. We also reformulate GP image-to-image convolutional mappings as multi-output GPs, leading to deep convolutional GPs. We show experimentally that our new kernel improves performance in both single-layer and deep models. We also demonstrate that our fully Bayesian approach improves on dropout-based Bayesian deep learning methods in terms of uncertainty and marginal likelihood estimates.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1529,1538,,,,,,,,,,,,,,,,WOS:000559931300080,0
C,"Kharkovskii, D; Ling, CK; Low, BKH",,"Chiappa, S; Calandra, R",,"Kharkovskii, Dmitrii; Ling, Chun Kai; Low, Bryan Kian Hsiang",,,Nonmyopic Gaussian Process Optimization with Macro-Actions,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"This paper presents a multi-staged approach to nonmyopic adaptive Gaussian process optimization (GPO) for Bayesian optimization (BO) of unknown, highly complex objective functions that, in contrast to existing nonmyopic adaptive BO algorithms, exploits the notion of macro-actions for scaling up to a further lookahead to match up to a larger available budget. To achieve this, we generalize GP upper confidence bound to a new acquisition function defined w.r.t. a nonmyopic adaptive macro-action policy, which is intractable to be optimized exactly due to an uncountable set of candidate outputs. The contribution of our work here is thus to derive a nonmyopic adaptive epsilon-Bayes-optimal macro-action GPO (epsilon-Macro-GPO) policy. To perform nonmyopic adaptive BO in real time, we then propose an asymptotically optimal anytime variant of our epsilon-Macro-GPO policy with a performance guarantee. We empirically evaluate the performance of our epsilon-Macro-GPO policy and its anytime variant in BO with synthetic and real-world datasets.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,4593,4603,,,,,,,,,,,,,,,,WOS:000559931301069,0
C,"Koskela, A; Jalko, J; Honkela, A",,"Chiappa, S; Calandra, R",,"Koskela, Antti; Jalko, Joonas; Honkela, Antti",,,Computing Tight Differential Privacy Guarantees Using FFT,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Differentially private (DP) machine learning has recently become popular. The privacy loss of DP algorithms is commonly reported using (epsilon, delta)-DP. In this paper, we propose a numerical accountant for evaluating the privacy loss for algorithms with continuous one dimensional output. This accountant can be applied to the subsampled multidimensional Gaussian mechanism which underlies the popular DP stochastic gradient descent. The proposed method is based on a numerical approximation of an integral formula which gives the exact (epsilon, delta)-values. The approximation is carried out by discretising the integral and by evaluating discrete convolutions using the fast Fourier transform algorithm. We give both theoretical error bounds and numerical error estimates for the approximation. Experimental comparisons with state-of-the-art techniques demonstrate significant improvements in bound tightness and/or computation time.",,,,,"Koskela, Antti/AAD-6202-2021",,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2560,2568,,,,,,,,,,,,,,,,WOS:000559931301085,0
C,"Liu, XR; Li, Y; Tang, JL; Yan, M",,"Chiappa, S; Calandra, R",,"Liu, Xiarui; Li, Yao; Tang, Jiliang; Yan, Ming",,,A Double Residual Compression Algorithm for Efficient Distributed Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Large-scale machine learning models are often trained by parallel stochastic gradient descent algorithms. However, the communication cost of gradient aggregation and model synchronization between the master and worker nodes becomes the major obstacle for efficient learning as the number of workers and the dimension of the model increase. In this paper, we propose DORE, a DOuble REsidual compression stochastic gradient descent algorithm, to reduce over 95% of the overall communication such that the obstacle can be immensely mitigated. Our theoretical analyses demonstrate that the proposed strategy has superior convergence properties for both strongly convex and nonconvex objective functions. The experimental results validate that DORE achieves the best communication efficiency while maintaining similar model accuracy and convergence speed in comparison with start-of-the-art baselines.",,,,,"Yan, Ming/V-3072-2017; Li, Yao/AAW-4613-2021","Yan, Ming/0000-0002-8686-3530; Li, Yao/0000-0001-9956-9851",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,133,142,,,,,,,,,,,,,,,,WOS:000559931302014,0
C,"Ma, XC; Blaschko, MB",,"Chiappa, S; Calandra, R",,"Ma, Xingchen; Blaschko, Matthew B.",,,Additive Tree-Structured Covariance Function for Conditional Parameter Spaces in Bayesian Optimization,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Bayesian optimization (BO) is a sample-efficient global optimization algorithm for black-box functions which are expensive to evaluate. Existing literature on model based optimization in conditional parameter spaces are usually built on trees. In this work, we generalize the additive assumption to tree-structured functions and propose an additive tree-structured covariance function, showing improved sample-efficiency, wider applicability and greater flexibility. Furthermore, by incorporating the structure information of parameter spaces and the additive assumption in the BO loop, we develop a parallel algorithm to optimize the acquisition function and this optimization can be performed in a low dimensional space. We demonstrate our method on an optimization benchmark function, as well as on a neural network model compression problem, and experimental results show our approach significantly outperforms the current state of the art for conditional parameter optimization including SMAC, TPE and Jenatton et al. (2017).",,,,,,"Blaschko, Matthew/0000-0002-2640-181X; Ma, Xingchen/0000-0001-9667-3768",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1015,1024,,,,,,,,,,,,,,,,WOS:000559931302024,0
C,"Nika, A; Elahi, S; Tekin, C",,"Chiappa, S; Calandra, R",,"Nika, Andi; Elahi, Sepehr; Tekin, Cem",,,Contextual Combinatorial Volatile Multi-armed Bandit with Adaptive Discretization,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We consider contextual combinatorial volatile multi-armed bandit (CCV-MAB), in which at each round, the learner observes a set of available base arms and their contexts, and then, selects a super arm that contains K base arms in order to maximize its cumulative reward. Under the semi-bandit feedback setting and assuming that the contexts lie in a space X endowed with the Euclidean norm and that the expected base arm outcomes (expected rewards) are Lipschitz continuous in the contexts (expected base arm outcomes), we propose an algorithm called Adaptive Contextual Combinatorial Upper Confidence Bound (ACC-UCB). This algorithm, which adaptively discretizes X to form estimates of base arm outcomes and uses an a -approximation oracle as a subroutine to select a super arm in each round, achieves (O) over tilde (T-)(((D) over bar +1)/((D) over bar +2)+epsilon) regret for any epsilon > 0, where D represents the approximate optimality dimension related to X. This dimension captures both the benignness of the base arm arrivals and the structure of the expected reward. In addition, we provide a recipe for obtaining more optimistic regret bounds by taking into account the volatility of the base arms and show that ACC-UCB achieves significant performance gains compared to the state-of-the-art for worker selection in mobile crowdsourcing.",,,,,,"Elahi, Sepehr/0000-0001-5494-6465",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1486,1495,,,,,,,,,,,,,,,,WOS:000559931302056,0
C,"Pope, P; Balaji, Y; Feizi, S",,"Chiappa, S; Calandra, R",,"Pope, Phillip; Balaji, Yogesh; Feizi, Soheil",,,Adversarial Robustness of Flow-Based Generative Models,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Flow-based generative models leverage invertible generator functions to fit a distribution to the training data using maximum likelihood. Despite their use in several application domains, robustness of these models to adversarial attacks has hardly been explored. In this paper, we study adversarial robustness of flow-based generative models both theoretically (for some simple models) and empirically (for more complex ones). First, we consider a linear flow-based generative model and compute optimal sample-specific and universal adversarial perturbations that maximally decrease the likelihood scores. Using this result, we study the robustness of the well-known adversarial training procedure, where we characterize the fundamental trade-off between model robustness and accuracy. Next, we empirically study the robustness of two prominent deep, nonlinear, flow-based generative models, namely GLOW and RealNVP. We design two types of adversarial attacks; one that minimizes the likelihood scores of in-distribution samples, while the other that maximizes the likelihood scores of out-of-distribution ones. We find that GLOW and RealNVP are extremely sensitive to both types of attacks. Finally, using a hybrid adversarial training procedure, we significantly boost the robustness of these generative models.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3795,3804,,,,,,,,,,,,,,,,WOS:000559931302079,0
C,"Tirinzoni, A; Lazaric, A; Restelli, M",,"Chiappa, S; Calandra, R",,"Tirinzoni, Andrea; Lazaric, Alessandro; Restelli, Marcello",,,A Novel Confidence-Based Algorithm for Structured Bandits,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We study finite-armed stochastic bandits where the rewards of each arm might be correlated to those of other arms. We introduce a novel phased algorithm that exploits the given structure to build confidence sets over the parameters of the true bandit problem and rapidly discard all sub-optimal arms. In particular, unlike standard bandit algorithms with no structure, we show that the number of times a suboptimal arm is selected may actually be reduced thanks to the information collected by pulling other arms. Furthermore, we show that, in some structures, the regret of an anytime extension of our algorithm is uniformly bounded over time. For these constant-regret structures, we also derive a matching lower bound. Finally, we demonstrate numerically that our approach better exploits certain structures than existing methods.",,,,,,"Restelli, Marcello/0000-0002-6322-1076",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303041,0
C,"Ardywibowo, R; Zhao, G; Wang, ZY; Mortazavi, B; Huang, S; Qian, XN",,"Chaudhuri, K; Sugiyama, M",,"Ardywibowo, Randy; Zhao, Guang; Wang, Zhangyang; Mortazavi, Bobak; Huang, Shuai; Qian, Xiaoning",,,Adaptive Activity Monitoring with Uncertainty Quantification in Switching Gaussian Process Models,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Emerging wearable sensors have enabled the unprecedented ability to continuously monitor human activities for healthcare purposes. However, with so many ambient sensors collecting different measurements, it becomes important not only to maintain good monitoring accuracy, but also low power consumption to ensure sustainable monitoring. This power-efficient sensing scheme can be achieved by deciding which group of sensors to use at a given time, requiring an accurate characterization of the trade-off between sensor energy usage and the uncertainty in ignoring certain sensor signals while monitoring. To address this challenge in the context of activity monitoring, we have designed an adaptive activity monitoring framework. We first propose a switching Gaussian process to model the observed sensor signals emitting from the underlying activity states. To efficiently compute the Gaussian process model likelihood and quantify the context prediction uncertainty, we propose a block circulant embedding technique and use Fast Fourier Transforms (FFT) for inference. By computing the Bayesian loss function tailored to switching Gaussian processes, an adaptive monitoring procedure is developed to select features from available sensors that optimize the trade-off between sensor power consumption and the prediction performance quantified by state prediction entropy. We demonstrate the effectiveness of our framework on the popular benchmark of UCI Human Activity Recognition using Smartphones.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,266,274,,,,,,,,,,,,,,,,WOS:000509687900028,0
C,"Chang, B; Pan, SY; Joe, H",,"Chaudhuri, K; Sugiyama, M",,"Chang, Bo; Pan, Shenyi; Joe, Harry",,,Vine copula structure learning via Monte Carlo tree search,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Monte Carlo tree search (MCTS) has been widely adopted in various game and planning problems. It can efficiently explore a search space with guided random sampling. In statistics, vine copulas are flexible multivariate dependence models that adopt vine structures, which are based on a hierarchy of trees to express conditional dependence, and bivariate copulas on the edges of the trees. The vine structure learning problem has been challenging due to the large search space. To tackle this problem, we propose a novel approach to learning vine structures using MCTS. The proposed method has significantly better performance over the existing methods under various experimental setups.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,353,361,,,,,,,,,,,,,,,,WOS:000509687900037,0
C,"Chen, C; Ni, XY; Bai, QX; Wang, YS",,"Chaudhuri, K; Sugiyama, M",,"Chen, Chao; Ni, Xiuyan; Bai, Qinxun; Wang, Yusu",,,A Topological Regularizer for Classifiers via Persistent Homology,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Regularization plays a crucial role in supervised learning. Most existing methods enforce a global regularization in a structure agnostic manner. In this paper, we initiate a new direction and propose to enforce the structural simplicity of the classification boundary by regularizing over its topological complexity. In particular, our measurement of topological complexity incorporates the importance of topological features (e.g., connected components, handles, and so on) in a meaningful manner, and provides a direct control over spurious topological structures. We incorporate the new measurement as a topological penalty in training classifiers. We also propose an efficient algorithm to compute the gradient of such penalty. Our method provides a novel way to topologically simplify the global structure of the model, without having to sacrifice too much of the flexibility of the model. We demonstrate the effectiveness of our new topological regularizer on a range of synthetic and real-world datasets.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902064,0
C,"Cheung, WC; Tan, VYF; Zhong, ZX",,"Chaudhuri, K; Sugiyama, M",,"Cheung, Wang Chi; Tan, Vincent Y. F.; Zhong, Zixin",,,A Thompson Sampling Algorithm for Cascading Bandits,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We design and analyze TS-CAscADE, a Thompson sampling algorithm for the cascading bandit problem. In TS-CAscADE, Bayesian estimates of the click probability are constructed using a univariate Gaussian; this leads to a more efficient exploration procedure vis-a-vis existing UCB-based approaches. We also incorporate the empirical variance of each item's click probability into the Bayesian updates. These two novel features allow us to prove an expected regret bound of the form (O) over tilde(root KLT) where L and K are the number of ground items and the number of items in the chosen list respectively and T >= L is the number of Thompson sampling update steps. This matches the state-of-theart regret bounds for UCB-based algorithms. More importantly, it is the first theoretical guarantee on a Thompson sampling algorithm for any stochastic combinatorial bandit problem model with partial feedback. Empirical experiments demonstrate superiority of TS CASCADE compared to existing UCB-based procedures in terms of the expected cumulative regret and the time complexity.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,438,447,,,,,,,,,,,,,,,,WOS:000509687900046,0
C,"Fadili, J; Garrigos, G; Malick, J; Peyre, G",,"Chaudhuri, K; Sugiyama, M",,"Fadili, Jalal; Garrigos, Guillaume; Malick, Jerome; Peyre, Gabriel",,,Model Consistency for Learning with Mirror-Stratifiable Regularizers,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Low-complexity non-smooth convex regularizers are routinely used to impose some structure (such as sparsity or low-rank) on the coefficients for linear predictors in supervised learning. Model consistency consists then in selecting the correct structure (for instance support or rank) by regularized empirical risk minimization. It is known that model consistency holds under appropriate non-degeneracy conditions. However such conditions typically fail for highly correlated designs and it is observed that regularization methods tend to select larger models. In this work, we provide the theoretical underpinning of this behavior using the notion of mirror-Stratifiable regularizers. This class of regularizers encompasses the most well-known in the literature, including the l(1) or trace norms. It brings into play a pair of primal-dual models, which in turn allows one to locate the structure of the solution using a specific dual certificate. We also show how this analysis is applicable to optimal solutions of the learning problem, and also to the iterates computed by a certain class of stochastic proximal-gradient algorithms.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901029,0
C,"Garber, D",,"Chaudhuri, K; Sugiyama, M",,"Garber, Dan",,,Logarithmic Regret for Online Gradient Descent Beyond Strong Convexity,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Hoffman's classical result gives a bound on the distance of a point from a convex and compact polytope in terms of the magnitude of violation of the constraints. Recently, several results showed that Hoffman's bound can be used to derive strongly-convex-like rates for first-order methods for offline convex optimization of curved, though not strongly convex, functions, over polyhedral sets. In this work, we use this classical result for the first time to obtain faster rates for online convex optimization over polyhedral sets with curved convex, though not strongly convex, loss functions. We show that under several reasonable assumptions on the data, the standard Online Gradient Descent algorithm guarantees logarithmic regret. To the best of our knowledge, the only previous algorithm to achieve logarithmic regret in the considered settings is the Online Newton Step algorithm which requires quadratic (in the dimension) memory and at least quadratic runtime per iteration, which greatly limits its applicability to large-scale problems. In particular, our results hold for semi-adversarial settings in which the data is a combination of an arbitrary (adversarial) sequence and a stochastic sequence, which might provide reasonable approximation for many real-world sequences, or under a natural assumption that the data is low-rank. We demonstrate via experiments that the regret of OGD is indeed comparable to that of ONS (and even far better) on curved though not strongly-convex losses.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,295,303,,,,,,,,,,,,,,,,WOS:000509687900031,0
C,"Huggins, JH; Campbell, T; Kasprzak, M; Broderick, T",,"Chaudhuri, K; Sugiyama, M",,"Huggins, Jonathan H.; Campbell, Trevor; Kasprzak, Mikolaj; Broderick, Tamara",,,Scalable Gaussian Process Inference with Finite-data Mean and Variance Guarantees,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Gaussian processes (GPs) offer a flexible class of priors for nonparametric Bayesian regression, but popular GP posterior inference methods are typically prohibitively slow or lack desirable finite-data guarantees on quality. We develop a scalable approach to approximate GP regression, with finite-data guarantees on the accuracy of our pointwise posterior mean and variance estimates. Our main contribution is a novel objective for approximate inference in the nonparametric setting: the preconditioned Fisher (pF) divergence. We show that unlike the Kullback Leibler divergence (used in variational inference), the pF divergence bounds the 2-Wasserstein distance, which in turn provides tight bounds on the pointwise error of mean and variance estimates. We demonstrate that, for sparse GP likelihood approximations, we can minimize the pF divergence efficiently. Our experiments show that optimizing the pF divergence has the same computational requirements as variational sparse GPs while providing comparable empirical performance in addition to our novel finite-data quality guarantees.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,796,805,,,,,,,,,,,,,,,,WOS:000509687900083,0
C,"Jang, B; Hero, A",,"Chaudhuri, K; Sugiyama, M",,"Jang, Byoungwook; Hero, Alfred",,,Minimum Volume Topic Modeling,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We propose a new topic modeling procedure that takes advantage of the fact that the Latent Dirichlet Allocation (LDA) log likelihood function is asymptotically equivalent to the logarithm of the volume of the topic simplex. This allows topic modeling to be reformulated as finding the probability simplex that minimizes its volume and encloses the documents that are represented as distributions over words. A convex relaxation of the minimum volume topic model optimization is proposed, and it is shown that the relaxed problem has the same global minimum as the original problem under the separability assumption and the sufficiently scattered assumption introduced by Arora et al. (2013) and Huang et al. (2016). A locally convergent alternating direction method of multipliers (ADMM) approach is introduced for solving the relaxed minimum volume problem. Numerical experiments illustrate the benefits of our approach in terms of computation time and topic recovery performance.",,,,,,"Hero, Alfred/0000-0002-2531-9670",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687903007,0
C,"Katariya, S; Kveton, B; Wen, Z; Potluru, VK",,"Chaudhuri, K; Sugiyama, M",,"Katariya, Sumeet; Kveton, Branislav; Wen, Zheng; Potluru, Vamsi Krishna",,,Conservative Exploration using Interleaving,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"In many practical problems, a learning agent may want to learn the best action in hindsight without ever taking a bad action, which is much worse than a default production action. In general, this is impossible because the agent has to explore unknown actions, some of which can be bad, to learn better actions. However, when the actions are structured, this is possible if the unknown action can be evaluated by interleaving it with the default action. We formalize this concept as learning in stochastic combinatorial semi-bandits with exchangeable actions. We design efficient learning algorithms for this problem, bound their n-step regret, and evaluate them on both synthetic and real-world problems. Our real-world experiments show that our algorithms can learn to recommend K most attractive movies without ever making disastrous recommendations, both overall and subject to a diversity constraint.",,,,,"wen, zheng/HII-3705-2022",,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,954,963,,,,,,,,,,,,,,,,WOS:000509687900099,0
C,"Mangoubi, O; Smith, A",,"Chaudhuri, K; Sugiyama, M",,"Mangoubi, Oren; Smith, Aaron",,,Mixing of Hamiltonian Monte Carlo on strongly log-concave distributions 2: Numerical integrators,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We obtain quantitative bounds on the mixing properties of the Hamiltonian Monte Carlo (HMC) algorithm with target distribution in d-dimensional Euclidean space, showing that HMC mixes quickly whenever the target log-distribution is strongly concave and has Lipschitz gradients. We use a coupling argument to show that the popular leapfrog implementation of HMC can sample approximately from the target distribution in a number of gradient evaluations which grows like d(boolean AND)1/2 with the dimension and grows at most polynomially in the strong convexity and Lipschitz-gradient constants. Our results significantly extend and improve on the dimension dependence of previous quantitative bounds on the mixing of HMC and of the unadjusted Langevin algorithm in this setting.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,586,595,,,,,,,,,,,,,,,,WOS:000509687900061,0
C,"Natarajan, N; Simmons, D; Datha, N; Jain, P; Gulwani, S",,"Chaudhuri, K; Sugiyama, M",,"Natarajan, Nagarajan; Simmons, Danny; Datha, Naren; Jain, Prateek; Gulwani, Sumit",,,Learning Natural Programs from a Few Examples in Real-Time,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Programming by examples (PBE) is a rapidly growing subfield of Al, that aims to synthesize user-intended programs using input-output examples from the task. As users can provide only a few I/O examples, capturing user-intent accurately and ranking user-intended programs over other programs is challenging even in the simplest of the domains Commercially deployed PBE systems often require years of engineering effort and domain expertise to devise ranking heuristics for real-time synthesis of accurate programs. But such heuristics may not cater to new domains, or even to a different segment of users from the same domain. In this work, we develop a novel, real-time, ML-based program r ing algorithm that enables synthesis of natural, userintended, personalized programs. We make two key technical contributions: 1) a new technique to embed programs in a vector space making them amenable to ML-formulations, 2) a novel foiiiiulation that interleaves program search with ranking, enabling real-time synthesis of accurate user-intended programs. We implement our solution in the state-of-the-art PROSE framework. The proposed approach learns the intended program with just one I/O example in a variety of real -world string/date/number manipulation tasks, and outperforms state-of-the-art neural synthesis methods along multiple metrics.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901079,0
C,"Requeima, J; Tebbutt, W; Bruinsma, W; Turner, RE",,"Chaudhuri, K; Sugiyama, M",,"Requeima, James; Tebbutt, Will; Bruinsma, Wessel; Turner, Richard E.",,,The Gaussian Process Autoregressive Regression Model (GPAR),"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Multi-output regression models must exploit dependencies between outputs to maximise predictive performance. The application of Gaussian processes (GPs) to this setting typically yields models that are computationally demanding and have limited representational power. We present the Gaussian Process Autoregressive Regression (GPAR) model, a scalable multi-output GP model that is able to capture nonlinear, possibly input-varying, dependencies between outputs in a simple and tractable way: the product rule is used to decompose the joint distribution over the outputs into a set of conditionals, each of which is modelled by a standard GP. GPAR's efficacy is demonstrated on a variety of synthetic and real-world problems, outperforming existing GP models and achieving state-of-the-art performance on established benchmarks.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901094,0
C,"Sen, R; Kandasamy, K; Shakkottai, S",,"Chaudhuri, K; Sugiyama, M",,"Sen, Rajat; Kandasamy, Kirthevasan; Shakkottai, Sanjay",,,Noisy Blackbox Optimization using Multi-fidelity Queries: A Tree Search Approach,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We study the problem of black-box optimization of a noisy function in the presence of low-cost approximations or fidelities, which is motivated by problems like hyper-parameter tuning. In hyper-parameter tuning evaluating the black-box function at a point involves training a learning algorithm on a large dataset at a particular hyper-parameter and evaluating the validation error. Even a single such evaluation can be prohibitively expensive. Therefore, it is beneficial to use lowcost approximations, like training the learning algorithm on a sub-sampled version of the whole data-set. These low-cost approximations/fidelities can however provide a biased and noisy estimate of the function value. In this work, we combine structured state-space exploration through hierarchical partitioning with querying these partitions at multiple fidelities, and develop a multi-fidelity bandit based tree-search algorithm for noisy blackbox optimization. We derive simple regret guarantees for our algorithm and validate its performance on real and synthetic datasets.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902015,0
C,"Staib, M; Wilder, B; Jegelka, S",,"Chaudhuri, K; Sugiyama, M",,"Staib, Matthew; Wilder, Bryan; Jegelka, Stefanie",,,Distributionally Robust Submodular Maximization,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Submodular functions have applications throughout machine learning, but in many settings, we do not have direct access to the underlying function f. We focus on stochastic functions that are given as an expectation of functions over a distribution P. In practice, we often have only a limited set of samples f(i) from P. The standard approach indirectly optimizes f by maximizing the sum of f(i). However, this ignores generalization to the true (unknown) distribution. In this paper, we achieve better performance on the actual underlying function f by directly optimizing a combination of bias and variance. Algorithmically, we accomplish this by showing how to carry out distributionally robust optimization (DRO) for submodular functions, providing efficient algorithms backed by theoretical guarantees which leverage several novel contributions to the general theory of DRO. We also show compelling empirical evidence that DRO improves generalization to the unknown stochastic submodular function.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,506,516,,,,,,,,,,,,,,,,WOS:000509687900053,0
C,"Yang, JS; Rao, V; Neville, J",,"Chaudhuri, K; Sugiyama, M",,"Yang, Jiasen; Rao, Vinayak; Neville, Jennifer",,,A Stein-Papangelou Goodness-of-Fit Test for Point Processes,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Point processes provide a powerful framework for modeling the distribution and interactions of events in time or space. Their flexibility has given rise to a variety of sophisticated models in statistics and machine learning, yet model diagnostic and criticism techniques remain underdeveloped. In this work, we propose a general Stein operator for point processes based on the Papangelou conditional intensity function. We then establish a kernel goodness-of-fit test by defining a Stein discrepancy measure for general point processes. Notably, our test also applies to non-Poisson point processes whose intensity functions contain intractable normalization constants due to the presence of complex interactions among points. We apply our proposed test to several point process models, and show that it outperforms a two-sample test based on the maximum mean discrepancy.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,226,235,,,,,,,,,,,,,,,,WOS:000509687900024,0
C,"Gower, RM; Le Roux, N; Bach, F",,"Storkey, A; PerezCruz, F",,"Gower, Robert M.; Le Roux, Nicolas; Bach, Francis",,,Tracking the gradients using the Hessian: A new look at variance reducing stochastic methods,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Our goal is to improve variance reducing stochastic methods through better control variates. We first propose a modification of SVRG which uses the Hessian to track gradients over time, rather than to recondition, increasing the correlation of the control variates and leading to faster theoretical convergence close to the optimum. We then propose accurate and computationally efficient approximations to the Hessian, both using a diagonal and a low-rank matrix. Finally, we demonstrate the effectiveness of our method on a wide range of problems.",,,,,"Gower, Robert Mansel/Y-8838-2019","Gower, Robert Mansel/0000-0002-2268-9780",,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300075,0
C,"Mohri, M; Yang, S",,"Storkey, A; PerezCruz, F",,"Mohri, Mehryar; Yang, Scott",,,Competing with Automata-based Expert Sequences,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"consider a general framework of online learning with expert advice where regret is defined with respect to sequences of experts accepted by a weighted automaton. Our framework covers several problems previously studied, including competing against k-shifting experts. We give a series of algorithms for this problem, including an automata-based algorithm extending weighted-majority and more efficient algorithms based on the notion of failure transitions. We further present efficient algorithms based on an approximation of the competitor automaton, in particular n-gram models obtained by minimizing the infinity-Renyi divergence, and present an extensive study of the approximation properties of such models. Finally, we also extend our algorithms and results to the framework of sleeping experts.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300181,0
C,"Pennington, J; Schoenholz, SS; Ganguli, S",,"Storkey, A; PerezCruz, F",,"Pennington, Jeffrey; Schoenholz, Samuel S.; Ganguli, Surya",,,The Emergence of Spectral Universality in Deep Networks,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Recent work has shown that tight concentration of the entire spectrum of singular values of a deep network's input-output Jacobian around one at initialization can speed up learning by orders of magnitude. Therefore, to guide important design choices, it is important to build a full theoretical understanding of the spectra of Jacobians at initialization. To this end, we leverage powerful tools from free probability theory to provide a detailed analytic understanding of how a deep network's Jacobian spectrum depends on various hyperparameters including the nonlinearity, the weight and bias distributions, and the depth. For a variety of nonlinearities, our work reveals the emergence of new universal limiting spectral distributions that remain concentrated around one even as the depth goes to infinity.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300201,0
C,"Rosenfeld, N; Globerson, A",,"Storkey, A; PerezCruz, F",,"Rosenfeld, Nir; Globerson, Amir",,,Semi-Supervised Learning with Competitive Infection Models,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"The goal in semi-supervised learning is to effectively combine labeled and unlabeled data. One way to do this is by encouraging smoothness across edges in a graph whose nodes correspond to input examples. In many graphbased methods, labels can be thought of as propagating over the graph, where the underlying propagation mechanism is based on random walks or on averaging dynamics. While theoretically elegant, these dynamics suffer from several drawbacks which can hurt predictive performance. Our goal in this work is to explore alternative mechanisms for propagating labels. In particular, we propose a method based on dynamic infection processes, where unlabeled nodes can be \infected with the label of their already infected neighbors. Our algorithm is efficient and scalable, and an analysis of the underlying optimization objective reveals a surprising relation to other Laplacian approaches. We conclude with a thorough set of experiments across multiple benchmarks and various learning settings.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300036,0
C,"Salimbeni, H; Eleftheriadis, S; Hensman, J",,"Storkey, A; PerezCruz, F",,"Salimbeni, Hugh; Eleftheriadis, Stefanos; Hensman, James",,,Natural Gradients in Practice: Non-Conjugate Variational Inference in Gaussian Process Models,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"The natural gradient method has been used effectively in conjugate Gaussian process models, but the non-conjugate case has been largely unexplored. We examine how natural gradients can be used in non-conjugate stochastic settings, together with hyperparameter learning. We conclude that the natural gradient can significantly improve performance in terms of wall-clock time. For ill-conditioned posteriors the benefit of the natural gradient method is especially pronounced, and we demonstrate a practical setting where ordinary gradients are unusable. We show how natural gradients can be computed efficiently and automatically in any parameterization, using automatic differentiation. Our code is integrated into the GPflow package.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300073,0
C,"Chang, YL; Li, Y; Ding, A; Dy, JG",,"Gretton, A; Robert, CC",,"Chang, Yale; Li, Yi; Ding, Adam (Aidong); Dy, Jennifer G.",,,A Robust-Equitable Copula Dependence Measure for Feature Selection,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Feature selection aims to select relevant features to improve the performance of predictors. Many feature selection methods depend on the choice of dependence measures. To select features that have complex nonlinear relationships with the response variable, the dependence measure should be equitable; i.e., it should treat linear and nonlinear relationships equally. In this paper, we introduce the concept of robust-equitability and identify a robust-equitable dependence measure robust copula dependence (RCD). This measure has the following advantages compared to existing dependence measures: it is robust to different relationship forms and robust to unequal sample sizes of different features. In contrast, existing dependence measures cannot take these factors into account simultaneously. Experiments on synthetic and real-world datasets confirm our theoretical analysis, and illustrate its advantage in feature selection.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,84,92,,,,,,,,,,,,,,,,WOS:000508662100010,0
C,"Hu, CW; Rai, P; Carin, L",,"Gretton, A; Robert, CC",,"Hu, Changwei; Rai, Piyush; Carin, Lawrence",,,Non-negative Matrix Factorization for Discrete Data with Hierarchical Side-Information,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"We present a probabilistic framework for efficient non-negative matrix factorization of discrete (count/binary) data with side-information. The side-information is given as a multi-level structure, taxonomy, or ontology, with nodes at each level being categorical-valued observations. For example, when modeling documents with a two-level side-information (documents being at level-zero), level-one may represent (one or more) authors associated with each document and level-two may represent affiliations of each author. The model easily generalizes to more than two levels (or taxonomy/ontology of arbitrary depth). Our model can learn embeddings of entities present at each level in the data/side-information hierarchy (e.g., documents, authors, affiliations, in the previous example), with appropriate sharing of information across levels. The model also enjoys full local conjugacy, facilitating efficient Gibbs sampling for model inference. Inference cost scales in the number of non-zero entries in the data matrix, which is especially appealing for real-world massive but sparse matrices. We demonstrate the effectiveness of the model on several real-world data sets.",,,,,,"Carin, Lawrence/0000-0001-6277-7948",,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,1124,1132,,,,,,,,,,,,,,,,WOS:000508662100122,0
C,"Nishiyama, Y; Afsharinejad, AH; Naruse, S; Boots, B; Song, L",,"Gretton, A; Robert, CC",,"Nishiyama, Yu; Afsharinejad, Amir Hossein; Naruse, Shunsuke; Boots, Byron; Song, Le",,,The Nonparametric Kernel Bayes Smoother,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Recently, significant progress has been made developing kernel mean expressions for Bayesian inference. An important success in this domain is the nonparametric kernel Bayes' filter (nKB-filter), which can be used for sequential inference in state space models. We expand upon this work by introducing a smoothing algorithm, the nonparametric kernel Bayes' smoother (nKB-smoother) which relies on kernel Bayesian inference through the kernel sum rule and kernel Bayes' rule. We derive the smoothing equations, analyze the computational cost, and show smoothing consistency. We summarize the algorithm, which is simple to implement, requiring only matrix multiplications and the output of the nKB-filter. Finally, we report experimental results that compare the nKB-smoother to previous parametric and nonparametric approaches to Bayesian filtering and smoothing. In the supplementary materials, we show that the combination of the nKB-filter and the nKB-smoother allows marginal kernel mean computation, which gives an alternative to kernel belief propagation.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,547,555,,,,,,,,,,,,,,,,WOS:000508662100060,0
C,"Song, Z; Henao, R; Carlson, D; Carin, L",,"Gretton, A; Robert, CC",,"Song, Zhao; Henao, Ricardo; Carlson, David; Carin, Lawrence",,,Learning Sigmoid Belief Networks via Monte Carlo Expectation Maximization,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Belief networks are commonly used generative models of data, but require expensive posterior estimation to train and test the model. Learning typically proceeds by posterior sampling, variational approximations, or recognition networks, combined with stochastic optimization. We propose using an online Monte Carlo expectation-maximization (MCEM) algorithm to learn the maximum a posteriori (MAP) estimator of the generative model or optimize the variational lower bound of a recognition network. The E-step in this algorithm requires posterior samples, which are already generated in current learning schema. For the M-step, we augment with Polya-Gamma (PG) random variables to give an analytic updating scheme. We show relationships to standard learning approaches by deriving stochastic gradient ascent in the MCEM framework. We apply the proposed methods to both binary and count data. Experimental results show that MCEM improves the convergence speed and often improves hold-out performance over existing learning methods. Our approach is readily generalized to other recognition networks.",,,,,"Song, Zhao/AAW-4042-2020","Carin, Lawrence/0000-0001-6277-7948; Henao, Ricardo/0000-0003-4980-845X; Carlson, David/0000-0003-1005-6385",,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,1347,1355,,,,,,,,,,,,,,,,WOS:000508662100146,0
C,"Teneva, N; Mudrakarta, PK; Kondor, R",,"Gretton, A; Robert, CC",,"Teneva, Nedelina; Mudrakarta, Pramod K.; Kondor, Risi",,,Multiresolution Matrix Compression,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Multiresolution Matrix Factorization (MMF) is a recently introduced method for finding multi-scale structure and defining wavelets on graphs and matrices. MMF can also be used for matrix compression (sketching). However, the original MMF algorithm of (Kondor et al., 2014) scales with n(3) or worse (where n is the number of rows/columns in the matrix to be factorized) making it infeasible for large scale problems. In this paper we describe pMMF, a fast parallel MMF algorithm, which can scale to n in the range of millions. Our experimental results show that when used for matrix compression, pMMF often achieves much lower error than competing algorithms (especially on network data), yet for sparse matrices its running time scales close to linearly with n.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,1441,1449,,,,,,,,,,,,,,,,WOS:000508662100156,0
C,"Weller, A; Rowland, M; Sontag, D",,"Gretton, A; Robert, CC",,"Weller, Adrian; Rowland, Mark; Sontag, David",,,Tightness of LP Relaxations for Almost Balanced Models,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Linear programming (LP) relaxations are widely used to attempt to identify a most likely configuration of a discrete graphical model. In some cases, the LP relaxation attains an optimum vertex at an integral location and thus guarantees an exact solution to the original optimization problem. When this occurs, we say that the LP relaxation is tight. Here we consider binary pairwise models and derive sufficient conditions for guaranteed tightness of (i) the standard LP relaxation on the local polytope LP+LOC, and (ii) the LP relaxation on the triplet-consistent polytope LP+TRI (the next level in the Sherali-Adams hierarchy). We provide simple new proofs of earlier results and derive significant novel results including that LP+TRI is tight for any model where each block is balanced or almost balanced, and a decomposition theorem that may be used to break apart complex models into smaller pieces. An almost balanced (sub-)model is one that contains no frustrated cycles except through one privileged variable.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,47,55,,,,,,,,,,,,,,,,WOS:000508662100006,0
C,"Pandey, G; Dukkipati, A",,"Kaski, S; Corander, J",,"Pandey, Gaurav; Dukkipati, Ambedkar",,,To go deep or wide in learning?,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"To achieve acceptable performance for AI tasks, one can either use sophisticated feature extraction methods as the first layer in a two-layered supervised learning model, or learn the features directly using a deep (multilayered) model. While the first approach is very problem-specific, the second approach has computational overheads in learning multiple layers and fine-tuning of the model. In this paper, we propose an approach called wide learning based on arc-cosine kernels, that learns a single layer of infinite width. We propose exact and inexact learning strategies for wide learning and show that wide learning with single layer outperforms single layer as well as deep architectures of finite width for some benchmark datasets.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,724,732,,,,,,,,,,,,,,,,WOS:000508355800080,0
C,"Bhaskara, A; Cutkosky, A; Kumar, R; Purohit, M",,"Banerjee, A; Fukumizu, K",,"Bhaskara, Aditya; Cutkosky, Ashok; Kumar, Ravi; Purohit, Manish",,,Power of Hints for Online Learning with Movement Costs,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We consider the online linear optimization problem with movement costs, a variant of online learning in which the learner must not only respond to cost vectors c(t) with points x(t) in order to maintain low regret, but is also penalized for movement by an additional cost parallel to x(t) - x(t+1)parallel to(1+epsilon) for some epsilon >= 0. Classically, s pimple algorithms that obtain the optimal root T regret already are very stable and do not incur a significant movement cost. However, recent work has shown that when the learning algorithm is provided with weak hint vectors that have a positive correlation with the costs, the regret can be significantly improved to log T. In this work, we study the stability of such algorithms, and provide matching upper and lower bounds showing that incorporating movement costs results in intricate tradeoffs between log T when epsilon >= 1 and root T regret when epsilon = 0.",,,,,"Purohit, Manish/ABD-3458-2021","Cutkosky, Ashok/0000-0002-3822-3029",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803040,0
C,"Cheraghchi, M; Grigorescu, E; Juba, B; Wimmer, K; Xie, N",,"Banerjee, A; Fukumizu, K",,"Cheraghchi, Mahdi; Grigorescu, Elena; Juba, Brendan; Wimmer, Karl; Xie, Ning",,,List Learning with Attribute Noise,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We introduce and study the model of list learning with attribute noise. Learning with attribute noise was introduced by Shackelford and Volper (COLT, 1988) as a variant of PAC learning, in which the algorithm has access to noisy examples and uncorrupted labels, and the goal is to recover an accurate hypothesis. Sloan (COLT, 1988) and Goldman and Sloan (Algorithmica, 1995) discovered information-theoretic limits to learning in this model, which have impeded further progress. In this article we extend the model to that of list learning, drawing inspiration from the list-decoding model in coding theory, and its recent variant studied in the context of learning. On the positive side, we show that sparse conjunctions can be efficiently list learned under some assumptions on the underlying ground-truth distribution. On the negative side, our results show that even in the list-learning model, efficient learning of parities and majorities is not possible, regardless of the representation used.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802066,0
C,"Ghoroghchian, N; Dasarathy, G; Draper, SC",,"Banerjee, A; Fukumizu, K",,"Ghoroghchian, Nafiseh; Dasarathy, Gautam; Draper, Stark C.",,,Graph Community Detection from Coarse Measurements: Recovery Conditions for the Coarsened Weighted Stochastic Block Model,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We study the problem of community recovery from coarse measurements of a graph. In contrast to the problem of community recovery of a fully observed graph, one often encounters situations when measurements of a graph are made at low-resolution, each measurement integrating across multiple graph nodes. Such low-resolution measurements effectively induce a coarse graph with its own communities. Our objective is to develop conditions on the graph structure, the quantity, and properties of measurements, under which we can recover the community organization in this coarse graph. In this paper, we build on the stochastic block model by mathematically formalizing the coarsening process, and characterizing its impact on the community members and connections. Through this novel setup and modeling, we characterize an error bound for community recovery. The error bound yields simple and closed-form asymptotic conditions to achieve the perfect recovery of the coarse graph communities.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,34258582,,,,,WOS:000659893804037,0
C,"Hanneke, S; Yang, L",,"Banerjee, A; Fukumizu, K",,"Hanneke, Steve; Yang, Liu",,,Toward a General Theory of Online Selective Sampling: Trading Off Mistakes and Queries,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"While the literature on the theory of pool-based active learning has seen much progress in the past 15 years, and is now fairly mature, much less is known about its cousin problem: online selective sampling. In the stochastic online learning setting, there is a stream of iid data, and the learner is required to predict a label for each instance, and we are interested in the rate of growth of the number of mistakes the learner makes. In the selective sampling variant of this problem, after each prediction, the learner can optionally request to observe the true classification of the point. This introduces a trade-off between the number of these queries and the number of mistakes as a function of the number T of samples in the sequence. This work explores various properties of the optimal trade-off curve, both abstractly (for general VC classes), and more-concretely for several constructed examples that expose important properties of the trade-off.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804078,0
C,"Holland, MJ",,"Banerjee, A; Fukumizu, K",,"Holland, Matthew J.",,,"Robustness and scalability under heavy tails, without strong convexity",24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Real-world data is laden with outlying values. The challenge for machine learning is that the learner typically has no prior knowledge of whether the feedback it receives (losses, gradients, etc.) will be heavy-tailed or not. In this work, we study a simple, cost-efficient algorithmic strategy that can be leveraged when both losses and gradients can be heavy-tailed. The core technique introduces a simple robust validation sub-routine, which is used to boost the confidence of inexpensive gradient-based sub-processes. Compared with recent robust gradient descent methods from the literature, dimension dependence (both risk bounds and cost) is substantially improved, without relying upon strong convexity or expensive per-step robustification. We also empirically show that the proposed procedure cannot simply be replaced with naive cross-validation.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801010,0
C,"Lamb, A; Goyal, A; Slowik, A; Mozer, M; Beaudoin, P; Bengio, Y",,"Banerjee, A; Fukumizu, K",,"Lamb, Alex; Goyal, Anirudh; Slowik, Agnieszka; Mozer, Michael; Beaudoin, Philippe; Bengio, Yoshua",,,Neural Function Modules with Sparse Arguments: A Dynamic Approach to Integrating Information across Layers,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Feed-forward neural networks consist of a sequence of layers, in which each layer performs some processing on the information from the previous layer. A downside to this approach is that each layer (or module, as multiple modules can operate in parallel) is tasked with processing the entire hidden state, rather than a particular part of the state which is most relevant for that module. Methods which only operate on a small number of input variables are an essential part of most programming languages, and they allow for improved modularity and code re-usability. Our proposed method, Neural Function Modules (NFM), aims to introduce the same structural capability into deep learning. Most of the work in the context of feed-forward networks combining top-down and bottom-up feedback is limited to classification problems. The key contribution of our work is to combine attention, sparsity, top-down and bottom-up feedback, in a flexible algorithm which, as we show, improves the results in standard classification, out-of-domain generalization, generative modeling, and learning representations in the context of reinforcement learning.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801016,0
C,"Liu, FH; Liao, ZY; Suykens, JAK",,"Banerjee, A; Fukumizu, K",,"Liu, Fanghui; Liao, Zhenyu; Suykens, Johan A. K.",,,Kernel regression in high dimensions: Refined analysis beyond double descent,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"In this paper, we provide a precise characterization of generalization properties of high dimensional kernel ridge regression across the under- and over-parameterized regimes, depending on whether the number of training data n exceeds the feature dimension d. By establishing a bias-variance decomposition of the expected excess risk, we show that, while the bias is (almost) independent of d and monotonically decreases with n, the variance depends on n; d and can be unimodal or monotonically decreasing under different regularization schemes. Our refined analysis goes beyond the double descent theory by showing that, depending on the data eigen-profile and the level of regularization, the kernel regression risk curve can be a double-descent-like, bell-shaped, or monotonic function of n. Experiments on synthetic and real data are conducted to support our theoretical findings.",,,,,"Liao, Zhenyu/AAF-3621-2020","LIAO, Zhenyu/0000-0002-1915-8559",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,649,+,,,,,,,,,,,,,,,,WOS:000659893800073,0
C,"Mansour, Y; Mohri, M; Ro, J; Suresh, AT; Wu, K",,"Banerjee, A; Fukumizu, K",,"Mansour, Yishay; Mohri, Mehryar; Ro, Jae; Suresh, Ananda Theertha; Wu, Ke",,,A Theory of Multiple-Source Adaptation with Limited Target Labeled Data,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We present a theoretical and algorithmic study of the multiple-source domain adaptation problem in the common scenario where the learner has access only to a limited amount of labeled target data, but where the learner has at their disposal a large amount of labeled data from multiple source domains. We show that a new family of algorithms based on model selection ideas benefits from very favorable guarantees in this scenario and discuss some theoretical obstacles affecting some alternative techniques. We also report the results of several experiments with our algorithms that demonstrate their practical e.ectiveness.",,,,,,"Ro, Jae Hun/0000-0002-4738-0595",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802079,0
C,"Marienwald, H; Fermanian, JB; Blanchard, G",,"Banerjee, A; Fukumizu, K",,"Marienwald, Hannah; Fermanian, Jean-Baptiste; Blanchard, Gilles",,,High-Dimensional Multi-Task Averaging and Application to Kernel Mean Embedding,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We propose an improved estimator for the multi-task averaging problem, whose goal is the joint estimation of the means of multiple distributions using separate, independent data sets. The naive approach is to take the empirical mean of each data set individually, whereas the proposed method exploits similarities between tasks, without any related information being known in advance. First, for each data set, similar or neighboring means are determined from the data by multiple testing. Then each naive estimator is shrunk towards the local average of its neighbors. We prove theoretically that this approach provides a reduction in mean squared error. This improvement can be significant when the dimension of the input space is large; demonstrating a blessing of dimensionality phenomenon. An application of this approach is the estimation of multiple kernel mean embeddings, which plays an important role in many modern applications. The theoretical results are verified on artificial and real world data.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802038,0
C,"Medina, AM; Syed, U; Vassilvitskii, S; Vitercik, E",,"Banerjee, A; Fukumizu, K",,"Medina, Andres Munoz; Syed, Umar; Vassilvitskii, Sergei; Vitercik, Ellen",,,Private optimization without constraint violations,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We study the problem of differentially private optimization with linear constraints when the right-hand-side of the constraints depends on private data. This type of problem appears in many applications, especially resource allocation. Previous research provided solutions that retained privacy but sometimes violated the constraints. In many settings, however, the constraints cannot be violated under any circumstances. To address this hard requirement, we present an algorithm that releases a nearly-optimal solution satisfying the constraints with probability 1. We also prove a lower bound demonstrating that the difference between the objective value of our algorithm's solution and the optimal solution is tight up to logarithmic factors among all differentially private algorithms. We conclude with experiments demonstrating that our algorithm can achieve nearly optimal performance while preserving privacy.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803011,0
C,"Reda, C; Kaufmann, E; Delahaye-Duriez, A",,"Banerjee, A; Fukumizu, K",,"Reda, Clemente; Kaufmann, Emilie; Delahaye-Duriez, Andree",,,Top-m identification for linear bandits,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Motivated by an application to drug repurposing, we propose the first algorithms to tackle the identification of the m >= 1 arms with largest means in a linear bandit model, in the fixed-confidence setting. These algorithms belong to the generic family of GapIndex Focused Algorithms (GIFA) that we introduce for Top-m identification in linear bandits. We propose a unified analysis of these algorithms, which shows how the use of features might decrease the sample complexity. We further validate these algorithms empirically on simulated data and on a simple drug repurposing task.",,,,,,"Delahaye-Duriez, Andree/0000-0003-4324-7372; Reda, Clemence/0000-0003-3238-0258",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801037,0
C,"Shah, A; Shah, D; Wornell, GW",,"Banerjee, A; Fukumizu, K",,"Shah, Abhin; Shah, Devavrat; Wornell, Gregory W.",,,On Learning Continuous Pairwise Markov Random Fields,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We consider learning a sparse pairwise Markov Random Field (MRF) with continuous valued variables from i.i.d samples. We adapt the algorithm of Vuffray et al. (2019) to this setting and provide finite-sample analysis revealing sample complexity scaling logarithmically with the number of variables, as in the discrete and Gaussian settings. Our approach is applicable to a large class of pairwise MRFs with continuous variables and also has desirable asymptotic properties, including consistency and normality under mild conditions. Further, we establish that the population version of the optimization criterion employed by Vuffray et al. (2019) can be interpreted as local maximum likelihood estimation (MLE). As part of our analysis, we introduce a robust variation of sparse linear regression a la Lasso, which may be of interest in its own right.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801042,0
C,"Watson, J; Lin, JA; Klink, P; Pajarinen, J; Peters, J",,"Banerjee, A; Fukumizu, K",,"Watson, Joe; Lin, Jihao Andreas; Klink, Pascal; Pajarinen, Joni; Peters, Jan",,,Latent Derivative Bayesian Last Layer Networks,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Bayesian neural networks (BNN) are powerful parametric models for nonlinear regression with uncertainty quantification. However, the approximate inference techniques for weight space priors suffer from several drawbacks. The 'Bayesian last layer' (BLL) is an alternative BNN approach that learns the feature space for an exact Bayesian linear model with explicit predictive distributions. However, its predictions outside of the data distribution (OOD) are typically overconfident, as the marginal likelihood objective results in a learned feature space that overfits to the data. We overcome this weakness by introducing a functional prior on the model's derivatives w.r.t. the inputs. Treating these Jacobians as latent variables, we incorporate the prior into the objective to influence the smoothness and diversity of the features, which enables greater predictive uncertainty. For the BLL, the Jacobians can be computed directly using forward mode automatic differentiation, and the distribution over Jacobians may be obtained in closed-form. We demonstrate this method enhances the BLL to Gaussian process-like performance on tasks where calibrated uncertainty is critical: OOD regression, Bayesian optimization and active learning, which include high-dimensional real-world datasets.",,,,,"Peters, Jan/P-6027-2019; Pajarinen, Joni/C-4189-2014","Peters, Jan/0000-0002-5266-8091; Pajarinen, Joni/0000-0003-4469-8191",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801047,0
C,"Wilkinson, WJ; Solin, A; Adam, V",,"Banerjee, A; Fukumizu, K",,"Wilkinson, William J.; Solin, Arno; Adam, Vincent",,,Sparse Algorithms for Markovian Gaussian Processes,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Approximate Bayesian inference methods that scale to very large datasets are crucial in leveraging probabilistic models for real-world time series. Sparse Markovian Gaussian processes combine the use of inducing variables with efficient Kalman filter-like recursions, resulting in algorithms whose computational and memory requirements scale linearly in the number of inducing points, whilst also enabling parallel parameter updates and stochastic optimisation. Under this paradigm, we derive a general site-based approach to approximate inference, whereby we approximate the non-Gaussian likelihood with local Gaussian terms, called sites. Our approach results in a suite of novel sparse extensions to algorithms from both the machine learning and signal processing literature, including variational inference, expectation propagation, and the classical nonlinear Kalman smoothers. The derived methods are suited to large time series, and we also demonstrate their applicability to spatio-temporal data, where the model has separate inducing points in both time and space.",,,,,"; Solin, Arno/G-6859-2012","Adam, Vincent/0000-0002-9953-3434; Solin, Arno/0000-0002-0958-7886",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802014,0
C,"Xu, ZP; Meisami, A; Tewari, A",,"Banerjee, A; Fukumizu, K",,"Xu, Ziping; Meisami, Amirhossein; Tewari, Ambuj",,,Decision Making Problems with Funnel Structure: A Multi-Task Learning Approach with Application to Email Marketing Campaigns,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"This paper studies the decision making problem with Funnel Structure. Funnel structure, a well-known concept in the marketing field, occurs in those systems where the decision maker interacts with the environment in a layered manner receiving far fewer observations from deep layers than shallow ones. For example, in the email marketing campaign application, the layers correspond to Open, Click and Purchase events. Conversions from Click to Purchase happen very infrequently because a purchase cannot be made unless the link in an email is clicked on. We formulate this challenging decision making problem as a contextual bandit with funnel structure and develop a multi-task learning algorithm that mitigates the lack of sufficient observations from deeper layers. We analyze both the prediction error and the regret of our algorithms. We verify our theory on prediction errors through a simple simulation. Experiments on both a simulated environment and an environment based on real-world data from a major email marketing company show that our algorithms offer significant improvement over previous methods.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,127,+,,,,,,,,,,,,,,,,WOS:000659893800015,0
C,"Yu, L; Kaufmann, T; Lederer, J",,"Banerjee, A; Fukumizu, K",,"Yu, Lu; Kaufmann, Tobias; Lederer, Johannes",,,False Discovery Rates in Biological Networks,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"The increasing availability of data has generated unprecedented prospects for network analyses in many biological fields, such as neuroscience (e.g., brain networks), genomics (e.g., gene-gene interaction networks), and ecology (e.g., species interaction networks). A powerful statistical framework for estimating such networks is Gaussian graphical models, but standard estimators for the corresponding graphs are prone to large numbers of false discoveries. In this paper, we introduce a novel graph estimator based on knockoffs that imitate the partial correlation structures of unconnected nodes. We then show that this new estimator provides accurate control of the false discovery rate and yet large power.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,163,+,,,,,,,,,,,,,,,,WOS:000659893800019,0
C,"Zhang, GD; Wang, YH",,"Banerjee, A; Fukumizu, K",,"Zhang, Guodong; Wang, Yuanhao",,,On the Suboptimality of Negative Momentum for Minimax Optimization,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Smooth game optimization has recently attracted great interest in machine learning as it generalizes the single-objective optimization paradigm. However, game dynamics is more complex due to the interaction between di.erent players and is therefore fundamentally di.erent from minimization, posing new challenges for algorithm design. Notably, it has been shown that negative momentum is preferred due to its ability to reduce oscillation in game dynamics. Nevertheless, the convergence rate of negative momentum was only established in simple bilinear games. In this paper, we extend the analysis to smooth and strongly-convex stronglyconcave minimax games by taking the variational inequality formulation. By connecting Polyak's momentum with Chebyshev polynomials, we show that negative momentum accelerates convergence of game dynamics locally, though with a suboptimal rate. To the best of our knowledge, this is the first work that provides an explicit convergence rate for negative momentum in this setting.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802053,0
C,"Zhu, JJ; Jitkrittum, W; Diehl, M; Scholkopf, B",,"Banerjee, A; Fukumizu, K",,"Zhu, Jia-Jie; Jitkrittum, Wittawat; Diehl, Moritz; Schoelkopf, Bernhard",,,Kernel Distributionally Robust Optimization: Generalized Duality Theorem and Stochastic Approximation,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We propose kernel distributionally robust optimization (Kernel DRO) using insights from the robust optimization theory and functional analysis. Our method uses reproducing kernel Hilbert spaces (RKHS) to construct a wide range of convex ambiguity sets, which can be generalized to sets based on integral probability metrics and finite-order moment bounds. This perspective unifies multiple existing robust and stochastic optimization methods. We prove a theorem that generalizes the classical duality in the mathematical problem of moments. Enabled by this theorem, we reformulate the maximization with respect to measures in DRO into the dual program that searches for RKHS functions. Using universal RKHSs, the theorem applies to a broad class of loss functions, lifting common limitations such as polynomial losses and knowledge of the Lipschitz constant. We then establish a connection between DRO and stochastic optimization with expectation constraints. Finally, we propose practical algorithms based on both batch convex solvers and stochastic functional gradient, which apply to general optimization and machine learning tasks.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,280,+,,,,,,,,,,,,,,,,WOS:000659893800032,0
C,"Zhu, ZW; Zhou, WJ",,"Banerjee, A; Fukumizu, K",,"Zhu, Ziwei; Zhou, Wenjing",,,Taming heavy-tailed features by shrinkage,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"In this work, we focus on a variant of the generalized linear model (GLM) called corrupted GLM (CGLM) with heavy-tailed features and responses. To robustify the statistical inference on this model, we propose to apply l(4)-norm shrinkage to the feature vectors in the low-dimensional regime and apply elementwise shrinkage to them in the highdimensional regime. Under bounded fourth moment assumptions, we show that the maximum likelihood estimator (MLE) based on the shrunk data enjoys nearly the minimax optimal rate with an exponential deviation bound. Our simulations demonstrate that the proposed feature shrinkage significantly enhances the statistical performance in linear regression and logistic regression on heavy-tailed data. Finally, we apply our shrinkage principle to guard against mislabeling and image noise in the human-written digit recognition problem. We add an l(4)-norm shrinkage layer to the original neural net and reduce the testing misclassification rate by more than 30% relatively in the presence of mislabeling and image noise.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803090,0
C,"Amin, K; Cortes, C; DeSalvo, G; Rostamizadeh, A",,"Chiappa, S; Calandra, R",,"Amin, Kareem; Cortes, Corinna; DeSalvo, Giulia; Rostamizadeh, Afshin",,,Understanding the Effects of Batching in Online Active Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Online active learning (AL) algorithms often assume immediate access to a label once a query has been made. However, due to practical constraints, the labels of these queried examples are generally only available in batches. In this work, we present an analysis for a generic class of batch online AL algorithms, which reveals that the effects of batching are in fact mild and only result in an additional label complexity term that is quasi-linear in the batch size. To our knowledge, this provides the first theoretical justification for such algorithms and we show how they can be applied to batch variants of three canonical online AL algorithms: IWAL, ORIWAL, and DHM. Finally, we also present empirical results across several benchmark datasets that corroborate these theoretical insights.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3482,3491,,,,,,,,,,,,,,,,WOS:000559931300011,0
C,"Bell, J; Bellet, A; Gascon, A; Kulkarni, T",,"Chiappa, S; Calandra, R",,"Bell, James; Bellet, Aurelien; Gascon, Adria; Kulkarni, Tejas",,,Private Protocols for U-Statistics in the Local Model and Beyond,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"In this paper, we study the problem of computing U-statistics of degree 2, i.e., quantities that come in the form of averages over pairs of data points, in the local model of differential privacy (LDP). The class of U-statistics covers many statistical estimates of interest, including Gini mean difference, Kendall's tau coefficient and Area under the ROC Curve (AUC), as well as empirical risk measures for machine learning problems such as ranking, clustering and metric learning. We first introduce an LDP protocol based on quantizing the data into bins and applying randomized response, which guarantees an epsilon-LDP estimate with a Mean Squared Error (MSE) of O(1/root n epsilon) under regularity assumptions on the U-statistic or the data distribution. We then propose a specialized protocol for AUC based on a novel use of hierarchical histograms that achieves MSE of O (alpha(3)/n epsilon(2)) for arbitrary data distribution. We also show that 2-party secure computation allows to design a protocol with MSE of O(1/n epsilon(2)), without any assumption on the kernel function or data distribution and with total communication linear in the number of users n. Finally, we evaluate the performance of our protocols through experiments on synthetic and real datasets.",,,,,,"Bellet, Aurelien/0000-0003-3440-1251",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1573,1582,,,,,,,,,,,,,,,,WOS:000559931300028,0
C,"Buesing, L; Weber, T; Hees, N",,"Chiappa, S; Calandra, R",,"Buesing, Lars; Weber, Theophane; Hees, Nicolas",,,Approximate Inference in Discrete Distributions with Monte Carlo Tree Search and Value Functions,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Exact probabilistic inference in discrete models is often prohibitively expensive, as it may require evaluating the (unnormalized) target density on its entire domain. Here we consider the setting where only a limited budget of calls to the unnormalized target density oracle is available, raising the challenge of where in the domain to allocate these function calls in order to construct a good approximate solution. We formulate this problem as an instance of sequential decision-making under uncertainty and leverage methods from reinforcement learning for probabilistic inference with budget constraints. In particular, we propose the TREESAMPLE algorithm, an adaptation of Monte Carlo Tree Search to approximate inference. This algorithm caches all previous queries to the density oracle in an explicit search tree, and dynamically allocates new queries based on a best-first heuristic for exploration, using existing upper confidence bound methods. Our non-parametric inference method can be effectively combined with neural networks that compile approximate conditionals of the target, which are then used to guide the inference search and enable generalization across multiple target distributions. We show empirically that TREESAMPLE outperforms standard approximate inference methods on synthetic factor graphs.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2581,2590,,,,,,,,,,,,,,,,WOS:000559931300039,0
C,"Fortuin, V; Baranchuk, D; Ratsch, G; Mandt, S",,"Chiappa, S; Calandra, R",,"Fortuin, Vincent; Baranchuk, Dmitry; Ratsch, Gunnar; Mandt, Stephan",,,GP-VAE: Deep Probabilistic Multivariate Time Series Imputation,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Multivariate time series with missing values are common in areas such as healthcare and finance, and have grown in number and complexity over the years. This raises the question whether deep learning methodologies can outperform classical data imputation methods in this domain. However, naive applications of deep learning fall short in giving reliable confidence estimates and lack interpretability. We propose a new deep sequential latent variable model for dimensionality reduction and data imputation. Our modeling assumption is simple and interpretable: the high dimensional time series has a lower-dimensional representation which evolves smoothly in time according to a Gaussian process. The non-linear dimensionality reduction in the presence of missing data is achieved using a VAE approach with a novel structured variational approximation. We demonstrate that our approach outperforms several classical and deep learning-based data imputation methods on high-dimensional data from the domains of computer vision and healthcare, while additionally improving the smoothness of the imputations and providing interpretable uncertainty estimates.",,,,,"Fortuin, Vincent/GXZ-5395-2022","Fortuin, Vincent/0000-0002-0640-2671",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1651,1660,,,,,,,,,,,,,,,,WOS:000559931301005,0
C,"Geng, Q; Ding, W; Guo, RQ; Kumar, S",,"Chiappa, S; Calandra, R",,"Geng, Quan; Ding, Wei; Guo, Ruiqi; Kumar, Sanjiv",,,Tight Analysis of Privacy and Utility Tradeoff in Approximate Differential Privacy,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We characterize the minimum noise amplitude and power for noise-adding mechanisms in (epsilon, delta)-differential privacy for single real-valued query function. We derive new lower bounds using the duality of linear programming, and new upper bounds by analyzing a special class of (epsilon, delta)-differentially private mechanisms, the truncated Laplacian mechanisms. We show that the multiplicative gap of the lower bounds and upper bounds goes to zero in various high privacy regimes, proving the tightness of the lower and upper bounds. In particular, our results close the previous constant multiplicative gap in the discrete setting. Numeric experiments show the improvement of the truncated Laplacian mechanism over the optimal Gaussian mechanism in all privacy regimes.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,89,98,,,,,,,,,,,,,,,,WOS:000559931301017,0
C,"Im, S; Qaem, MM; Moseley, B; Sun, XR; Zhou, R",,"Chiappa, S; Calandra, R",,"Im, Sungjin; Qaem, Mahshid Montazer; Moseley, Benjamin; Sun, Xiaorui; Zhou, Rudy",,,Fast Noise Removal for k-Means Clustering,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"This paper considers k-means clustering in the presence of noise. It is known that k-means clustering is highly sensitive to noise, and thus noise should be removed to obtain a quality solution. A popular formulation of this problem is called k-means clustering with outliers. The goal of k-means clustering with outliers is to discard up to a specified number z of points as noise/outliers and then find a k-means solution on the remaining data. The problem has received significant attention, yet current algorithms with theoretical guarantees suffer from either high running time or inherent loss in the solution quality. The main contribution of this paper is two-fold. Firstly, we develop a simple greedy algorithm that has provably strong worst case guarantees. The greedy algorithm adds a simple preprocessing step to remove noise, which can be combined with any k-means clustering algorithm. This algorithm gives the first pseudo-approximation-preserving reduction from k-means with outliers to k-means without outliers. Secondly, we show how to construct a coreset of size O(k log n). When combined with our greedy algorithm, we obtain a scalable, near linear time algorithm. The theoretical contributions are verified experimentally by demonstrating that the algorithm quickly removes noise and obtains a high-quality clustering.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,456,465,,,,,,,,,,,,,,,,WOS:000559931301052,0
C,"Izbicki, R; Shimizu, GY; Stern, RB",,"Chiappa, S; Calandra, R",,"Izbicki, Rafael; Shimizu, Gilson Y.; Stern, Rafael B.",,,Flexible distribution-free conditional predictive bands using density estimators,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Conformal methods create prediction bands that control average coverage assuming solely i.i.d. data. Besides average coverage, one might also desire to control conditional coverage, that is, coverage for every new testing point. However, without strong assumptions, conditional coverage is unachievable. Given this limitation, the literature has focused on methods with asymptotical conditional coverage. In order to obtain this property, these methods require strong conditions on the dependence between the target variable and the features. We introduce two conformal methods based on conditional density estimators that do not depend on this type of assumption to obtain asymptotic conditional coverage: Dist-split and CD-split. While Dist-split asymptotically obtains optimal intervals, which are easier to interpret than general regions, CD-split obtains optimal size regions, which are smaller than intervals. CD-split also obtains local coverage by creating prediction bands locally on a partition of the features space. This partition is data-driven and scales to high-dimensional settings. In a wide variety of simulated scenarios, our methods have a better control of conditional coverage and have smaller length than previously proposed methods.",,,,,"Shimizu, Gilson Yuuji/AAA-8325-2022","Shimizu, Gilson Yuuji/0000-0003-3711-5592",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3068,3076,,,,,,,,,,,,,,,,WOS:000559931301056,0
C,"Jin, Y; Loukas, A; Jaja, JF",,"Chiappa, S; Calandra, R",,"Jin, Yu; Loukas, Andreas; Jaja, Joseph F.",,,Graph Coarsening with Preserved Spectral Properties,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"In graph coarsening, one aims to produce a coarse graph of reduced size while preserving important graph properties. However, as there is no consensus on which specific graph properties should be preserved by coarse graphs, measuring the differences between original and coarse graphs remains a key challenge. This work relies on spectral graph theory to justify a distance function constructed to measure the similarity between original and coarse graphs. We show that the proposed spectral distance captures the structural differences in the graph coarsening process. We also propose graph coarsening algorithms that aim to minimize the spectral distance. Experiments show that the proposed algorithms can outperform previous graph coarsening methods in graph classification and stochastic block recovery tasks.",,,,,,"Loukas, Andreas/0000-0003-4866-1599",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,4452,4461,,,,,,,,,,,,,,,,WOS:000559931301063,0
C,"Khaled, A; Mishchenko, K; Richtarik, P",,"Chiappa, S; Calandra, R",,"Khaled, Ahmed; Mishchenko, Konstantin; Richtarik, Peter",,,Tighter Theory for Local SGD on Identical and Heterogeneous Data,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We provide a new analysis of local SGD, removing unnecessary assumptions and elaborating on the difference between two data regimes: identical and heterogeneous. In both cases, we improve the existing theory and provide values of the optimal stepsize and optimal number of local iterations. Our bounds are based on a new notion of variance that is specific to local SGD methods with different data. The tightness of our results is guaranteed by recovering known statements when we plug H = 1, where H is the number of local steps. The empirical evidence further validates the severe impact of data heterogeneity on the performance of local SGD.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,4519,4528,,,,,,,,,,,,,,,,WOS:000559931300027,0
C,"Kim, C; Bayati, M",,"Chiappa, S; Calandra, R",,"Kim, Carolyn; Bayati, Mohsen",,,Recommendation on a Budget: Column Space Recovery from Partially Observed Entries with Random or Active Sampling,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We analyze alternating minimization for column space recovery of a partially observed, approximately low rank matrix with a growing number of columns and a fixed budget of observations per column. We prove that if the budget is greater than the rank of the matrix, column space recovery succeeds - as the number of columns grows, the estimate from alternating minimization converges to the true column space with probability tending to one. From our proof techniques, we naturally formulate an active sampling strategy for choosing entries of a column that is theoretically and empirically (on synthetic and real data) better than the commonly studied uniformly random sampling strategy.",,,,,,"Bayati, Mohsen/0000-0002-7280-912X",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,445,454,,,,,,,,,,,,,,,,WOS:000559931301072,0
C,"Koskela, A; Honkela, A",,"Chiappa, S; Calandra, R",,"Koskela, Antti; Honkela, Antti",,,Learning Rate Adaptation for Differentially Private Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Differentially private learning has recently emerged as the leading approach for privacy-preserving machine learning. Differential privacy can complicate learning procedures because each access to the data needs to be carefully designed and carries a privacy cost. For example, standard parameter tuning with a validation set cannot be easily applied. In this paper, we propose a differentially private algorithm for the adaptation of the learning rate for differentially private stochastic gradient descent (SGD) that avoids the need for validation set use. The idea for the adaptiveness comes from the technique of extrapolation in numerical analysis: to get an estimate for the error against the gradient flow we compare the result obtained by one full step and two half-steps. We prove the privacy of the method using the moments accountant mechanism. This allows us to compute tight privacy bounds. Empirically we show that our method is competitive with manually tuned commonly used optimisation methods for training deep neural networks and differentially private variational inference.",,,,,"Koskela, Antti/AAD-6202-2021",,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2465,2474,,,,,,,,,,,,,,,,WOS:000559931301084,0
C,"Mialon, G; d'Aspremont, A; Mairal, J",,"Chiappa, S; Calandra, R",,"Mialon, Gregoire; d'Aspremont, Alexandre; Mairal, Julien",,,Screening Data Points in Empirical Risk Minimization via Ellipsoidal Regions and Safe Loss Functions,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We design simple screening tests to automatically discard data samples in empirical risk minimization without losing optimization guarantees. We derive loss functions that produce dual objectives with a sparse solution. We also show how to regularize convex losses to ensure such a dual sparsity-inducing property, and propose a general method to design screening tests for classification or regression based on ellipsoidal approximations of the optimal set. In addition to producing computational gains, our approach also allows us to compress a dataset into a subset of representative points.",,,,,"Mairal, Julien/AAL-5611-2021",,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3610,3619,,,,,,,,,,,,,,,,WOS:000559931302042,0
C,"Qin, Y; Peng, J; Zhou, Y",,"Chiappa, S; Calandra, R",,"Qin, Yue; Peng, Jian; Zhou, Yuan",,,A PTAS for the Bayesian Thresholding Bandit Problem,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"In this paper, we study the Bayesian thresholding bandit problem (BTBP), where the goal is to adaptively make a budget of Q queries to n stochastic arms and determine the label for each arm (whether its mean reward is closer to 0 or 1). We present a polynomial-time approximation scheme for the BTBP with runtime O(f(epsilon) + Q) that achieves expected labeling accuracy at least (OPT(Q) - epsilon), where f (center dot) is a function that only depends on and OPT(Q) is the optimal expected accuracy achieved by any algorithm. For any fixed epsilon > 0, our algorithm runs in time linear with Q. The main algorithmic ideas we use include discretization employed in the PTASs for many dynamic programming algorithms (such as Knapsack), as well as many problem specific techniques such as proving an upper bound on the number of query numbers for any arm made by an almost optimal policy, and establishing the smoothness property of the OPT(center dot) curve, etc.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2455,2463,,,,,,,,,,,,,,,,WOS:000559931302083,0
C,"Rontsis, N; Goulart, PJ",,"Chiappa, S; Calandra, R",,"Rontsis, Nikitas; Goulart, Paul J.",,,Optimal Approximation of Doubly Stochastic Matrices,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We consider the least-squares approximation of a matrix C in the set of doubly stochastic matrices with the same sparsity pattern as C. Our approach is based on applying the well-known Alternating Direction Method of Multipliers (ADMM) to a reformulation of the original problem. Our resulting algorithm requires an initial Cholesky factorization of a positive definite matrix that has the same sparsity pattern as C + I followed by simple iterations whose complexity is linear in the number of nonzeros in C, thus ensuring excellent scalability and speed. We demonstrate the advantages of our approach in a series of experiments on problems with up to 82 million nonzeros; these include normalizing large scale matrices arising from the 3D structure of the human genome, clustering applications, and the SuiteSparse matrix library. Overall, our experiments illustrate the outstanding scalability of our algorithm; matrices with millions of nonzeros can be approximated in a few seconds on modest desktop computing hardware.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3589,3597,,,,,,,,,,,,,,,,WOS:000559931302094,0
C,"Shang, XD; de Heide, R; Kaufmann, E; Menard, P; Valko, M",,"Chiappa, S; Calandra, R",,"Shang, Xuedong; de Heide, Rianne; Kaufmann, Emilie; Menard, Pierre; Valko, Michal",,,Fixed-Confidence Guarantees for Bayesian Best-Arm Identification,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We investigate and provide new insights on the sampling rule called Top-Two Thompson Sampling (TTTS). In particular, we justify its use for fixed-confidence best-arm identification. We further propose a variant of TTTS called Top-Two Transportation Cost (T3C), which disposes of the computational burden of TTTS. As our main contribution, we provide the first sample complexity analysis of TTTS and T3C when coupled with a very natural Bayesian stopping rule, for bandits with Gaussian rewards, solving one of the open questions raised by Russo (2016). We also provide new posterior convergence results for TTTS under two models that are commonly used in practice: bandits with Gaussian and Bernoulli rewards and conjugate priors.",,,,,"Shang, Xuedong/AAD-2793-2022","Shang, Xuedong/0000-0002-1537-6540",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303010,0
C,"Sidford, A; Wang, MD; Yang, LF; Ye, YY",,"Chiappa, S; Calandra, R",,"Sidford, Aaron; Wang, Mengdi; Yang, Lin F.; Ye, Yinyu",,,Solving Discounted Stochastic Two-Player Games with Near-Optimal Time and Sample Complexity,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"In this paper we settle the sampling complexity of solving discounted two-player turn-based zero-sum stochastic games up to poly-logarithmic factors. Given a stochastic game with discount factor gamma is an element of (0, 1) we provide an algorithm that computes an epsilon-optimal strategy with high-probability given O((1 - gamma)(-3) epsilon(-2)) samples from the transition function for each state-action-pair. Our algorithm runs in time nearly linear in the number of samples and uses space nearly linear in the number of state-action pairs. As stochastic games generalize Markov decision processes (MDPs) our runtime and sample complexities are optimal due to Azar et al. (2013). We achieve our results by showing how to generalize a nearoptimal Q-learning based algorithms for MDP, in particular Sidford et al. (2018a), to two-player strategy computation algorithms. This overcomes limitations of standard Q-learning and strategy iteration or alternating minimization based approaches and we hope will pave the way for future reinforcement learning results by facilitating the extension of MDP results to multi-agent settings with little loss.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303021,0
C,"Singh, SP; Hug, A; Dieuleveut, A; Jaggi, M",,"Chiappa, S; Calandra, R",,"Singh, Sidak Pal; Hug, Andreas; Dieuleveut, Aymeric; Jaggi, Martin",,,Context Mover's Distance & Barycenters: Optimal Transport of Contexts for Building Representations,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We present a framework for building unsupervised representations of entities and their compositions, where each entity is viewed as a probability distribution rather than a vector embedding. In particular, this distribution is supported over the contexts which co-occur with the entity and are embedded in a suitable low-dimensional space. This enables us to consider representation learning from the perspective of Optimal Transport and take advantage of its tools such as Wasserstein distance and barycenters. We elaborate how the method can be applied for obtaining unsupervised representations of text and illustrate the performance (quantitatively as well as qualitatively) on tasks such as measuring sentence similarity, word entailment and similarity, where we empirically observe significant gains (e.g., 4.1% relative improvement over Sent2vec, GenSen). The key benefits of the proposed approach include: (a) capturing uncertainty and polysemy via modeling the entities as distributions, (b) utilizing the underlying geometry of the particular task (with the ground cost), (c) simultaneously providing interpretability with the notion of optimal transport between contexts and (d) easy applicability on top of existing point embedding methods. The code, as well as pre-built histograms, are available under https://hub.com/contextsmover/.",,,,,,"Jaggi, Martin/0000-0003-1579-5558",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303023,0
C,"Virtanen, S; Girolami, M",,"Chiappa, S; Calandra, R",,"Virtanen, Seppo; Girolami, Mark",,,Dynamic content based ranking,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We introduce a novel state space model for a set of sequentially time-stamped partial rankings of items and textual descriptions for the items. Based on the data, the model infers text-based themes that are predictive of the rankings enabling forecasting tasks and performing trend analysis. We propose a scaled Gamma process based prior for capturing the underlying dynamics. Based on two challenging and contemporary real data collections, we show the model infers meaningful and useful textual themes as well as performs better than existing related dynamic models.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303056,0
C,"Wang, PA; Proutiere, A; Ariu, K; Jedra, Y; Russo, A",,"Chiappa, S; Calandra, R",,"Wang, Po-An; Proutiere, Alexandre; Ariu, Kaito; Jedra, Yassir; Russo, Alessio",,,Optimal Algorithms for Multiplayer Multi-Armed Bandits,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"The paper addresses various Multiplayer Multi-Armed Bandit (MMAB) problems, where M decision-makers, or players, collaborate to maximize their cumulative reward. We first investigate the MMAB problem where players selecting the same arms experience a collision (and are aware of it) and do not collect any reward. For this problem, we present DPE1 (Decentralized Parsimonious Exploration), a decentralized algorithm that achieves the same asymptotic regret as that obtained by an optimal centralized algorithm. DPE1 is simpler than the state-of-the-art algorithm SIC-MMAB Boursier and Pen-het (2019), and yet offers better performance guarantees. We then study the MMAB problem without collision, where players may select the same arm. Players sit on vertices of a graph, and in each round, they are able to send a message to their neighbours in the graph. We present DPE2, a simple and asymptotically optimal algorithm that outperforms the state-of-the-art algorithm DD-UCB Martinez-Rubio et al. (2019). Besides, under DPE2, the expected number of bits transmitted by the players in the graph is finite.",,,,,,"Jedra, Yassir/0000-0002-4403-1066; Russo, Alessio/0000-0001-9083-5260",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303071,0
C,"Zrnic, T; Jiang, DL; Ramdas, A; Jordan, MI",,"Chiappa, S; Calandra, R",,"Zrnic, Tijana; Jiang, Daniel L.; Ramdas, Aaditya; Jordan, Michael I.",,,The Power of Batching in Multiple Hypothesis Testing,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"One important partition of algorithms for controlling the false discovery rate (FDR) in multiple testing is into offline and online algorithms. The first generally achieve significantly higher power of discovery, while the latter allow making decisions sequentially as well as adaptively formulating hypotheses based on past observations. Using existing methodology, it is unclear how one could trade off the benefits of these two broad families of algorithms, all the while preserving their formal FDR guarantees. To this end, we introduce Batch(BH) and Batch(St-BH), algorithms for controlling the FDR when a possibly infinite sequence of batches of hypotheses is tested by repeated application of one of the most widely used offline algorithms, the Benjamini-Hochberg (BH) method or Storey's improvement of the BH method. We show that our algorithms interpolate between existing online and offline methodology, thus trading off the best of both worlds.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3806,3814,,,,,,,,,,,,,,,,WOS:000559931304028,0
C,"Amid, E; Warmuth, MK; Srinivasan, S",,"Chaudhuri, K; Sugiyama, M",,"Amid, Ehsan; Warmuth, Manfred K.; Srinivasan, Sriram",,,Two-temperature logistic regression based on the Tsallis divergence,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We develop a variant of multiclass logistic regression that is significantly more robust to noise. The algorithm has one weight vector per class and the surrogate loss is a function of the linear activations (one per class). The surrogate loss of an example with linear activation vector a and class c has the form -log(t1) exp(t2)(a(c) - G(t2)(a)) where the two temperatures t(1) and t(2) temper the log and exp, respectively, and G(t2) (a) is a scalar value that generalizes the log-partition function. We motivate this loss using the Tsallis divergence. Our method allows transitioning between non-convex and convex losses by the choice of the temperature parameters. As the temperature l(1) of the logarithm becomes smaller than the temperature t(2) of the exponential, the surrogate loss becomes quasi convex. Various tunings of the temperatures recover previous methods and tuning the degree of non-convexity is crucial in the experiments. In particular, quasi-convexity and boundedness of the loss provide significant robustness to the outliers. We explain this by showing that t(1) < 1 caps the surrogate loss and t(2) > 1 makes the predictive distribution have a heavy tail. We show that the surrogate loss is Bayes-consistent, even in the non-convex case. Additionally, we provide efficient iterative algorithms for calculating the log-partition value only in a few number of iterations. Our compelling experimental results on large real-world datasets show the advantage of using the two-temperature variant in the noisy as well as the noise free case.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902045,0
C,"Cheng, CA; Yan, XY; Theodorou, EA; Boots, B",,"Chaudhuri, K; Sugiyama, M",,"Cheng, Ching-An; Yan, Xinyan; Theodorou, Evangelos A.; Boots, Byron",,,Accelerating Imitation Learning with Predictive Models,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Sample efficiency is critical in solving real-world reinforcement learning problems where agent-environment interactions can be costly. Imitation learning from expert advice has proved to be an effective strategy for reducing the number of interactions required to train a policy. Online imitation learning, which interleaves policy evaluation and policy optimization, is a particularly effective technique with provable performance guarantees. In this work, we seek to further accelerate the convergence rate of online imitation learning, thereby making it more sample efficient. We propose two model-based algorithms inspired by Follow-the-Leader (FTL) with prediction: MoBIL-VI based on solving variational inequalities and MOBIL-PRox based on stochastic first-order updates. These two methods leverage a model to predict future gradients to speed up policy learning. When the model oracle is learned online, these algorithms can provably accelerate the best known convergence rate up to an order. Our algorithms can be viewed as a generalization of stochastic MIRROR-PROX (Juditsky et al., 2011), and admit a simple constructive FTL-style analysis of performance.",,,,,"Cheng, Ching-An/AAZ-1802-2020; Yan, Xinyan/AAT-3052-2021; Yan, Xinyan/AAO-3563-2021","Cheng, Ching-An/0000-0002-0610-2070; ",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687903025,0
C,"Dai, B; Dai, HJ; Gretton, A; Song, L; Schuurmans, D; He, N",,"Chaudhuri, K; Sugiyama, M",,"Dai, Bo; Dai, Hanjun; Gretton, Arthur; Song, Le; Schuurmans, Dale; He, Niao",,,Kernel Exponential Family Estimation via Doubly Dual Embedding,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We investigate penalized maximum log-likelihood estimation for exponential family distributions whose natural parameter resides in a reproducing kernel Hilbert space. Key to our approach is a novel technique, doubly dual embedding, that avoids computation of the partition function. This technique also allows the development of a flexible sampling strategy that amortizes the cost of Monte-Carlo sampling in the inference stage. The resulting estimator can be easily generalized to kernel conditional exponential families. We establish a connection between kernel exponential family estimation and MMD-GANs, revealing a new perspective for understanding GANs. Compared to the score matching based estimators, the proposed method improves both memory and time efficiency while enjoying stronger statistical properties, such as fully capturing smoothness in its statistical convergence rate while the score matching estimator appears to saturate. Finally, we show that the proposed estimator empirically outperforms state-of-the-art methods in both kernel exponential family estimation and its conditional extension.",,,,,"Dai, Hanjun/AAQ-8943-2021","Gretton, Arthur/0000-0003-3169-7624",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902038,0
C,"Jung, A; Vesselinova, N",,"Chaudhuri, K; Sugiyama, M",,"Jung, A.; Vesselinova, N.",,,Analysis of Network Lasso for Semi-Supervised Regression,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We apply network Lasso to semi-supervised regression problems involving network-structured data. This approach lends quite naturally to highly scalable learning algorithms in the form of message passing over an empirical graph which represents the network structure of the data. By using a simple non-parametric regression model, which is motivated by a clustering hypothesis, we provide an analysis of the estimation error incurred by network Lasso. This analysis reveals conditions on the network structure and the available training data which guarantee network Lasso to be accurate. Remarkably, the accuracy of network Lasso is related to the existence of sufficiently large network flows over the empirical graph. Thus, our analysis reveals a connection between network Lasso and maximum flow problems.",,,,,"Jung, Alexander/M-4407-2016","Jung, Alexander/0000-0001-7538-0990",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,380,387,,,,,,,,,,,,,,,,WOS:000509687900040,0
C,"Kamiya, S; Miyashiro, R; Takano, Y",,"Chaudhuri, K; Sugiyama, M",,"Kamiya, Shunsuke; Miyashiro, Ryuhei; Takano, Yuichi",,,Feature subset selection for the multinomial logit model via mixed-integer optimization,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"This paper is concerned with a feature subset selection problem for the multinomial logit (MNL) model. There are several convex approximation algorithms for this problem, but to date the only exact algorithms are those for the binomial logit model. In this paper, we propose an exact algorithm to solve the problem for the MNL model. Our algorithm is based on a mixed-integer optimization approach with an outer approximation method. We prove the convergence properties of the algorithm for more general models including generalized linear models for multiclass classification. We also propose approximation of loss functions to accelerate the algorithm computationally. Numerical experiments demonstrate that our exact and approximation algorithms achieve better generalization performance than does an L-1-regularization method.",,,,,"Miyashiro, Ryuhei/B-9929-2013","Miyashiro, Ryuhei/0000-0003-3828-4013",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901031,0
C,"Levy, KY; Krause, A",,"Chaudhuri, K; Sugiyama, M",,"Levy, Kfir Y.; Krause, Andreas",,,Projection Free Online Learning over Smooth Sets,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"The projection operation is a crucial step in applying Online Gradient Descent (OGD) and its stochastic version SGD. Unfortunately, in some cases, projection is computationally demanding and inhibits us from applying OGD. In this work, we focus on the special case where the constraint set is smooth and we have an access to gradient and value oracles of the constraint function. Under these assumptions we design a new approximate projection operation that necessitates only logarithmically many calls to these oracles. We further show that combining OGD with this new approximate projection, results in a projectionfree variant that recovers the standard rates of the fully projected version. This applies to both convex and strongly-convex online settings.",,,,,,"Krause, Andreas/0000-0001-7260-9673",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901052,0
C,"Li, YT; Murias, M; Major, S; Dawson, G; Carlson, DE",,"Chaudhuri, K; Sugiyama, M",,"Li, Yitong; Murias, Michael; Major, Samantha; Dawson, Geraldine; Carlson, David E.",,,On Target Shift in Adversarial Domain Adaptation,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Discrepancy between training and testing domains is a fundamental problem in the generalization of machine learning techniques. Recently, several approaches have been proposed to learn domain invariant feature representations through adversarial deep learning. However, label shift, where the percentage of data in each class is different between domains, has received less attention. Label shift naturally arises in many contexts, especially in behavioral studies where the behaviors are freely chosen. In this work, we propose a method called Domain Adversarial nets for Target Shift (DAYS) to address label shift while learning a domain invariant representation. This is accomplished by using distribution matching to estimate label proportions in a blind test set. We extend this framework to handle multiple domains by developing a scheme to upweight source domains most similar to the target domain. Empirical results show that this framework performs well under large label shift in synthetic and real experiments, demonstrating the practical importance.",,,,,,"Carlson, David/0000-0003-1005-6385; Dawson, Geraldine/0000-0003-1410-2764",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,616,625,,,,,,,,,,,,,,,,WOS:000509687900064,0
C,"Ma, YF; Wang, YX; Narayanaswamy, B",,"Chaudhuri, K; Sugiyama, M",,"Ma, Yifei; Wang, Yu-Xiang; Narayanaswamy, Balakrishnan (Murali)",,,Imitation-Regularized Offline Learning,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We study the problem of offline learning in automated decision systems under the contextual bandits model. We are given logged historical data consisting of contexts, (randomized) actions, and (nonnegative) rewards. A common goal is to evaluate what would happen if different actions were taken in the same contexts, so as to optimize the action policies accordingly. The typical approach to this problem, inverse probability weighted estimation (IPWE) [5], requires logged action probabilities, which may be missing in practice due to engineering complications. Even when available, small action probabilities cause large uncertainty in IPWE, rendering the corresponding results insignificant. To solve both problems, we show how one can use policy improvement (PIL) objectives, regularized by policy imitation (IML). We motivate and analyze PIL as an extension to Clipped-IPWE, by showing that both are lower-bound surrogates to the vanilla IPWE. We also formally connect IML to IPWE variance estimation [31] and natural policy gradients. Without probability logging, our PIL-IML interpretations justify and improve, by reward-weighting, the state-of-art cross-entropy (CE) loss that predicts the action items among all action candidates available in the same contexts. With probability logging, our main theoretical contribution connects IML-underfitting to the existence of either confounding variables or model misspecification. We show the value and accuracy of our insights by simulations based on Simpson's paradox, standard UCI multiclass-to-bandit conversions and on the Criteo counterfactual analysis challenge dataset.",,,,,,"Wang, Yu-Xiang/0000-0002-6403-212X",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687903001,0
C,"Malinsky, D; Spirtes, P",,"Chaudhuri, K; Sugiyama, M",,"Malinsky, Daniel; Spirtes, Peter",,,Learning the Structure of a Nonstationary Vector Autoregression,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We adapt graphical causal structure learning methods to apply to nonstationary time series data, specifically to processes that exhibit stochastic trends. We modify the likelihood component of the BIC score used by score-based search algorithms, such that it remains a consistent selection criterion for integrated or cointegrated processes. We use this modified score in conjunction with the SVAR-GFCI algorithm [15], which allows us to recover qualitative structural information about the underlying data-generating process even in the presence of latent (unmeasured) factors. We demonstrate our approach on both simulated and real macroeconomic data.",,,,,"Spirtes, Peter/GYD-5724-2022",,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687903004,0
C,"Molchanov, D; Kharitonov, V; Sobolev, A; Vetrov, D",,"Chaudhuri, K; Sugiyama, M",,"Molchanov, Dmitry; Kharitonov, Valery; Sobolev, Artem; Vetrov, Dmitry",,,Doubly Semi-Implicit Variational Inference,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We extend the existing framework of semi-implicit variational inference (SIVI) and introduce doubly semi-implicit variational inference (DSIVI), a way to perform variational inference and learning when both the approximate posterior and the prior distribution are semi-implicit. In other words, DSIVI performs inference in models where the prior and the posterior can be expressed as an intractable infinite mixture of some analytic density with a highly flexible implicit mixing distribution. We provide a sandwich bound on the evidence lower bound (ELBO) objective that can be made arbitrarily tight. Unlike discriminator-based and kernel-based approaches to implicit variational inference, DSIVI optimizes a proper lower bound on ELBO that is asymptotically exact. We evaluate DSIVI on a set of problems that benefit from implicit priors. In particular, we show that DSIVI gives rise to a simple modification of VampPrior, the current state-of-the-art prior for variational autoencoders, which improves its performance.",,,,,"Molchanov, Dmitry/Y-6008-2018",,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902066,0
C,"Okuno, A; Kim, G; Shimodaira, H",,"Chaudhuri, K; Sugiyama, M",,"Okuno, Akifumi; Kim, Geewook; Shimodaira, Hidetoshi",,,Graph Embedding with Shifted Inner Product Similarity and Its Improved Approximation Capability,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We propose shifted inner-product similarity (SIPS), which is a novel yet very simple extension of the ordinary inner-product similarity (IPS) for neural-network based graph embedding (GE). In contrast to IPS, that is limited to approximating positive-definite (PD) similarities, SIPS goes beyond the limitation by introducing bias terms in IPS; we theoretically prove that SIPS is capable of approximating not only PD but also conditionally PD (CPD) similarities with many examples such as cosine similarity, negative Poincare distance and negative Wasserstein distance. Since SIPS with sufficiently large neural networks learns a variety of similarities, SIPS alleviates the need for configuring the similarity function of GE. Approximation error rate is also evaluated, and experiments on two real-world datasets demonstrate that graph embedding using SIPS indeed outperforms existing methods.",,,,,"Shimodaira, Hidetoshi/B-9127-2008","Shimodaira, Hidetoshi/0000-0002-3371-7724",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,644,653,,,,,,,,,,,,,,,,WOS:000509687900067,0
C,"Papamakarios, G; Sterratt, DC; Murray, I",,"Chaudhuri, K; Sugiyama, M",,"Papamakarios, George; Sterratt, David C.; Murray, Iain",,,Sequential Neural Likelihood: Fast Likelihood-free Inference with Autoregressive Flows,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We present Sequential Neural Likelihood (SNL), a new method for Bayesian inference in simulator models, where the likelihood is intractable but simulating data from the model is possible. SNL trains an autoregressive flow on simulated data in order to learn a model of the likelihood in the region of high posterior density. A sequential training procedure guides simulations and reduces simulation cost by orders of magnitude. We show that SNL is more robust, more accurate and requires less tuning than related neural-based methods, and we discuss diagnostics for assessing calibration, convergence and goodness-of-fit.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,837,848,,,,,,,,,,,,,,,,WOS:000509687900087,0
C,"Song, JL; Chen, YX; Yue, YS",,"Chaudhuri, K; Sugiyama, M",,"Song, Jialin; Chen, Yuxin; Yue, Yisong",,,A General Framework for Multi-fidelity Bayesian Optimization with Gaussian Processes,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"How can we efficiently gather information to optimize an unknown function, when presented with multiple, mutually dependent information sources with different costs? For example, when optimizing a physical system, intelligently trading off computer simulations and real-world tests can lead to significant savings. Existing multi-fidelity Bayesian optimization methods, such as multi-fidelity GP-UCB or Entropy Search-based approaches, either make simplistic assumptions on the interaction among different fidelities or use simple heuristics that lack theoretical guarantees. In this paper, we study multifidelity Bayesian optimization with complex structural dependencies among multiple outputs, and propose MF-MI-Greedy, a principled algorithmic framework for addressing this problem. In particular, we model different fidelities using additive Gaussian processes based on shared latent relationships with the target function. Then we use cost-sensitive mutual information gain for efficient Bayesian optimization. We propose a simple notion of regret which incorporates the varying cost of different fidelities, and prove that MF-MI-Greedy achieves low regret. We demonstrate the strong empirical performance of our algorithm on both synthetic and real-world datasets.",,,,,,"Chen, Yuxin/0000-0003-2133-140X",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687903022,0
C,"Uhrenholt, AK; Jensen, BS",,"Chaudhuri, K; Sugiyama, M",,"Uhrenholt, Anders Kirk; Jensen, Bjorn Sand",,,Efficient Bayesian Optimization for Target Vector Estimation,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We consider the problem of estimating target, vector by querying an unknown multi-output function which is stochastic and expensive to evaluate. Through sequential experimental design the aim is to minimize the squared Euclidean distance between the output of the function and the target vector. Applying standard single-objective Bayesian nation to this problem is both wasteful, since individual output components are never observed, and imprecise since the predictive distribution for new inputs will be symmetric and have support in the negative domain. We address this issue by proposing a Gaussian process model that takes into account the individual function outputs and derive a distribution over the resulting 2-norm. Furthermore, we derive computationally efficient acquisition functions and evaluate the resulting optimization framework on several synthetic benchmark functions and a real-world problem. The results demonstrate a significant improvement over standard Bayesian optimization methods based on both standard and Warped Gaussian processes.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902073,0
C,"Usmanova, I; Krause, A; Kamgarpour, M",,"Chaudhuri, K; Sugiyama, M",,"Usmanova, Ilnura; Krause, Andreas; Kamgarpour, Maryam",,,Safe Convex Learning under Uncertain Constraints,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We address the problem of minimizing a convex smooth function f(x) over a compact polyhedral set D given a stochastic zeroth-order constraint feedback model. This problem arises in safety-critical machine learning applications, such as personalized medicine and robotics. In such cases, one needs to ensure constraints are satisfied while exploring the decision space to find optimum of the loss function. We propose a new variant of the Frank-Wolfe algorithm, which applies to the case of uncertain linear constraints. Using robust optimization, we provide the convergence rate of the algorithm while guaranteeing feasibility of all iterates, with high probability.(1)",,,,,,"Krause, Andreas/0000-0001-7260-9673",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902016,0
C,"von Kugelgen, J; Mey, A; Loog, M",,"Chaudhuri, K; Sugiyama, M",,"von Kuegelgen, Julius; Mey, Alexander; Loog, Marco",,,Semi-Generative Modelling: Covariate-Shift Adaptation with Cause and Effect Features,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Current methods for covariate-shift adaptation use unlabelled data to compute importance weights or domain-invariant features, while the final model is trained on labelled data only. Here, we consider a particular case of covariate shift which allows us also to learn from unlabelled data, that is, combining adaptation with semi-supervised learning. Using ideas from causality, we argue that this requires learning with both causes, X-C, and effects, X-E, of a target variable, Y, and show how this setting leads to what we call a semi-generative model, P(Y, X-E vertical bar X-C, theta). Our approach is robust to domain shifts in the distribution of causal features and leverages unlabelled data by learning a direct map from causes to effects. Experiments on synthetic data demonstrate significant improvements in classification over purely-supervised and importance-weighting baselines.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901042,0
C,"Bagaria, V; Kamath, GM; Ntranos, V; Zhang, MJ; Tse, DN",,"Storkey, A; PerezCruz, F",,"Bagaria, Vivek; Kamath, Govinda M.; Ntranos, Vasilis; Zhang, Martin J.; Tse, David N.",,,Medoids in Almost-Linear Time via Multi-Armed Bandits,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Computing the medoid of a large number of points in high-dimensional space is an increasingly common operation in many data science problems. We present an algorithm Med-dit to compute the medoid with high probability, which uses O(n log n) distance evaluations. Med-dit is based on a connection with the multi-armed bandit problem. We evaluate the performance of Med-dit empirically on the Netflix-prize and single-cell RNA-Seq datasets, containing hundreds of thousands of points living in tens of thousands of dimensions, and observe a 5-10x improvement in performance over the current state of the art.",,,,,"Zhang, Martin Jinye/AAF-2206-2019","Zhang, Martin Jinye/0000-0003-0006-2466",,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300053,0
C,"Blondel, M; Seguy, V; Rolet, A",,"Storkey, A; PerezCruz, F",,"Blondel, Mathieu; Seguy, Vivien; Rolet, Antoine",,,Smooth and Sparse Optimal Transport,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Entropic regularization is quickly emerging as a new standard in optimal transport (OT). It enables to cast the OT computation as a differentiable and unconstrained convex optimization problem, which can be efficiently solved using the Sinkhorn algorithm. However, entropy keeps the transportation plan strictly positive and therefore completely dense, unlike unregularized OT. This lack of sparsity can be problematic in applications where the transportation plan itself is of interest. In this paper, we explore regularizing the primal and dual OT formulations with a strongly convex term, which corresponds to relaxing the dual and primal constraints with smooth approximations. We show how to incorporate squared 2-norm and group lasso regularizations within that framework, leading to sparse and group-sparse transportation plans. On the theoretical side, we bound the approximation error introduced by regularizing the primal and dual formulations. Our results suggest that, for the regularized primal, the approximation error can often be smaller with squared 2-norm than with entropic regularization. We showcase our proposed framework on the task of color transfer.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300093,0
C,"Eisen, M; Mokhtari, A; Ribeiro, A",,"Storkey, A; PerezCruz, F",,"Eisen, Mark; Mokhtari, Aryan; Ribeiro, Alejandro",,,Large Scale Empirical Risk Minimization via Truncated Adaptive Newton Method,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Most second order methods are inapplicable to large scale empirical risk minimization (ERM) problems because both, the number of samples N and number of parameters p are large. Large N makes it costly to evaluate Hessians and large p makes it costly to invert Hessians. This paper propose a novel adaptive sample size second-order method, which reduces the cost of computing the Hessian by solving a sequence of ERM problems corresponding to a subset of samples and lowers the cost of computing the Hessian inverse using a truncated eigenvalue decomposition. Although the sample size is grown at a geometric rate, it is shown that it is sufficient to run a single iteration in each growth stage to track the optimal classifier to within its statistical accuracy. This results in convergence to the optimal classifier associated with the whole set in a number of iterations that scales with log(N). The use of a truncated eigenvalue decomposition result in the cost of each iteration being of order p(2). Theoretical performance gains manifest in practical implementations.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300151,0
C,"Grover, A; Markov, T; Attia, P; Jin, N; Perkins, N; Cheong, B; Chen, M; Yang, Z; Harris, S; Chueh, W; Ermon, S",,"Storkey, A; PerezCruz, F",,"Grover, Aditya; Markov, Todor; Attia, Peter; Jin, Norman; Perkins, Nicholas; Cheong, Bryan; Chen, Michael; Yang, Zi; Harris, Stephen; Chueh, William; Ermon, Stefano",,,Best arm identification in multi-armed bandits with delayed feedback,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"We propose a generalization of the best arm identification problem in stochastic multi-armed bandits (MAB) to the setting where every pull of an arm is associated with delayed feedback. The delay in feedback increases the effective sample complexity of standard algorithms, but can be offset if we have access to partial feedback received before a pull is completed. We propose a general framework to model the relationship between partial and delayed feedback, and as a special case we introduce efficient algorithms for settings where the partial feedback are biased or unbiased estimators of the delayed feedback. Additionally, we propose a novel extension of the algorithms to the parallel MAB setting where an agent can control a batch of arms. Our experiments in real-world settings, involving policy search and hyperparameter optimization in computational sustainability domains for fast charging of batteries and wildlife corridor construction, demonstrate that exploiting the structure of partial feedback can lead to significant improvements over baselines in both sequential and parallel MAB.",,,,,"Attia, Peter/ABD-1427-2020","Attia, Peter/0000-0003-4745-5726",,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300088,0
C,"Kim, SJ; Lim, J; Won, JH",,"Storkey, A; PerezCruz, F",,"Kim, Seung-Jean; Lim, Johan; Won, Joong-Ho",,,Nonparametric Sharpe Ratio Function Estimation in Heteroscedastic Regression Models via Convex Optimization,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"We consider maximum likelihood estimation (MLE) of heteroscedastic regression models based on a new parametrization of the likelihood in terms of the Sharpe ratio function, or the ratio of the mean and volatility functions. While with a standard parametrization the MLE problem is not convex and hence hard to solve globally, our parametrization leads to a functional that is jointly convex in the Sharpe ratio and inverse volatility functions. The major difficulty with the resulting infinite-dimensional convex program is the shape constraint on the inverse volatility function. We propose to solve the problem by solving a sequence of finite-dimensional convex programs with increasing dimensions, which can be done globally and efficiently. We demonstrate that, when the goal is to estimate the Sharpe ratio function directly, the finite-sample performance of the proposed estimation method is superior to existing methods that estimate the mean and variance functions separately. When applied to a financial dataset, our method captures a well-known covariate-dependent effect on the Shape ratio.",,,,,"Won, Joong-Ho/GPF-9286-2022",,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300156,0
C,"Martinez-Cantin, R; Tee, K; McCourt, M",,"Storkey, A; PerezCruz, F",,"Martinez-Cantin, Ruben; Tee, Kevin; McCourt, Michael",,,Practical Bayesian optimization in the presence of outliers,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Inference in the presence of outliers is an important field of research as outliers are ubiquitous and may arise across a variety of problems and domains. Bayesian optimization is method that heavily relies on probabilistic inference. This allows outstanding sample efficiency because the probabilistic machinery provides a memory of the whole optimization process. However, that virtue becomes a disadvantage when the memory is populated with outliers, inducing bias in the estimation. In this paper, we present an empirical evaluation of Bayesian optimization methods in the presence of outliers. The empirical evidence shows that Bayesian optimization with robust regression often produces suboptimal results. We then propose a new algorithm which combines robust regression (a Gaussian process with Student-t likelihood) with outlier diagnostics to classify data points as outliers or inliers. By using an scheduler for the classification of outliers, our method is more efficient and has better convergence over the standard robust regression. Furthermore, we show that even in controlled situations with no expected outliers, our method is able to produce better results.",,,,,"Martinez-Cantin, Ruben/F-8981-2011","Martinez-Cantin, Ruben/0000-0002-6741-844X",,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300180,0
C,"Olfat, M; Aswani, A",,"Storkey, A; PerezCruz, F",,"Olfat, Matt; Aswani, Anil",,,Spectral Algorithms for Computing Fair Support Vector Machines,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Classifiers and rating scores are prone to implicitly codifying biases, which may be present in the training data, against protected classes (i.e., age, gender, or race). So it is important to understand how to design classifiers and scores that prevent discrimination in predictions. This paper develops computationally tractable algorithms for designing accurate but fair support vector machines (SVM's). Our approach imposes a constraint on the covariance matrices conditioned on each protected class, which leads to a nonconvex quadratic constraint in the SVM formulation. We develop iterative algorithms to compute fair linear and kernel SVM's, which solve a sequence of relaxations constructed using a spectral decomposition of the nonconvex constraint. Its effectiveness in achieving high prediction accuracy while ensuring fairness is shown through numerical experiments on several data sets.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300202,0
C,"Perolat, J; Piot, B; Pietquin, O",,"Storkey, A; PerezCruz, F",,"Perolat, Julien; Piot, Bilal; Pietquin, Olivier",,,Actor-Critic Fictitious Play in Simultaneous Move Multistage Games,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Fictitious play is a game theoretic iterative procedure meant to learn an equilibrium in normal form games. However, this algorithm requires that each player has full knowledge of other players' strategies. Using an architecture inspired by actor-critic algorithms, we build a stochastic approximation of the fictitious play process. This procedure is online, decentralized (an agent has no information of others' strategies and rewards) and applies to multistage games (a generalization of normal form games). In addition, we prove convergence of our method towards a Nash equilibrium in both the cases of zero-sum two-player multistage games and cooperative multistage games. We also provide empirical evidence of the soundness of our approach on the game of Alesia with and without function approximation.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300097,0
C,"Rosenfeld, N; Mansour, Y; Yom-Tov, E",,"Storkey, A; PerezCruz, F",,"Rosenfeld, Nir; Mansour, Yishay; Yom-Tov, Elad",,,Discriminative Learning of Prediction Intervals,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"In this work we consider the task of constructing prediction intervals in an inductive batch setting. We present a discriminative learning framework which optimizes the expected error rate under a budget constraint on the interval sizes. Most current methods for constructing prediction intervals offer guarantees for a single new test point. Applying these methods to multiple test points can result in a high computational overhead and degraded statistical guarantees. By focusing on expected errors, our method allows for variability in the per-example conditional error rates. As we demonstrate both analytically and empirically, this flexibility can increase the overall accuracy, or alternatively, reduce the average interval size. While the problem we consider is of a regressive flavor, the loss we use is combinatorial. This allows us to provide PAC-style, finite-sample guarantees. Computationally, we show that our original objective is NP-hard, and suggest a tractable convex surrogate. We conclude with a series of experimental evaluations.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300037,0
C,"Chen, XL; Monfort, M; Liu, AQ; Ziebart, BD",,"Gretton, A; Robert, CC",,"Chen, Xiangli; Monfort, Mathew; Liu, Anqi; Ziebart, Brian D.",,,Robust Covariate Shift Regression,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"In many learning settings, the source data available to train a regression model differs from the target data it encounters when making predictions due to input distribution shift. Appropriately dealing with this situation remains an important challenge. Existing methods attempt to reweight the source data samples to better represent the target domain, but this introduces strong inductive biases that are highly extrapolative and can often err greatly in practice. We propose a robust approach for regression under covariate shift that embraces the uncertainty resulting from sample selection bias by producing regression models that are explicitly robust to it. We demonstrate the benefits of our approach on a number of regression tasks.",,,,,,"Ziebart, Brian/0000-0003-4041-6871",,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,1270,1279,,,,,,,,,,,,,,,,WOS:000508662100138,0
C,"Hu, XW; Prashanth, LA; Gyorgy, A; Szepesvari, C",,"Gretton, A; Robert, CC",,"Hu, Xiaowei; Prashanth, L. A.; Gyorgy, Andras; Szepesvari, Csaba",,,(Bandit) Convex Optimization with Biased Noisy Gradient Oracles,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Algorithms for bandit convex optimization and online learning often rely on constructing noisy gradient estimates, which are then used in appropriately adjusted first-order algorithms, replacing actual gradients. Depending on the properties of the function to be optimized and the nature of noise in the bandit feedback, the bias and variance of gradient estimates exhibit various tradeoffs. In this paper we propose a novel framework that replaces the specific gradient estimation methods with an abstract oracle. With the help of the new framework we unify previous works, reproducing their results in a clean and concise fashion, while, perhaps more importantly, the framework also allows us to formally show that to achieve the optimal root-n rate either the algorithms that use existing gradient estimators, or the proof techniques used to analyze them have to go beyond what exists today.",,,,,,"Gyorgy, Andras/0000-0003-0586-4337",,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,819,828,,,,,,,,,,,,,,,,WOS:000508662100089,0
C,"Pu, YC; Yuan, X; Stevens, A; Li, CY; Carin, L",,"Gretton, A; Robert, CC",,"Pu, Yunchen; Yuan, Xin; Stevens, Andrew; Li, Chunyuan; Carin, Lawrence",,,A Deep Generative Deconvolutional Image Model,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"A deep generative model is developed for representation and analysis of images, based on a hierarchical convolutional dictionary-learning framework. Stochastic unpooling is employed to link consecutive layers in the model, yielding top-down image generation. A Bayesian support vector machine is linked to the top-layer features, yielding max-margin discrimination. Deep deconvolutional inference is employed when testing, to infer the latent features, and the top-layer features are connected with the max-margin classifier for discrimination tasks. The model is efficiently trained using a Monte Carlo expectation-maximization (MCEM) algorithm; the algorithm is implemented on graphical processor units (GPU) to enable large-scale learning, and fast testing. Excellent results are obtained on several benchmark datasets, including ImageNet, demonstrating that the proposed model achieves results that are highly competitive with similarly sized convolutional neural networks.",,,,,,"Stevens, Andrew/0000-0002-8474-8483; Yuan, Xin/0000-0002-8311-7524; Carin, Lawrence/0000-0001-6277-7948",,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,741,750,,,,,,,,,,,,,,,,WOS:000508662100081,0
C,"Ravi, S; Diao, QM",,"Gretton, A; Robert, CC",,"Ravi, Sujith; Diao, Qiming",,,Large Scale Distributed Semi-Supervised Learning Using Streaming Approximation,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Traditional graph-based semi-supervised learning (SSL) approaches are not suited for massive data and large label scenarios since they scale linearly with the number of edges |E| and distinct labels m. To deal with the large label size problem, recent works propose sketch-based methods to approximate the label distribution per node thereby achieving a space reduction from O(m) to O(logm), under certain conditions. In this paper, we present a novel streaming graph-based SSL approximation that effectively captures the sparsity of the label distribution and further reduces the space complexity per node to O(1). We also provide a distributed version of the algorithm that scales well to large data sizes. Experiments on real-world datasets demonstrate that the new method achieves better performance than existing state-of-the-art algorithms with significant reduction in memory footprint. Finally, we propose a robust graph augmentation strategy using unsupervised deep learning architectures that yields further significant quality gains for SSL in natural language applications.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,519,528,,,,,,,,,,,,,,,,WOS:000508662100057,0
C,"Srinivasa, C; Ravanbakhsh, S; Frey, B",,"Gretton, A; Robert, CC",,"Srinivasa, Christopher; Ravanbakhsh, Siamak; Frey, Brendan",,,Survey Propagation beyond Constraint Satisfaction Problems,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Survey propagation (SP) is a message passing procedure that attempts to model all the fixed points of Belief Propagation (BP), thereby improving BP's approximation in loopy graphs where BP's assumptions do not hold. For this, SP messages represent distributions over BP messages. Unfortunately this requirement makes SP intractable beyond constraint satisfaction problems because, to perform general SP updates, one has to operate on distributions over a continuous domain. We propose an approximation scheme to efficiently extend the application of SP to marginalization in binary pairwise graphical models. Our approximate SP has O(DK log(DK)tau) complexity per iteration, where tau is the complexity of BP per iteration, D is the maximum node degree and K is a resolution constant controlling the approximation's fidelity. Our experiments show that this method can track many BP fixed points, achieving a high marginalization accuracy within a few iterations, in difficult settings where BP is often non-convergent and inaccurate.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,286,295,,,,,,,,,,,,,,,,WOS:000508662100032,0
C,"Tang, C; Monteleoni, C",,"Gretton, A; Robert, CC",,"Tang, Cheng; Monteleoni, Claire",,,On Lloyd's algorithm: new theoretical insights for clustering in practice,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"We provide new analyses of Lloyd's algorithm (1982), commonly known as the k-means clustering algorithm. Kumar and Kannan (2010) showed that running k-SVD followed by a constant approximation k-means algorithm, and then Lloyd's algorithm, will correctly cluster nearly all of the dataset with respect to the optimal clustering, provided the dataset satisfies a deterministic clusterability assumption. This method is viewed as the Swiss Army knife for clustering problems, subsuming popular generative models such as Gaussian mixtures. However, it is tailored to high dimensional data, i.e., when d >> k. We analyze Lloyd's algorithm for general d without using the spectral projection, which leads to a weaker assumption in the case d < k. Surprisingly, we show that a simple and scalable heuristic that combines random sampling with Single-Linkage serves as a good seeding algorithm for Lloyd's algorithm under this assumption. We then study stopping criteria for Lloyd's algorithm under the lens of clusterability, accompanied by controlled simulations.",,,,,,"Monteleoni, Claire/0000-0002-9488-0517",,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,1280,1289,,,,,,,,,,,,,,,,WOS:000508662100139,0
C,"Tschiatschek, S; Djolonga, J; Krause, A",,"Gretton, A; Robert, CC",,"Tschiatschek, Sebastian; Djolonga, Josip; Krause, Andreas",,,Learning Probabilistic Submodular Diversity Models Via Noise Contrastive Estimation,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Modeling diversity of sets of items is important in many applications such as product recommendation and data summarization. Probabilistic submodular models, a family of models including the determinantal point process, form a natural class of distributions, encouraging effects such as diversity, repulsion and coverage. Current models, however, are limited to small and medium number of items due to the high time complexity for learning and inference. In this paper, we propose FLID, a novel log-submodular diversity model that scales to large numbers of items and can be efficiently learned using noise contrastive estimation. We show that our model achieves state of the art performance in terms of model fit, but can be also learned orders of magnitude faster. We demonstrate the wide applicability of our model using several experiments.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,770,779,,,,,,,,,,,,,,,,WOS:000508662100084,0
C,"Ding, WC; Rohban, MH; Ishwar, P; Saligrama, V",,"Kaski, S; Corander, J",,"Ding, Weicong; Rohban, Mohammad H.; Ishwar, Prakash; Saligrama, Venkatesh",,,Efficient Distributed Topic Modeling with Provable Guarantees,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,Topic modeling for large-scale distributed web-collections requires distributed techniques that account for both computational and communication costs. We consider topic modeling under the separability assumption and develop novel computationally efficient methods that provably achieve the statistical performance of the state-of-the-art centralized approaches while requiring insignificant communication between the distributed document collections. We achieve trade-offs between communication and computation without actually transmitting the documents. Our scheme is based on exploiting the geometry of normalized word-word co-occurrence matrix and viewing each row of this matrix as a vector in a high-dimensional space. We relate the solid angle subtended by extreme points of the convex hull of these vectors to topic identities and construct distributed schemes to identify topics.,,,,,"Rohban, Mohammad Hossein/ABB-2845-2020","Saligrama, Venkatesh/0000-0002-0675-2268; Ishwar, Prakash/0000-0002-2621-1549",,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,167,175,,,,,,,,,,,,,,,,WOS:000508355800019,0
C,"Guzman-Rivera, A; Kohli, P; Batra, D; Rutenbar, RA",,"Kaski, S; Corander, J",,"Guzman-Rivera, Abner; Kohli, Pushmeet; Batra, Dhruv; Rutenbar, Rob A.",,,Efficiently Enforcing Diversity in Multi-Output Structured Prediction,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"This paper proposes a novel method for efficiently generating multiple diverse predictions for structured prediction problems. Existing methods like SDPPs or DivMBest work by making a series of predictions where each prediction is made after considering the predictions that came before it. Such approaches are inherently sequential and computationally expensive. In contrast, our method, Diverse Multiple Choice Learning, learns a set of models to make multiple independent, yet diverse, predictions at test-time. We achieve this by including a diversity encouraging term in the loss function used for training the models. This approach encourages diversity in the predictions while preserving computational efficiency at test-time. Experimental results on a number of challenging problems show that our method learns models that not only predict more diverse results than competing methods, but are also able to generalize better and produce results with high test accuracy.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,284,292,,,,,,,,,,,,,,,,WOS:000508355800032,0
C,"Hensman, J; Zwiessele, M; Lawrence, ND",,"Kaski, S; Corander, J",,"Hensman, James; Zwiessele, Max; Lawrence, Neil D.",,,Tilted Variational Bayes,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"inference. Using some of the constructs from expectation propagation (EP), we derive a lower bound of the marginal likelihood in a similar fashion to variational Bayes (VB). The method combines some of the benefits of VB and EP: it can be used with light-tailed likelihoods (where traditional VB fails), and it provides a lower bound on the marginal likelihood. We apply the method to Gaussian process classification, a situation where the Kullback-Leibler divergence minimized in traditional VB can be infinite, and to robust Gaussian process regression, where the inference process is dramatically simplified in comparison to EP. Code to reproduce all the experiments can be found at github. com/SheffieldML/TVB.",,,,,,"Hensman, James/0000-0002-4989-3589",,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,356,364,,,,,,,,,,,,,,,,WOS:000508355800040,0
C,"Huang, RT; Szepesvari, C",,"Kaski, S; Corander, J",,"Huang, Ruitong; Szepesvari, Csaba",,,A Finite-Sample Generalization Bound for Semiparametric Regression: Partially Linear Models,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"In this paper we provide generalization bounds for semiparametric regression with the so-called partially linear models where the regression function is written as the sum of a linear parametric and a nonlinear, non-parametric function, the latter taken from a some set H with finite entropy-integral. The problem is technically challenging because the parametric part is unconstrained and the model is underdetermined, while the response is allowed to be unbounded with subgaussian tails. Under natural regularity conditions, we bound the generalization error as a function of the metric entropy of H and the dimension of the linear model. Our main tool is a ratio-type concentration inequality for increments of empirical processes, based on which we are able to give an exponential tail bound on the size of the parametric component. We also provide a comparison to alternatives of this technique and discuss why and when the unconstrained parametric part in the model may cause a problem in terms of the expected risk. We also explain by means of a specific example why this problem cannot be detected using the results of classical asymptotic analysis often seen in the statistics literature.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,402,410,,,,,,,,,,,,,,,,WOS:000508355800045,0
C,"Jensen, ST; Foster, DP",,"Kaski, S; Corander, J",,"Jensen, Shane T.; Foster, Dean P.",,,A Level-set Hit-and-run Sampler for Quasi-Concave Distributions,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"We develop a new sampling strategy that uses the hit-and-run algorithm within level sets of a target density. Our method can be applied to any quasi-concave density, which covers a broad class of models. Standard sampling methods often perform poorly on densities that are high-dimensional or multi-modal. Our level set sampler performs well in high-dimensional settings, which we illustrate on a spike-and-slab mixture model. We also extend our method to exponentially-tilted quasi-concave densities, which arise in Bayesian models consisting of a log-concave likelihood and quasi-concave prior density. We illustrate our exponentially-tilted level-set sampler on a Cauchy-normal model where our sampler is better able to handle a high-dimensional and multi-modal posterior distribution compared to Gibbs sampling and Hamiltonian Monte Carlo.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,439,447,,,,,,,,,,,,,,,,WOS:000508355800049,0
C,"Kim, DK; Der, M; Saul, LK",,"Kaski, S; Corander, J",,"Kim, Do-kyum; Der, Matthew; Saul, Lawrence K.",,,A Gaussian Latent Variable Model for Large Margin Classification of Labeled and Unlabeled Data,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"We investigate a Gaussian latent variable model for semi-supervised learning of linear large margin classifiers. The model's latent variables encode the signed distance of examples to the separating hyperplane, and we constrain these variables, for both labeled and unlabeled examples, to ensure that the classes are separated by a large margin. Our approach is based on similar intuitions as semi-supervised support vector machines (S-3 VMs), but these intuitions are formalized in a probabilistic framework. Within this framework we are able to derive an especially simple Expectation-Maximization (EM) algorithm for learning. The algorithm alternates between applying Bayes rule to fill in the latent variables (the E-step) and performing an unconstrained least-squares regression to update the weight vector (the M-step). For the best results it is necessary to constrain the unlabeled data to have a similar ratio of positive to negative examples as the labeled data. Within our model this constraint renders exact inference intractable, but we show that a Lyapunov central limit theorem (for sums of independent, but non-identical random variables) provides an excellent approximation to the true posterior distribution. We perform experiments on large-scale text classification and find that our model significantly outperforms existing implementations of S-3 VMs.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,484,492,,,,,,,,,,,,,,,,WOS:000508355800054,0
C,"Kiraly, FJ; Ehler, M",,"Kaski, S; Corander, J",,"Kiraly, Franz J.; Ehler, Martin",,,Algebraic Reconstruction Bounds and Explicit Inversion for Phase Retrieval at the Identifiability Threshold,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"We study phase retrieval from rank-one magnitude and more general linear magnitude measurements of an unknown signal as an algebraic estimation problem. It is verified that a certain number of generic rank-one or generic linear measurements are sufficient to enable signal reconstruction for generic signals, and slightly more generic measurements yield reconstructability for all signals. Our results solve few open problems stated in the recent literature. Furthermore, we show how the algebraic estimation problem can be solved by a closed-form algebraic estimation technique, termed ideal regression, providing non-asymptotic success guarantees.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,503,511,,,,,,,,,,,,,,,,WOS:000508355800056,0
C,"Liu, J; Zhang, CM; Burnside, E; Page, D",,"Kaski, S; Corander, J",,"Liu, Jie; Zhang, Chunming; Burnside, Elizabeth; Page, David",,,Learning Heterogeneous Hidden Markov Random Fields,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"Hidden Markov random fields (HMRFs) are conventionally assumed to be homogeneous in the sense that the potential functions are invariant across different sites. However in some biological applications, it is desirable to make HMRFs heterogeneous, especially when there exists some background knowledge about how the potential functions vary. We formally define heterogeneous HMRFs and propose an EM algorithm whose M-step combines a contrastive divergence learner with a kernel smoothing step to incorporate the background knowledge. Simulations show that our algorithm is effective for learning heterogeneous HMRFs and outperforms alternative binning methods. We learn a heterogeneous HMRF in a real-world study.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,576,584,,,,,,,,,,,25404989,,,,,WOS:000508355800064,0
C,"Behrmann, J; Vicol, P; Wang, KC; Grosse, R; Jacobsen, JH",,"Banerjee, A; Fukumizu, K",,"Behrmann, Jens; Vicol, Paul; Wang, Kuan-Chieh; Grosse, Roger; Jacobsen, Jorn-Henrik",,,Understanding and Mitigating Exploding Inverses in Invertible Neural Networks,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Invertible neural networks (INNs) have been used to design generative models, implement memory-saving gradient computation, and solve inverse problems. In this work, we show that commonly-used INN architectures suffer from exploding inverses and are thus prone to becoming numerically non-invertible. Across a wide range of INN use-cases, we reveal failures including the non-applicability of the change-of-variables formula on in- and out-of-distribution (OOD) data, incorrect gradients for memory-saving backprop, and the inability to sample from normalizing flow models. We further derive bi-Lipschitz properties of atomic building blocks of common architectures. These insights into the stability of INNs then provide ways forward to remedy these failures. For tasks where local invertibility is sufficient, like memory-saving backprop, we propose a flexible and efficient regularizer. For problems where global invertibility is necessary, such as applying normalizing flows on OOD data, we show the importance of designing stable INN building blocks.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802019,0
C,"Charles, Z; Konecny, J",,"Banerjee, A; Fukumizu, K",,"Charles, Zachary; Konecny, Jakub",,,Convergence and Accuracy Trade-Offs in Federated Learning and Meta-Learning,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We study a family of algorithms, which we refer to as local update methods, generalizing many federated and meta-learning algorithms. We prove that for quadratic models, local update methods are equivalent to first-order optimization on a surrogate loss we exactly characterize. Moreover, fundamental algorithmic choices (such as learning rates) explicitly govern a trade-off between the condition number of the surrogate loss and its alignment with the true loss. We derive novel convergence rates showcasing these trade-offs and highlight their importance in communication-limited settings. Using these insights, we are able to compare local update methods based on their convergence/accuracy trade-off, not just their convergence to critical points of the empirical loss. Our results shed new light on a broad range of phenomena, including the efficacy of server momentum in federated learning and the impact of proximal client updates.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803013,0
C,"Chikahara, Y; Sakaue, S; Fujino, A; Kashima, H",,"Banerjee, A; Fukumizu, K",,"Chikahara, Yoichi; Sakaue, Shinsaku; Fujino, Akinori; Kashima, Hisashi",,,Learning Individually Fair Classifier with Path-Specific Causal-Effect Constraint,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Machine learning is used to make decisions for individuals in various fields, which require us to achieve good prediction accuracy while ensuring fairness with respect to sensitive features (e.g., race and gender). This problem, however, remains difficult in complex real-world scenarios. To quantify unfairness under such situations, existing methods utilize path-specific causal effects. However, none of them can ensure fairness for each individual without making impractical functional assumptions about the data. In this paper, we propose a far more practical framework for learning an individually fair classifier. To avoid restrictive functional assumptions, we define the probability of individual unfairness (PIU) and solve an optimization problem where PIU's upper bound, which can be estimated from data, is controlled to be close to zero. We elucidate why our method can guarantee fairness for each individual. Experimental results show that our method can learn an individually fair classifier at a slight cost of accuracy.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,145,+,,,,,,,,,,,,,,,,WOS:000659893800017,0
C,"de la Concha, A; Vayatis, N; Kalogeratos, A",,"Banerjee, A; Fukumizu, K",,"de la Concha, Alejandro; Vayatis, Nicolas; Kalogeratos, Argyris",,,Offline detection of change-points in the mean for stationary graph signals,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"This paper addresses the problem of segmenting a stream of graph signals : we aim to detect changes in the mean of a multivariate signal defined over the nodes of a known graph. We propose an offline method that relies on the concept of graph signal stationarity and allows the convenient translation of the problem from the original vertex domain to the spectral domain (Graph Fourier Transform), where it is much easier to solve. Although the obtained spectral representation is sparse in real applications, to the best of our knowledge this property has not been sufficiently exploited in the existing related literature. Our change-point detection method adopts a model selection approach that takes into account the sparsity of the spectral representation and determines automatically the number of changepoints. Our detector comes with a proof of a non-asymptotic oracle inequality. Numerical experiments demonstrate the performance of the proposed method.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804016,0
C,"Ding, TY; Zhu, ZH; Tsakiris, M; Vidal, R; Robinson, D",,"Banerjee, A; Fukumizu, K",,"Ding, Tianyu; Zhu, Zhihui; Tsakiris, Manolis; Vidal, Rene; Robinson, Daniel",,,Dual Principal Component Pursuit for Learning a Union of Hyperplanes: Theory and Algorithms,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"State-of-the-art subspace clustering methods are based on convex formulations whose theoretical guarantees require the subspaces to be low-dimensional. Dual Principal Component Pursuit (DPCP) is a non-convex method that is specifically designed for learning high-dimensional subspaces, such as hyperplanes. However, existing analyses of DPCP in the multi-hyperplane case lack a precise characterization of the distribution of the data and involve quantities that are difficult to interpret. Moreover, the provable algorithm based on recursive linear programming is not efficient. In this paper, we introduce a new notion of geometric dominance, which explicitly captures the distribution of the data, and derive both geometric and probabilistic conditions under which a global solution to DPCP is a normal vector to a geometrically dominant hyperplane. We then prove that the DPCP problem for a union of hyperplanes satisfies a Riemannian regularity condition, and use this result to show that a scalable Riemannian subgradient method exhibits (local) linear convergence to the normal vector of the geometrically dominant hyperplane. Finally, we show that integrating DPCP into popular subspace clustering schemes, such as K-ensembles, leads to superior or competitive performance over the state-of-the-art in clustering hyperplanes.",,,,,"Zhu, Zhihui/AAR-5029-2020","Zhu, Zhihui/0000-0002-3856-0375",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803054,0
C,"Fraboni, Y; Vidal, R; Lorenzi, M",,"Banerjee, A; Fukumizu, K",,"Fraboni, Yann; Vidal, Richard; Lorenzi, Marco",,,Free-rider Attacks on Model Aggregation in Federated Learning,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Free-rider attacks against federated learning consist in dissimulating participation to the federated learning process with the goal of obtaining the final aggregated model without actually contributing with any data. This kind of attacks is critical in sensitive applications of federated learning, where data is scarce and the model has high commercial value. We introduce here the first theoretical and experimental analysis of free-rider attacks on federated learning schemes based on iterative parameters aggregation, such as FedAvg or FedProx, and provide formal guarantees for these attacks to converge to the aggregated models of the fair participants. We first show that a straightforward implementation of this attack can be simply achieved by not updating the local parameters during the iterative federated optimization. As this attack can be detected by adopting simple countermeasures at the server level, we subsequently study more complex disguising schemes based on stochastic updates of the free-rider parameters. We demonstrate the proposed strategies on a number of experimental scenarios, in both iid and non-iid settings. We conclude by providing recommendations to avoid free-rider attacks in real world applications of federated learning, especially in sensitive domains where security of data and models is critical.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802025,0
C,"Guan, ZW; Xu, TY; Liang, YB",,"Banerjee, A; Fukumizu, K",,"Guan, Ziwei; Xu, Tengyu; Liang, Yingbin",,,When Will Generative Adversarial Imitation Learning Algorithms Attain Global Convergence,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Generative adversarial imitation learning (GAIL) is a popular inverse reinforcement learning approach for jointly optimizing policy and reward from expert trajectories. A primary question about GAIL is whether applying a certain policy gradient algorithm to GAIL attains a global minimizer (i.e., yields the expert policy), for which existing understanding is very limited. Such global convergence has been shown only for the linear (or linear-type) MDP and linear (or linearizable) reward. In this paper, we study GAIL under general MDP and for nonlinear reward function classes (as long as the objective function is strongly concave with respect to the reward parameter). We characterize the global convergence with a sublinear rate for a broad range of commonly used policy gradient algorithms, all of which are implemented in an alternating manner with stochastic gradient ascent for reward update, including projected policy gradient (PPG)-GAIL, Frank-Wolfe policy gradient (FWPG)-GAIL, trust region policy optimization (TRPO)-GAIL and natural policy gradient (NPG)-GAIL. This is the first systematic theoretical study of GAIL for global convergence.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801038,0
C,"Kovalev, D; Koloskova, A; Jaggi, M; Richtarik, P; Stich, SU",,"Banerjee, A; Fukumizu, K",,"Kovalev, Dmitry; Koloskova, Anastasia; Jaggi, Martin; Richtarik, Peter; Stich, Sebastian U.",,,A Linearly Convergent Algorithm for Decentralized Optimization: Sending Less Bits for Free!,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Decentralized optimization methods enable on-device training of machine learning models without a central coordinator. In many scenarios communication between devices is energy demanding and time consuming and forms the bottleneck of the entire system. We propose a new randomized first-order method which tackles the communication bottleneck by applying randomized compression operators to the communicated messages. By combining our scheme with a new variance reduction technique that progressively throughout the iterations reduces the adverse effect of the injected quantization noise, we obtain a scheme that converges linearly on strongly convex decentralized problems while using compressed communication only. We prove that our method can solve the problems without any increase in the number of communications compared to the baseline which does not perform any communication compression while still allowing for a significant compression factor which depends on the conditioning of the problem and the topology of the network. We confirm our theoretical findings in numerical experiments.",,,,,,"Jaggi, Martin/0000-0003-1579-5558; Kovalev, Dmitry/0000-0003-1467-2994",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804088,0
C,"Nikolopoulos, P; Srinivasavaradhan, SR; Guo, T; Fragouli, C; Diggavi, S",,"Banerjee, A; Fukumizu, K",,"Nikolopoulos, Pavlos; Srinivasavaradhan, Sundara Rajan; Guo, Tao; Fragouli, Christina; Diggavi, Suhas",,,Group testing for connected communities,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"In this paper, we propose algorithms that leverage a known community structure to make group testing more efficient. We consider a population organized in disjoint communities: each individual participates in a community, and its infection probability depends on the community (s)he participates in. Use cases include families, students who participate in several classes, and workers who share common spaces. Group testing reduces the number of tests needed to identify the infected individuals by pooling diagnostic samples and testing them together. We show that if we design the testing strategy taking into account the community structure, we can significantly reduce the number of tests needed for adaptive and non-adaptive group testing, and can improve the reliability in cases where tests are noisy.",,,,,"Guo, Tao/HDN-2914-2022; Nikolopoulos, Pavlos/AAA-1794-2022","Guo, Tao/0000-0002-8991-6757; ",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802080,0
C,"Qiu, HJ; Li, C; Weng, Y; Sun, Z; He, XY; Zhao, QB",,"Banerjee, A; Fukumizu, K",,"Qiu, Hejia; Li, Chao; Weng, Ying; Sun, Zhun; He, Xingyu; Zhao, Qibin",,,On the Memory Mechanism of Tensor-Power Recurrent Models,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"ETensor-power (TP) recurrent model is a family of non-linear dynamical systems, of which the recurrence relation consists of a p-fold (a.k.a., degree-p) tensor product. Despite such the model frequently appears in the advanced recurrent neural networks (RNNs), to this date there is limited study on its memory property, a critical characteristic in sequence tasks. In this work, we conduct a thorough investigation of the memory mechanism of TP recurrent models. Theoretically, we prove that a large degree p is an essential condition to achieve the long memory effect, yet it would lead to unstable dynamical behaviors. Empirically, we tackle this issue by extending the degree p from discrete to a differentiable domain, such that it is efficiently learnable from a variety of datasets. Taken together, the new model is expected to benefit from the long memory effect in a stable manner. We experimentally show that the proposed model achieves competitive performance compared to various advanced RNNs in both the single-cell and seq2seq architectures.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804044,0
C,"Reddi, SJ; Pasumarthi, RK; Menon, AK; Rawat, AS; Yu, F; Kim, S; Veit, A; Kumar, S",,"Banerjee, A; Fukumizu, K",,"Reddi, Sashank J.; Pasumarthi, Rama Kumar; Menon, Aditya Krishna; Rawat, Ankit Singh; Yu, Felix; Kim, Seungyeon; Veit, Andreas; Kumar, Sanjiv",,,RANKDISTIL: Knowledge Distillation for Ranking,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Knowledge distillation is an approach to improve the performance of a student model by using the knowledge of a complex teacher. Despite its success in several deep learning applications, the study of distillation is mostly confined to classification settings. In particular, the use of distillation in top-k ranking settings, where the goal is to rank k most relevant items correctly, remains largely unexplored. In this paper, we study such ranking problems through the lens of distillation. We present a distillation framework for top-k ranking and draw connections with the existing ranking methods. The core idea of this framework is to preserve the ranking at the top by matching the order of items of student and teacher, while penalizing large scores for items ranked low by the teacher. Building on this, we develop a novel distillation approach, RankDistil, specifically catered towards ranking problems with a large number of items to rank, and establish statistical basis for the method. Finally, we conduct experiments which demonstrate that RankDistil yields benefits over commonly used baselines for ranking problems.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802083,0
C,"Zhang, SRO; Moscovich, A; Singer, A",,"Banerjee, A; Fukumizu, K",,"Zhang, Sharon; Moscovich, Amit; Singer, Amit",,,Product Manifold Learning,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We consider dimensionality reduction for data sets with two or more independent degrees of freedom. For example, measurements of deformable shapes with several parts that move independently fall under this characterization. Mathematically, if the space of each continuous independent motion is a manifold, then their combination forms a product manifold. In this paper, we present an algorithm for manifold factorization given a sample of points from the product manifold. Our algorithm is based on spectral graph methods for manifold learning and the separability of the Laplacian operator on product spaces. Recovering the factors of a manifold yields meaningful lower-dimensional representations, allowing one to focus on particular aspects of the data space while ignoring others. We demonstrate the potential use of our method for an important and challenging problem in structural biology: mapping the motions of proteins and other large molecules using cryo-electron microscopy data sets.",,,,,"Moscovich, Amit/Y-4085-2018","Moscovich, Amit/0000-0002-1289-8052",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803087,0
C,"Andrews, B; Spirtes, P; Cooper, GF",,"Chiappa, S; Calandra, R",,"Andrews, Bryan; Spirtes, Peter; Cooper, Gregory F.",,,On the Completeness of Causal Discovery in the Presence of Latent Confounding with Tiered Background Knowledge,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"The discovery of causal relationships is a core part of scientific research. Accordingly, over the past several decades, algorithms have been developed to discover the causal structure for a system of variables from observational data. Learning ancestral graphs is of particular interest due to their ability to represent latent confounding implicitly with bi-directed edges. The well-known FCI algorithm provably recovers an ancestral graph for a system of variables encoding the sound and complete set of causal relationships identifiable from observational data.(1) Additional causal relationships become identifiable with the incorporation of background knowledge; however, it is not known for what types of knowledge FCI remains complete. In this paper, we define tiered background knowledge and show that FCI is sound and complete with the incorporation of this knowledge.",,,,,"Spirtes, Peter/GYD-5724-2022",,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,4002,4010,,,,,,,,,,,,,,,,WOS:000559931300014,0
C,"Azizian, W; Scieur, D; Mitliagkas, I; Lacoste-Julien, S; Gidel, G",,"Chiappa, S; Calandra, R",,"Azizian, Waiss; Scieur, Damien; Mitliagkas, Ioannis; Lacoste-Julien, Simon; Gidel, Gauthier",,,Accelerating Smooth Games by Manipulating Spectral Shapes,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We use matrix iteration theory to characterize acceleration in smooth games. We define the spectral shape of a family of games as the set containing all eigenvalues of the Jacobians of standard gradient dynamics in the family. Shapes restricted to the real line represent well-understood classes of problems, like minimization. Shapes spanning the complex plane capture the added numerical challenges in solving smooth games. In this framework, we describe gradient-based methods, such as extragradient, as transformations on the spectral shape. Using this perspective, we propose an optimal algorithm for bilinear games. For smooth and strongly monotone operators, we identify a continuum between convex minimization, where acceleration is possible using Polyak's momentum, and the worst case where gradient descent is optimal. Finally, going beyond first-order methods, we propose an accelerated version of consensus optimization.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1705,1714,,,,,,,,,,,,,,,,WOS:000559931300020,0
C,"Cao, YS; Xu, P",,"Chiappa, S; Calandra, R",,"Cao, Yanshuai; Xu, Peng",,,Better Long-Range Dependency By Bootstrapping A Mutual Information Regularizer,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"In this work, we develop a novel regularizer to improve the learning of long-range dependency of sequence data. Applied on language modelling, our regularizer expresses the inductive bias that sequence variables should have high mutual information even though the model might not see abundant observations for complex long-range dependency. We show how the next sentence prediction (classification) heuristic can be derived in a principled way from our mutual information estimation framework, and be further extended to maximize the mutual information of sequence variables. The proposed approach not only is effective at increasing the mutual information of segments under the learned model but more importantly, leads to a higher likelihood on holdout data, and improved generation quality. Code is released at https://github.com/BorealisAI/BMI.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3991,4000,,,,,,,,,,,,,,,,WOS:000559931300043,0
C,"Chaturvedi, A; Scarlett, J",,"Chiappa, S; Calandra, R",,"Chaturvedi, Anamay; Scarlett, Jonathan",,,Learning Gaussian Graphical Models via Multiplicative Weights,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Graphical model selection in Markov random fields is a fundamental problem in statistics and machine learning. Two particularly prominent models, the Ising model and Gaussian model, have largely developed in parallel using different (though often related) techniques, and several practical algorithms with rigorous sample complexity bounds have been established for each. In this paper, we adapt a recently proposed algorithm of Klivans and Meka (FOCS, 2017), based on the method of multiplicative weight updates, from the Ising model to the Gaussian model, via non-trivial modifications to both the algorithm and its analysis. The algorithm enjoys a sample complexity bound that is qualitatively similar to others in the literature, has a low runtime O(mp(2)) in the case of m samples and p nodes, and can trivially be implemented in an online manner.",,,,,"Scarlett, Jonathan/AGK-0892-2022",,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1104,1113,,,,,,,,,,,,,,,,WOS:000559931300051,0
C,"Chen, L; Zhang, MR; Hassani, H; Karbasi, A",,"Chiappa, S; Calandra, R",,"Chen, Lin; Zhang, Mingrui; Hassani, Hamed; Karbasi, Amin",,,Black Box Submodular Maximization: Discrete and Continuous Settings,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"In this paper, we consider the problem of black box continuous submodular maximization where we only have access to the function values and no information about the derivatives is provided. For a monotone and continuous DR-submodular function, and subject to a bounded convex body constraint, we propose Black-box Continuous Greedy, a derivative-free algorithm that provably achieves the tight [(1 - 1/e)OPT - epsilon] approximation guarantee with O (d/epsilon(3)) function evaluations. We then extend our result to the stochastic setting where function values are subject to stochastic zero-mean noise. It is through this stochastic generalization that we revisit the discrete submodular maximization problem and use the multi-linear extension as a bridge between discrete and continuous settings. Finally, we extensively evaluate the performance of our algorithm on continuous and discrete submodular objective functions using both synthetic and real data.",,,,,"Chen, Lin/CAH-1961-2022","Chen, Lin/0000-0003-0349-6577",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1058,1069,,,,,,,,,,,,,,,,WOS:000559931300056,0
C,"Clemencon, S; Vogel, R",,"Chiappa, S; Calandra, R",,"Clemencon, Stephan; Vogel, Robin",,,A Multiclass Classification Approach to Label Ranking,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"In multiclass classification, the goal is to learn how to predict a random label Y, valued in y = {1 K} with K > 3, based upon observing a r.v. X, taking its values in Ng with q > 1 say, by means of a classification rule g : y with minimum probability of error P{Y g(X)}. However, in a wide variety of situations, the task targeted may be more ambitious, consisting in sorting all the possible label values y that may be assigned to X by decreasing order of the posterior probability ny(X) = P{Y = y X}. This article is devoted to the analysis of this statistical learning problem, halfway between multiclass classification and posterior probability estimation (regression) and referred to as label ranking here. We highlight the fact that it can be viewed as a specific variant of ranking median regression (RMR), where, rather than observing a random permutation assigned to the input vector X and drawn from a BradleyTerry-Luce-Plackett model with conditional preference vector (771(X),..., 77K (X)), the sole information available for training a label ranking rule is the label Y ranked on top, namely 1(1). Inspired by recent results in RMR, we prove that under appropriate noise conditions, the One-Versus-One (OVO) approach to multiclassification yields, as a by-product, an optimal ranking of the labels with overwhelming probability. Beyond theoretical guarantees, the relevance of the approach to label ranking promoted in this article is supported by experimental results.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303057,0
C,"Janzing, D; Minorics, L; Blobaum, P",,"Chiappa, S; Calandra, R",,"Janzing, Dominik; Minorics, Lenon; Bloebaum, Patrick",,,Feature relevance quantification in explainable AI: A causal problem,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We discuss promising recent contributions on quantifying feature relevance using Shapley values, where we observed some confusion on which probability distribution is the right one for dropped features. We argue that the confusion is based on not carefully distinguishing between observational and interventional conditional probabilities and try a clarification based on Pearl's seminal work on causality. We conclude that unconditional rather than conditional expectations provide the right notion of dropping features in contradiction to the theoretical justification of the software package SHAP. Parts of SHAP are unaffected because unconditional expectations (which we argue to be conceptually right) are used as approximation for the conditional ones, which encouraged others to 'improve' SHAP in a way that we believe to be flawed.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2907,2915,,,,,,,,,,,,,,,,WOS:000559931301060,0
C,"Li, ZZ; Li, J",,"Chiappa, S; Calandra, R",,"Li, Zhize; Li, Jian",,,A Fast Anderson-Chebyshev Acceleration for Nonlinear Optimization,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Anderson acceleration (or Anderson mixing) is an efficient acceleration method for fixed point iterations x(t+1) = G(x(t)), e.g., gradient descent can be viewed as iteratively applying the operation G(x) x aVf (x). It is known that Anderson acceleration is quite efficient in practice and can be viewed as an extension of Krylov subspace methods for nonlinear problems. In this paper, we show that Anderson acceleration with Chebyshev polynomial can achieve the optimal convergence rate 0(A/T In E), which improves the previous result O(i In E) provided by (Toth and Kelley, 2015) for quadratic functions. Moreover, we provide a convergence analysis for minimizing general nonlinear problems. Besides, if the hyperparameters (e.g., the Lipschitz smooth parameter L) are not available, we propose a guessing algorithm for guessing them dynamically and also prove a similar convergence rate. Finally, the experimental results demonstrate that the proposed Anderson-Chebyshev acceleration method converges significantly faster than other algorithms, e.g., vanilla gradient descent (GD), Nesterov's Accelerated GD. Also, these algorithms combined with the proposed guessing algorithm (guessing the hyperparameters dynamically) achieve much better performance.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1047,1056,,,,,,,,,,,,,,,,WOS:000559931302004,0
C,"Ramponi, G; Likmeta, A; Metelli, AM; Tirinzoni, A; Restelli, M",,"Chiappa, S; Calandra, R",,"Ramponi, Giorgia; Likmeta, Amarildo; Metelli, Alberto Maria; Tirinzoni, Andrea; Restelli, Marcello",,,Truly Batch Model-Free Inverse Reinforcement Learning about Multiple Intentions,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We consider Inverse Reinforcement Learning (IRL) about multiple intentions, i.e., the problem of estimating the unknown reward functions optimized by a group of experts that demonstrate optimal behaviors. Most of the existing algorithms either require access to a model of the environment or need to repeatedly compute the optimal policies for the hypothesized rewards. However, these requirements are rarely met in real-world applications, in which interacting with the environment can be expensive or even dangerous. In this paper, we address the IRL about multiple intentions in a fully model-free and batch setting. We first cast the single IRL problem as a constrained likelihood maximization and then we use this formulation to cluster agents based on the likelihood of the assignment. In this way, we can efficiently solve, without interactions with the environment, both the IRL and the clustering problem. Finally, we evaluate the proposed methodology on simulated domains and on a real-world social-network application.",,,,,"Metelli, Alberto Maria/AAY-5206-2020","Metelli, Alberto Maria/0000-0002-3424-5212; Likmeta, Amarildo/0000-0002-4227-0741",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2359,2368,,,,,,,,,,,,,,,,WOS:000559931302088,0
C,"Srinivasa, RS; Davenport, MA; Romberg, J",,"Chiappa, S; Calandra, R",,"Srinivasa, Rakshith S.; Davenport, Mark A.; Romberg, Justin",,,Sample complexity bounds for localized sketching,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We consider sketched approximate matrix multiplication and ridge regression in the novel setting of localized sketching, where at any given point, only part of the data matrix is available. This corresponds to a block diagonal structure on the sketching matrix. We show that, under mild conditions, block diagonal sketching matrices require only O(sr/epsilon(2)) and O(sd(lambda)/epsilon) total sample complexity for matrix multiplication and ridge regression, respectively. This matches the state-of-the-art bounds that are obtained using global sketching matrices. The localized nature of sketching considered allows for different parts of the data matrix to be sketched independently and hence is more amenable to computation in distributed and streaming settings and results in a smaller memory and computational footprint.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303028,0
C,"Staerman, G; Mozharovskyi, P; Clemencon, S",,"Chiappa, S; Calandra, R",,"Staerman, Guillaume; Mozharovskyi, Pavlo; Clemencon, Stephan",,,The Area of the Convex Hull of Sampled Curves: a Robust Functional Statistical Depth Measure,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"With the ubiquity of sensors in the IoT era, statistical observations are becoming increasingly available in the form of massive (multivariate) time-series. Formulated as unsupervised anomaly detection tasks, an abundance of applications like aviation safety management, the health monitoring of complex infrastructures or fraud detection can now rely on such functional data, acquired and stored with an ever finer granularity. The concept of statistical depth, which reflects centrality of an arbitrary observation w.r.t. a statistical population may play a crucial role in this regard, anomalies corresponding to observations with 'small' depth. Supported by sound theoretical and computational developments in the recent decades, it has proven to be extremely useful, in particular in functional spaces. However, most approaches documented in the literature consist in evaluating independently the centrality of each point forming the time series and consequently exhibit a certain insensitivity to possible shape changes. In this paper, we propose a novel notion of functional depth based on the area of the convex hull of sampled curves, capturing gradual departures from centrality, even beyond the envelope of the data, in a natural fashion. We discuss practical relevance of commonly imposed axioms on functional depths and investigate which of them are satisfied by the notion of depth we promote here. Estimation and computational issues are also addressed and various numerical experiments provide empirical evidence of the relevance of the approach proposed.",,,,,"Mozharovskyi, Pavlo/AAC-7929-2021",,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303029,0
C,"Uziel, G",,"Chiappa, S; Calandra, R",,"Uziel, Guy",,,Nonparametric Sequential Prediction While Deep Learning the Kernel,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"The research on online learning under stationary and ergodic processes has been mainly focused on achieving asymptotic guarantees. Although all the methods pursue the same asymptotic goal, their performance varies when handling finite sample datasets and depends heavily on which predefined density estimation method is chosen. In this paper, therefore, we propose a novel algorithm that simultaneously satisfies a short-term goal, to perform as good as the best choice in hindsight of a data-adaptive kernel, learned using a deep neural network, and a long-term goal, to achieve the same theoretical asymptotic guarantee. We present theoretical proofs for our algorithms and demonstrate the validity of our method on the online portfolio selection problem.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303050,0
C,"Wolfer, G; Kontorovich, A",,"Chiappa, S; Calandra, R",,"Wolfer, Geoffrey; Kontorovich, Aryeh",,,Minimax Testing of Identity to a Reference Ergodic Markov Chain,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We exhibit an efficient procedure for testing, based on a single long state sequence, whether an unknown Markov chain is identical to or epsilon-far from a given reference chain. We obtain nearly matching (up to logarithmic factors) upper and lower sample complexity bounds for our notion of distance, which is based on total variation. Perhaps surprisingly, we discover that the sample complexity depends solely on the properties of the known reference chain and does not involve the unknown chain at all, which is not even assumed to be ergodic.",,,,,,"Wolfer, Geoffrey/0000-0002-5388-7640",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303080,0
C,"Wu, WQ; Yang, J; Shen, C",,"Chiappa, S; Calandra, R",,"Wu, Weiqiang; Yang, Jing; Shen, Cong",,,Stochastic Linear Contextual Bandits with Diverse Contexts,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"In this paper, we investigate the impact of context diversity on stochastic linear contextual bandits. As opposed to the previous view that contexts lead to more difficult bandit learning, we show that when the contexts are sufficiently diverse, the learner is able to utilize the information obtained during exploitation to shorten the exploration process, thus achieving reduced regret. We design the LinUCB-d algorithm, and propose a novel approach to analyze its regret performance. The main theoretical result is that under the diverse context assumption, the cumulative expected regret of LinUCB-d is bounded by a constant. As a by-product, our results improve the previous understanding of LinUCB and strengthen its performance guarantee.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303083,0
C,"Yang, YY; Rashtchian, C; Wang, YZ; Chaudhuri, K",,"Chiappa, S; Calandra, R",,"Yang, Yao-Yuan; Rashtchian, Cyrus; Wang, Yizhen; Chaudhuri, Kamalika",,,Robustness for Non-Parametric Classification: A Generic Attack and Defense,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Adversarially robust machine learning has received much recent attention. However, prior attacks and defenses for non-parametric classifiers have been developed in an ad-hoc or classifier-specific basis. In this work, we take a holistic look at adversarial examples for non-parametric classifiers, including nearest neighbors, decision trees, and random forests. We provide a general defense method, adversarial pruning, that works by preprocessing the dataset to become well-separated. To test our defense, we provide a novel attack that applies to a wide range of non-parametric classifiers. Theoretically, we derive an optimally robust classifier, which is analogous to the Bayes Optimal. We show that adversarial pruning can be viewed as a finite sample approximation to this optimal classifier. We empirically show that our defense and attack are either better than or competitive with prior work on non-parametric classifiers. Overall, our results provide a strong and broadly-applicable baseline for future work on robust non-parametrics.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303093,0
C,"Zhang, JY; Zhang, RY; Carin, L; Chen, CY",,"Chiappa, S; Calandra, R",,"Zhang, Jianyi; Zhang, Ruiyi; Carin, Lawrence; Chen, Changyou",,,Stochastic Particle-Optimization Sampling and the Non-Asymptotic Convergence Theory,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Particle-optimization-based sampling (POS) is a recently developed effective sampling technique that interactively updates a set of particles to approximate a target distribution. A representative algorithm is the Stein variational gradient descent (SVGD). We prove, under certain conditions, SVGD experiences a theoretical pitfall, i.e., particles tend to collapse. As a remedy, we generalize POS to a stochastic setting by injecting random noise into particle updates, thus termed stochastic particle-optimization sampling (SPOS). Notably, for the first time, we develop non asymptotic convergence theory for the SPOS framework (related to SVGD), characterizing algorithm convergence in terms of the 1-Wasserstein distance w.r.t. the numbers of particles and iterations. Somewhat surprisingly, with the same number of updates (not too large) for each particle, our theory suggests adopting more particles does not necessarily lead to a better approximation of a target distribution, due to limited computational budget and numerical errors. This phenomenon is also observed in SVGD and verified via a synthetic experiment. Extensive experimental results verify our theory and demonstrate the effectiveness of our proposed framework.",,,,,"Zhang, Ruiyi/AAB-8402-2021",,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1877,1886,,,,,,,,,,,,,,,,WOS:000559931304010,0
C,"Zhao, P; Zhang, LJ; Jiang, YA; Zhou, ZH",,"Chiappa, S; Calandra, R",,"Zhao, Peng; Zhang, Lijun; Jiang, Yuan; Zhou, Zhi-Hua",,,A Simple Approach for Non-stationary Linear Bandits,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"This paper investigates the problem of non-stationary linear bandits, where the unknown regression parameter is evolving over time. Previous studies have adopted sophisticated mechanisms, such as sliding window or weighted penalty to achieve near-optimal dynamic regret. In this paper, we demonstrate that a simple restarted strategy is sufficient to attain the same regret guarantee. Specifically, we design an UCB-type algorithm to balance exploitation and exploration, and restart it periodically to handle the drift of unknown parameters. Let T be the time horizon, d be the dimension, and P-T be the path-length that measures the fluctuation of the evolving unknown parameter, our approach enjoys an (O) over tilde (d(2/3)(1 + P-T)T-1/3(2/3)) dynamic regret, which is nearly optimal, matching the Omega(d(2/3)(1+P-T)T-1/3(2/3)) minimax lower bound up to logarithmic factors. Empirical studies also validate the efficacy of our approach.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,746,754,,,,,,,,,,,,,,,,WOS:000559931304016,0
C,"Chen, YZ; Gutmann, MU",,"Chaudhuri, K; Sugiyama, M",,"Chen, Yanzhi; Gutmann, Michael U.",,,Adaptive Gaussian Copula ABC,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Approximate Bayesian computation (ABC) is a set of techniques for Bayesian inference when the likelihood is intractable but sampling from the model is possible. This work presents a simple yet effective ABC algorithm based on the combination of two classical ABC approaches regression ABC and sequential ABC. The key idea is that rather than learning the posterior directly, we first target another auxiliary distribution that can be learned accurately by existing methods, through which we then subsequently learn the desired posterior with the help of a Gaussian copula. During this process, the complexity of the model changes adaptively according to the data at hand. Experiments on a synthetic dataset as well as three real-world inference tasks demonstrates that the proposed method is fast, accurate, and easy to use.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901065,0
C,"Delgosha, P; Goela, N",,"Chaudhuri, K; Sugiyama, M",,"Delgosha, Payam; Goela, Naveen",,,Deep Switch Networks for Generating Discrete Data and Language,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Multilayer switch networks are proposed as artificial generators of high-dimensional discrete data (e.g., binary vectors, categorical data, natural language, network log files, and discrete-valued time series). Unlike deconvolution networks which generate continuous-valued data and which consist of upsampling filters and reverse pooling layers, multilayer switch networks are composed of adaptive switches which model conditional distributions of discrete random variables. An interpretable, statistical framework is introduced for training these nonlinear networks based on a maximum-likelihood objective function. To learn network parameters, stochastic gradient descent is applied to the objective, and is stable until convergence. This direct optimization does not involve back-propagation over separate encoder and decoder networks, or adversarial training of dueling networks. While training remains tractable for moderately sized networks, Markov-chain Monte Carlo (MCMC) approximations of gradients are derived for deep networks which contain latent variables. The statistical framework is evaluated on synthetic data, high-dimensional binary data of handwritten digits, and web-crawled natural language data. Aspects of the model's framework such as interpretability, computational complexity, and generalization ability are discussed.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687903012,0
C,"Gao, WH; Makkuva, AV; Oh, S; Viswanath, P",,"Chaudhuri, K; Sugiyama, M",,"Gao, Weihao; Makkuva, Ashok Vardhan; Oh, Sewoong; Viswanath, Pramod",,,Learning One-hidden-layer Neural Networks under General Input Distributions,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Significant advances have been made recently on training neural networks, where the main challenge is in solving an optimization problem with abundant critical points. However, existing approaches to address this issue crucially rely on a restrictive assumption: the training data is drawn from a Gaussian distribution. In this paper, we provide a novel unified framework to design loss functions with desirable landscape properties for a wide range of general input distributions. On these loss functions, remarkably, stochastic gradient descent theoretically recovers the true parameters with global initializations and empirically outperforms the existing approaches. Our loss function design bridges the notion of score functions with the topic of neural network optimization. Central to our approach is the task of estimating the score function from samples, which is of basic and independent interest to theoretical statistics. Traditional estimation methods (example: kernel based) fail right at the outset; we bring statistical methods of local likelihood to design a novel estimator of score functions, that provably adapts to the local geometry of the unknown density.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901103,0
C,"Kushagra, S; Ben-David, S; Ilyas, IF",,"Chaudhuri, K; Sugiyama, M",,"Kushagra, Shrinu; Ben-David, Shai; Ilyas, Ihab F.",,,Semi-supervised clustering for de-duplication,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Data de-duplication is the task of detecting multiple records that correspond to the same real-world entity in a database. In this work, we view de-duplication as a clustering problem where the goal is to put records corresponding to the same physical entity in the same cluster and putting records corresponding to different physical entities into different clusters. We introduce a framework which we call promise correlation clustering. Given a complete graph G with the edges labeled 0 and 1, the goal is to find a clustering that minimizes the number of 0 edges within a cluster plus the number of 1 edges across different clusters (or correlation loss). The optimal clustering can also be viewed as a complete graph G* with edges corresponding to points in the same cluster being labeled 0 and other edges being labeled 1. Under the promise that the edge difference between G and G* is small, we prove that finding the optimal clustering (or G*) is still NP-Hard. [Ashtiani et al., 2016] introduced the framework of semi-supervised clustering, where the learning algorithm has access to an oracle, which answers whether two points belong to the same or different clusters. We further prove that even with access to a same-cluster oracle, the promise version is NP-Hard as long as the number queries to the oracle is not too large (o(n) where n is the number of vertices). Given these negative results, we consider a restricted version of correlation clustering. As before, the goal is to find a clustering that minimizes the correlation loss. However, we restrict ourselves to a given class F of clusterings. We offer a semi-supervised algorithmic approach to solve the restricted variant with success guarantees.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901073,0
C,"LeJeune, D; Baraniuk, RG; Heckel, R",,"Chaudhuri, K; Sugiyama, M",,"LeJeune, Daniel; Baraniuk, Richard G.; Heckel, Reinhard",,,Adaptive Estimation for Approximate k-Nearest-Neighbor Computations,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Algorithms often carry out equally many computations for easy and hard problem instances. In particular, algorithms for finding nearest neighbors typically have the same running time regardless of the particular problem instance. In this paper, we consider the approximate k-nearest-neighbor problem, which is the problem of finding a subset of 0(k) points in a given set of points that contains the set of k nearest neighbors of a given query point. We propose an algorithm based on adaptively estimating the distances, and show that it is essentially optimal out of algorithms that are only allowed to adaptively estimate distances. We then demonstrate both theoretically and experimentally that the algorithm can achieve significant speedups relative to the naive method.",,,,,"Baraniuk, Richard/ABA-1743-2020; Heckel, Reinhard/AAV-7904-2020",,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687903016,0
C,"Li, ZL; Xia, T; Lou, XY; Xu, KH; Wang, SJ; Xiao, J",,"Chaudhuri, K; Sugiyama, M",,"Li, Zhongliang; Xia, Tian; Lou, Xingyu; Xu, Kaihe; Wang, Shaojun; Xiao, Jing",,,Adversarial Discrete Sequence Generation without Explicit Neural Networks as Discriminators,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"This paper presents a novel approach to train GANs for discrete sequence generation without resorting to an explicit neural network as the discriminator. We show that when an alternative mini-max optimization procedure is performed for the value function where a closed form solution for the discriminator exists in the maximization step, it is equivalent to directly optimizing the Jensen-Shannon divergence (JSD) between the generator's distribution and the empirical distribution over the training data without sampling from the generator, thus optimizing the JSD becomes computationally tractable to train the generator that generates sequences of discrete data. Extensive experiments on synthetic data and real-world tasks demonstrate significant improvements over existing methods to train GANs that generate discrete sequences.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687903015,0
C,"Louppe, G; Hermans, J; Cranmer, K",,"Chaudhuri, K; Sugiyama, M",,"Louppe, Gilles; Hermans, Joeri; Cranmer, Kyle",,,Adversarial Variational Optimization of Non-Differentiable Simulators,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Complex computer simulators are increasingly used across fields of science as generative models tying parameters of an underlying theory to experimental observations. Inference in this setup is often difficult, as simulators rarely admit a tractable density or likelihood function. We introduce Adversarial Variational Optimization (AVO), a likelihood-free inference algorithm for fitting a non-differentiable generative model incorporating ideas from generative adversarial networks, variational optimization and empirical Bayes. We adapt the training procedure of generative adversarial networks by replacing the differentiable generative network with a domain-specific simulator. We solve the resulting non-differentiable minimax problem by minimizing variational upper bounds of the two adversarial objectives. Effectively, the procedure results in learning a proposal distribution over simulator parameters, such that the JS divergence between the marginal distribution of the synthetic data and the empirical distribution of observed data is minimized. We evaluate and compare the method with simulators producing both discrete and continuous data.",,,,,"Louppe, Gilles/D-1923-2017","Louppe, Gilles/0000-0002-2082-3106",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901050,0
C,"Paananen, T; Piironen, J; Andersen, MR; Vehtari, A",,"Chaudhuri, K; Sugiyama, M",,"Paananen, Topi; Piironen, Juho; Andersen, Michael Riis; Vehtari, Aki",,,Variable selection for Gaussian processes via sensitivity analysis of the posterior predictive distribution,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Variable selection for Gaussian process models is often done using automatic relevance determination, which uses the inverse length-scale parameter of each input variable as a proxy for variable relevance. This implicitly determined relevance has several drawbacks that prevent the selection of optimal input variables in terms of predictive performance. To improve on this, we propose two novel variable selection methods for Gaussian process models that utilize the predictions of a full model in the vicinity of the training points and thereby rank the variables based on their predictive relevance. Our empirical results on synthetic and real world data sets demonstrate improved variable selection compared to automatic relevance determination in terms of variability and predictive performance.",,,,,"Piironen, Juho/GSJ-3745-2022",,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901082,0
C,"Pacheco, J; Fisher, JW",,"Chaudhuri, K; Sugiyama, M",,"Pacheco, Jason; Fisher, John W., III",,,Variational Information Planning for Sequential Decision Making,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We consider the setting of sequential decision making where, at each stage, potential actions are evaluated based on expected reduction in posterior uncertainty, given by mutual information (MI). As MI typically lacks a closed form, we propose an approach which maintains variational approximations of, both, the posterior and MI utility. Our planning objective extends an established variational bound on MI to the setting of sequential planning. The result, variational information planning (VIP), is an efficient method for sequential decision making. We further establish convexity of the variational planning objective and, under conditional exponential family approximations, we show that the optimal MI bound arises from a relaxation of the well-known exponential family moment matching property. We demonstrate VIP for sensor selection, experiment design, and active learning, where it meets or exceeds methods requiring more computation, or those specialized to the task.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902008,0
C,"Rhodes, B; Gutmann, MU",,"Chaudhuri, K; Sugiyama, M",,"Rhodes, Benjamin; Gutmann, Michael U.",,,Variational Noise-Contrastive Estimation,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Unnormalised latent variable models are a broad and flexible class of statistical models. However, learning their parameters from data is intractable, and few estimation techniques are currently available for such models. To increase the number of techniques in our arsenal, we propose variational noise-contrastive estimation (VNCE), building on NCE which is a method that only applies to unnormalised models. The core idea is to use a variational lower bound to the NCE objective function, which can be optimised in the same fashion as the evidence lower bound (ELBO) in standard variational inference (VI). We prove that VNCE can be used for both parameter estimation of unnormalised models and posterior inference of latent variables. The developed theory shows that VNCE has the same level of generality as standard VI, meaning that advances made there can be directly imported to the unnormalised setting. We validate VNCE on toy models and apply it to a realistic problem of estimating an undirected graphical model from incomplete data.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902081,0
C,"Tarbouriech, J; Lazaric, A",,"Chaudhuri, K; Sugiyama, M",,"Tarbouriech, Jean; Lazaric, Alessandro",,,Active Exploration in Markov Decision Processes,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We introduce the active exploration problem in Markov decision processes (MDPs). Each state of the MDP is characterized by a random value and the learner should gather samples to estimate the mean value of each state as accurately as possible. Similarly to active exploration in multi-armed bandit (MAB), states may have different levels of noise, so that the higher the noise, the more samples are needed. As the noise level is initially unknown, we need to trade off the exploration of the environment to estimate the noise and the exploitation of these estimates to compute a policy maximizing the accuracy of the mean predictions. We introduce a novel learning algorithm to solve this problem showing that active exploration in MDPs may be significantly more difficult than in MAB. We also derive a heuristic procedure to mitigate the negative effect of slowly mixing policies. Finally, we validate our findings on simple numerical simulations.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901002,0
C,"Zhang, DT; Jung, YH; Tewari, A",,"Chaudhuri, K; Sugiyama, M",,"Zhang, Daniel T.; Jung, Young Hun; Tewari, Ambuj",,,Online Multiclass Boosting with Bandit Feedback,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We present online boosting algorithms for multiclass classification with bandit feedback, where the learner only receives feedback about the correctness of its prediction. We propose an unbiased estimate of the loss using a randomized prediction, allowing the model to update its weak learners with limited information. Using the unbiased estimate, we extend two full information boosting algorithms (Jung et al., 2017) to the bandit setting. We prove that the asymptotic error bounds of the bandit algorithms exactly match their full information counterparts. The cost of restricted feedback is reflected in the larger sample complexity. Experimental results also support our theoretical findings, and performance of the proposed models is comparable to that of an existing bandit boosting algorithm, which is limited to use binary weak learners.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901020,0
C,"Bogunovic, I; Zhao, JY; Cevher, V",,"Storkey, A; PerezCruz, F",,"Bogunovic, Ilija; Zhao, Junyao; Cevher, Volkan",,,Robust Maximization of Non-Submodular Objectives,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"We study the problem of maximizing a monotone set function subject to a cardinality constraint k in the setting where some number of elements tau is deleted from the returned set. The focus of this work is on the worst-case adversarial setting. While there exist constant-factor guarantees when the function is submodular [1, 2], there are no guarantees for non-submodular objectives. In this work, we present a new algorithm OBLIVIOUS-GREEDY and prove the first constant-factor approximation guarantees for a wider class of non-submodular objectives. The obtained theoretical bounds are the first constant-factor bounds that also hold in the linear regime, i.e. when the number of deletions tau is linear in k. Our bounds depend on established parameters such as the submodularity ratio and some novel ones such as the inverse curvature. We bound these parameters for two important objectives including support selection and variance reduction. Finally, we numerically demonstrate the robust performance of OBLIVIOUS-GREEDY for these two objectives on various datasets.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300094,0
C,"Cavazza, J; Haeffele, BD; Lane, C; Morerio, P; Murino, V; Vidal, R",,"Storkey, A; PerezCruz, F",,"Cavazza, Jacopo; Haeffele, Benjamin D.; Lane, Connor; Morerio, Pietro; Murino, Vittorio; Vidal, Rene",,,Dropout as a Low-Rank Regularizer for Matrix Factorization,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Dropout is a simple yet effective regularization technique that has been applied to various machine learning tasks, including linear classification, matrix factorization (MF) and deep learning. However, despite its solid empirical performance, the theoretical properties of dropout as a regularizer remain quite elusive. In this paper, we present a theoretical analysis of dropout for MF, where Bernoulli random variables are used to drop columns of the factors. We demonstrate the equivalence between dropout and a fully deterministic model for MF in which the factors are regularized by the sum of the product of squared Euclidean norms of the columns. Additionally, we investigate the case of a variable sized factorization and we prove that dropout is equivalent to a convex approximation problem with (squared) nuclear norm regularization. As a consequence, we conclude that dropout induces a low-rank regularizer that results in a data dependent singular-value thresholding.",,,,,,"Murino, Vittorio/0000-0002-8645-2328",,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300046,0
C,"Derezinski, ML; Warmuth, MK",,"Storkey, A; PerezCruz, F",,"Derezinski, Micha L.; Warmuth, Manfred K.",,,Subsampling for Ridge Regression via Regularized Volume Sampling,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Given n vectors x(i) is an element of R-d, we want to fit a linear regression model for noisy labels y(i) is an element of R. The ridge estimator is a classical solution to this problem. However, when labels are expensive, we are forced to select only a small subset of vectors x(i) for which we obtain the labels y(i). We propose a new procedure for selecting the subset of vectors, such that the ridge estimator obtained from that subset offers strong statistical guarantees in terms of the mean squared prediction error over the entire dataset of n labeled vectors. The number of labels needed is proportional to the statistical dimension of the problem which is often much smaller than d. Our method is an extension of a joint subsampling procedure called volume sampling. A second major contribution is that we speed up volume sampling so that it is essentially as efficient as leverage scores, which is the main i.i.d. subsampling procedure for this task. Finally, we show theoretically and experimentally that volume sampling has a clear advantage over any i.i.d. sampling when labels are expensive.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300076,0
C,"Kamthe, S; Deisenroth, MP",,"Storkey, A; PerezCruz, F",,"Kamthe, Sanket; Deisenroth, Marc Peter",,,Data-Efficient Reinforcement Learning with Probabilistic Model Predictive Control,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Trial-and-error based reinforcement learning (RL) has seen rapid advancements in recent times, especially with the advent of deep neural networks. However, the majority of autonomous RL algorithms require a large number of interactions with the environment. A large number of interactions may be impractical in many real-world applications, such as robotics, and many practical systems have to obey limitations in the form of state space or control constraints. To reduce the number of system interactions while simultaneously handling constraints, we propose a model-based RL framework based on probabilistic Model Predictive Control (MPC). In particular, we propose to learn a probabilistic transition model using Gaussian Processes (GPs) to incorporate model uncertainty into long-term predictions, thereby, reducing the impact of model errors. We then use MPC to find a control sequence that minimises the expected long-term cost. We provide theoretical guarantees for first-order optimality in the GP-based transition models with deterministic approximate inference for long-term planning. We demonstrate that our approach does not only achieve state-of-the-art data efficiency, but also is a principled way for RL in constrained environments.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300178,0
C,"Li, TY; Rabusseau, G; Preup, D",,"Storkey, A; PerezCruz, F",,"Li, Tianyu; Rabusseau, Guillaume; Preup, Doina",,,Nonlinear Weighted Finite Automata,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Weighted finite automata (WFA) can expressively model functions defined over strings but are inherently linear models. Given the recent successes of nonlinear models in machine learning, it is natural to wonder whether extending WFA to the nonlinear setting would be beneficial. In this paper, we propose a novel model of neural network based nonlinear WFA model (NL-WFA) along with a learning algorithm. Our learning algorithm is inspired by the spectral learning algorithm for WFA and relies on a nonlinear decomposition of the so-called Hankel matrix, by means of an auto-encoder network. The expressive power of NL-WFA and the proposed learning algorithm are assessed on both synthetic and real world data, showing that NL-WFA can lead to smaller model sizes and infer complex grammatical structures from data.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300072,0
C,"Madry, A; Mitrovic, S; Schmidt, L",,"Storkey, A; PerezCruz, F",,"Madry, Aleksander; Mitrovic, Slobodan; Schmidt, Ludwig",,,A Fast Algorithm for Separated Sparsity via Perturbed Lagrangians,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Sparsity-based methods are widely used in machine learning, statistics, and signal processing. There is now a rich class of structured sparsity approaches that expand the modeling power of the sparsity paradigm and incorporate constraints such as group sparsity, graph sparsity, or hierarchical sparsity. While these sparsity models offer improved sample complexity and better interpretability, the improvements come at a computational cost: it is often challenging to optimize over the (non-convex) constraint sets that capture various sparsity structures. In this paper, we make progress in this direction in the context of separated sparsity - a fundamental sparsity notion that captures exclusion constraints in linearly ordered data such as time series. While prior algorithms for computing a projection onto this constraint set required quadratic time, we provide a perturbed Lagrangian relaxation approach that computes provably exact projection in only nearly-linear time. Although the sparsity constraint is nonconvex, our perturbed Lagrangian approach is still guaranteed to find a globally optimal solution. In experiments, our new algorithms offer a 10x speed-up already on moderately-size inputs.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300003,0
C,"Yang, SCH; Yu, Y; Givchi, A; Wang, P; Vong, WK; Shafto, P",,"Storkey, A; PerezCruz, F",,"Yang, Scott Cheng-Hsin; Yu, Yue; Givchi, Arash; Wang, Pei; Vong, Wai Keen; Shafto, Patrick",,,Optimal Cooperative Inference,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Cooperative transmission of data fosters rapid accumulation of knowledge by efficiently combining experiences across learners. Although well studied in human learning and increasingly in machine learning, we lack formal frameworks through which we may reason about the benefits and limitations of cooperative inference. We present such a framework. We introduce novel indices for measuring the effectiveness of probabilistic and cooperative information transmission. We relate our indices to the well-known Teaching Dimension in deterministic settings. We prove conditions under which optimal cooperative inference can be achieved, including a representation theorem that constrains the form of inductive biases for learners optimized for cooperative inference. We conclude by demonstrating how these principles may inform the design of machine learning algorithms and discuss implications for human and machine learning.",,,,,"Yu, Yue/AAE-8379-2020; Yang, Scott Cheng-Hsin/GMW-8090-2022","Yu, Yue/0000-0003-0505-7084; ",,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300040,0
C,"Jun, KS; Jamieson, K; Nowak, R; Zhu, XJ",,"Gretton, A; Robert, CC",,"Jun, Kwang-Sung; Jamieson, Kevin; Nowak, Robert; Zhu, Xiaojin",,,Top Arm Identification in Multi-Armed Bandits with Batch Arm Pulls,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"We introduce a new multi-armed bandit (MAB) problem in which arms must be sampled in batches, rather than one at a time. This is motivated by applications in social media monitoring and biological experimentation where such batch constraints naturally arise. This paper develops and analyzes algorithms for batch MABs and top arm identification, for both fixed confidence and fixed budget settings. Our main theoretical results show that the batch constraint does not significantly affect the sample complexity of top arm identification compared to unconstrained MAB algorithms. Alternatively, if one views a batch as the fundamental sampling unit, then the results can be interpreted as showing that the sample complexity of batch MABs can be significantly less than traditional MABs. We demonstrate the new batch MAB algorithms with simulations and in two interesting real-world applications: (i) microwell array experiments for identifying genes that are important in virus replication and (ii) finding the most active users in Twitter on a specific topic.",,,,,,"Jun, Kwang-Sung/0000-0001-5483-3161",,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,139,148,,,,,,,,,,,,,,,,WOS:000508662100016,0
C,"Kalofolias, V",,"Gretton, A; Robert, CC",,"Kalofolias, Vassilis",,,How to learn a graph from smooth signals,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"We propose a framework to learn the graph structure underlying a set of smooth signals. Given X is an element of R-mxn whose rows reside on the vertices of an unknown graph, we learn the edge weights w is an element of R-+(m(m-1)/2) under the smoothness assumption that tr((XLX)-L-inverted perpendicular) is small, where L is the graph Laplacian. We show that the problem is a weighted l-1 minimization that leads to naturally sparse solutions. We prove that the standard graph construction with Gaussian weights w(ij) = exp (-1/sigma(2)parallel to x(i)-x(j)parallel to(2)) and the previous state of the art are special cases of our framework. We propose a new model and present efficient, scalable primal-dual based algorithms both for this and the previous state of the art, to evaluate their performance on artificial and real data. The new model performs best in most settings.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,920,929,,,,,,,,,,,,,,,,WOS:000508662100100,0
C,"Kocal, T; Neu, G; Valko, M",,"Gretton, A; Robert, CC",,"Kocal, Tomas; Neu, Gergely; Valko, Michal",,,Online learning with noisy side observations,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"We propose a new partial-observability model for online learning problems where the learner, besides its own loss, also observes some noisy feedback about the other actions, depending on the underlying structure of the problem. We represent this structure by a weighted directed graph, where the edge weights are related to the quality of the feedback shared by the connected nodes. Our main contribution is an efficient algorithm that guarantees a regret of (O) over tilde(root alpha*T) after T rounds, where alpha* is a novel graph property that we call the effective independence number. Our algorithm is completely parameter-free and does not require knowledge (or even estimation) of alpha*. For the special case of binary edge weights, our setting reduces to the partial-observability models of Mannor & Shamir (2011) and Alon et al. (2013) and our algorithm recovers the near-optimal regret bounds.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,1186,1194,,,,,,,,,,,,,,,,WOS:000508662100129,0
C,"Murray, I; Graham, MM",,"Gretton, A; Robert, CC",,"Murray, Iain; Graham, Matthew M.",,,Pseudo-Marginal Slice Sampling,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Markov chain Monte Carlo (MCMC) methods asymptotically sample from complex probability distributions. The pseudo-marginal MCMC framework only requires an unbiased estimator of the unnormalized probability distribution function to construct a Markov chain. However, the resulting chains are harder to tune to a target distribution than conventional MCMC, and the types of updates available are limited. We describe a general way to clamp and update the random numbers used in a pseudo-marginal method's unbiased estimator. In this framework we can use slice sampling and other adaptive methods. We obtain more robust Markov chains, which often mix more quickly.",,,,,"Robinson, Christopher/FKL-4907-2022",,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,911,919,,,,,,,,,,,,,,,,WOS:000508662100099,0
C,"Shang, FH; Liu, YY; Cheng, J",,"Gretton, A; Robert, CC",,"Shang, Fanhua; Liu, Yuanyuan; Cheng, James",,,Tractable and Scalable Schatten Quasi-Norm Approximations for Rank Minimization,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"The Schatten quasi-norm was introduced to bridge the gap between the trace norm and rank function. However, existing algorithms are too slow or even impractical for large-scale problems. Motivated by the equivalence relation between the trace norm and its bilinear spectral penalty, we define two tractable Schatten norms, i.e. the bi-trace and tri-trace norms, and prove that they are in essence the Schatten-1/2 and 1/3 quasi-norms, respectively. By applying the two defined Schatten quasi-norms to various rank minimization problems such as MC and RPCA, we only need to solve much smaller factor matrices. We design two efficient linearized alternating minimization algorithms to solve our problems and establish that each bounded sequence generated by our algorithms converges to a critical point. We also provide the restricted strong convexity (RSC) based and MC error bounds for our algorithms. Our experimental results verified both the efficiency and effectiveness of our algorithms compared with the state-of-the-art methods.",,,,,"liu, yuanyuan/GWZ-5838-2022",,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,620,629,,,,,,,,,,,,,,,,WOS:000508662100068,0
C,"Wang, JL; Kolar, M; Srebro, N",,"Gretton, A; Robert, CC",,"Wang, Jialei; Kolar, Mladen; Srebro, Nathan",,,Distributed Multi-Task Learning,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"We consider the problem of distributed multi-task learning, where each machine learns a separate, but related, task. Specifically, each machine learns a linear predictor in high-dimensional space, where all tasks share the same small support. We present a communication-efficient estimator based on the debiased lasso and show that it is comparable with the optimal centralized method.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,751,760,,,,,,,,,,,,,,,,WOS:000508662100082,0
C,"Hoffman, MW; Shahriari, B; de Freitas, N",,"Kaski, S; Corander, J",,"Hoffman, Matthew W.; Shahriari, Bobak; de Freitas, Nando",,,On correlation and budget constraints in model-based bandit optimization with application to automatic machine learning,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"We address the problem of finding the maximizer of a nonlinear function that can only be evaluated, subject to noise, at a finite number of query locations. Further, we will assume that there is a constraint on the total number of permitted function evaluations. We introduce a Bayesian approach for this problem and show that it empirically outperforms both the existing frequentist counterpart and other Bayesian optimization methods. The Bayesian approach places emphasis on detailed modelling, including the modelling of correlations among the arms. As a result, it can perform well in situations where the number of arms is much larger than the number of allowed function evaluation, whereas the frequentist counterpart is inapplicable. This feature enables us to develop and deploy practical applications, such as automatic machine learning toolboxes. The paper presents comprehensive comparisons of the proposed approach with many Bayesian and bandit optimization techniques, the first comparison of many of these methods in the literature.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,365,374,,,,,,,,,,,,,,,,WOS:000508355800041,0
C,"Al-Shedivat, M; Li, L; Xing, E; Talwalkar, A",,"Banerjee, A; Fukumizu, K",,"Al-Shedivat, Maruan; Li, Liam; Xing, Eric; Talwalkar, Ameet",,,Data Efficiency of Meta-learning,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Meta-learning has enabled learning statistical models that can be quickly adapted to new prediction tasks. Motivated by use-cases in personalized federated learning, we study the often overlooked aspect of the modern meta-learning algorithms-their data efficiency. To shed more light on which methods are more efficient, we use techniques from algorithmic stability to derive bounds on the transfer risk that have important practical implications, indicating how much supervision is needed and how it must be allocated for each method to attain the desired level of generalization. Further, we introduce a new simple framework for evaluating meta-learning methods under a limit on the available supervision, conduct an empirical study of MAML, Reptile, and PROTONETS, and demonstrate the differences in the behavior of these methods on few-shot and federated learning benchmarks. Finally, we propose active meta-learning, which incorporates active data selection into learning-to-learn, leading to better performance of all methods in the limited supervision regime.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801066,0
C,"Bas-Serrano, J; Curi, S; Krause, A; Neu, G",,"Banerjee, A; Fukumizu, K",,"Bas-Serrano, Joan; Curi, Sebastian; Krause, Andreas; Neu, Gergely",,,Logistic Q-Learning,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We propose a new reinforcement learning algorithm derived from a regularized linear-programming formulation of optimal control in MDPs. The method is closely related to the classic Relative Entropy Policy Search (REPS) algorithm of Peters et al. (2010), with the key difference that our method introduces a Q-function that enables efficient exact model-free implementation. The main feature of our algorithm (called Q-REPS) is a convex loss function for policy evaluation that serves as a theoretically sound alternative to the widely used squared Bellman error. We provide a practical saddle-point optimization method for minimizing this loss function and provide an error-propagation analysis that relates the quality of the individual updates to the performance of the output policy. Finally, we demonstrate the effectiveness of our method on a range of benchmark problems.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804036,0
C,"Chen, CL; Zhang, JW; Li, S; Zhao, PL; Luo, ZQ",,"Banerjee, A; Fukumizu, K",,"Chen, Congliang; Zhang, Jiawei; Shen, Li; Zhao, Peilin; Luo, Zhi-Quan",,,Communication Efficient Primal-Dual Algorithm for Nonconvex Nonsmooth Distributed Optimization,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Decentralized optimization frequently appears in large scale machine learning problems. However, few works have been focused on solving the decentralized optimization problems under the difficult nonconvex nonsmooth setting. In this paper, we propose a distributed primal-dual algorithm to solve this type of problems in a decentralized manner and the proposed algorithm can achieve an O (1/epsilon(2)) iteration complexity to attain an epsilon-solution, which is the well-known lower iteration complexity bound for nonconvex optimization. Furthermore, to reduce communication overhead, we also modify our algorithm by compressing the vectors exchanged between nodes. The iteration complexity of the algorithm with compression is still O(1/epsilon(2)). To our knowledge, it is the first algorithm achieving this rate under a nonconvex, nonsmooth decentralized setting with compression. Besides, we apply the proposed algorithm to solve nonconvex linear regression problem and train a deep learning model, both of which demonstrate the efficacy of the proposed algorithms.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801091,0
C,"Khemakhem, I; Monti, RP; Leech, R; Hyvarinen, A",,"Banerjee, A; Fukumizu, K",,"Khemakhem, Ilyes; Monti, Ricardo P.; Leech, Robert; Hyvarinen, Aapo",,,Causal Autoregressive Flows,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Two apparently unrelated fields - normalizing flows and causality - have recently received considerable attention in the machine learning community. In this work, we highlight an intrinsic correspondence between a simple family of autoregressive normalizing flows and identifiable causal models. We exploit the fact that autoregressive flow architectures define an ordering over variables, analogous to a causal ordering, to show that they are well-suited to performing a range of causal inference tasks, ranging from causal discovery to making interventional and counterfactual predictions. First, we show that causal models derived from both affine and additive autoregressive flows with fixed orderings over variables are identifiable, i.e. the true direction of causal influence can be recovered. This provides a generalization of the additive noise model well-known in causal discovery. Second, we derive a bivariate measure of causal direction based on likelihood ratios, leveraging the fact that flow models can estimate normalized log-densities of data. Third, we demonstrate that flows naturally allow for direct evaluation of both interventional and counterfactual queries, the latter case being possible due to the invertible nature of flows. Finally, throughout a series of experiments on synthetic and real data, the proposed method is shown to outperform current approaches for causal discovery as well as making accurate interventional and counterfactual predictions.",,,,,,"Hyvarinen, Aapo/0000-0002-5806-4432",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804026,0
C,"Laclau, C; Redko, I; Choudhary, M; Largeron, C",,"Banerjee, A; Fukumizu, K",,"Laclau, Charlotte; Redko, Ievgen; Choudhary, Manvi; Largeron, Christine",,,All of the Fairness for Edge Prediction with Optimal Transport,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Machine learning and data mining algorithms have been increasingly used recently to support decision-making systems in many areas of high societal importance such as healthcare, education, or security. While being very efficient in their predictive abilities, the deployed algorithms sometimes tend to learn an inductive model with a discriminative bias due to the presence of this latter in the learning sample. This problem gave rise to a new field of algorithmic fairness where the goal is to correct the discriminative bias introduced by a certain attribute in order to decorrelate it from the model's output. In this paper, we study the problem of fairness for the task of edge prediction in graphs, a largely underinvestigated scenario compared to a more popular setting of fair classification. To this end, we formulate the problem of fair edge prediction, analyze it theoretically, and propose an embedding-agnostic repairing procedure for the adjacency matrix of an arbitrary graph with a trade-off between the group and individual fairness. We experimentally show the versatility of our approach and its capacity to provide explicit control over different notions of fairness and prediction accuracy.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802017,0
C,"Li, YT; Wang, CH; Cheng, G",,"Banerjee, A; Fukumizu, K",,"Li, Yuantong; Wang, Chi-Hua; Cheng, Guang",,,Online Forgetting Process for Linear Regression Models,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Motivated by the EU's Right To Be Forgotten regulation, we initiate a study of statistical data deletion problems where users' data are accessible only for a limited period of time. This setting is formulated as an online supervised learning task with constant memory limit. We propose a deletion-aware algorithm FIFD-OLS for the low dimensional case, and witness a catastrophic rank swinging phenomenon due to the data deletion operation, which leads to statistical inefficiency. As a remedy, we propose the FIFD-Adaptive Ridge algorithm with a novel online regularization scheme, that effectively offsets the uncertainty from deletion. In theory, we provide the cumulative regret upper bound for both online forgetting algorithms. In the experiment, we showed FIFD-Adaptive Ridge outperforms the ridge regression algorithm with fixed regularization level, and hopefully sheds some light on more complex statistical models.",,,,,,"Li, Yuantong/0000-0001-7420-2332",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,217,225,,,,,,,,,,,,,,,,WOS:000659893800025,0
C,"Ramchandran, S; Koskinen, M; Lahdesmaki, H",,"Banerjee, A; Fukumizu, K",,"Ramchandran, Siddharth; Koskinen, Miika; Lahdesmaki, Harri",,,Latent Gaussian process with composite likelihoods and numerical quadrature,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Clinical patient records are an example of high-dimensional data that is typically collected from disparate sources and comprises of multiple likelihoods with noisy as well as missing values. In this work, we propose an unsupervised generative model that can learn a low-dimensional representation among the observations in a latent space, while making use of all available data in a heterogeneous data setting with missing values. We improve upon the existing Gaussian process latent variable model (GPLVM) by incorporating multiple likelihoods and deep neural network parameterised back-constraints to create a non-linear dimensionality reduction technique for heterogeneous data. In addition, we develop a variational inference method for our model that uses numerical quadrature. We establish the effectiveness of our model and compare against existing GPLVM methods on a standard benchmark dataset as well as on clinical data of Parkinson's disease patients treated at the HUS Helsinki University Hospital.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804048,0
C,"Wang, Z; Zhang, CC; Singh, MK; Riek, LD; Chaudhuri, K",,"Banerjee, A; Fukumizu, K",,"Wang, Zhi; Zhang, Chicheng; Singh, Manish Kumar; Riek, Laurel D.; Chaudhuri, Kamalika",,,Multitask Bandit Learning Through Heterogeneous Feedback Aggregation,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"In many real-world applications, multiple agents seek to learn how to perform highly related yet slightly different tasks in an online bandit learning protocol. We formulate this problem as the epsilon-multi-player multi-armed bandit problem, in which a set of players concurrently interact with a set of arms, and for each arm, the reward distributions for all players are similar but not necessarily identical. We develop an upper confidence boundbased algorithm, ROBUSTAGG(epsilon), that adaptively aggregates rewards collected by different players. In the setting where an upper bound on the pairwise dissimilarities of reward distributions between players is known, we achieve instance-dependent regret guarantees that depend on the amenability of information sharing across players. We complement these upper bounds with nearly matching lower bounds. In the setting where pairwise dissimilarities are unknown, we provide a lower bound, as well as an algorithm that trades off minimax regret guarantees for adaptivity to unknown similarity structure.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801084,0
C,"Wei, YD; Sheth, R; Khardon, R",,"Banerjee, A; Fukumizu, K",,"Wei, Yadi; Sheth, Rishit; Khardon, Roni",,,Direct Loss Minimization for Sparse Gaussian Processes,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"The paper provides a thorough investigation of Direct Loss Minimization (DLM), which optimizes the posterior to minimize predictive loss, in sparse Gaussian processes. For the conjugate case, we consider DLM for log-loss and DLM for square loss showing a significant performance improvement in both cases. The application of DLM in non-conjugate cases is more complex because the logarithm of expectation in the log-loss DLM objective is often intractable and simple sampling leads to biased estimates of gradients. The paper makes two technical contributions to address this. First, a new method using product sampling is proposed, which gives unbiased estimates of gradients (uPS) for the objective function. Second, a theoretical analysis of biased Monte Carlo estimates (bMC) shows that stochastic gradient descent converges despite the biased gradients. Experiments demonstrate empirical success of DLM. A comparison of the sampling methods shows that, while uPS is potentially more sample-efficient, bMC provides a better tradeoff in terms of convergence time and computational efficiency.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803012,0
C,"Xu, TY; Liang, YY",,"Banerjee, A; Fukumizu, K",,"Xu, Tengyu; Liang, Yingbin",,,Sample Complexity Bounds for Two Timescale Value-based Reinforcement Learning Algorithms,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Two timescale stochastic approximation (SA) has been widely used in value-based reinforcement learning algorithms. In the policy evaluation setting, it can model the linear and nonlinear temporal difference learning with gradient correction (TDC) algorithms as linear SA and nonlinear SA, respectively. In the policy optimization setting, two timescale nonlinear SA can also model the greedy gradient-Q (Greedy-GQ) algorithm. In previous studies, the non-asymptotic analysis of linear TDC and Greedy-GQ has been studied in the Markovian setting, with diminishing or accuracy-dependent stepsize. For the nonlinear TDC algorithm, only the asymptotic convergence has been established. In this paper, we study the non-asymptotic convergence rate of two timescale linear and nonlinear TDC and Greedy-GQ under Markovian sampling and with accuracy-independent constant stepsize. For linear TDC, we provide a novel non-asymptotic analysis and show that it attains an epsilon-accurate solution with the optimal sample complexity of O(epsilon(-1) log(1/epsilon)) under a constant stepsize. For nonlinear TDC and Greedy-GQ, we show that both algorithms attain epsilon-accurate stationary solution with sample complexity O(epsilon(-2)). It is the first non-asymptotic convergence result established for nonlinear TDC under Markovian sampling and our result for Greedy-GQ outperforms the previous result orderwisely by a factor of O(epsilon(-1) log(1/epsilon)).",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801004,0
C,"Xu, ZQ; Li, P",,"Banerjee, A; Fukumizu, K",,"Xu, Zhiqiang; Li, Ping",,,On the Faster Alternating Least-Squares for CCA,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We study alternating least-squares (ALS) for canonical correlation analysis (CCA). Recent research shows that the alternating least-squares solver for k-CCA can be directly accelerated with momentum and prominent performance gain has been observed in practice for the resulting simple algorithm. However, despite the simplicity, it is difficult for the accelerated rate to be analyzed in theory in order to explain and match the empirical performance gain. By looking into two neighboring iterations, in this work, we propose an even simpler variant of the faster alternating least-squares solver. Instead of applying momentum to each update for acceleration, the proposed variant only leverages momentum at every other iteration and can converge at a provably faster linear rate of nearly square-root dependence on the singular value gap of the whitened cross-covariance matrix. In addition to the high consistency between theory and practice, experimental studies also show that our variant of the alternating leastsquares algorithm as a block CCA solver is even more pass efficient than other variants.",,,,,"Xu, Zhiqiang/AAB-7414-2022",,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801094,0
C,"Chen, C; Gu, M; Zhang, ZH; Zhang, WN; Yu, Y",,"Chiappa, S; Calandra, R",,"Chen, Cheng; Gu, Ming; Zhang, Zhihua; Zhang, Weinan; Yu, Yong",,,Efficient Spectrum-Revealing CUR Matrix Decomposition,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"The CUR matrix decomposition is an important tool for low-rank matrix approximation. It approximates a data matrix though selecting a small number of columns and rows of the matrix. Those CUR algorithms with gap-dependent approximation bounds can obtain high approximation quality for matrices with good singular value spectrum decay, but they have impractically high time complexities. In this paper, we propose a novel CUR algorithm based on truncated LU factorization with an efficient variant of complete pivoting. Our algorithm has gap-dependent approximation bounds on both spectral and Frobenius norms while maintaining high efficiency. Numerical experiments demonstrate the effectiveness of our algorithm and verify our theoretical guarantees.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,766,774,,,,,,,,,,,,,,,,WOS:000559931300054,0
C,"Karimi, AH; Barthe, G; Balle, B; Valera, I",,"Chiappa, S; Calandra, R",,"Karimi, Amir-Hossein; Barthe, Gilles; Balle, Borja; Valera, Isabel",,,Model-Agnostic Counterfactual Explanations for Consequential Decisions,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Predictive models are being increasingly used to support consequential decision making at the individual level in contexts such as pretrial bail and loan approval. As a result, there is increasing social and legal pressure to provide explanations that help the affected individuals not only to understand why a prediction was output, but also how to act to obtain a desired outcome. To this end, several works have proposed optimization-based methods to generate nearest counterfactual explanations. However, these methods are often restricted to a particular subset of models (e.g., decision trees or linear models) and differentiable distance functions. In contrast, we build on standard theory and tools from formal verification and propose a novel algorithm that solves a sequence of satisfiability problems, where both the distance function (objective) and predictive model (constraints) are represented as logic formulae. As shown by our experiments on real-world data, our algorithm is: i) model-agnostic ({non-}linear, {non-}differentiable, {non-}convex); ii) data-type-agnostic (heterogeneous features); iii) distance-agnostic (l(0), l(1), l(infinity), and combinations thereof); iv) able to generate plausible and diverse counterfactuals for any sample (i.e., 100% coverage); and v) at provably optimal distances.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,895,904,,,,,,,,,,,,,,,,WOS:000559931301065,0
C,"Kirschner, J; Bogunovic, I; Jegelka, S; Krause, A",,"Chiappa, S; Calandra, R",,"Kirschner, Johannes; Bogunovic, Ilija; Jegelka, Stefanie; Krause, Andreas",,,Distributionally Robust Bayesian Optimization,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Robustness to distributional shift is one of the key challenges of contemporary machine learning. Attaining such robustness is the goal of distributionally robust optimization, which seeks a solution to an optimization problem that is worst-case robust under a specified distributional shift of an uncontrolled covariate. In this paper, we study such a problem when the distributional shift is measured via the maximum mean discrepancy (MMD). For the setting of zeroth-order, noisy optimization, we present a novel distributionally robust Bayesian optimization algorithm (DRBO). Our algorithm provably obtains sub-linear robust regret in various settings that differ in how the uncertain covariate is observed. We demonstrate the robust performance of our method on both synthetic and real-world benchmarks.",,,,,,"Krause, Andreas/0000-0001-7260-9673; Kirschner, Johannes/0000-0002-7228-8280",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1921,1930,,,,,,,,,,,,,,,,WOS:000559931301078,0
C,"Merigot, Q; Delalande, A; Chazal, F",,"Chiappa, S; Calandra, R",,"Merigot, Quentin; Delalande, Alex; Chazal, Frederic",,,Quantitative Stability of Optimal Transport Maps and Linearization of the 2-Wasserstein Space,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"This work studies an explicit embedding of the set of probability measures into a Hilbert space, defined using optimal transport maps from a reference probability density. This embedding linearizes to some extent the 2-Wasserstein space and is shown to be bi-Holder continuous. It enables the direct use of generic supervised and unsupervised learning algorithms on measure data consistently w.r.t. the Wasserstein geometry.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3186,3195,,,,,,,,,,,,,,,,WOS:000559931302041,0
C,"Wang, Y; Wang, LB",,"Chiappa, S; Calandra, R",,"Wang, Yue; Wang, Linbo",,,Causal inference in degenerate systems: An impossibility result,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Causal relationships among variables are commonly represented via directed acyclic graphs. There are many methods in the literature to quantify the strength of arrows in a causal acyclic graph. These methods, however, have undesirable properties when the causal system represented by a directed acyclic graph is degenerate. In this paper, we characterize a degenerate causal system using multiplicity of Markov boundaries. We show that in this case, it is impossible to find an identifiable quantitative measure of causal effects that satisfy a set of natural criteria. To supplement the impossibility result, we also develop algorithms to identify degenerate causal systems from observed data. Performance of our algorithms is investigated through synthetic data analysis.",,,,,"Wang, Yue/GZG-6184-2022; wang, lili/HDL-7210-2022; wang, lin/GSE-3040-2022","Wang, Yue/0000-0001-5918-7525",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303067,0
C,"Acharya, J; Sun, ZT; Zhang, HY",,"Chaudhuri, K; Sugiyama, M",,"Acharya, Jayadev; Sun, Ziteng; Zhang, Huanyu",,,"Hadamard Response: Estimating Distributions Privately, Efficiently, and with Little Communication","22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We study the problem of estimating kary distributions under s-local differential privacy. n samples are distributed across users who send privatized versions of their sample to a central server. All previously known sample optimal algorithms require linear (in k) communication from each user in the high privacy regime (s = 0(1)), and run in time that grows as n k, which is prohibitive for a large domain size k. We propose Hadamard Response (HR), a local privatization scheme that requires no shared randomness and is symmetric with respect to the users. HR has order optimal sample complexity for all s, a communication of at most log k + 2 bits per user, and nearly linear running time of O(n + k). HR is based on Hadamard matrices, and is simple to implement. The statistical performance relies on the coding theoretic aspects of Hadamard matrices, ie, the large Hamming distance between the rows. Computational efficiency is achieved by using the Fast Walsh-Hadamard transform. We compare our approach with Randomized Response (RR), RAPPOR, and subset-selection mechanisms (SS), both theoretically, and experimentally. For k 10000, our algorithm runs about 100x faster than SS, and RAPPOR.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901017,0
C,"Alvarez, MA; Ward, WOC; Guarnizo, C",,"Chaudhuri, K; Sugiyama, M",,"Alvarez, Mauricio A.; Ward, Wil O. C.; Guarnizo, Cristian",,,Non-linear process convolutions for multi-output Gaussian processes,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"The paper introduces a non-linear version of the process convolution formalism for building covariance functions for multi-output Gaussian processes. The non-linearity is introduced via Volterra series, one series per each output. We provide closed-form expressions for the mean function and the covariance function of the approximated Gaussian process at the output of the Volterra series. The mean function and covariance function for the joint Gaussian process are derived using formulae for the product moments of Gaussian variables. We compare the performance of the non-linear model against the classical process convolution approach in one synthetic dataset and two real datasets.",,,,,,"Guarnizo Lemus, Cristian/0000-0002-3853-7976",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902002,0
C,"Bauer, A; Nakajima, S; Gornitz, N; Muller, KR",,"Chaudhuri, K; Sugiyama, M",,"Bauer, Alexander; Nakajima, Shinichi; Goernitz, Nico; Mueller, Klaus-Robert",,,Partial Optimality of Dual Decomposition for MAP Inference in Pairwise MRFs,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Markov random fields (MRFs) are a powerful tool for modelling statistical dependencies for a set of random variables using a graphical representation. An important computational problem related to MRFs, called maximum a posteriori (MAP) inference, is finding a joint variable assignment with the maximal probability. It is well known that the two popular optimisation techniques for this task, linear programming (LP) relaxation and dual decomposition (DD), have a strong connection both providing an optimal solution to the MAP problem when a corresponding LP relaxation is tight. However, less is known about their relationship in the opposite and more realistic case. In this paper, we explain how the fully integral assignments obtained via DD partially agree with the optimal fractional assignments via LP relaxation when the latter is not tight. In particular, for binary pairwise MRFs the corresponding result suggests that both methods share the partial optimality property of their solutions.",,,,,"Mueller, Klaus-Robert/Y-3547-2019","Mueller, Klaus-Robert/0000-0002-3861-7685",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901077,0
C,"Chai, H; Garnett, R",,"Chaudhuri, K; Sugiyama, M",,"Chai, Henry; Garnett, Roman",,,Improving Quadrature for Constrained Integrands,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We present an improved Bayesian framework for performing inference of affine transformations of constrained functions. We focus on quadrature with nonnegative functions, a common task in Bayesian inference. We consider constraints on the range of the function of interest, such as nonnegativity or boundedness. Although our framework is general, we derive explicit approximation schemes for these constraints, and argue for the use of a log transformation for functions with high dynamic range such as likelihood surfaces. We propose a novel method for optimizing hyperparameters in this framework: we optimize the marginal likelihood in the original space, as opposed to in the transformed space. The result is a model that better explains the actual data. Experiments on synthetic and real-world data demonstrate our framework achieves superior estimates using less wall-clock time than existing Bayesian quadrature procedures.",,,,,,"Chai, Henry/0000-0003-3323-3903",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902082,0
C,"Laude, E; Wu, T; Cremers, D",,"Chaudhuri, K; Sugiyama, M",,"Laude, Emanuel; Wu, Tao; Cremers, Daniel",,,Optimization of Inf-Convolution Regularized Nonconvex Composite Problems,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"In this work, we consider nonconvex composite problems that involve inf-convolution with a Legendre function, which gives rise to an anisotropic generalization of the proximal mapping and Moreau-envelope. In a convex setting such problems can be solved via alternating minimization of a splitting formulation, where the consensus constraint is penalized with a Legendre function. In contrast, for nonconvex models it is in general unclear that this approach yields stationary points to the infimal convolution problem. To this end we analytically investigate local regularity properties of the Moreau-envelope function under prox-regularity, which allows us to establish the equivalence between stationary points of the splitting model and the original inf-convolution model. We apply our theory to characterize stationary points of the penalty objective, which is minimized by the elastic averaging SCD (EASGD) method for distributed training. Numerically, we demonstrate the practical relevance of the proposed approach on the important task of distributed training; of deep neural networks.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,547,556,,,,,,,,,,,,,,,,WOS:000509687900057,0
C,"Li, BC; Chen, TY; Giannakis, GB",,"Chaudhuri, K; Sugiyama, M",,"Li, Bingcong; Chen, Tianyi; Giannakis, Georgios B.",,,Bandit Online Learning with Unknown Delays,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"This paper deals with bandit online learning, where feedback of unknown delay can emerge in non-stochastic multi-armed bandit (MAB) and bandit convex optimization (BCO) settings. MAB and BCO require only values of the objective function to become available through feedback, and are used to estimate the gradient appearing in the corresponding iterative algorithms. Since the challenging case of feedback with unknown delays prevents one from constructing the sought gradient estimates, existing MAB and BCO algorithms become intractable. Delayed exploration, exploitation, and exponential (DEXP3) iterations, along with delayed bandit gradient descent (DBGD) iterations are developed for MAB and BCO with unknown delays, respectively. Based on a unifying analysis framework, it is established that both DEXP3 and DBGD guarantee an (O) over tilde(root K(T + D)) regret, where D denotes the delay accumulated over T slots, and K represents the number of arms in MAB or the dimension of decision variables in BCO. Numerical tests using both synthetic and real data validate DEXP3 and DBGD.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901004,0
C,"Liang, TY; Stokes, J",,"Chaudhuri, K; Sugiyama, M",,"Liang, Tengyuan; Stokes, James",,,Interaction Matters: A Note on Non-asymptotic Local Convergence of Generative Adversarial Networks,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Motivated by the pursuit of a systematic computational and algorithmic understanding of Generative Adversarial Networks (GANs), we present a simple yet unified non-asymptotic local convergence theory for smooth two-player games, which subsumes several discrete-time gradient-based saddle point dynamics. The analysis reveals the surprising nature of the off-diagonal interaction term as both a blessing and a curse. On the one hand, this interaction term explains the origin of the slow-down effect in the convergence of Simultaneous Gradient Ascent (SGA) to stable Nash equilibria. On the other hand, for the unstable equilibria, exponential convergence can be proved thanks to the interaction term, for four modified dynamics proposed to stabilize GAN training: Optimistic Mirror Descent (OMD), Consensus Optimization (CO), Implicit Updates (IU) and Predictive Method (PM). The analysis uncovers the intimate connections among these stabilizing techniques, and provides detailed characterization on the choice of learning rate. As a by-product, we present a new analysis for OMD proposed in Daskalakis, Ilyas, Syrgkanis, and Zeng [2017] with improved rates.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,907,915,,,,,,,,,,,,,,,,WOS:000509687900094,0
C,"Subbaswamy, A; Schulam, P; Saria, S",,"Chaudhuri, K; Sugiyama, M",,"Subbaswamy, Adarsh; Schulam, Peter; Saria, Suchi",,,Preventing Failures Due to Dataset Shift: Learning Predictive Models That Transport,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Classical supervised learning produces unreliable models when training and target distributions differ, with most existing solutions requiring samples from the target domain. We propose a proactive approach which learns a relationship in the training domain that will generalize to the target domain by incorporating prior knowledge of aspects of the data generating process that are expected to differ as expressed in a causal selection diagram. Specifically, we remove variables generated by unstable mechanisms from the joint factorization to yield the Surgery Estimator an interventional distribution that is invariant to the differences across environments. We prove that the surgery estimator finds stable relationships in strictly more scenarios than previous approaches which only consider conditional relationships, and demonstrate this in simulated experiments. We also evaluate on real world data for which the true causal diagram is unknown, performing competitively against entirely data-driven approaches.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687903018,0
C,"Tarnowski, W; Warchol, P; Jastrzebski, S; Tabor, J; Nowak, MA",,"Chaudhuri, K; Sugiyama, M",,"Tarnowski, Wojciech; Warchol, Piotr; Jastrzebski, Stanislaw; Tabor, Jacek; Nowak, Maciej A.",,,Dynamical Isometry is Achieved in Residual Networks in a Universal Way for any Activation Function,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We demonstrate that in residual neural networks (ResNets) dynamical isometry is achievable irrespective of the activation function used. We do that by deriving, with the help of Free Probability and Random Matrix Theories, a universal formula for the spectral density of the input-output Jacobian at initialization, in the large network width and depth limit. The resulting singular value spectrum depends on a single parameter, which we calculate for a variety of popular activation functions, by analyzing the signal propagation in the artificial neural network. We corroborate our results with numerical simulations of both random matrices and ResNets applied to the CIFAR-10 classification problem. Moreover, we study consequences of this universal behavior for the initial and late phases of the learning processes. We conclude by drawing attention to the simple fact, that initialization acts as a confounding factor between the choice of activation function and the rate of learning. We propose that in ResNets this can be resolved based on our results by ensuring the same level of dynamical isometry at initialization.",,,,,"Jastrzbski, Stanisaw/AAX-4621-2020","Jastrzbski, Stanisaw/0000-0003-4138-1818",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902028,0
C,"Vashishth, S; Yadav, P; Bhandari, M; Talukdar, P",,"Chaudhuri, K; Sugiyama, M",,"Vashishth, Shikhar; Yadav, Prateek; Bhandari, Manik; Talukdar, Partha",,,Confidence-based Graph Convolutional Networks for Semi-Supervised Learning,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Predicting properties of nodes in a graph is an important problem with applications in a variety of domains. Graph-based Semi Supervised Learning (SSL) methods aim to address this problem by labeling a small subset of the nodes as seeds and then utilizing the graph structure to predict label scores for the rest of the nodes in the graph. Recently, Graph Convolutional Networks (GCNs) have achieved impressive performance on the graph-based SSL task. In addition to label scores, it is also desirable to have confidence scores associated with them. Unfortunately, confidence estimation in the context of GCN has not been previously explored. We fill this important gap in this paper and propose ConfGCN, which estimates labels scores along with their confidences jointly in GCN-based setting. ConfGCN uses these estimated confidences to determine the influence of one node on another during neighborhood aggregation, thereby acquiring anisotropicl capabilities. Through extensive analysis and experiments on standard benchmarks, we find that ConfGCN is able to outperform state-of-the-art baselines. We have made ConfGCN's source code available to encourage reproducible research.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901087,0
C,"Wang, YX; Balle, B; Kasiviswanathan, SP",,"Chaudhuri, K; Sugiyama, M",,"Wang, Yu-Xiang; Balle, Borja; Kasiviswanathan, Shiva Prasad",,,Subsampled Renyi Differential Privacy and Analytical Moments Accountant,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We study the problem of subsampling in differential privacy (DP), a question that is the centerpiece behind many successful differentially private machine learning algorithms. Specifically, we provide a tight upper bound on the Renyi Differential Privacy (RDP) (Mironov, 2017) parameters for algorithms that: (1) subsample the dataset, and then (2) apply a randomized mechanism M to the subsample, in terms of the RDP parameters of M and the subsampling probability parameter. Our results generalize the moments accounting technique, developed by Abadi et al. (2016) for the Gaussian mechanism, to any subsampled RDP mechanism.",,,,,,"Wang, Yu-Xiang/0000-0002-6403-212X",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901028,0
C,"Zhang, HY; Sharan, V; Charikar, M; Liang, YY",,"Chaudhuri, K; Sugiyama, M",,"Zhang, Hongyang; Sharan, Vatsal; Charikar, Moses; Liang, Yingyu",,,Recovery Guarantees For Quadratic Tensors With Sparse Observations,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We consider the tensor completion problem of predicting the missing entries of a tensor. The commonly used CP model has a triple product form, but an alternate family of quadratic models which are the sum of pairwise products instead of a triple product have emerged from applications such as recommendation systems. Non-convex methods are the method of choice for learning quadratic models, and this work examines their sample complexity and error guarantee. Our main result is that with the number of samples being only linear in the dimension, all local minima of the mean squared error objective are global minima and recover the original tensor. We substantiate our theoretical results with experiments on synthetic and real world data.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687903039,0
C,"Bartunov, S; Vetrov, DP",,"Storkey, A; PerezCruz, F",,"Bartunov, Sergey; Vetrov, Dmitry P.",,,Few-shot Generative Modelling with Generative Matching Networks,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Despite recent advances, the remaining bottlenecks in deep generative models are necessity of extensive training and difficulties with generalization from small number of training examples. We develop a new generative model called Generative Matching Network which is inspired by the recently proposed matching networks for one-shot learning in discriminative tasks. By conditioning on the additional input dataset, our model can instantly learn new concepts that were not available in the training data but conform to a similar generative process. The proposed framework does not explicitly restrict diversity of the conditioning data and also does not require an extensive inference procedure for training or adaptation. Our experiments on the Omniglot dataset demonstrate that Generative Matching Networks significantly improve predictive performance on the fly as more additional data is available and outperform existing state of the art conditional generative models.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300071,0
C,"Cao, Y; Xie, LY; Xie, Y; Xu, H",,"Storkey, A; PerezCruz, F",,"Cao, Yang; Xie, Liyan; Xie, Yao; Xu, Huan",,,Nearly second-order optimality of online joint detection and estimation via one-sample update schemes,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Sequential hypothesis test and change-point detection when the distribution parameters are unknown is a fundamental problem in statistics and machine learning. We show that for such problems, detection procedures based on sequential likelihood ratios with simple one-sample update estimates such as online mirror descent are nearly second-order optimal. This means that the upper bound for the algorithm performance meets the lower bound asymptotically up to a log-log factor in the false-alarm rate when it tends to zero. This is a blessing, since although the generalized likelihood ratio (GLR) statistics are optimal theoretically, but they cannot be computed recursively, and their exact computation usually requires infinite memory of historical data. We prove the nearly second-order optimality by making a connection between sequential change-point detection and online convex optimization and leveraging the logarithmic regret bound property of online mirror descent algorithm. Numerical examples validate our theory.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300055,0
C,"Clemencon, S; Portier, F",,"Storkey, A; PerezCruz, F",,"Clemencon, Stephan; Portier, Francois",,,Beating Monte Carlo Integration: a Nonasymptotic Study of Kernel Smoothing Methods,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Evaluating integrals is an ubiquitous issue and Monte Carlo methods, exploiting advances in random number generation over the last decades, offer a popular and powerful alternative to integration deterministic techniques, unsuited in particular when the domain of integration is complex. This paper is devoted to the study of a kernel smoothing based competitor built from a sequence of n >= 1 i.i.d random vectors with arbitrary continuous probability distribution f (x)dx, originally proposed in [7], from a nonasymptotic perspective. We establish a probability bound showing that the method under study, though biased, produces an estimate approximating the target integral integral(x is an element of Rd) phi(x)dx with an error bound of order o(1/root n) uniformly over a class Phi of functions phi, under weak complexity/smoothness assumptions related to the class Phi, outperforming Monte-Carlo procedures. This striking result is shown to derive from an appropriate decomposition of the maximal deviation between the target integrals and their estimates, highlighting the remarkable benefit to averaging strongly dependent terms regarding statistical accuracy in this situation. The theoretical analysis then rests on sharp probability inequalities for degenerate U-statistics. It is illustrated by numerical results in the context of covariate shift regression, providing empirical evidence of the relevance of the approach.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300058,0
C,"Genevay, A; Peyre, G; Cuturi, M",,"Storkey, A; PerezCruz, F",,"Genevay, Aude; Peyre, Gabriel; Cuturi, Marco",,,Learning Generative Models with Sinkhorn Divergences,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"The ability to compare two degenerate probability distributions, that is two distributions supported on low-dimensional manifolds in much higher-dimensional spaces, is a crucial factor in the estimation of generative models. It is therefore no surprise that optimal transport (OT) metrics and their ability to handle measures with non-overlapping supports have emerged as a promising tool. Yet, training generative machines using OT raises formidable computational and statistical challenges, because of (i) the computational burden of evaluating OT losses, (ii) their instability and lack of smoothness, (iii) the difficulty to estimate them, as well as their gradients, in high dimension. This paper presents the first tractable method to train large scale generative models using an OT-based loss called Sinkhorn loss which tackles these three issues by relying on two key ideas: (a) entropic smoothing, which turns the original OT loss into a differentiable and more robust quantity that can be computed using Sinkhorn fixed point iterations; (b) algorithmic (automatic) differentiation of these iterations with seamless GPU execution. Additionally, Entropic smoothing generates a family of losses interpolating between Wasserstein (OT) and Energy distance/Maximum Mean Discrepancy (MMD) losses, thus allowing to find a sweet spot leveraging the geometry of OT on the one hand, and the favorable high-dimensional sample complexity of MMD, which comes with unbiased gradient estimates.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300168,0
C,"Kasai, H; Sato, H; Mishra, B",,"Storkey, A; PerezCruz, F",,"Kasai, Hiroyuki; Sato, Hiroyuki; Mishra, Bamdev",,,Riemannian stochastic quasi-Newton algorithm with variance reduction and its convergence analysis,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Stochastic variance reduction algorithms have recently become popular for minimizing the average value of a large but finite number of loss functions. This paper proposes a Riemannian stochastic quasi-Newton algorithm with variance reduction (R-SQN-VR). We present convergence analyses of the R-SQN-VR on both non-convex and retraction strongly convex functions with retraction and vector transport. The proposed algorithm is tested on the Riemannian centroid computation on the symmetric positive-definite manifold and the low-rank matrix completion on the Grassmann manifold. In all cases, the proposed algorithm outperforms the state-of-the-art Riemannian batch and stochastic gradient algorithms.",,,,,"Sato, Hiroyuki/AAF-8944-2020","Sato, Hiroyuki/0000-0003-1399-8140; Mishra, Bamdev/0000-0001-7430-2843",,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300029,0
C,"Locatello, F; Khanna, R; Ghosh, J; Ratsch, G",,"Storkey, A; PerezCruz, F",,"Locatello, Francesco; Khanna, Rajiv; Ghosh, Joydeep; Ratsch, Gunnar",,,Boosting Variational Inference: an Optimization Perspective,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Variational inference is a popular technique to approximate a possibly intractable Bayesian posterior with a more tractable one. Recently, boosting variational inference [20, 4] has been proposed as a new paradigm to approximate the posterior by a mixture of densities by greedily adding components to the mixture. However, as is the case with many other variational inference algorithms, its theoretical properties have not been studied. In the present work, we study the convergence properties of this approach from a modern optimization viewpoint by establishing connections to the classic Frank-Wolfe algorithm. Our analyses yields novel theoretical insights regarding the sufficient conditions for convergence, explicit rates, and algorithmic simplifications. Since a lot of focus in previous works for variational inference has been on tractability, our work is especially important as a much needed attempt to bridge the gap between probabilistic models and their corresponding theoretical properties.",,,,,"Locatello, Francesco/GQY-6025-2022; Khanna, Rajiv/GPK-2566-2022","Khanna, Rajiv/0000-0003-1314-3126",,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300049,0
C,"Zhao, RB; Cevher, V",,"Storkey, A; PerezCruz, F",,"Zhao, Renbo; Cevher, Volkan",,,Stochastic Three-Composite Convex Minimization with a Linear Operator,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"We develop a primal-dual convex minimization framework to solve a class of stochastic convex three-composite problem with a linear operator. We consider the cases where the problem is both convex and strongly convex and analyze the convergence of the proposed algorithm in both cases. In addition, we extend the proposed framework to deal with additional constraint sets and multiple non-smooth terms. We provide numerical evidence on graph-guided sparse logistic regression, fused lasso and overlapped group lasso, to demonstrate the superiority of our approach to the state-of-the-art.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300081,0
C,"Guo, ZD; Doroudi, S; Brunskill, E",,"Gretton, A; Robert, CC",,"Guo, Zhaohan Daniel; Doroudi, Shayan; Brunskill, Emma",,,A PAC RL Algorithm for Episodic POMDPs,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Many interesting real world domains involve reinforcement learning (RL) in partially observable environments. Efficient learning in such domains is important, but existing sample complexity bounds for partially observable RL are at least exponential in the episode length. We give, to our knowledge, the first partially observable RL algorithm with a polynomial bound on the number of episodes on which the algorithm may not achieve near-optimal performance. Our algorithm is suitable for an important class of episodic POMDPs. Our approach builds on recent advances in method of moments for latent variable model estimation.",,,,,,"Doroudi, Shayan/0000-0002-0602-1406; Guo, Zhaohan/0000-0002-2497-6441",,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,510,518,,,,,,,,,,,,,,,,WOS:000508662100056,0
C,"van de Meent, JW; Paige, B; Tolpin, D; Wood, F",,"Gretton, A; Robert, CC",,"van de Meent, Jan-Willem; Paige, Brooks; Tolpin, David; Wood, Frank",,,Black-Box Policy Search with Probabilistic Programs,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"In this work we show how to represent policies as programs: that is, as stochastic simulators with tunable parameters. To learn the parameters of such policies we develop connections between black box variational inference and existing policy search approaches. We then explain how such learning can be implemented in a probabilistic programming system. Using our own novel implementation of such a system we demonstrate both conciseness of policy representation and automatic policy parameter learning for a set of canonical reinforcement learning problems.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,1195,1204,,,,,,,,,,,,,,,,WOS:000508662100130,0
C,"Wang, Z; Zhou, BL; Jegelka, S",,"Gretton, A; Robert, CC",,"Wang, Zi; Zhou, Bolei; Jegelka, Stefanie",,,Optimization as Estimation with Gaussian Processes in Bandit Settings,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Recently, there has been rising interest in Bayesian optimization - the optimization of an unknown function with assumptions usually expressed by a Gaussian Process (GP) prior. We study an optimization strategy that directly uses an estimate of the argmax of the function. This strategy offers both practical and theoretical advantages: no tradeoff parameter needs to be selected, and, moreover, we establish close connections to the popular GP-UCB and GP-PI strategies. Our approach can be understood as automatically and adaptively trading off exploration and exploitation in GP-UCB and GP-PI. We illustrate the effects of this adaptive tuning via bounds on the regret as well as an extensive empirical evaluation on robotics and vision tasks, demonstrating the robustness of this strategy for a range of performance criteria.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,1022,1031,,,,,,,,,,,,,,,,WOS:000508662100111,0
C,"Balcan, MF; Berlind, C",,"Kaski, S; Corander, J",,"Balcan, Maria-Florina; Berlind, Christopher",,,A New Perspective on Learning Linear Separators with Large LqLp Margins,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,We give theoretical and empirical results that provide new insights into large margin learning. We prove a bound on the generalization error of learning linear separators with large LqLp margins (where L-q and L-p are dual norms) for any finite p >= 1. The bound leads to a simple data-dependent sufficient condition for fast learning in addition to extending and improving upon previous results. We also provide the first study that shows the benefits of taking advantage of margins with p < 2 over margins with p >= 2. Our experiments confirm that our theoretical results are relevant in practice.,,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,68,76,,,,,,,,,,,,,,,,WOS:000508355800008,0
C,"Bargi, A; Xu, RYD; Ghahramani, Z; Piccardi, M",,"Kaski, S; Corander, J",,"Bargi, Ava; Xu, Richard Yi Da; Ghahramani, Zoubin; Piccardi, Massimo",,,A Non-parametric Conditional Factor Regression Model for Multi-Dimensional Input and Response,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"In this paper, we propose a non-parametric conditional factor regression (NCFR) model for domains with multi-dimensional input and response. NCFR enhances linear regression in two ways: a) introducing low-dimensional latent factors leading to dimensionality reduction and b) integrating the Indian Buffet Process as prior for the latent layer to dynamically derive an optimal number of sparse factors. Thanks to IBP's enhancements to the latent factors, NCFR can significantly avoid over-fitting even in the case of a very small sample size compared to the dimensionality. Experimental results on three diverse datasets comparing NCRF to a few baseline alternatives give evidence of its robust learning, remarkable predictive performance, good mixing and computational efficiency.",,,,,,"Piccardi, Massimo/0000-0001-9250-6604; Xu, Richard Yi Da/0000-0003-2080-4762",,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,77,85,,,,,,,,,,,,,,,,WOS:000508355800009,0
C,"Hennig, P; Hauberg, S",,"Kaski, S; Corander, J",,"Hennig, Philipp; Hauberg, Soren",,,Probabilistic Solutions to Differential Equations and their Application to Riemannian Statistics,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"We study a probabilistic numerical method for the solution of both boundary and initial value problems that returns a joint Gaussian process posterior over the solution. Such methods have concrete value in the statistics on Riemannian manifolds, where non-analytic ordinary differential equations are involved in virtually all computations. The probabilistic formulation permits marginalising the uncertainty of the numerical solution such that statistics are less sensitive to inaccuracies. This leads to new Riemannian algorithms for mean value computations and principal geodesic analysis. Marginalisation also means results can be less precise than point estimates, enabling a noticeable speed-up over the state of the art. Our approach is an argument for a wider point that uncertainty caused by numerical calculations should be tracked throughout the pipeline of machine learning algorithms.",,,,,"Hauberg, Soren/L-2104-2016","Hauberg, Soren/0000-0001-7223-877X",,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,347,355,,,,,,,,,,,,,,,,WOS:000508355800039,0
C,"Khan, ME; Ko, YJ; Seeger, M",,"Kaski, S; Corander, J",,"Khan, Mohammad Emtiyaz; Ko, Young Jun; Seeger, Matthias",,,Scalable Collaborative Bayesian Preference Learning,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"Learning about users' utilities from preference, discrete choice or implicit feedback data is of integral importance in e-commerce, targeted advertising and web search. Due to the sparsity and diffuse nature of data, Bayesian approaches hold much promise, yet most prior work does not scale up to realistic data sizes. We shed light on why inference for such settings is computationally difficult for standard machine learning methods, most of which focus on predicting explicit ratings only. To simplify the difficulty, we present a novel expectation maximization algorithm, driven by expectation propagation approximate inference, which scales to very large datasets without requiring strong factorization assumptions. Our utility model uses both latent bilinear collaborative filtering and non-parametric Gaussian process (GP) regression. In experiments on large real-world datasets, our method gives substantially better results than either matrix factorization or GPs in isolation, and converges significantly faster.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,475,483,,,,,,,,,,,,,,,,WOS:000508355800053,0
C,"Qian, J; Saligrama, V; Chen, YT",,"Kaski, S; Corander, J",,"Qian, Jing; Saligrama, Venkatesh; Chen, Yuting",,,Connected Sub-graph Detection,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"We characterize the family of connected subgraphs in terms of linear matrix inequalities (LMI) with additional integrality constraints. We then show that convex relaxations of the integral LMI lead to parameterization of all weighted connected subgraphs. These developments allow for optimizing arbitrary graph functionals under connectivity constraints. For concreteness we consider the connected sub-graph detection problem that arises in a number of applications including network intrusion, disease outbreaks, and video surveillance. In these applications feature vectors are associated with nodes and edges of a graph. The problem is to decide whether or not the null hypothesis is true based on the measured features. For simplicity we consider the elevated mean problem wherein feature values at various nodes are distributed IID under the null hypothesis. The non-null (positive) hypothesis is distinguished from the null hypothesis by the fact that feature values on some unknown connected sub-graph has elevated mean.",,,,,,"Saligrama, Venkatesh/0000-0002-0675-2268",,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,796,804,,,,,,,,,,,,,,,,WOS:000508355800088,0
C,"Rasiwasia, N; Mahajan, D; Mahadevan, V; Aggarwal, G",,"Kaski, S; Corander, J",,"Rasiwasia, Nikhil; Mahajan, Dhruv; Mahadevan, Vijay; Aggarwal, Gaurav",,,Cluster Canonical Correlation Analysis,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"In this paper we present cluster canonical correlation analysis (cluster-CCA) for joint dimensionality reduction of two sets of data points. Unlike the standard pairwise correspondence between the data points, in our problem each set is partitioned into multiple clusters or classes, where the class labels define correspondences between the sets. Cluster-CCA is able to learn discriminant low dimensional representations that maximize the correlation between the two sets while segregating the different classes on the learned space. Furthermore, we present a kernel extension, kernel cluster canonical correlation analysis (cluster-KCCA) that extends cluster-CCA to account for non-linear relationships. Cluster-(K)CCA is shown to be computationally efficient, the complexity being similar to standard (K)CCA. By means of experimental evaluation on benchmark datasets, cluster-(K)CCA is shown to achieve state of the art performance for cross-modal retrieval tasks.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,823,831,,,,,,,,,,,,,,,,WOS:000508355800091,0
C,"Solin, A; Sarkka, S",,"Kaski, S; Corander, J",,"Solin, Arno; Sarkka, Simo",,,Explicit Link Between Periodic Covariance Functions and State Space Models,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"This paper shows how periodic covariance functions in Gaussian process regression can be reformulated as state space models, which can be solved with classical Kalman filtering theory. This reduces the problematic cubic complexity of Gaussian process regression in the number of time steps into linear time complexity. The representation is based on expanding periodic covariance functions into a series of stochastic resonators. The explicit representation of the canonical periodic covariance function is written out and the expansion is shown to uniformly converge to the exact covariance function with a known convergence rate. The framework is generalized to quasi-periodic covariance functions by introducing damping terms in the system and applied to two sets of real data. The approach could be easily extended to non-stationary and spatio-temporal variants.",,,,,"Solin, Arno/G-6859-2012","Solin, Arno/0000-0002-0958-7886",,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,904,912,,,,,,,,,,,,,,,,WOS:000508355800100,0
C,"Borovitskiy, V; Mostowsky, P; Azangulov, I; Deisenroth, MP; Terenin, A; Durrande, N",,"Banerjee, A; Fukumizu, K",,"Borovitskiy, Viacheslav; Mostowsky, Peter; Azangulov, Iskander; Deisenroth, Marc Peter; Terenin, Alexander; Durrande, Nicolas",,,Matern Gaussian Processes on Graphs,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Gaussian processes are a versatile framework for learning unknown functions in a manner that permits one to utilize prior information about their properties. Although many different Gaussian process models are readily available when the input space is Euclidean, the choice is much more limited for Gaussian processes whose input space is an undirected graph. In this work, we leverage the stochastic partial differential equation characterization of Matern Gaussian processes-a widely-used model class in the Euclidean setting-to study their analog for undirected graphs. We show that the resulting Gaussian processes inherit various attractive properties of their Euclidean and Riemannian analogs and provide techniques that allow them to be trained using standard methods, such as inducing points. This enables graph Matern Gaussian processes to be employed in mini-batch and non-conjugate settings, thereby making them more accessible to practitioners and easier to deploy within larger learning frameworks.",,,,,"; Borovitskiy, Viacheslav/R-7216-2017","Mostowsky, Peter/0000-0002-1961-4155; Borovitskiy, Viacheslav/0000-0002-3539-333X",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803015,0
C,"Chewi, S; Clancy, J; Le Gouic, T; Rigollet, P; Stepaniants, G; Stromme, AJ",,"Banerjee, A; Fukumizu, K",,"Chewi, Sinho; Clancy, Julien; Le Gouic, Thibaut; Rigollet, Philippe; Stepaniants, George; Stromme, Austin J.",,,Fast and Smooth Interpolation on Wasserstein Space,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We propose a new method for smoothly interpolating probability measures using the geometry of optimal transport. To that end, we reduce this problem to the classical Euclidean setting, allowing us to directly leverage the extensive toolbox of spline interpolation. Unlike previous approaches to measurevalued splines, our interpolated curves (i) have a clear interpretation as governing particle flows, which is natural for applications, and (ii) come with the first approximation guarantees on Wasserstein space. Finally, we demonstrate the broad applicability of our interpolation methodology by fitting surfaces of measures using thin-plate splines.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803067,0
C,"Demirel, I; Tekin, C",,"Banerjee, A; Fukumizu, K",,"Demirel, Ilker; Tekin, Cem",,,Combinatorial Gaussian Process Bandits with Probabilistically Triggered Arms,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Combinatorial bandit models and algorithms are used in many sequential decision-making tasks ranging from item list recommendation to influence maximization. Typical algorithms proposed for combinatorial bandits, including combinatorial UCB (CUCB) and combinatorial Thompson sampling (CTS) do not exploit correlations between base arms during the learning process. Moreover, their regret is usually analyzed under independent base arm outcomes. In this paper, we use Gaussian Processes (GPs) to model correlations between base arms. In particular, we consider a combinatorial bandit model with probabilistically triggered arms, and assume that the expected base arm outcome function is a sample from a GP. We assume that the learner has access to an exact computation oracle, which returns an optimal solution given expected base arm outcomes, and analyze the regret of Combinatorial Gaussian Process Upper Confidence Bound (ComGP-UCB) algorithm for this setting. Under (triggering probability modulated) Lipschitz continuity assumption on the expected reward function, we derive (O(root mTlogT gamma(PTA)(T,mu))) O (m root T log T/p*) upper bounds for the regret of ComGP-UCB that hold with high probability, where m denotes the number of base arms, p* denotes the minimum non-zero triggering probability, and gamma(PTA)(T,mu) denotes the pseudo-information gain. Finally, we show via simulations that when the correlations between base arm outcomes are strong, ComGP-UCB significantly outperforms CUCB and CTS.",,,,,,"Demirel, Ilker/0000-0003-1035-8500",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804062,0
C,"Ding, MC; Daskalakis, C; Feizi, S",,"Banerjee, A; Fukumizu, K",,"Ding, Mucong; Daskalakis, Constantinos; Feizi, Soheil",,,GANs with Conditional Independence Graphs: On Subadditivity of Probability Divergences,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Generative Adversarial Networks (GANs) are modern methods to learn the underlying distribution of a data set. GANs have been widely used in sample synthesis, de-noising, domain transfer, etc. GANs, however, are designed in a model-free fashion where no additional information about the underlying distribution is available. In many applications, however, practitioners have access to the underlying independence graph of the variables, either as a Bayesian network or a Markov Random Field (MRF). We ask: how can one use this additional information in designing model-based GANs? In this paper, we provide theoretical foundations to answer this question by studying subadditivity properties of probability divergences, which establish upper bounds on the distance between two high-dimensional distributions by the sum of distances between their marginals over (local) neighborhoods of the graphical structure of the Bayes-net or the MRF. We prove that several popular probability divergences satisfy some notion of subadditivity under mild conditions. These results lead to a principled design of a model-based GAN that uses a set of simple discriminators on the neighborhoods of the Bayes-net/MRF, rather than a giant discriminator on the entire network, providing significant statistical and computational benefits. Our experiments on synthetic and real-world datasets demonstrate the benefits of our principled design of model-based GANs.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804047,0
C,"Doan, T; Bennani, M; Mazoure, B; Rabusseau, G; Alquier, P",,"Banerjee, A; Fukumizu, K",,"Doan, Thang; Bennani, Mehdi; Mazoure, Bogdan; Rabusseau, Guillaume; Alquier, Pierre",,,A Theoretical Analysis of Catastrophic Forgetting through the NTK Overlap Matrix,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Continual learning (CL) is a setting in which an agent has to learn from an incoming stream of data during its entire lifetime. Although major advances have been made in the field, one recurring problem which remains unsolved is that of Catastrophic Forgetting (CF). While the issue has been extensively studied empirically, little attention has been paid from a theoretical angle. In this paper, we show that the impact of CF increases as two tasks increasingly align. We introduce a measure of task similarity called the NTK overlap matrix which is at the core of CF. We analyze common projected gradient algorithms and demonstrate how they mitigate forgetting. Then, we propose a variant of Orthogonal Gradient Descent (OGD) which leverages structure of the data through Principal Component Analysis (PCA). Experiments support our theoretical findings and show how our method can help reduce CF on classical CL datasets.",,,,,,"Alquier, Pierre/0000-0003-4249-7337",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801033,0
C,"Jazbec, M; Ashman, M; Fortuin, V; Pearce, M; Mandt, S; Ratsch, G",,"Banerjee, A; Fukumizu, K",,"Jazbec, Metod; Ashman, Matthew; Fortuin, Vincent; Pearce, Michael; Mandt, Stephan; Raetsch, Gunnar",,,Scalable Gaussian Process Variational Autoencoders,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Conventional variational autoencoders fail in modeling correlations between data points due to their use of factorized priors. Amortized Gaussian process inference through GPVAEs has led to significant improvements in this regard, but is still inhibited by the intrinsic complexity of exact GP inference. We improve the scalability of these methods through principled sparse inference approaches. We propose a new scalable GPVAE model that outperforms existing approaches in terms of runtime and memory footprint, is easy to implement, and allows for joint end-to-end optimization of all components.",,,,,"Fortuin, Vincent/GXZ-5395-2022","Fortuin, Vincent/0000-0002-0640-2671",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804025,0
C,"Liu, FH; Huang, XL; Chen, YY; Suykens, JAK",,"Banerjee, A; Fukumizu, K",,"Liu, Fanghui; Huang, Xiaolin; Chen, Yingyi; Suykens, Johan A. K.",,,Fast learning in reproducing kernel Krein spaces via signed measures,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"In this paper, we attempt to solve a long-lasting open question for non-positive definite (non-PD) kernels in machine learning community: can a given non-PD kernel be decomposed into the difference of two PD kernels (termed as positive decomposition)? We cast this question as a distribution view by introducing the signed measure, which transforms positive decomposition to measure decomposition: a series of non-PD kernels can be associated with the linear combination of specific finite Borel measures. In this manner, our distribution-based framework provides a sufficient and necessary condition to answer this open question. Specifically, this solution is also computationally implementable in practice to scale non-PD kernels in large sample cases, which allows us to devise the first random features algorithm to obtain an unbiased estimator. Experimental results on several benchmark datasets verify the effectiveness of our algorithm over the existing methods.",,,,,"; Suykens, Johan/C-9781-2014","Chen, Yingyi/0000-0002-5571-9050; Suykens, Johan/0000-0002-8846-6352",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,388,+,,,,,,,,,,,,,,,,WOS:000659893800044,0
C,"Russac, Y; Faury, L; Cappe, O; Garivier, A",,"Banerjee, A; Fukumizu, K",,"Russac, Yoan; Faury, Louis; Cappe, Olivier; Garivier, Aurelien",,,Self-Concordant Analysis of Generalized Linear Bandits with Forgetting,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Contextual sequential decision problems with categorical or numerical observations are ubiquitous and Generalized Linear Bandits (GLB) offer a solid theoretical framework to address them. In contrast to the case of linear bandits, existing algorithms for GLB have two drawbacks undermining their applicability. First, they rely on excessively pessimistic concentration bounds due to the non-linear nature of the model. Second, they require either non-convex projection steps or burn-in phases to enforce boundedness of the estimators. Both of these issues are worsened when considering non-stationary models, in which the GLB parameter may vary with time. In this work, we focus on self-concordant GLB (which include logistic and Poisson regression) with forgetting achieved either by the use of a sliding window or exponential weights. We propose a novel confidence-based algorithm for the maximum-likehood estimator with forgetting and analyze its perfomance in abruptly changing environments. These results as well as the accompanying numerical simulations highlight the potential of the proposed approach to address non-stationarity in GLB.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,658,+,,,,,,,,,,,,,,,,WOS:000659893800074,0
C,"Takai, Y; Sannai, A; Cordonnier, M",,"Banerjee, A; Fukumizu, K",,"Takai, Yuuki; Sannai, Akiyoshi; Cordonnier, Matthieu",,,On the Number of Linear Functions Composing Deep Neural Network: Towards a Refined Definition of Neural Networks Complexity,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"The classical approach to measure the expressive power of deep neural networks with piecewise linear activations is based on counting their maximum number of linear regions. This complexity measure is quite relevant to understand general properties of the expressivity of neural networks such as the benefit of depth over width. Nevertheless, it appears limited when it comes to comparing the expressivity of different network architectures. This lack becomes particularly prominent when considering permutation-invariant networks, due to the symmetrical redundancy among the linear regions. To tackle this, we propose a refined definition of piecewise linear function complexity: instead of counting the number of linear regions directly, we first introduce an equivalence relation among the linear functions composing a piecewise linear function and then count those linear functions relative to that equivalence relation. Our new complexity measure can clearly distinguish between the two aforementioned models, is consistent with the classical measure, and increases exponentially with depth.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804057,0
C,"Vankadara, LC; Bordt, S; von Luxburg, U; Ghoshdastidar, D",,"Banerjee, A; Fukumizu, K",,"Vankadara, Leena C.; Bordt, Sebastian; von Luxburg, Ulrike; Ghoshdastidar, Debarghya",,,Recovery Guarantees for Kernel-based Clustering under Non-parametric Mixture Models,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Despite the ubiquity of kernel-based clustering, surprisingly few statistical guarantees exist beyond settings that consider strong structural assumptions on the data generation process. In this work, we take a step towards bridging this gap by studying the statistical performance of kernel-based clustering algorithms under non-parametric mixture models. We provide necessary and sufficient separability conditions under which these algorithms can consistently recover the underlying true clustering. Our analysis provides guarantees for kernel clustering approaches without structural assumptions on the form of the component distributions. Additionally, we establish a key equivalence between kernel-based dataclustering and kernel density-based clustering. This enables us to provide consistency guarantees for kernel-based estimators of nonparametric mixture models. Along with theoretical implications, this connection could have practical implications, including in the systematic choice of the bandwidth of the Gaussian kernel in the context of clustering.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804059,0
C,"Wang, Z; Xing, W; Kirby, RM; Zhe, S",,"Banerjee, A; Fukumizu, K",,"Wang, Zheng; Xing, Wei; Kirby, Robert M.; Zhe, Shandian",,,Multi-Fidelity High-Order Gaussian Processes for Physical Simulation,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"The key task of physical simulation is to solve partial differential equations (PDEs) on discretized domains, which is known to be costly. In particular, high-fidelity solutions are much more expensive than low-fidelity ones. To reduce the cost, we consider novel Gaussian process (GP) models that leverage simulation examples of different fidelities to predict high-dimensional PDE solution outputs. Existing GP methods are either not scalable to high-dimensional outputs or lack effective strategies to integrate multi-fidelity examples. To address these issues, we propose Multi-Fidelity High-Order Gaussian Process (MFHoGP) that can capture complex correlations both between the outputs and between the fidelities to enhance solution estimation, and scale to large numbers of outputs. Based on a novel nonlinear coregionalization model, MFHoGP propagates bases throughout fidelities to fuse information, and places a deep matrix GP prior over the basis weights to capture the (nonlinear) relationships across the fidelities. To improve inference efficiency and quality, we use bases decomposition to largely reduce the model parameters, and layer-wise matrix Gaussian posteriors to capture the posterior dependency and to simplify the computation. Our stochastic variational learning algorithm successfully handles millions of outputs without extra sparse approximations. We show the advantages of our method in several typical applications.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801008,0
C,"Zhou, YL; Renduchintala, A; Li, X; Wang, SD; Mehdad, Y; Ghoshal, A",,"Banerjee, A; Fukumizu, K",,"Zhou, Yilun; Renduchintala, Adithya; Li, Xian; Wang, Sida; Mehdad, Yashar; Ghoshal, Asish",,,Towards Understanding the Behaviors of Optimal Deep Active Learning Algorithms,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Active learning (AL) algorithms may achieve better performance with fewer data because the model guides the data selection process. While many algorithms have been proposed, there is little study on what the optimal AL algorithm looks like, which would help researchers understand where their models fall short and iterate on the design. In this paper, we present a simulated annealing algorithm to search for this optimal oracle and analyze it for several tasks. We present qualitative and quantitative insights into the behaviors of this oracle, comparing and contrasting them with those of various heuristics. Moreover, we are able to consistently improve the heuristics using one particular insight. We hope that our findings can better inform future active learning research. The code is available at https://github.com/ YilunZhou/optimal-active-learning.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801079,0
C,"Bong, HJ; Li, WS; Shrotriya, S; Rinaldo, A",,"Chiappa, S; Calandra, R",,"Bong, Heejong; Li, Wanshan; Shrotriya, Shamindra; Rinaldo, Alessandro",,,Nonparametric Estimation in the Dynamic Bradley-Terry Model,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,We propose a time-varying generalization of the Bradley-Terry model that allows for non-parametric modeling of dynamic global rankings of distinct teams. We develop a novel estimator that relies on kernel smoothing to preprocess the pairwise comparisons over time and is applicable in sparse settings where the Bradley-Terry may not be fit. We obtain necessary and sufficient conditions for the existence and uniqueness of our estimator. We also derive time-varying oracle bounds for both the estimation error and the excess risk in the model-agnostic setting where the Bradley-Terry model is not necessarily the true data generating process. We thoroughly test the practical effectiveness of our model using both simulated and real world data and suggest an efficient data-driven approach for bandwidth tuning.,,,,,"Bong, Heejong/HDM-9793-2022","Bong, Heejong/0000-0002-3906-2175",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3317,3325,,,,,,,,,,,,,,,,WOS:000559931300036,0
C,"Hsu, CCY; Hardt, M; Hardt, M",,"Chiappa, S; Calandra, R",,"Hsu, Chloe Ching-Yun; Hardt, Michaela; Hardt, Moritz",,,Linear Dynamics: Clustering without identification,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Linear dynamical systems are a fundamental and powerful parametric model class. However, identifying the parameters of a linear dynamical system is a venerable task, permitting provably efficient solutions only in special cases. This work shows that the eigen-spectrum of unknown linear dynamics can be identified without full system identification. We analyze a computationally efficient and provably convergent algorithm to estimate the eigenvalues of the state-transition matrix in a linear dynamical system. When applied to time series clustering, our algorithm can efficiently cluster multidimensional time series with temporal offsets and varying lengths, under the assumption that the time series are generated from linear dynamical systems. Evaluating our algorithm on both synthetic data and real electrocardiogram (ECG) signals, we see improvements in clustering quality over existing baselines.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,918,928,,,,,,,,,,,,,,,,WOS:000559931301043,0
C,"Ji, DJ; Lu, JW; Zhang, YL; Gao, SY; Zhao, HY",,"Chiappa, S; Calandra, R",,"Ji, Dingjue; Lu, Junwei; Zhang, Yiliang; Gao, Siyuan; Zhao, Hongyu",,,Inference of Dynamic Graph Changes for Functional Connectome,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Dynamic functional connectivity is an effective measure for the brain's responses to continuous stimuli. We propose an inferential method to detect the dynamic changes of brain networks based on time-varying graphical models. Whereas most existing methods focus on testing the existence of change points, the dynamics in the brain network offer more signals in many neuroscience studies. We propose a novel method to conduct hypothesis testing on changes in dynamic brain networks. We introduce a bootstrap statistic to approximate the supreme of the high-dimensional empirical processes over dynamically changing edges. Our simulations show that this framework can capture the change points with changed connectivity. Finally, we apply our method to a brain imaging dataset under a natural audio-video stimulus and illustrate that we are able to detect temporal changes in brain networks. The functions of the identified regions are consistent with specific emotional annotations, which are closely associated with changes inferred by our method.",,,,,"Gao, Siyuan/AAA-1650-2022","Gao, Siyuan/0000-0002-8925-7042",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3230,3239,,,,,,,,,,,,,,,,WOS:000559931301061,0
C,"Mroueh, Y",,"Chiappa, S; Calandra, R",,"Mroueh, Youssef",,,Wasserstein Style Transfer,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We propose Gaussian optimal transport for image style transfer in an Encoder/Decoder framework. Optimal transport for Gaussian measures has closed forms Monge mappings from source to target distributions. Moreover, interpolating between a content and a style image can be seen as geodesics in the Wasserstein Geometry. Using this insight, we show how to mix different target styles, using Wasserstein barycenter of Gaussian measures. Since Gaussians are closed under Wasserstein barycenter, this allows us a simple style transfer and style mixing and interpolation. Moreover we show how mixing different styles can be achieved using other geodesic metrics between gaussians such as the Fisher Rao metric, while the transport of the content to the new interpolate style is still performed with Gaussian OT maps. Our simple methodology allows to generate new stylized content interpolating between many artistic styles. The metric used in the interpolation results in different stylizations. A demo is available on https: //wasserstein-transfer.github.ic.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,842,851,,,,,,,,,,,,,,,,WOS:000559931302051,0
C,"Saad, FA; Freer, CE; Rinard, MC; Mansinghka, VK",,"Chiappa, S; Calandra, R",,"Saad, Feras A.; Freer, Cameron E.; Rinard, Martin C.; Mansinghka, Vikash K.",,,The Fast Loaded Dice Roller: A Near-Optimal Exact Sampler for Discrete Probability Distributions,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"This paper introduces a new algorithm for the fundamental problem of generating a random integer from a discrete probability distribution using a source of independent and unbiased random coin flips. We prove that this algorithm, which we call the Fast Loaded Dice Roller (FLDR), is highly efficient in both space and time: (i) the size of the sampler is guaranteed to be linear in the number of bits needed to encode the input distribution; and (ii) the expected number of bits of entropy it consumes per sample is at most 6 bits more than the information-theoretically optimal rate. We present fast implementations of the linear-time preprocessing and near-optimal sampling algorithms using unsigned integer arithmetic. Empirical evaluations on a broad set of probability distributions establish that FLDR is 2x-10x faster in both preprocessing and sampling than multiple baseline algorithms, including the widely-used alias and interval samplers. It also uses up to 10000x less space than the information-theoretically optimal sampler, at the expense of less than 1.5x runtime overhead.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1036,1045,,,,,,,,,,,,,,,,WOS:000559931302098,0
C,"Sakaue, S",,"Chiappa, S; Calandra, R",,"Sakaue, Shinsaku",,,Guarantees of Stochastic Greedy Algorithms for Non-monotone Submodular Maximization with Cardinality Constraint,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Submodular maximization with a cardinality constraint can model various problems, and those problems are often very large in practice. For the case where objective functions are monotone, many fast approximation algorithms have been developed. The stochastic greedy algorithm (SG) is one such algorithm, which is widely used thanks to its simplicity, efficiency, and high empirical performance. However, its approximation guarantee has been proved only for monotone objective functions. When it comes to non-monotone objective functions, existing approximation algorithms are inefficient relative to the fast algorithms developed for the case of monotone objectives. In this paper, we prove that SG (with slight modification) can achieve almost 1/4-approximation guarantees in expectation in linear time even if objective functions are non-monotone. Our result provides a constant-factor approximation algorithm with the fewest oracle queries for non-monotone submodular maximization with a cardinality constraint. Experiments validate the performance of (modified) SG.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303001,0
C,"Zhang, Y; Jarrett, D; van der Schaar, M",,"Chiappa, S; Calandra, R",,"Zhang, Yao; Jarrett, Daniel; van der Schaar, Mihaela",,,Stepwise Model Selection for Sequence Prediction via Deep Kernel Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"An essential problem in automated machine learning (AutoML) is that of model selection. A unique challenge in the sequential setting is the fact that the optimal model itself may vary over time, depending on the distribution of features and labels available up to each point in time. In this paper, we propose a novel Bayesian optimization (BO) algorithm to tackle the challenge of model selection in this setting. This is accomplished by treating the performance at each time step as its own black-box function. In order to solve the resulting multiple black-box function optimization problem jointly and efficiently, we exploit potential correlations among black-box functions using deep kernel learning (DKL). To the best of our knowledge, we are the first to formulate the problem of stepwise model selection (SMS) for sequence prediction, and to design and demonstrate an efficient joint learning algorithm for this purpose. Using multiple real-world datasets, we verify that our proposed method outperforms both standard BO and multi-objective BO algorithms on a variety of sequence prediction tasks.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2304,2313,,,,,,,,,,,,,,,,WOS:000559931304012,0
C,"Aglietti, V; Damoulas, T; Bonilla, EV",,"Chaudhuri, K; Sugiyama, M",,"Aglietti, Virginia; Damoulas, Theodoros; Bonilla, Edwin, V",,,Efficient Inference in Multi-task Cox Process Models,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We generalize the log Gaussian Cox process (LGCP) framework to model multiple correlated point data jointly. The observations are treated as realizations of multiple LGGPs, whose log intensities are given by linear combinations of latent functions drawn from Gaussian process priors. The combination coefficients are also drawn from Gaussian processes and can incorporate additional dependencies. We derive closed-form expressions for the moments of the intensity functions and develop an efficient variational inference algorithm that is orders of magnitude faster than competing deterministic and stochastic approximations of multivariate LGGPs, coregionalization models, and multi-task permanental processes. Our approach outperforms these benchmarks in multiple problems, offering the current state of the art in modeling multivariate point processes.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,537,546,,,,,,,,,,,,,,,,WOS:000509687900056,0
C,"Andrade, D; Okajima, Y",,"Chaudhuri, K; Sugiyama, M",,"Andrade, Daniel; Okajima, Yuzuru",,,Efficient Bayes Risk Estimation for Cost-Sensitive Classification,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"In some real world applications, acquiring covariates for classification can be cost-intensive and should be limited as much as possible. For example, in the medical setting, a doctor cannot just perform all possible types of tests to classify whether the patient has diabetes or not. The decision of classifying or acquiring more covariates before classifying is dependent on the costs of new covariates and the expected optimal cost of misclassification (Bayes risk). However, estimating the latter is a formidable task due to the estimation of a high dimensional probability density and intractable integrals. In this work, we show that for linear classifiers this task can be considerably simplified, leading to a one dimensional integral for which we propose an efficient approximation. Experimental results on three datasets show consistent improvements over previously proposed methods for cost-sensitive classification. We also demonstrate that our proposed Bayes risk estimation procedure can benefit from additional unlabeled data which can be helpful when only small amount of labeled data is available.",,,,,"Andrade, Daniel/AAD-3721-2019","Andrade, Daniel/0000-0002-1123-4369",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687903044,0
C,"Charikar, M; Chatziafratis, V; Niazadeh, R; Yaroslavtsev, G",,"Chaudhuri, K; Sugiyama, M",,"Charikar, Moses; Chatziafratis, Vaggos; Niazadeh, Rad; Yaroslavtsev, Grigory",,,Hierarchical Clustering for Euclidean Data,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Recent works on Hierarchical Clustering (HC), a well-studied problem in exploratory data analysis, have focused on optimizing various objective functions for this problem under arbitrary similarity measures. In this paper we take the first step and give novel scalable algorithms for this problem tailored to Euclidean data in R-d and under vector-based similarity measures, a prevalent model in several typical machine learning applications. We focus primarily on the popular Gaussian kernel and other related measures, presenting our results through the lens of the objective introduced recently by [MW17]. We show that the approximation factor in [MW17] can be improved for Euclidean data. We further demonstrate both theoretically and experimentally that our algorithms scale to very high dimension d, while outperforming average-linkage and showing competitive results against other less scalable approaches.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902079,0
C,"Choromanski, K; Pacchiano, A; Pennington, J; Tang, YH",,"Chaudhuri, K; Sugiyama, M",,"Choromanski, Krzysztof; Pacchiano, Aldo; Pennington, Jeffrey; Tang, Yunhao",,,KAMA-NNs: low-dimensional rotation based neural networks,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We present new architectures for feedforward neural networks built from products of learned or random low-dimensional rotations that offer substantial space compression and computational speedups in comparison to the unstructured baselines. Models using them are also competitive with the baselines and often, due to imposed orthogonal structure, outperform baselines accuracy-wise. We propose to use our architectures in two settings. We show that in the non-adaptive scenario (random neural networks) they lead to asymptotically more accurate, space-efficient and faster estimators of the so-called PNG-kernels (for any activation function defining the PNG). This generalizes several recent theoretical results about orthogonal estimators (e.g. orthogonal JLTs, orthogonal estimators of angular kernels and more). In the adaptive setting we propose efficient algorithms for learning products of low-dimensional rotations and show how our architectures can be used to improve space and time complexity of state of the art reinforcement learning (RL) algorithms (e.g. PPO, TRPO). Here they offer up to 7x compression of the network in comparison to the unstructured baselines and outperform reward-wise state of the art structured neural networks offering similar computational gains and based on low displacement rank matrices.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,236,245,,,,,,,,,,,,,,,,WOS:000509687900025,0
C,"Czarnecki, WM; Pascanu, R; Osindero, S; Jayakumar, SM; Swirszcz, G; Jaderberg, M",,"Chaudhuri, K; Sugiyama, M",,"Czarnecki, Wojciech M.; Pascanu, Razvan; Osindero, Simon; Jayakumar, Siddhant M.; Swirszcz, Grzegorz; Jaderberg, Max",,,Distilling Policy Distillation,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"The transfer of knowledge from one policy to another is an important tool in Deep Reinforcement Learning. This process, referred to as distillation, has been used to great success, for example, by enhancing the optimisation of agents, leading to stronger performance faster, on harder domains [26, 32, 5, 8]. Despite the widespread use and conceptual simplicity of distillation, many different formulations are used in practice, and the subtle variations between them can often drastically change the performance and the resulting objective that is being optimised. In this work, we rigorously explore the entire landscape of policy distillation, comparing the motivations and strengths of each variant through theoretical and empirical analysis. Our results point to three distillation techniques, that are preferred depending on specifics of the task. Specifically a newly proposed expected entropy regularised distillation allows for quicker learning in a wide range of situations, while still guaranteeing convergence.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901039,0
C,"de Roos, F; Hennig, P",,"Chaudhuri, K; Sugiyama, M",,"de Roos, Filip; Hennig, Philipp",,,Active Probabilistic Inference on Matrices for Pre-Conditioning in Stochastic Optimization,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Pre-conditioning is a well-known concept that can significantly improve the convergence of optimization algorithms. For noise-free problems, where good pre-conditioners are not known a priori, iterative linear algebra methods offer one way to efficiently construct them. For the stochastic optimization problems that dominate contemporary machine learning, however, this approach is not readily available. We propose an iterative algorithm inspired by classic iterative linear solvers that uses a probabilistic model to actively infer a pre-conditioner in situations where Hessian-projections can only be constructed with strong Gaussian noise. The algorithm is empirically demonstrated to efficiently construct effective pre-conditioners for stochastic gradient descent and its variants. Experiments on problems of comparably low dimensionality show improved convergence. In very high-dimensional problems, such as those encountered in deep learning, the pre-conditioner effectively becomes an automatic learning-rate adaptation scheme, which we also empirically show to work well.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901051,0
C,"Du, SS; Hu, W",,"Chaudhuri, K; Sugiyama, M",,"Du, Simon S.; Hu, Wei",,,Linear Convergence of the Primal-Dual Gradient Method for Convex-Concave Saddle Point Problems without Strong Convexity,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We consider the convex-concave saddle point problem min(x) max(y) f (x)-1y(inverted perpendicular)Ax-g(y) where f is smooth and convex and g is smooth and strongly convex. We prove that if the coupling matrix A has full column rank, the vanilla primal-dual gradient method can achieve linear convergence even if f is not strongly convex. Our result generalizes previous work which either requires f and g to be quadratic functions or requires proximal mappings for both f and g. We adopt a novel analysis technique that in each iteration uses a ghost update as a reference, and show that the iterates in the primal-dual gradient method converge to this ghost sequence. Using the same technique we further give an analysis for the primal-dual stochastic variance reduced gradient method for convex-concave saddle point problems with a finite-sum structure.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,196,205,,,,,,,,,,,,,,,,WOS:000509687900021,0
C,"Ghadiri, M; Schmidt, M",,"Chaudhuri, K; Sugiyama, M",,"Ghadiri, Mehrdad; Schmidt, Mark",,,Distributed Maximization of Submodular plus Diversity Functions for Multi-label Feature Selection on Huge Datasets,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"There are many problems in machine learning and data mining which are equivalent to selecting a non-redundant, high quality set of objects. Recommender systems, feature selection, and data summarization are among many applications of this. In this paper, we consider this problem as an optimization problem that seeks to maximize the sum of a sum-sum diversity function and a non-negative monotone submodular function. The diversity function addresses the redundancy, and the submodular function controls the predictive quality. We consider the problem in big data settings (in other words, distributed and streaming settings) where the data cannot be stored on a single machine or the process time is too high for a single machine. We show that a greedy algorithm achieves a constant factor approximation of the optimal solution in these settings. Moreover, we formulate the multi-label feature selection problem as such an optimization problem. This formulation combined with our algorithm leads to the first distributed multi-label feature selection method. We compare the performance of this method with centralized multi-label feature selection methods in the literature, and we show that its performance is comparable or in some cases is even better than current centralized multi-label feature selection methods.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902013,0
C,"Grave, E; Joulin, A; Berthet, Q",,"Chaudhuri, K; Sugiyama, M",,"Grave, Edouard; Joulin, Armand; Berthet, Quentin",,,Unsupervised Alignment of Embeddings with Wasserstein Procrustes,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We consider the task of aligning two sets of points in high dimension, which has many applications in natural language processing and computer vision. As an example, it was recently shown that it is possible to infer a bilingual lexicon, without supervised data, by aligning word embeddings trained on monolingual data. These recent advances are based on adversarial training to learn the mapping between the two embeddings. In this paper, we propose to use an alternative formulation, based on the joint estimation of an orthogonal matrix and a permutation matrix. While this problem is not convex, we propose to initialize our optimization algorithm by using a convex relaxation, traditionally considered for the graph isomorphism problem. We propose a stochastic algorithm to minimize our cost function on large scale problems. Finally, we evaluate our method on the problem of unsupervised word translation, by aligning word embeddings trained on monolingual data. On this task, our method obtains state of the art results, while requiring less computational resources than competing approaches.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901096,0
C,"Kallus, N; Mao, XJ; Zhou, A",,"Chaudhuri, K; Sugiyama, M",,"Kallus, Nathan; Mao, Xiaojie; Zhou, Angela",,,Interval Estimation of Individual-Level Causal Effects Under Unobserved Confounding,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We study the problem of learning conditional average treatment effects (CATE) from observational data with unobserved confounders. The CATE function maps baseline covariates to individual causal effect predictions and is key for personalized assessments. Recent work has focused on how to learn CATE under unconfoundedness, i.e., when there are no unobserved confounders. Since CATE may not be identified when unconfoundedness is violated, we develop a functional interval estimator that predicts bounds on the individual causal effects under realistic violations of unconfoundedness. Our estimator takes the form of a weighted kernel estimator with weights that vary adversarially. We prove that our estimator is sharp in that it converges exactly to the tightest bounds possible on CATE when there may be unobserved confounders. Further, we prove that personalized decision rules derived from our estimator achieve optimal minimax regret asymptotically. We assess our approach in a simulation study as well as demonstrate its application by comparing conclusions from a real observational study and clinical trial.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902034,0
C,"Kuznetsov, V; Mariet, Z",,"Chaudhuri, K; Sugiyama, M",,"Kuznetsov, Vitaly; Mariet, Zelda",,,Foundations of Sequence-to-Sequence Modeling for Time Series,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"The availability of large amounts of time series data, paired with the performance of deep-learning algorithms on a broad class of problems, has recently led to significant interest in the use of sequence-to-sequence models for time series forecasting. We provide the first theoretical analysis of this time series forecasting framework. We include a comparison of sequence-to-sequence modeling to classical time series models, and as such our theory can serve as a quantitative guide for practitioners choosing between different modeling methodologies.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,408,417,,,,,,,,,,,,,,,,WOS:000509687900043,0
C,"Laforgue, P; Clemencon, S; d'Alche-Buc, F",,"Chaudhuri, K; Sugiyama, M",,"Laforgue, Pierre; Clemencon, Stephan; d'Alche-Buc, Florence",,,Autoencoding any Data through Kernel Autoencoders,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"This paper investigates a novel algorithmic approach to data representation based on kernel methods. Assuming that the observations lie in a Hilbert space chi, the introduced Kernel Autoencoder (KAE) is the composition of mappings from vector-valued Reproducing Kernel Hilbert Spaces (vv-RKHSs) that minimizes the expected reconstruction error. Beyond a first extension of the autoencoding scheme to possibly infinite dimensional Hilbert spaces, KAE further allows to autoencode any kind of data by choosing chi to be itself a RKHS. A theoretical analysis of the model is carried out, providing a generalization bound, and shedding light on its connection with Kernel Principal Component Analysis. The proposed algorithms are then detailed at length: they crucially rely on the form taken by the minimizers, revealed by a dedicated Representer Theorem. Finally, numerical experiments on both simulated data and real labeled graphs (molecules) provide empirical evidence of the KAE performances.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901011,0
C,"Mariet, Z; Gartrell, M; Sra, S",,"Chaudhuri, K; Sugiyama, M",,"Mariet, Zelda; Gartrell, Mike; Sra, Suvrit",,,Learning Determinantal Point Processes by Corrective Negative Sampling,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Determinantal Point Processes (DPPs) have attracted significant interest from the machine learning community due to their ability to elegantly and tractably model the delicate balance between quality and diversity of sets. DPPs are commonly learned from data using maximum likelihood estimation (MLE). While fitting observed sets well, MLE for DPPs may also assign high likelihoods to unobserved sets that are far from the true generative distribution of the data. To address this issue, which reduces the quality of the learned model, we introduce a novel optimization problem, Contrastive Estimation (CE), which encodes information about negative samples into the basic learning model. CE is grounded in the successful use of negative information in machine vision and language modeling. Depending on the chosen negative distribution (which may be static or evolve during optimization), CE assumes two different forms, which we analyze theoretically and experimentally. We evaluate our new model on real-world datasets; on a challenging dataset, CE learning delivers a considerable improvement in predictive performance over a DPP learned without using contrastive information.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902031,0
C,"Nikolakakis, KE; Kalogerias, DS; Sarwate, AD",,"Chaudhuri, K; Sugiyama, M",,"Nikolakakis, Konstantinos E.; Kalogerias, Dionysios S.; Sarwate, Anand D.",,,Learning Tree Structures from Noisy Data,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We provide high-probability sample complexity guarantees for exact structure recovery of tree-structured graphical models, when only noisy observations of the respective vertex emissions are available. We assume that the hidden variables follow either an Ising model or a Gaussian graphical model, and the observables are noise-corrupted versions of the hidden variables: We consider multiplicative 1 binary noise for Ising models, and additive Gaussian noise for Gaussian models. Such hidden models arise naturally in a variety of applications such as physics, biology, computer science, and finance. We study the impact of measurement noise on the task of learning the underlying tree structure via the well-known Chow-Liu algorithm, and provide formal sample complexity guarantees for exact recovery. In particular, for a tree with p vertices and probability of failure delta > 0, we show that the number of necessary samples for exact structure recovery is of the order of O(log(p/delta)) for Ising models (which remains the same as in the noiseless case), and O(polylog(p/delta)) for Gaussian models.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901085,0
C,"Ren, WB; Liu, J; Shroff, NB",,"Chaudhuri, K; Sugiyama, M",,"Ren, Wenbo; Liu, Jia; Shroff, Ness B.",,,Exploring k out of Top rho Fraction of Arms in Stochastic Bandits,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"This paper studies the problem of identifying any k distinct arms among the top rho fraction (e.g., top 5%) of arms from a finite or infinite set with a probably approximately correct (PAC) tolerance epsilon. We consider two cases: (i) when the threshold of the top arms' expected rewards is known and (ii) when it is unknown. We prove lower bounds for the four variants (finite or infinite arms, and known or unknown threshold), and propose algorithms for each. Two of these algorithms are shown to be sample complexity optimal (up to constant factors) and the other two are optimal up to a log factor. Results in this paper provide up to rho n/k reductions compared with the k-exploration algorithms that focus on finding the (PAC) best k arms out of n arms. We also numerically show improvements over the state-of-the-art.",,,,,"Liu, Jia/G-2981-2016; Liu, Jia/ABB-2195-2020","Liu, Jia/0000-0001-8844-3233; ",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902089,0
C,"Schulam, P; Saria, S",,"Chaudhuri, K; Sugiyama, M",,"Schulam, Peter; Saria, Suchi",,,Can You Trust This Prediction? Auditing Pointwise Reliability After Learning,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"To use machine learning in high stakes applications (e.g. medicine), we need tools for building confidence in the system and evaluating whether it is reliable. Methods to improve model reliability often require new learning algorithms (e.g. using Bayesian inference to obtain uncertainty estimates). An alternative is to audit a model after it is trained. In this paper, we describe resampling uncertainty estimation (RUE), an algorithm to audit the pointwise reliability of predictions. Intuitively, RUE estimates the amount that a prediction would change if the model had been fit on different training data. The algorithm uses the gradient and Hessian of the model's loss function to create an ensemble of predictions. Experimentally, we show that RUE more effectively detects inaccurate predictions than existing tools for auditing reliability subsequent to training. We also show that RUE can create predictive distributions that are competitive with state-of-the-art methods like Monte Carlo dropout, probabilistic back-propagation, and deep ensembles, but does not depend on specific algorithms at train-time like these methods do.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901007,0
C,"Tian, L; Nie, FP; Li, XL",,"Chaudhuri, K; Sugiyama, M",,"Tian, Lai; Nie, Feiping; Li, Xuelong",,,A Unified Weight Learning Paradigm for Multi-view Learning,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Learning a set of weights to combine views linearly forms a series of popular schemes in multi-view learning. Three weight learning paradigms, i.e., Norm Regularization (NR), Exponential Decay (ED), and p-th Root Loss (pRL), are widely used in the literature, while the relations between them and the limiting behaviors of them are not well understood yet. In this paper, we present a Unified Paradigm (UP) that contains the aforementioned three popular paradigms as special cases. Specifically, we extend the domain of hyper-parameters of NR from positive to real numbers and show this extension bridges NR, ED, and pRL. Besides, we provide detailed discussion on the weights sparsity, hyperparameter setting, and counterintuitive limiting behavior of these paradigms. Furthermore, we show the generality of our technique with examples in Multi-Task Learning and Fuzzy Clustering. Our results may provide in-sights to understand existing algorithms better and inspire research on new weight learning schemes. Numerical results support our theoretical analysis.",,,,,"Li, Xuelong/ABF-3381-2020",,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902086,0
C,"Besserve, M; Shajarisales, N; Scholkopf, B; Janzing, D",,"Storkey, A; PerezCruz, F",,"Besserve, Michel; Shajarisales, Naji; Schoelkopf, Bernhard; Janzing, Dominik",,,Group invariance principles for causal generative models,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"The postulate of independence of cause and mechanism (ICM) has recently led to several new causal discovery algorithms. The interpretation of independence and the way it is utilized, however, varies across these methods. Our aim in this paper is to propose a group theoretic framework for ICM to unify and generalize these approaches. In our setting, the cause-mechanism relationship is assessed by perturbing it with random group transformations. We show that the group theoretic view encompasses previous ICM approaches and provides a very general tool to study the structure of data generating mechanisms with direct applications to machine learning.",,,,,"Scholkopf, Bernhard/A-7570-2013","Scholkopf, Bernhard/0000-0002-8177-0925",,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300059,0
C,"Gidel, G; Pedregosa, F; Lacoste-Julien, S",,"Storkey, A; PerezCruz, F",,"Gidel, Gauthier; Pedregosa, Fabian; Lacoste-Julien, Simon",,,Frank-Wolfe Splitting via Augmented Lagrangian Method,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Minimizing a function over an intersection of convex sets is an important task in optimization that is often much more challenging than minimizing it over each individual constraint set. While traditional methods such as Frank-Wolfe (FW) or proximal gradient descent assume access to a linear or quadratic oracle on the intersection, splitting techniques take advantage of the structure of each sets, and only require access to the oracle on the individual constraints. In this work, we develop and analyze the Frank-Wolfe Augmented Lagrangian (FW-AL) algorithm, a method for minimizing a smooth function over convex compact sets related by a linear consistency constraint that only requires access to a linear minimization oracle over the individual constraints. It is based on the Augmented Lagrangian Method (ALM), also known as Method of Multipliers, but unlike most existing splitting methods, it only requires access to linear (instead of quadratic) minimization oracles. We use recent advances in the analysis of Frank-Wolfe and the alternating direction method of multipliers algorithms to prove a sublinear convergence rate for FW-AL over general convex compact sets and a linear convergence rate over polytopes.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300152,0
C,"Gruber, A; Yanover, C; El-Hay, T; Sonnerborg, A; Borghi, V; Incardona, F; Goldschmidt, Y",,"Storkey, A; PerezCruz, F",,"Gruber, Amit; Yanover, Chen; El-Hay, Tal; Sonnerborg, Anders; Borghi, Vanni; Incardona, Francesca; Goldschmidt, Yaara",,,Factorial HMMs with Collapsed Gibbs Sampling for Optimizing Long-term HIV Therapy,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Combined antiretroviral therapies (CART) can successfully suppress HIV in the serum and bring its viral load below detection rate. However, drug resistance remains a major challenge. As resistance patterns vary between patients, personalized therapy is required. Automatic systems for therapy personalization exist and were shown to better predict therapy outcome than HIV experts in some settings. However, these systems focus only on selecting the therapy most likely to suppress the virus for several weeks, a choice that may be suboptimal over the longer term due to evolution of drug resistance. We present a novel generative model for HIV drug resistance evolution. This model is based on factorial HMMs, applying a novel collapsed Gibbs Sampling algorithm for approximate learning. Using the suggested model, we obtain better therapy outcome predictions than existing methods and recommend therapies that may be more effective over the long term. We demonstrate our results using simulated data and using real data from the EuResist dataset.",,,,,"Yanover, Chen/A-3754-2012","Yanover, Chen/0000-0003-3663-4286",,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300034,0
C,"Jung, YH; Tewari, A",,"Storkey, A; PerezCruz, F",,"Jung, Young Hun; Tewari, Ambuj",,,Online Boosting Algorithms for Multi-label Ranking,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"We consider the multi-label ranking approach to multi-label learning. Boosting is a natural method for multi-label ranking as it aggregates weak predictions through majority votes, which can be directly used as scores to produce a ranking of the labels. We design online boosting algorithms with provable loss bounds for multi-label ranking. We show that our first algorithm is optimal in terms of the number of learners required to attain a desired accuracy, but it requires knowledge of the edge of the weak learners. We also design an adaptive algorithm that does not require this knowledge and is hence more practical. Experimental results on real data sets demonstrate that our algorithms are at least as good as existing batch boosting algorithms.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300030,0
C,"Noshad, M; Hero, AO",,"Storkey, A; PerezCruz, F",,"Noshad, Morteza; Hero, Alfred O., III",,,Scalable Hash-Based Estimation of Divergence Measures,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"We propose a scalable divergence estimation method based on hashing. Consider two continuous random variables X and Y whose densities have bounded support. We consider a particular locality sensitive random hashing, and consider the ratio of samples in each hash bin having non-zero numbers of Y samples. We prove that the weighted average of these ratios over all of the hash bins converges to f-divergences between the two samples sets. We derive the MSE rates for two families of smooth functions; the Holder smoothness class and differentiable functions. In particular, it is proved that if the density functions have bounded derivatives up to the order d, where d is the dimension of samples, the optimal parametric MSE rate of O(1/N) can be achieved. The computational complexity is shown to be O(N), which is optimal. To the best of our knowledge, this is the first empirical divergence estimator that has optimal computational complexity and can achieve the optimal parametric MSE estimation rate of O(1/N)",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300196,0
C,"Rowland, M; Bellemare, MG; Dabney, W; Munos, R; Teh, YW",,"Storkey, A; PerezCruz, F",,"Rowland, Mark; Bellemare, Marc G.; Dabney, Will; Munos, Remi; Teh, Yee Whye",,,An Analysis of Categorical Distributional Reinforcement Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Distributional approaches to value-based reinforcement learning model the entire distribution of returns, rather than just their expected values, and have recently been shown to yield state-of-the-art empirical performance. This was demonstrated by the recently proposed C51 algorithm, based on categorical distributional reinforcement learning (CDRL) [Bellemare et al., 2017a]. However, the theoretical properties of CDRL algorithms are not yet well understood. In this paper, we introduce a framework to analyse CDRL algorithms, establish the importance of the projected distributional Bellman operator in distributional RL, draw fundamental connections between CDRL and the Cramer distance, and give a proof of convergence for sample-based categorical distributional reinforcement learning algorithms.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300004,0
C,"Xue, LR; Kpotufe, S",,"Storkey, A; PerezCruz, F",,"Xue, Lirong; Kpotufe, Samory",,,"Achieving the time of 1-NN, but the accuracy of k-NN","INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"We propose a simple approach which, given distributed computing resources, can nearly achieve the accuracy of k-NN prediction, while matching (or improving) the faster prediction time of 1-NN. The approach consists of aggregating denoised 1-NN predictors over a small number of distributed subsamples. We show, both theoretically and experimentally, that small subsample sizes suffice to attain similar performance as k-NN, without sacrificing the computational efficiency of 1-NN.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300170,0
C,"Boissier, M; Lyu, SW; Ying, YM; Zhou, DX",,"Gretton, A; Robert, CC",,"Boissier, Martin; Lyu, Siwei; Ying, Yiming; Zhou, Ding-Xuan",,,Fast Convergence of Online Pairwise Learning Algorithms,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Pairwise learning usually refers to a learning task which involves a loss function depending on pairs of examples, among which most notable ones are bipartite ranking, metric learning and AUC maximization. In this paper, we focus on online learning algorithms for pairwise learning problems without strong convexity, for which all previously known algorithms achieve a convergence rate of O(1/root T) after T iterations. In particular, we study an online learning algorithm for pairwise learning with a least-square loss function in an unconstrained setting. We prove that the convergence of its last iterate can converge to the desired minimizer at a rate arbitrarily close to O(1/T) up to logarithmic factor. The rates for this algorithm are established in high probability under the assumptions of polynomially decaying step sizes.",,,,,"Zhou, Ding-Xuan/B-3160-2013; Ying, Yiming/AGD-7246-2022","Zhou, Ding-Xuan/0000-0003-0224-9216; Ying, Yiming/0000-0001-7345-6672",,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,204,212,,,,,,,,,,,,,,,,WOS:000508662100023,0
C,"Kusner, MJ; Sun, Y; Sridharan, K; Weinberger, KQ",,"Gretton, A; Robert, CC",,"Kusner, Matt J.; Sun, Yu; Sridharan, Karthik; Weinberger, Kilian Q.",,,Private Causal Inference,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Causal inference deals with identifying which random variables cause or control other random variables. Recent advances on the topic of causal inference based on tools from statistical estimation and machine learning have resulted in practical algorithms for causal inference. Causal inference has the potential to have significant impact on medical research, prevention and control of diseases, and identifying factors that impact economic changes to name just a few. However, these promising applications for causal inference are often ones that involve sensitive or personal data of users that need to be kept private (e.g., medical records, personal finances, etc). Therefore, there is a need for the development of causal inference methods that preserve data privacy. We study the problem of inferring causality using the current, popular causal inference framework, the additive noise model (ANM) while simultaneously ensuring privacy of the users. Our framework provides differential privacy guarantees for a variety of ANM variants. We run extensive experiments, and demonstrate that our techniques are practical and easy to implement.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,1308,1317,,,,,,,,,,,,,,,,WOS:000508662100142,0
C,"Li, WZ; Ahn, SJ; Welling, M",,"Gretton, A; Robert, CC",,"Li, Wenzhe; Ahn, Sungjin; Welling, Max",,,Scalable MCMC for Mixed Membership Stochastic Blockmodels,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,We propose a stochastic gradient Markov chain Monte Carlo (SG-MCMC) algorithm for scalable inference in mixed-membership stochastic blockmodels (MMSB). Our algorithm is based on the stochastic gradient Riemannian Langevin sampler and achieves both faster speed and higher accuracy at every iteration than the current state-of-the-art algorithm based on stochastic variational inference. In addition we develop an approximation that can handle models that entertain a very large number of communities. The experimental results show that SG-MCMC strictly dominates competing algorithms in all cases.,,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,723,731,,,,,,,,,,,,,,,,WOS:000508662100079,0
C,"Mei, JC; Zhang, H; Lu, BL",,"Gretton, A; Robert, CC",,"Mei, Jincheng; Zhang, Hao; Lu, Bao-Liang",,,On the Reducibility of Submodular Functions,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"The scalability of submodular optimization methods is critical for their usability in practice. In this paper, we study the reducibility of submodular functions, a property that enables us to reduce the solution space of submodular optimization problems without performance loss. We introduce the concept of reducibility using marginal gains. Then we show that by adding perturbation, we can endow irreducible functions with reducibility, based on which we propose the perturbation-reduction optimization framework. Our theoretical analysis proves that given the perturbation scales, the reducibility gain could be computed, and the performance loss has additive upper bounds. We further conduct empirical studies and the results demonstrate that our proposed framework significantly accelerates existing optimization methods for irreducible submodular functions with a cost of only small performance losses.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,186,194,,,,,,,,,,,,,,,,WOS:000508662100021,0
C,"Rockova, V; Moran, GE; George, EI",,"Gretton, A; Robert, CC",,"Rockova, Veronika; Moran, Gemma E.; George, Edward, I",,,Determinantal Regularization for Ensemble Variable Selection,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Recent years have seen growing interest in deterministic search approaches to spike-and-slab Bayesian variable selection. Such methods have focused on the goal of finding a global mode to identify a best model. However, the report of a single model will be a misleading reflection of the model uncertainty inherent in a highly multimodal posterior. Motivated by non-parametric variational Bayes strategies, we move beyond this limitation by proposing an ensemble optimization approach to identify a collection of representative posterior modes. By deploying determinantal penalty functions as diversity regularizers, our approach performs regularization over multiple locations of the posterior. The key driver of these determinantal penalties is a kernel function that induces repulsion in the latent model space domain.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,1105,1113,,,,,,,,,,,,,,,,WOS:000508662100120,0
C,"Tenzer, Y; Soloveytchik, I; Wiesel, A; Elidan, G",,"Gretton, A; Robert, CC",,"Tenzer, Yaniv; Soloveytchik, Ilya; Wiesel, Ami; Elidan, Gal",,,Generalized Ideal Parent (GIP): Discovering non-Gaussian Hidden Variables,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"A formidable challenge in uncertainty modeling in general, and when learning Bayesian networks in particular, is the discovery of unknown hidden variables. Few works that tackle this task are typically limited to discrete or Gaussian domains, or to tree structures. We propose a novel approach for discovering hidden variables in flexible non-Gaussian domains using the powerful class of Gaussian copula networks. Briefly, we define the concept of a hypothetically optimal predictor of variable, and show how it can be used to discover useful hidden variables in the expressive framework of copula networks. We demonstrate the merit of our approach for learning succinct models that generalize well in several real-life domains.",,,,,,"Elidan, Gal/0000-0001-5365-599X",,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,222,230,,,,,,,,,,,,,,,,WOS:000508662100025,0
C,"Tran, D; Kim, M; Doshi-Velez, F",,"Gretton, A; Robert, CC",,"Tran, Dustin; Kim, Minjae; Doshi-Velez, Finale",,,Spectral M-estimation with Applications to Hidden Markov Models,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Method of moment estimators exhibit appealing statistical properties, such as asymptotic un-biasedness, for nonconvex problems. However, they typically require a large number of samples and are extremely sensitive to model mis-specification. In this paper, we apply the framework of M-estimation to develop both a generalized method of moments procedure and a principled method for regularization. Our proposed Mestimator obtains optimal sample efficiency rates (in the class of moment-based estimators) and the same well-known rates on prediction accuracy as other spectral estimators. It also makes it straightforward to incorporate regularization into the sample moment conditions. We demonstrate empirically the gains in sample efficiency from our approach on hidden Markov models.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,1421,1430,,,,,,,,,,,,,,,,WOS:000508662100154,0
C,"Ghassemi, NH; Deisenroth, MP",,"Kaski, S; Corander, J",,"Ghassemi, Nooshin Haji; Deisenroth, Marc Peter",,,Analytic Long-Term Forecasting with Periodic Gaussian Processes,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"Gaussian processes are a state-of-the-art method for learning models from data. Data with an underlying periodic structure appears in many areas, e.g., in climatology or robotics. It is often important to predict the long-term evolution of such a time series, and to take the inherent periodicity explicitly into account. In a Gaussian process, periodicity can be accounted for by an appropriate kernel choice. However, the standard periodic kernel does not allow for analytic long-term forecasting. To address this shortcoming, we re-parametrize the periodic kernel, which, in combination with a double approximation, allows for analytic long-term forecasting of a periodic state evolution with Gaussian processes. Our model allows for probabilistic long-term forecasting of periodic processes, which can be valuable in Bayesian decision making, optimal control, reinforcement learning, and robotics.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,303,311,,,,,,,,,,,,,,,,WOS:000508355800034,0
C,"Gopalan, P; Ruiz, FJR; Ranganath, R; Blei, DM",,"Kaski, S; Corander, J",,"Gopalan, Prem; Ruiz, Francisco J. R.; Ranganath, Rajesh; Blei, David M.",,,Bayesian Nonparametric Poisson Factorization for Recommendation Systems,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"We develop a Bayesian nonparametric Poisson factorization model for recommendation systems. Poisson factorization implicitly models each user's limited budget of attention (or money) that allows consumption of only a small subset of the available items. In our Bayesian nonparametric variant, the number of latent components is theoretically unbounded and effectively estimated when computing a posterior with observed user behavior data. To approximate the posterior, we develop an efficient variational inference algorithm. It adapts the dimensionality of the latent components to the data, only requires iteration over the user/item pairs that have been rated, and has computational complexity on the same order as for a parametric model with fixed dimensionality. We studied our model and algorithm with large real-world data sets of user-movie preferences. Our model cases the computational burden of searching for the number of latent components and gives better predictive performance than its parametric counterpart.",,,,,,"RODRIGUEZ RUIZ, FRANCISCO JESUS/0000-0002-2200-901X",,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,275,283,,,,,,,,,,,,,,,,WOS:000508355800031,0
C,"Heins, K; Stern, H",,"Kaski, S; Corander, J",,"Heins, Kevin; Stern, Hal",,,A Statistical Model for Event Sequence Data,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"The identification of recurring patterns within a sequence of events is an important task in behavior research. In this paper, we consider a general probabilistic framework for identifying such patterns, by distinguishing between events that belong to a pattern and events that occur as part of background processes. The event processes, both for background events and events that are part of recurring patterns, are modeled as competing renewal processes. Using this framework, we develop an inference procedure to detect the sequences present in observed data. Our method is compared to a current approach used within the ethology literature on both simulated data and data collected to study the impact of fragmented and unpredictable maternal behavior on cognitive development of children.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,338,346,,,,,,,,,,,28435511,,,,,WOS:000508355800038,0
C,"Lefakis, L; Fleuret, F",,"Kaski, S; Corander, J",,"Lefakis, Leonidas; Fleuret, Francois",,,Jointly Informative Feature Selection,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"We propose several novel criteria for the selection of groups of jointly informative continuous features in the context of classification. Our approach is based on combining a Gaussian modeling of the feature responses, with derived upper bounds on their mutual information with the class label and their joint entropy. We further propose specific algorithmic implementations of these criteria which reduce the computational complexity of the algorithms by up to two-orders of magnitude, making these strategies tractable in practice. Experiments on multiple computer-vision data-bases, and using several types of classifiers, show that this class of methods outperforms state-of-the-art baselines, both in terms of speed and classification accuracy.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,567,575,,,,,,,,,,,,,,,,WOS:000508355800063,0
C,"Smith, D; Gogate, V",,"Kaski, S; Corander, J",,"Smith, David; Gogate, Vibhav",,,Loopy Belief Propagation in the Presence of Determinism,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"It is well known that loopy Belief propagation (LBP) performs poorly on probabilistic graphical models (PGMs) with determinism. In this paper, we propose a new method for remedying this problem. The key idea in our method is finding a reparameterization of the graphical model such that LBP, when run on the reparameterization, is likely to have better convergence properties than LBP on the original graphical model. We propose several schemes for finding such reparameterizations, all of which leverage unique properties of zeros as well as research on LBP convergence done over the last decade. Our experimental evaluation on a variety of PGMs clearly demonstrates the promise of our method - it often yields accuracy and convergence time improvements of an order of magnitude or more over LBP.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,895,903,,,,,,,,,,,,,,,,WOS:000508355800099,0
C,"Behzadian, B; Russel, RH; Petrik, M; Ho, CP",,"Banerjee, A; Fukumizu, K",,"Behzadian, Bahram; Russel, Reazul Hasan; Petrik, Marek; Ho, Chin Pang",,,Optimizing Percentile Criterion Using Robust MDPs,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We address the problem of computing reliable policies in reinforcement learning problems with limited data. In particular, we compute policies that achieve good returns with high confidence when deployed. This objective, known as the percentile criterion, can be optimized using Robust MDPs (RMDPs). RMDPs generalize MDPs to allow for uncertain transition probabilities chosen adversarially from given ambiguity sets. We show that the RMDP solution's sub-optimality depends on the spans of the ambiguity sets along the value function. We then propose new algorithms that minimize the span of ambiguity sets defined by weighted L-1 and L-infinity norms. Our primary focus is on Bayesian guarantees, but we also describe how our methods apply to frequentist guarantees and derive new concentration inequalities for weighted L-1 and L-infinity norms. Experimental results indicate that our optimized ambiguity sets improve significantly on prior construction methods.",,,,,,"Ho, Chin Pang/0000-0002-2143-978X; Petrik, Marek/0000-0002-4568-7948",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801026,0
C,"Blum, A; Dan, C; Seddighin, S",,"Banerjee, A; Fukumizu, K",,"Blum, Avrim; Dan, Chen; Seddighin, Saeed",,,Learning Complexity of Simulated Annealing,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Simulated annealing is an effective and general means of optimization. It is in fact inspired by metallurgy, where the temperature of a material determines its behavior in thermodynamics. Likewise, in simulated annealing, the actions that the algorithm takes depend entirely on the value of a variable which captures the notion of temperature. Typically, simulated annealing starts with a high temperature, which makes the algorithm pretty unpredictable, and gradually cools the temperature down to become more stable. A key component that plays a crucial role in the performance of simulated annealing is the criteria under which the temperature changes, namely, the cooling schedule. Motivated by this, we study the following question in this work: Given enough samples to the instances of a specific class of optimization problems, can we design near-optimal cooling schedules that minimize the runtime or maximize the success rate of the algorithm on average when the underlying problem is drawn uniformly at random from the same class? We provide positive results both in terms of sample complexity and simulation complexity(1). For sample complexity, we show that (O) over tilde(root m) samples suffice to find an approximately optimal cooling schedule of length m. We complement this result by giving a lower bound of (Omega) over tilde (m(1/3)) on the sample complexity of any learning algorithm that provides an almost optimal cooling schedule. These results are general and rely on no assumption. For simulation complexity, however, we make additional assumptions to measure the success rate of an algorithm. To this end, we introduce the monotone stationary graph that models the performance of simulated annealing. Based on this model, we present polynomial time algorithms with provable guarantees for the learning problem.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801085,0
C,"Cohen-Addad, V; Guedj, B; Kanade, V; Rom, G",,"Banerjee, A; Fukumizu, K",,"Cohen-Addad, Vincent; Guedj, Benjamin; Kanade, Varun; Rom, Guy",,,Online k-means Clustering,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We study the problem of learning a clustering of an online set of points. The specific formulation we use is the k-means objective: At each time step the algorithm has to maintain a set of k candidate centers and the loss incurred by the algorithm is the squared distance between the new point and the closest center. The goal is to minimize regret with respect to the best solution to the k-means objective in hindsight. We show that provided the data lies in a bounded region, learning is possible, namely an implementation of the Multiplicative Weights Update Algorithm (MWUA) using a discretized grid achieves a regret bound of O (root T) in expectation. We also present an online-to-offline reduction that shows that an efficient no-regret online algorithm (despite being allowed to choose a different set of candidate centers at each round) implies an offine efficient algorithm for the k-means problem, which is known to be NP-hard. In light of this hardness, we consider the slightly weaker requirement of comparing regret with respect to (1 + epsilon)OPT and present a no-regret algorithm with runtime O T poly(log(T), k, d, 1/epsilon)(O(kd))). Our algorithm is based on maintaining a set of points of bounded size which is a coreset that helps identifying the relevant regions of the space for running an adaptive, more efficient, variant of the MWUA. We show that simpler online algorithms, such as Follow The Leader (FTL), fail to produce sublinear regret in the worst case. We also report preliminary experiments with synthetic and real-world data. Our theoretical results answer an open question of Dasgupta (2008).",,,,,,"Guedj, Benjamin/0000-0003-1237-7430",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801039,0
C,"Han, YJ",,"Banerjee, A; Fukumizu, K",,"Han, Yanjun",,,On the High Accuracy Limitation of Adaptive Property Estimation,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Recent years have witnessed the success of adaptive (or unified) approaches in estimating symmetric properties of discrete distributions, where the learner first obtains a distribution estimator independent of the target property, and then plugs the estimator into the target property as the final estimator. Several such approaches have been proposed and proved to be adaptively optimal, i.e. they achieve the optimal sample complexity for a large class of properties within a low accuracy, especially for a large estimation error epsilon >> n(-1/3) where n is the sample size. In this paper, we characterize the high accuracy limitation, or the penalty for adaptation, for general adaptive approaches. Specifically, we obtain the first known adaptation lower bound that under a mild condition, any adaptive approach cannot achieve the optimal sample complexity for every 1-Lipschitz property within accuracy epsilon << n(-1/3). In particular, this result disproves a conjecture in [Acharya et al., 2017a] that the profile maximum likelihood (PML) plug-in approach is optimal in property estimation for all ranges of epsilon, and confirms a conjecture in [Han and Shiragur, 2021] that their competitive analysis of the PML is tight.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801015,0
C,"Harder, F; Adamczewski, K; Park, M",,"Banerjee, A; Fukumizu, K",,"Harder, Frederik; Adamczewski, Kamil; Park, Mijung",,,DP-MERF: Differentially Private Mean Embeddings with Random Features for Practical Privacy-Preserving Data Generation,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We propose a differentially private data generation paradigm using random feature representations of kernel mean embeddings when comparing the distribution of true data with that of synthetic data. We exploit the random feature representations for two important benefits. First, we require a minimal privacy cost for training deep generative models. This is because unlike kernel-based distance metrics that require computing the kernel matrix on all pairs of true and synthetic data points, we can detach the data-dependent term from the term solely dependent on synthetic data. Hence, we need to perturb the data-dependent term only once and then use it repeatedly during the generator training. Second, we can obtain an analytic sensitivity of the kernel mean embedding as the random features are norm bounded by construction. This removes the necessity of hyper-parameter search for a clipping norm to handle the unknown sensitivity of a generator network. We provide several variants of our algorithm, differentially-private mean embeddings with random features (DP-MERF) to jointly generate labels and input features for datasets such as heterogeneous tabular data and image data. Our algorithm achieves drastically better privacy-utility trade-offs than existing methods when tested on several datasets.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802022,0
C,"Hoffman, MD; Radul, A; Sountsov, P",,"Banerjee, A; Fukumizu, K",,"Hoffman, Matthew D.; Radul, Alexey; Sountsov, Pavel",,,An Adaptive MCMC Scheme for Setting Trajectory Lengths in Hamiltonian Monte Carlo,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Hamiltonian Monte Carlo (HMC) is a powerful MCMC algorithm based on simulating Hamiltonian dynamics. Its performance depends strongly on choosing appropriate values for two parameters: the step size used in the simulation, and how long the simulation runs for. The step-size parameter can be tuned using standard adaptive-MCMC strategies, but it is less obvious how to tune the simulation-length parameter. The no-U-turn sampler (NUTS) eliminates this problematic simulation-length parameter, but NUTS's relatively complex control flow makes it difficult to efficiently run many parallel chains on accelerators such as GPUs. NUTS also spends some extra gradient evaluations relative to HMC in order to decide how long to run each iteration without violating detailed balance. We propose ChEES-HMC, a simple adaptive-MCMC scheme for automatically tuning HMC's simulation-length parameter, which minimizes a proxy for the autocorrelation of the state's second moments. We evaluate ChEES-HMC and NUTS on many tasks, and find that ChEES-HMC typically yields larger effective sample sizes per gradient evaluation than NUTS does. When running many chains on a GPU, ChEES-HMC can also run significantly more gradient evaluations per second than NUTS, allowing it to quickly provide accurate estimates of posterior expectations.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804069,0
C,"Monath, N; Zaheer, M; Dubey, A; Ahmed, A; McCallum, A",,"Banerjee, A; Fukumizu, K",,"Monath, Nicholas; Zaheer, Manzil; Dubey, Avinava; Ahmed, Amr; McCallum, Andrew",,,DAG-Structured Clustering by Nearest Neighbors,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Hierarchical clusterings compactly encode multiple granularities of clusters within a tree structure. Hierarchies, by definition, fail to capture di.erent flat partitions that are not subsumed in one another. In this paper, we advocate for an alternative structure for representing multiple clusterings, a directed acyclic graph (DAG). By allowing nodes to have multiple parents, DAG structures are not only more flexible than trees, but also allow for points to be members of multiple clusters. We describe a scalable algorithm, Llama, which simply merges nearest neighbor substructures to form a DAG structure. Llama discovers structures that are more accurate than state-of-the-art tree-based techniques while remaining scalable to large-scale clustering benchmarks. Additionally, we support the proposed algorithm with theoretical guarantees on separated data, including types of data that cannot be correctly clustered by tree-based algorithms.",,,,,"Zaheer, Manzil/ABG-6249-2021",,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803044,0
C,"Mondelli, M; Venkataramanan, R",,"Banerjee, A; Fukumizu, K",,"Mondelli, Marco; Venkataramanan, Ramji",,,Approximate Message Passing with Spectral Initialization for Generalized Linear Models,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We consider the problem of estimating a signal from measurements obtained via a generalized linear model. We focus on estimators based on approximate message passing (AMP), a family of iterative algorithms with many appealing features: the performance of AMP in the high-dimensional limit can be succinctly characterized under suitable model assumptions; AMP can also be tailored to the empirical distribution of the signal entries, and for a wide class of estimation problems, AMP is conjectured to be optimal among all polynomial-time algorithms. However, a major issue of AMP is that in many models (such as phase retrieval), it requires an initialization correlated with the ground-truth signal and independent from the measurement matrix. Assuming that such an initialization is available is typically not realistic. In this paper, we solve this problem by proposing an AMP algorithm initialized with a spectral estimator. With such an initialization, the standard AMP analysis fails since the spectral estimator depends in a complicated way on the design matrix. Our main contribution is a rigorous characterization of the performance of AMP with spectral initialization in the high-dimensional limit. The key technical idea is to define and analyze a two-phase artificial AMP algorithm that first produces the spectral estimator, and then closely approximates the iterates of the true AMP. We also provide numerical results that demonstrate the validity of the proposed approach.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,397,+,,,,,,,,,,,,,,,,WOS:000659893800045,0
C,"Pacchiano, A; Ghavamzadeh, M; Bartlett, P; Jiang, H",,"Banerjee, A; Fukumizu, K",,"Pacchiano, Aldo; Ghavamzadeh, Mohammad; Bartlett, Peter; Jiang, Heinrich",,,Stochastic Bandits with Linear Constraints,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We study a constrained contextual linear bandit setting, where the goal of the agent is to produce a sequence of policies, whose expected cumulative reward over the course of multiple rounds is maximum, and each one of them has an expected cost below a certain threshold. We propose an upper-confidence bound algorithm for this problem, called optimistic pessimistic linear bandit (OPLB), and prove a sublinear bound on its regret that is inversely proportional to the difference between the constraint threshold and the cost of a known feasible action. Our algorithm balances exploration and constraint satisfaction using a novel idea that scales the radii of the reward and cost confidence sets with different scaling factors. We further specialize our results to multi-armed bandits and propose a computationally efficient algorithm for this setting and prove a a regret bound that is better than simply casting multi-armed bandits as an instance of linear bandits and using the regret bound of OPLB. We also prove a lower- bound for the problem studied in the paper and provide simulations to validate our theoretical results. Finally, we show how our algorithm and analysis can be extended to multiple constraints and to the case when the cost of the feasible action is unknown.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803041,0
C,"Plummer, S; Zhou, S; Bhattacharya, A; Dunson, D; Pati, D",,"Banerjee, A; Fukumizu, K",,"Plummer, Sean; Zhou, Shuang; Bhattacharya, Anirban; Dunson, David; Pati, Debdeep",,,Statistical Guarantees for Transformation Based Models with Applications to Implicit Variational Inference,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Transformation-based methods have been an attractive approach in non-parametric inference for problems such as unconditional and conditional density estimation due to their unique hierarchical structure that models the data as flexible transformation of a set of common latent variables. More recently, transformation-based models have been used in variational inference (VI) to construct flexible implicit families of variational distributions. However, their use in both non-parametric inference and variational inference lacks theoretical justification. We provide theoretical justification for the use of non-linear latent variable models (NL-LVMs) in non-parametric inference by showing that the support of the transformation induced prior in the space of densities is sufficiently large in the L-1 sense. We also show that, when a Gaussian process (GP) prior is placed on the transformation function, the posterior concentrates at the optimal rate up to a logarithmic factor. Adopting the flexibility demonstrated in the non-parametric setting, we use the NL-LVM to construct an implicit family of variational distributions, deemed GP-IVI. We delineate sufficient conditions under which GP-IVI achieves optimal risk bounds and approximates the true posterior in the sense of the Kullback-Leibler divergence. To the best of our knowledge, this is the first work on providing theoretical guarantees for implicit variational inference.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802092,0
C,"Song, S; Steinke, T; Thakkar, O; Thakurta, A",,"Banerjee, A; Fukumizu, K",,"Song, Shuang; Steinke, Thomas; Thakkar, Om; Thakurta, Abhradeep",,,Evading the Curse of Dimensionality in Unconstrained Private GLMs,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We revisit the well-studied problem of differentially private empirical risk minimization (ERM). We show that for unconstrained convex generalized linear models (GLMs), one can obtain an excess empirical risk of (O) over tilde root rank/epsilon/n), where rank is the rank of the feature matrix in the GLM problem, n is the number of data samples, and epsilon is the privacy parameter. This bound is attained via differentially private gradient descent (DP-GD). Furthermore, via the first lower bound for unconstrained private ERM, we show that our upper bound is tight. In sharp contrast to the constrained ERM setting, there is no dependence on the dimensionality of the ambient model space (p). (Notice that rank <= min{n, p}. Besides, we obtain an analogous excess population risk bound which depends on rank instead of p. For the smooth non-convex GLM setting (i.e., where the objective function is non-convex but preserves the GLM structure), we further show that DP-GD attains a dimension-independent convergence of (O) over tilde root rank/epsilon/n) to a first-order-stationary-point of the underlying objective. Finally, we show that for convex GLMs, a variant of DP-GD commonly used in practice (which involves clipping the individual gradients) also exhibits the same dimension-independent convergence to the minimum of a well-defined objective. To that end, we provide a structural lemma that characterizes the effect of clipping on the optimization profile of DP-GD.",,,,,"song, song/GWN-2626-2022",,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803020,0
C,"Duy, VNL; Takeuchi, I",,"Banerjee, A; Fukumizu, K",,"Vo Nguyen Le Duy; Takeuchi, Ichiro",,,Parametric Programming Approach for More Powerful and General Lasso Selective Inference,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Selective Inference (SI) has been actively studied in the past few years for conducting inference on the features of linear models that are adaptively selected by feature selection methods such as Lasso. The basic idea of SI is to make inference conditional on the selection event. Unfortunately, the main limitation of the original SI approach for Lasso is that the inference is conducted not only conditional on the selected features but also on their signs - this leads to loss of power because of over-conditioning. Although this limitation can be circumvented by considering the union of such selection events for all possible combinations of signs, this is only feasible when the number of selected features is sufficiently small. To address this computational bottleneck, we propose a parametric programming-based method that can conduct SI without conditioning on signs even when we have thousands of active features. The main idea is to compute the continuum path of Lasso solutions in the direction of the selected test statistic, and identify the subset of the data space corresponding to the feature selection event by following the solution path. The proposed parametric programming-based method not only avoids the aforementioned computational bottleneck but also improves the performance and practicality of SI for Lasso in various respects. We conduct several experiments to demonstrate the effectiveness and efficiency of our proposed method.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801014,0
C,"Wolter, M; Garcke, J",,"Banerjee, A; Fukumizu, K",,"Wolter, Moritz; Garcke, Jochen",,,Adaptive wavelet pooling for convolutional neural networks,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Convolutional neural networks (CNN)s have become the go-to choice for most image and video processing tasks. Most CNN architectures rely on pooling layers to reduce the resolution along spatial dimensions. The reduction allows subsequent deep convolution layers to operate with greater efficiency. This paper introduces adaptive wavelet pooling layers, which employ fast wavelet transforms (FWT) to reduce the feature resolution. The FWT decomposes the input features into multiple scales reducing the feature dimensions by removing the fine-scale subbands. Our approach adds extra flexibility through wavelet-basis function optimization and coefficient weighting at different scales. The adaptive wavelet layers integrate directly into well-known CNNs like the LeNet, Alexnet, or Densenet architectures. Using these networks, we validate our approach and find competitive performance on the MNIST, CIFAR-10, and SVHN (street view house numbers) data-sets.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802035,0
C,"Abel, D; Umbanhowar, N; Khetarpal, K; Arumugam, D; Precup, D; Littman, M",,"Chiappa, S; Calandra, R",,"Abel, David; Umbanhowar, Nate; Khetarpal, Khimya; Arumugam, Dilip; Precup, Doina; Littman, Michael",,,Value Preserving State-Action Abstractions,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"The Abstraction can improve the sample efficiency of reinforcement learning. However, the process of abstraction inherently discards information, potentially compromising an agents ability to represent high-value policies. To mitigate this, we here introduce combinations of state abstractions and options that are guaranteed to preserve the representation of near-optimal policies. We first define phi-relative options, a general formalism for analyzing the value loss of options paired with a state abstraction, and present necessary and sufficient conditions for phi-relative options to preserve near-optimal behavior in any finite Markov Decision Process. We further show that, under appropriate assumptions, phi-relative options can be composed to induce hierarchical abstractions that are also guaranteed to represent high-value policies.ion can improve the sample efficiency of reinforcement learning. However, the process of abstraction inherently discards information, potentially compromising an agents ability to represent high-value policies. To mitigate this, we here introduce combinations of state abstractions and options that are guaranteed to preserve the representation of near-optimal policies. We first define phi-relative options, a general formalism for analyzing the value loss of options paired with a state abstraction, and present necessary and sufficient conditions for phi-relative options to preserve near-optimal behavior in any finite Markov Decision Process. We further show that, under appropriate assumptions, phi-relative options can be composed to induce hierarchical abstractions that are also guaranteed to represent high-value policies.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1639,1649,,,,,,,,,,,,,,,,WOS:000559931300002,0
C,"Haller, S; Prakash, M; Hutschenreiter, L; Pietzsch, T; Rother, C; Jug, F; Swoboda, P; Savchynskyy, B",,"Chiappa, S; Calandra, R",,"Haller, Stefan; Prakash, Mangal; Hutschenreiter, Lisa; Pietzsch, Tobias; Rother, Carsten; Jug, Florian; Swoboda, Paul; Savchynskyy, Bogdan",,,A Primal-Dual Solver for Large-Scale Tracking-by-Assignment,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We propose a fast approximate solver for the combinatorial problem known as tracking-by-assignment, which we apply to cell tracking. The latter plays a key role in discovery in many life sciences, especially in cell and developmental biology. So far, in the most general setting this problem was addressed by off-the-shelf solvers like Gurobi, whose run time and memory requirements rapidly grow with the size of the input. In contrast, for our method this growth is nearly linear. Our contribution consists of a new (1) decomposable compact representation of the problem; (2) dual block-coordinate ascent method for optimizing the decomposition-based dual; and (3) primal heuristics that reconstructs a feasible integer solution based on the dual information. Compared to solving the problem with Gurobi, we observe an up to 60 times speed-up, while reducing the memory footprint significantly. We demonstrate the efficacy of our method on real-world tracking problems.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2539,2548,,,,,,,,,,,,,,,,WOS:000559931301031,0
C,"Lauer, F",,"Chiappa, S; Calandra, R",,"Lauer, Fabien",,,Risk Bounds for Learning Multiple Components with Permutation-Invariant Losses,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"This paper proposes a simple approach to derive efficient error bounds for learning multiple components with sparsity-inducing regularization. We show that for such regularization schemes, known decompositions of the Rademacher complexity over the components can be used in a more efficient manner to result in tighter bounds without too much effort. We give examples of application to switching regression and center-based clustering/vector quantization. Then, the complete workflow is illustrated on the problem of subspace clustering, for which decomposition results were not previously available. For all these problems, the proposed approach yields risk bounds with mild dependencies on the number of components and completely removes this dependence for nonconvex regularization schemes that could not be handled by previous methods.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1178,1186,,,,,,,,,,,,,,,,WOS:000559931301094,0
C,"Mihelich, M; Dognin, C; Shu, Y; Blot, M",,"Chiappa, S; Calandra, R",,"Mihelich, Martin; Dognin, Charles; Shu, Yan; Blot, Michael",,,A Characterization of Mean Squared Error for Estimator with Bagging,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Bagging can significantly improve the generalization performance of unstable machine learning algorithms such as trees or neural networks. Though bagging is now widely used in practice and many empirical studies have explored its behavior, we still know little about the theoretical properties of bagged predictions. In this paper, we theoretically investigate how the bagging method can reduce the Mean Squared Error (MSE) when applied on a statistical estimator. First, we prove that for any estimator, increasing the number of bagged estimators N in the average can only reduce the MSE. This intuitive result, observed empirically and discussed in the literature, has not yet been rigorously proved. Second, we focus on the standard estimator of variance called unbiased sample variance and we develop an exact analytical expression of the MSE for this estimator with bagging. This allows us to rigorously discuss the number of iterations N and the batch size m of the bagging method. From this expression, we state that only if the kurtosis of the distribution is greater than 3/2, the MSE of the variance estimator can be reduced with bagging. This result is important because it demonstrates that for distribution with low kurtosis, bagging can only deteriorate the performance of a statistical prediction. Finally, we propose a novel general-purpose algorithm to estimate with high precision the variance of a sample.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,288,296,,,,,,,,,,,,,,,,WOS:000559931302043,0
C,"Pamfil, R; Sriwattanaworachai, N; Desai, S; Pilgerstorfer, P; Beaumont, P; Georgatzis, K; Aragam, B",,"Chiappa, S; Calandra, R",,"Pamfil, Roxana; Sriwattanaworachai, Nisara; Desai, Shaan; Pilgerstorfer, Philip; Beaumont, Paul; Georgatzis, Konstantinos; Aragam, Bryon",,,DYNOTEARS: Structure Learning from Time-Series Data,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We revisit the structure learning problem for dynamic Bayesian networks and propose a method that simultaneously estimates contemporaneous (intra-slice) and time-lagged (interslice) relationships between variables in a time-series. Our approach is score-based, and revolves around minimizing a penalized loss subject to an acyclicity constraint. To solve this problem, we leverage a recent algebraic result characterizing the acyclicity constraint as a smooth equality constraint. The resulting algorithm, which we call DYNOTEARS, outperforms other methods on simulated data, especially in high-dimensions as the number of variables increases. We also apply this algorithm on real datasets from two different domains, finance and molecular biology, and analyze the resulting output. Compared to state-of-the-art methods for learning dynamic Bayesian networks, our method is both scalable and accurate on real data. The simple formulation and competitive performance of our method make it suitable for a variety of problems where one seeks to learn connections between variables across time.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1595,1604,,,,,,,,,,,,,,,,WOS:000559931302061,0
C,"Park, S; Bastani, O; Weimer, J; Lee, I",,"Chiappa, S; Calandra, R",,"Park, Sangdon; Bastani, Osbert; Weimer, James; Lee, Insup",,,Calibrated Prediction with Covariate Shift via Unsupervised Domain Adaptation,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Reliable uncertainty estimates are an important tool for helping autonomous agents or human decision makers understand and leverage predictive models. However, existing approaches to estimating uncertainty largely ignore the possibility of covariate shift i.e., where the real-world data distribution may differ from the training distribution. As a consequence, existing algorithms can overestimate certainty, possibly yielding a false sense of confidence in the predictive model. We propose an algorithm for calibrating predictions that accounts for the possibility of covariate shift, given labeled examples from the training distribution and unlabeled examples from the real-world distribution. Our algorithm uses importance weighting to correct for the shift from the training to the real-world distribution. However, importance weighting relies on the training and real-world distributions to be sufficiently close. Building on ideas from domain adaptation, we additionally learn a feature map that tries to equalize these two distributions. In an empirical evaluation, we show that our proposed approach outperforms existing approaches to calibrated prediction when there is covariate shift.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3219,3228,,,,,,,,,,,,,,,,WOS:000559931302066,0
C,"Pearce, T; Leibfried, F; Brintrup, A; Zaki, M; Neely, A",,"Chiappa, S; Calandra, R",,"Pearce, Tim; Leibfried, Felix; Brintrup, Alexandra; Zaki, Mohamed; Neely, Andy",,,Uncertainty in Neural Networks: Approximately Bayesian Ensembling,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Understanding the uncertainty of a neural network's (NN) predictions is essential for many purposes. The Bayesian framework provides a principled approach to this, however applying it to NNs is challenging due to large numbers of parameters and data. Ensembling NNs provides an easily implementable, scalable method for uncertainty quantification, however, it has been criticised for not being Bayesian. This work proposes one modification to the usual process that we argue does result in approximate Bayesian inference; regularising parameters about values drawn from a distribution which can be set equal to the prior. A theoretical analysis of the procedure in a simplified setting suggests the recovered posterior is centred correctly but tends to have underestimated marginal variance, and overestimated correlation. However, two conditions can lead to exact recovery. We argue that these conditions are partially present in NNs. Empirical evaluations demonstrate it has an advantage over standard ensembling, and is competitive with variational methods. Interactive demo: teapearce.github.io",,,,,"Zaki, Mohamed/W-5494-2019","Zaki, Mohamed/0000-0003-0264-2691",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,234,243,,,,,,,,,,,,,,,,WOS:000559931302068,0
C,"Shen, ZY; Heinonen, M; Kaski, S",,"Chiappa, S; Calandra, R",,"Shen, Zheyang; Heinonen, Markus; Kaski, Samuel",,,Learning spectrograms with convolutional spectral kernels,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We introduce the convolutional spectral kernel (CSK), a novel family of non-stationary, non-parametric covariance kernels for Gaussian process (GP) models, derived from the convolution between two imaginary radial basis functions. We present a principled framework to interpret CSK, as well as other deep probabilistic models, using approximated Fourier transform, yielding a concise representation of input-frequency spectrogram. Observing through the lens of the spectrogram, we provide insight on the interpretability of deep models. We then infer the functional hyperparameters using scalable variational and MCMC methods. On small- and medium-sized spatiotemporal datasets, we demonstrate improved generalization of GP models when equipped with CSK, and their capability to extract non-stationary periodic patterns.",,,,,"Kaski, Samuel/B-6684-2008","Kaski, Samuel/0000-0003-1925-9154",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303014,0
C,"Shui, CJ; Zhou, F; Gagne, C; Wang, BY",,"Chiappa, S; Calandra, R",,"Shui, Changjian; Zhou, Fan; Gagne, Christian; Wang, Boyu",,,Deep Active Learning: Unified and Principled Method for Query and Training,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"In this paper, we are proposing a unified and principled method for both the querying and training processes in deep batch active learning. We are providing theoretical insights from the intuition of modeling the interactive procedure in active learning as distribution matching, by adopting the Wasserstein distance. As a consequence, we derived a new training loss from the theoretical analysis, which is decomposed into optimizing deep neural network parameters and batch query selection through alternative optimization. In addition, the loss for training a deep neural network is naturally formulated as a min-max optimization problem through leveraging the unlabeled data information. Moreover, the proposed principles also indicate an explicit uncertainty-diversity trade-off in the query batch selection. Finally, we evaluate our proposed method on different benchmarks, consistently showing better empirical performances and a better time-efficient query strategy compared to the baselines.",,,,,"Shui, Changjian/HCI-4782-2022; Zhou, Fan/ABH-2640-2020","Shui, Changjian/0000-0001-6447-6559; Zhou, Fan/0000-0003-1736-2641; Wang, Boyu/0000-0002-7413-4162",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303020,0
C,"Yuan, H; Liang, YY",,"Chiappa, S; Calandra, R",,"Yuan, Hui; Liang, Yingyu",,,Learning Entangled Single-Sample Distributions via Iterative Trimming,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"In the setting of entangled single-sample distributions, the goal is to estimate some common parameter shared by a family of distributions, given one single sample from each distribution. We study mean estimation and linear regression under general conditions, and analyze a simple and computationally efficient method based on iteratively trimming samples and re-estimating the parameter on the trimmed sample set. We show that the method in logarithmic iterations outputs an estimation whose error only depends on the noise level of the [alpha n]-th noisiest data point where alpha is a constant and n is the sample size. This means it can tolerate a constant fraction of high-noise points. These are the first such results under our general conditions with computationally efficient estimators. It also justifies the wide application and empirical success of iterative trimming in practice. Our theoretical results are complemented by experiments on synthetic data.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303100,0
C,"Zhang, Y; Bellot, A; van der Schaar, M",,"Chiappa, S; Calandra, R",,"Zhang, Yao; Bellot, Alexis; van der Schaar, Mihaela",,,Learning Overlapping Representations for the Estimation of Individualized Treatment Effects,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"The choice of making an intervention depends on its potential benefit or harm in comparison to alternatives. Estimating the likely outcome of alternatives from observational data is a challenging problem as all outcomes are never observed, and selection bias precludes the direct comparison of differently intervened groups. Despite their empirical success, we show that algorithms that learn domain-invariant representations of inputs (on which to make predictions) are often inappropriate, and develop generalization bounds that demonstrate the dependence on domain overlap and highlight the need for invertible latent maps. Based on these results, we develop a deep kernel regression algorithm and posterior regularization framework that substantially outperforms the state-of-the-art on a variety of benchmarks data sets.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1005,1013,,,,,,,,,,,,,,,,WOS:000559931304009,0
C,"Cucuringu, M; Davies, P; Glielmo, A; Tyagi, H",,"Chaudhuri, K; Sugiyama, M",,"Cucuringu, Mihai; Davies, Peter; Glielmo, Aldo; Tyagi, Hemant",,,SPONGE: A generalized eigenproblem for clustering signed networks,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We introduce a principled and theoretically sound spectral method for k-way clustering in signed graphs, where the affinity measure between nodes takes either positive or negative values. Our approach is motivated by social balance theory, where the task of clustering aims to decompose the network into disjoint groups, such that individuals within the same group are connected by as many positive edges as possible, while individuals from different groups are connected by as many negative edges as possible. Our algorithm relies on a generalized eigenproblem formulation inspired by recent work on constrained clustering. We provide theoretical guarantees for our approach in the setting of a signed stochastic block model, by leveraging tools from matrix perturbation theory and random matrix theory. An extensive set of numerical experiments on both synthetic and real data shows that our approach compares favorably with state-of-the-art methods for signed clustering, especially for large number of clusters and sparse measurement graphs.",,,,,"Glielmo, Aldo/GZG-2658-2022",,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901014,0
C,"Degenne, R; Nedelec, T; Calauzenes, C; Perchet, V",,"Chaudhuri, K; Sugiyama, M",,"Degenne, Remy; Nedelec, Thomas; Calauzenes, Clement; Perchet, Vianney",,,"Bridging the gap between regret minimization and best arm identification, with application to A/B tests","22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"State of the art online learning procedures focus either on selecting the best alternative (best arm identification) or on minimizing the cost (the regret). We merge these two objectives by providing the theoretical analysis of cost minimizing algorithms that are also (5-PAC (with a proven guaranteed bound on the decision time), hence fulfilling at the same time regret minimization and best arm identification. This analysis sheds light on the common observation that ill-callibrated UCB-algorithms minimize regret while still identifying quickly the best arm. We also extend these results to the non-iid case faced by many practitioners. This provides a technique to make cost versus decision time compromise when doing adaptive tests with applications ranging from website A/B testing to clinical trials.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902004,0
C,"Du, JL; Linero, AR",,"Chaudhuri, K; Sugiyama, M",,"Du, Junliang; Linero, Antonio R.",,,Interaction Detection with Bayesian Decision Tree Ensembles,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Methods based on Bayesian decision tree ensembles have proven valuable in constructing high-quality predictions, and are particularly attractive in certain settings because they encourage low-order interaction effects. Despite adapting to the presence of low-order interactions for prediction purpose, we show that Bayesian decision tree ensembles are generally anti-conservative for the purpose of conducting interaction detection. We address this problem by introducing Dirichlet process forests (DP-Forests), which leverage the presence of low-order interactions by clustering the trees so that trees within the same cluster focus on detecting a specific interaction. We show on both simulated and benchmark data that DP-Forests perform well relative to existing interaction detection techniques for detecting low-order interactions, attaining very low false-positive and false-negative rates while maintaining the same performance for prediction using a comparable computational budget.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,108,117,,,,,,,,,,,,,,,,WOS:000509687900012,0
C,"He, JY; Yalov, S; Hahn, PR",,"Chaudhuri, K; Sugiyama, M",,"He, Jingyu; Yalov, Saar; Hahn, P. Richard",,,XBART: Accelerated Bayesian Additive Regression Trees,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Bayesian additive regression trees (BART) (Chipman et al., 2010) is a powerful predictive model that often outperforms alternative models at out-of-sample prediction. BART is especially well-suited to settings with unstructured predictor variables and substantial sources of unmeasured variation as is typical in the social, behavioral and health sciences. This paper develops a modified version of BART that is amenable to fast posterior estimation. We present a stochastic hill climbing algorithm that matches the remarkable predictive accuracy of previous BART implementations, but is many times faster and less memory intensive. Simulation studies show that the new method is comparable in computation time and more accurate at function estimation than both random forests and gradient boosting.",,,,,,"He, Jingyu/0000-0003-0370-8867",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901018,0
C,"Hollocou, A; Bonald, T; Lelarge, M",,"Chaudhuri, K; Sugiyama, M",,"Hollocou, Alexandre; Bonald, Thomas; Lelarge, Marc",,,Modularity-based Sparse Soft Graph Clustering,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Clustering is a central problem in machine learning for which graph-based approaches have proven their efficiency. In this paper, we study a relaxation of the modularity maximization problem, well-known in the graph partitioning literature. A solution of this relaxation gives to each element of the dataset a probability to belong to a given cluster, whereas a solution of the standard modularity problem is a partition. We introduce an efficient optimization algorithm to solve this relaxation, that is both memory efficient and local. Furthermore, we prove that our method includes, as a special case, the Louvain optimization scheme, a state-of-the-art technique to solve the traditional modularity problem. Experiments on both synthetic and real-world data illustrate that our approach provides meaningful information on various types of data.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,323,332,,,,,,,,,,,,,,,,WOS:000509687900034,0
C,"Lacotte, J; Ghavamzadeh, M; Chow, YL; Pavone, M",,"Chaudhuri, K; Sugiyama, M",,"Lacotte, Jonathan; Ghavamzadeh, Mohammad; Chow, Yinlam; Pavone, Marco",,,Risk-Sensitive Generative Adversarial Imitation Learning,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We study risk-sensitive imitation learning where the agent's goal is to perform at least as well as the expert in terms of a risk profile. We first formulate our risk-sensitive imitation learning setting. We consider the generative adversarial approach to imitation learning (GAIL) and derive an optimization problem for our formulation, which we call it risk-sensitive GAIL (RS-GAIL). We then derive two different versions of our RS-GAIL optimization problem that aim at matching the risk profiles of the agent and the expert w.r.t. Jensen-Shannon (JS) divergence and Wasserstein distance, and develop risk-sensitive generative adversarial imitation learning algorithms based on these optimization problems. We evaluate the performance of our algorithms and compare them with GAIL and the risk-averse imitation learning (RAIL) algorithms in two MuJoCo and two OpenAI classical control tasks.",,,,,,"Pavone, Marco/0000-0002-0206-4337",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902021,0
C,"Pierson, E; Koh, PW; Hashimoto, T; Koller, D; Leskovec, J; Eriksson, N; Liang, P",,"Chaudhuri, K; Sugiyama, M",,"Pierson, Emma; Koh, Pang Wei; Hashimoto, Tatsunori; Koller, Daphne; Leskovec, Jure; Eriksson, Nicholas; Liang, Percy",,,Inferring Multidimensional Rates of Aging from Cross-Sectional Data,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Modeling how individuals evolve over time is a fundamental problem in the natural and social sciences. However, existing datasets are often cross-sectional with each individual observed only once, making it impossible to apply traditional time-series methods. Motivated by the study of human aging, we present an interpretable latent-variable model that learns temporal dynamics from cross-sectional data. Our model represents each individual's features over time as a nonlinear function of a low-dimensional, linearly-evolving latent state. We prove that when this nonlinear function is constrained to be order-isomorphic, the model family is identifiable solely from cross-sectional data provided the distribution of time-independent variation is known. On the UK Biobank human health dataset, our model reconstructs the observed data while learning interpretable rates of aging associated with diseases, mortality, and aging risk factors.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,97,107,,,,,,,,,,,31538144,,,,,WOS:000509687900011,0
C,"Vemula, A; Sun, W; Bagnell, JA",,"Chaudhuri, K; Sugiyama, M",,"Vemula, Anirudh; Sun, Wen; Bagnell, J. Andrew",,,Contrasting Exploration in Parameter and Action Space: A Zeroth-Order Optimization Perspective,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Black-box optimizers that explore in parameter space have often been shown to outperform more sophisticated action space exploration methods developed specifically for the reinforcement learning problem. We examine these black-box methods closely to identify situations in which they are worse than action space exploration methods and those in which they are superior. Through simple theoretical analyses, we prove that complexity of exploration in parameter space depends on the dimensionality of parameter space, while complexity of exploration in action space depends on both the dimensionality of action space and horizon length. This is also demonstrated empirically by comparing simple exploration methods on several model problems, including Contextual Bandit, Linear Regression and Reinforcement Learning in continuous control.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902100,0
C,"Vikram, S; Hoffman, MD; Johnson, MJ",,"Chaudhuri, K; Sugiyama, M",,"Vikram, Sharad; Hoffman, Matthew D.; Johnson, Matthew J.",,,The LORACs Prior for VAEs: Letting the Trees Speak for the Data,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"In variational autoencoders, the prior on the latent codes z is often treated as an afterthought, but the prior shapes the kind of latent representation that the model learns. If the goal is to learn a representation that is interpretable and useful, then the prior should reflect the ways in which the high-level factors that describe the data vary. The default prior is an isotropic normal, but if the natural factors of variation in the dataset exhibit discrete structure or are not independent, then the isotropic-normal prior will actually encourage learning representations that mask this structure. To alleviate this problem, we propose using a flexible Bayesian nonparametric hierarchical clustering prior based on the time-marginalized coalescent (TMC). To scale learning to large datasets, we develop a new inducing-point approximation and inference algorithm. We then apply the method without supervision to several datasets and examine the interpretability and practical performance of the inferred hierarchies and learned latent space.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687903036,0
C,"Wang, BY; Zhang, HJ; Liu, P; Shen, ZB; Pineau, J",,"Chaudhuri, K; Sugiyama, M",,"Wang, Boyu; Zhang, Hejia; Liu, Peng; Shen, Zebang; Pineau, Joelle",,,Multitask Metric Learning: Theory and Algorithm,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"In this paper, we study the problem of multi-task metric learning (mtML). We first examine the generalization bound of the regularized mtML formulation based on the notion of algorithmic stability, proving the convergence rate of mtML and revealing the trade-off between the tasks. Moreover, we also establish the theoretical connection between the mtML, single-task learning and pooling-task learning approaches. In addition, we present a novel boosting-based mtML (mt-BML) algorithm, which scales well with the feature dimension of the data. Finally, we also devise an efficient second-order Riemannian retraction operator which is tailored specifically to our mt-BML algorithm. It produces a low-rank solution of mtML to reduce the model complexity, and may also improve generalization performances. Extensive evaluations on several benchmark data sets verify the effectiveness of our learning algorithm.",,,,,,"Wang, Boyu/0000-0002-7413-4162",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687903043,0
C,"Wang, P; Paranamana, P; Shafto, P",,"Chaudhuri, K; Sugiyama, M",,"Wang, Pei; Paranamana, Pushpi; Shafto, Patrick",,,Generalizing the theory of cooperative inference,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Cooperative information sharing is important to theories of human learning and has potential implications for machine learning. Prior work derived conditions for achieving optimal Cooperative Inference given relatively restrictive assumptions. We demonstrate convergence for any discrete joint distribution, robustness through equivalence classes and stability under perturbation, and effectiveness by deriving bounds from structural properties of the original joint distribution. We provide geometric interpretations, connections to and implications for optimal transport and to importance sampling, and conclude by outlining open questions and challenges to realizing the promise of Cooperative Inference.",,,,,,"wang, pei/0000-0002-8866-7349",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901092,0
C,"Wu, MK; Goodman, N; Ermon, S",,"Chaudhuri, K; Sugiyama, M",,"Wu, Mike; Goodman, Noah; Ermon, Stefano",,,Differentiable Antithetic Sampling for Variance Reduction in Stochastic Variational Inference,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Stochastic optimization techniques are standard in variational inference algorithms. These methods estimate gradients by approximating expectations with independent Monte Carlo samples. In this paper, we explore a technique that uses correlated, but more representative, samples to reduce variance. Specifically, we show how to generate antithetic samples with sample moments that match the population moments of an underlying proposal distribution. Combining a differentiable antithetic sampler with modern stochastic variational inference, we showcase the effectiveness of this approach for learning a deep generative model. An implementation is available at https://github.com/mhw32/ antithetic-vae-public.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902095,0
C,"Abid, BK; Gower, RM",,"Storkey, A; PerezCruz, F",,"Abid, Brahim Khalil; Gower, Robert M.",,,Greedy stochastic algorithms for entropy-regularized optimal transport problems,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Optimal transport (OT) distances are finding evermore applications in machine learning and computer vision, but their wide spread use in larger-scale problems is impeded by their high computational cost. In this work we develop a family of fast and practical stochastic algorithms for solving the optimal transport problem with an entropic penalization. This work extends the recently developed Greenkhorn algorithm, in the sense that, the Greenkhorn algorithm is a limiting case of this family. We also provide a simple and general convergence theorem for all algorithms in the class, with rates that match the best known rates of Greenkorn and the Sinkhorn algorithm, and conclude with numerical experiments that show under what regime of penalization the new stochastic methods are faster than the aforementioned methods.",,,,,"Gower, Robert Mansel/Y-8838-2019","Gower, Robert Mansel/0000-0002-2268-9780",,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300157,0
C,"Diao, HA; Song, Z; Sun, W; Woodruff, DP",,"Storkey, A; PerezCruz, F",,"Diao, Huaian; Song, Zhao; Sun, Wen; Woodruff, David P.",,,Sketching for Kronecker Product Regression and P-splines,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"TENSORSKETCH is an oblivious linear sketch introduced in (Pagh, 2013) and later used in (Pham and Pagh, 2013) in the context of SVMs for polynomial kernels. It was shown in (Avron et al., 2014) that TENSORSKETCH provides a subspace embedding, and therefore can be used for canonical correlation analysis, low rank approximation, and principal component regression for the polynomial kernel. We take TENSORSKETCH outside of the context of polynomials kernels, and show its utility in applications in which the underlying design matrix is a Kronecker product of smaller matrices. This allows us to solve Kronecker product regression and non-negative Kronecker product regression, as well as regularized spline regression. Our main technical result is then in extending TENSORSKETCH to other norms. That is, TENSORSKETCH only provides input sparsity time for Kronecker product regression with respect to the 2-norm. We show how to solve Kronecker product regression with respect to the 1-norm in time sublinear in the time required for computing the Kronecker product, as well as for more general p-norms.",,,,,"Diao, Huaian/E-2996-2018","Diao, Huaian/0000-0002-3787-9608",,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300136,0
C,"Huang, CD; Yao, Y",,"Storkey, A; PerezCruz, F",,"Huang, Chendi; Yao, Yuan",,,A Unified Dynamic Approach to Sparse Model Selection,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Sparse model selection is ubiquitous from linear regression to graphical models where regularization paths, as a family of estimators upon the regularization parameter varying, are computed when the regularization parameter is unknown or decided data-adaptively. Traditional computational methods rely on solving a set of optimization problems where the regularization parameters are fixed on a grid that might be inefficient. In this paper, we introduce a simple iterative regularization path, which follows the dynamics of a sparse Mirror Descent algorithm or a generalization of Linearized Bregman Iterations with nonlinear loss. Its performance is competitive to glmnet with a further bias reduction. A path consistency theory is presented that under the Restricted Strong Convexity (RSC) and the Irrepresentable Condition (IRR), the path will first evolve in a subspace with no false positives and reach an estimator that is sign-consistent or of minimax optimal l(2) error rate. Early stopping regularization is required to prevent overfitting. Application examples are given in sparse logistic regression and Ising models for NIPS coauthorship.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300214,0
C,"Jahnichen, P; Wenzel, F; Kloft, M; Mandt, S",,"Storkey, A; PerezCruz, F",,"Jaehnichen, Patrick; Wenzel, Florian; Kloft, Marius; Mandt, Stephan",,,Scalable Generalized Dynamic Topic Models,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Dynamic topic models (DTMs) model the evolution of prevalent themes in literature, online media, and other forms of text over time. DTMs assume that word co-occurrence statistics change continuously and therefore impose continuous stochastic process priors on their model parameters. These dynamical priors make inference much harder than in regular topic models, and also limit scalability. In this paper, we present several new results around DTMs. First, we extend the class of tractable priors from Wiener processes to the generic class of Gaussian processes (GPs). This allows us to explore topics that develop smoothly over time, that have a long-term memory or are temporally concentrated (for event detection). Second, we show how to perform scalable approximate inference in these models based on ideas around stochastic variational inference and sparse Gaussian processes. This way we can train a rich family of DTMs to massive data. Our experiments on several large-scale datasets show that our generalized model allows us to find interesting patterns that were not accessible by previous approaches.",,,,,,"Wenzel, Florian/0000-0002-0368-2727",,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300149,0
C,"Mercado, P; Gautier, A; Tudisco, F; Hein, M",,"Storkey, A; PerezCruz, F",,"Mercado, Pedro; Gautier, Antoine; Tudisco, Francesco; Hein, Matthias",,,The Power Mean Laplacian for Multilayer Graph Clustering,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Multilayer graphs encode different kind of interactions between the same set of entities. When one wants to cluster such a multilayer graph, the natural question arises how one should merge the information from different layers. We introduce in this paper a one-parameter family of matrix power means for merging the Laplacians from different layers and analyze it in expectation in the stochastic block model. We show that this family allows to recover ground truth clusters under different settings and verify this in real world data. While computing the matrix power mean can be very expensive for large graphs, we introduce a numerical scheme to efficiently compute its eigenvectors for the case of large sparse graphs.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300191,0
C,"Nie, XK; Tian, XY; Taylor, J; Zou, J",,"Storkey, A; PerezCruz, F",,"Nie, Xinkun; Tian, Xiaoying; Taylor, Jonathan; Zou, James",,,Why Adaptively Collected Data Have Negative Bias and How to Correct for It,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"From scientific experiments to online A/B testing, the previously observed data often affects how future experiments are performed, which in turn affects which data will be collected. Such adaptivity introduces complex correlations between the data and the collection procedure. In this paper, we prove that when the data collection procedure satisfies natural conditions, then sample means of the data have systematic negative biases. As an example, consider an adaptive clinical trial where additional data points are more likely to be tested for treatments that show initial promise. Our surprising result implies that the average observed treatment effects would underestimate the true effects of each treatment. We quantitatively analyze the magnitude and behavior of this negative bias in a variety of settings. We also propose a novel debiasing algorithm based on selective inference techniques. In experiments, our method can effectively reduce bias and estimation error.",,,,,"Zhang, James/HHS-8616-2022",,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300132,0
C,"Sharir, O; Shashua, A",,"Storkey, A; PerezCruz, F",,"Sharir, Or; Shashua, Amnon",,,Sum-Product-Quotient Networks,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"We present a novel tractable generative model that extends Sum-Product Networks (SPNs) and significantly boosts their power. We call it Sum-Product-Quotient Networks (SPQNs), whose core concept is to incorporate conditional distributions into the model by direct computation using quotient nodes, e.g. P(A vertical bar B)= P(A,B)/P(B). We provide sufficient conditions for the tractability of SPQNs that generalize and relax the decomposable and complete tractability conditions of SPNs. These relaxed conditions give rise to an exponential boost to the expressive efficiency of our model, i.e. we prove that there are distributions which SPQNs can compute efficiently but require SPNs to be of exponential size. Thus, we narrow the gap in expressivity between tractable graphical models and other Neural Network-based generative models.",,,,,,"Sharir, Or/0000-0003-4957-8957",,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300056,0
C,"Wang, WL; Gan, Z; Wang, WQ; Shen, DH; Huang, JJ; Ping, W; Satheesh, S; Carin, L",,"Storkey, A; PerezCruz, F",,"Wang, Wenlin; Gan, Zhe; Wang, Wenqi; Shen, Dinghan; Huang, Jiaji; Ping, Wei; Satheesh, Sanjeev; Carin, Lawrence",,,Topic Compositional Neural Language Model,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"We propose a Topic Compositional Neural Language Model (TCNLM), a novel method designed to simultaneously capture both the global semantic meaning and the local word-ordering structure in a document. The TCNLM learns the global semantic coherence of a document via a neural topic model, and the probability of each learned latent topic is further used to build a Mixture-ofExperts (MoE) language model, where each expert (corresponding to one topic) is a recurrent neural network (RNN) that accounts for learning the local structure of a word sequence. In order to train the MoE model efficiently, a matrix factorization method is applied, by extending each weight matrix of the RNN to be an ensemble of topic-dependent weight matrices. The degree to which each member of the ensemble is used is tied to the document-dependent probability of the corresponding topics. Experimental results on several corpora show that the proposed approach outperforms both a pure RNN-based model and other topic-guided language models. Further, our model yields sensible topics, and also has the capacity to generate meaningful sentences conditioned on given topics.",,,,,,"Carin, Lawrence/0000-0001-6277-7948",,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300038,0
C,"Bhowmik, A; Liu, N; Zhong, EH; Bhaskar, BN; Rajan, S",,"Gretton, A; Robert, CC",,"Bhowmik, Avradeep; Liu, Nathan; Zhong, Erheng; Bhaskar, Badri Narayan; Rajan, Suju",,,Geometry Aware Mappings for High Dimensional Sparse Factors,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"While matrix factorisation models are ubiquitous in large scale recommendation and search, real time application of such models requires inner product computations over an intractably large set of item factors. In this manuscript we present a novel framework that uses the inverted index representation to exploit structural properties of sparse vectors to significantly reduce the run time computational cost of factorisation models. We develop techniques that use geometry aware permutation maps on a tessellated unit sphere to obtain high dimensional sparse embeddings for latent factors with sparsity patterns related to angular closeness of the original latent factors. We also design several efficient and deterministic realisations within this framework and demonstrate with experiments that our techniques lead to faster run time operation with minimal loss of accuracy.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,455,463,,,,,,,,,,,,,,,,WOS:000508662100050,0
C,"Carpentier, A; Schluter, T",,"Gretton, A; Robert, CC",,"Carpentier, Alexandra; Schlueter, Teresa",,,Learning relationships between data obtained independently,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"The aim of this paper is to provide a new method for learning the relationships between data that have been obtained independently. Unlike existing methods like matching, the proposed technique does not require any contextual information, provided that the dependency between the variables of interest is monotone. It can therefore be easily combined with matching in order to exploit the advantages of both methods. This technique can be described as a mix between quantile matching, and deconvolution. We provide for it a theoretical and an empirical validation.",,,,,"Carpentier, Alexandra/R-8179-2018","Carpentier, Alexandra/0000-0002-1194-7385",,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,658,666,,,,,,,,,,,,,,,,WOS:000508662100072,0
C,"Frellsen, J; Winther, O; Ghahramani, Z; Ferkinghoff-Borg, J",,"Gretton, A; Robert, CC",,"Frellsen, Jes; Winther, Ole; Ghahramani, Zoubin; Ferkinghoff-Borg, Jesper",,,Bayesian generalised ensemble Markov chain Monte Carlo,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Bayesian generalised ensemble (BayesGE) is a new method that addresses two major drawbacks of standard Markov chain Monte Carlo algorithms for inference in high-dimensional probability models: inapplicability to estimate the partition function and poor mixing properties. BayesGE uses a Bayesian approach to iteratively update the belief about the density of states (distribution of the log likelihood under the prior) for the model, with the dual purpose of enhancing the sampling efficiency and making the estimation of the partition function tractable. We benchmark BayesGE on Ising and Potts systems and show that it compares favourably to existing state-of-the-art methods.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,408,416,,,,,,,,,,,,,,,,WOS:000508662100045,0
C,"Goix, N; Sabourin, A; Clemencon, S",,"Gretton, A; Robert, CC",,"Goix, Nicolas; Sabourin, Anne; Clemencon, Stephan",,,Sparse Representation of Multivariate Extremes with Applications to Anomaly Ranking,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Extremes play a special role in Anomaly Detection. Beyond inference and simulation purposes, probabilistic tools borrowed from Extreme Value Theory (EVT), such as the angular measure, can also be used to design novel statistical learning methods for Anomaly Detection/ranking. This paper proposes a new algorithm based on multivariate EVT to learn how to rank observations in a high dimensional space with respect to their degree of 'abnormality'. The procedure relies on an original dimension-reduction technique in the extreme domain that possibly produces a sparse representation of multivariate extremes and allows to gain insight into the dependence structure thereof, escaping the curse of dimensionality. The representation output by the unsupervised methodology we propose here can be combined with any Anomaly Detection technique tailored to non-extreme data. As it performs linearly with the dimension and almost linearly in the data (in O(dn log n)), it fits to large scale problems. The approach in this paper is novel in that EVT has never been used in its multivariate version in the field of Anomaly Detection. Illustrative experimental results provide strong empirical evidence of the relevance of our approach.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,75,83,,,,,,,,,,,,,,,,WOS:000508662100009,0
C,"Goldstein, T; Taylor, G; Barabin, K; Sayre, K",,"Gretton, A; Robert, CC",,"Goldstein, Tom; Taylor, Gavin; Barabin, Kawika; Sayre, Kent",,,Unwrapping ADMM: Efficient Distributed Computing via Transpose Reduction,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Recent approaches to distributed model fitting rely heavily on consensus ADMM, where each node solves small sub-problems using only local data. We propose iterative methods that solve global sub-problems over an entire distributed dataset. This is possible using transpose reduction strategies that allow a single node to solve least-squares over massive datasets without putting all the data in one place. This results in simple iterative methods that avoid the expensive inner loops required for consensus methods. We analyze the convergence rates of the proposed schemes and demonstrate the efficiency of this approach by fitting linear classifiers and sparse linear models to large datasets using a distributed implementation with up to 20,000 cores in far less time than previous approaches.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,1151,1158,,,,,,,,,,,,,,,,WOS:000508662100125,0
C,"Li, CT; Jegelka, S; Sra, S",,"Gretton, A; Robert, CC",,"Li, Chengtao; Jegelka, Stefanie; Sra, Suvrit",,,Efficient Sampling for k-Determinantal Point Processes,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Determinantal Point Processes (DPPs) are elegant probabilistic models of repulsion and diversity over discrete sets of items. But their applicability to large sets is hindered by expensive cubic-complexity matrix operations for basic tasks such as sampling. In light of this, we propose a new method for approximate sampling from discrete k-DPPs. Our method takes advantage of the diversity property of subsets sampled from a DPP, and proceeds in two stages: first it constructs coresets for the ground set of items; thereafter, it efficiently samples subsets based on the constructed coresets. As opposed to previous approaches, our algorithm aims to minimize the total variation distance to the original distribution. Experiments on both synthetic and real datasets indicate that our sampling algorithm works efficiently on large data sets, and yields more accurate samples than previous approaches.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,1328,1337,,,,,,,,,,,,,,,,WOS:000508662100144,0
C,"Ni, RK; Gu, QQ",,"Gretton, A; Robert, CC",,"Ni, Renkun; Gu, Quanquan",,,Optimal Statistical and Computational Rates for One Bit Matrix Completion,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"We consider one bit matrix completion under rank constraint. We present an estimator based on rank constrained maximum likelihood estimation, and an efficient greedy algorithm to solve it approximately based on an extension of conditional gradient descent. The output of the proposed algorithm converges at a linear rate to the underlying true low-rank matrix up to the optimal statistical estimation error rate, i.e., O(root rn log(n)/|Omega|), where n is the dimension of the underlying matrix and |Omega| is the number of observed entries. Our work establishes the first computationally efficient approach with provable guarantee for optimal estimation in one bit matrix completion. Our theory is supported by thorough numerical results.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,426,434,,,,,,,,,,,,,,,,WOS:000508662100047,0
C,"Rashwan, A; Zhao, H; Poupart, P",,"Gretton, A; Robert, CC",,"Rashwan, Abdullah; Zhao, Han; Poupart, Pascal",,,Online and Distributed Bayesian Moment Matching for Parameter Learning in Sum-Product Networks,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Probabilistic graphical models provide a general and flexible framework for reasoning about complex dependencies in noisy domains with many variables. Among the various types of probabilistic graphical models, sum-product networks (SPNs) have recently generated some interest because exact inference can always be done in linear time with respect to the size of the network. This is particularly attractive since it means that learning an SPN from data always yields a tractable model for inference. However, existing parameter learning algorithms for SPNs operate in batch mode and do not scale easily to large datasets. In this work, we explore online algorithms to ensure that parameter learning can also be done tractably with respect to the amount of data. More specifically, we propose a new Bayesian moment matching (BMM) algorithm that operates naturally in an online fashion and that can be easily distributed. We demonstrate the effectiveness and scalability of BMM in comparison to online extensions of gradient descent, exponentiated gradient and expectation maximization on 20 classic benchmarks and 4 large scale datasets.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,1469,1477,,,,,,,,,,,,,,,,WOS:000508662100159,0
C,"van der Zander, B; Liskiewicz, M",,"Gretton, A; Robert, CC",,"van der Zander, Benito; Liskiewicz, Maciej",,,On Searching for Generalized Instrumental Variables,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Instrumental Variables are a popular way to identify the direct causal effect of a random variable X on a variable Y. Often no single instrumental variable exists, although it is still possible to find a set of generalized instrumental variables (GIVs) and identify the causal effect of all these variables at once. Till now it was not known how to find GIVs systematically or even test efficiently, if given variables satisfy GIV conditions. We provide fast algorithms for searching and testing restricted cases of GIVs. However, we prove that in the most general case it is NP-hard to verify if given variables fulfill the conditions of a general instrumental sets",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,1214,1222,,,,,,,,,,,,,,,,WOS:000508662100132,0
C,"Wei, XK; Yu, PS",,"Gretton, A; Robert, CC",,"Wei, Xiaokai; Yu, Philip S.",,,Unsupervised Feature Selection by Preserving Stochastic Neighbors,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Feature selection is an important technique for alleviating the curse of dimensionality. Unsupervised feature selection is more challenging than its supervised counterpart due to the lack of labels. In this paper, we present an effective method, Stochastic Neighbor-preserving Feature Selection (SNFS), for selecting discriminative features in unsupervised setting. We employ the concept of stochastic neighbors and select the features that can best preserve such stochastic neighbors by minimizing the Kullback-Leibler (KL) Divergence between neighborhood distributions. The proposed approach measures feature utility jointly in a nonlinear way and discriminative features can be selected due to its 'push-pull' property. We develop an efficient algorithm for optimizing the objective function based on projected quasi-Newton method. Moreover, few existing methods provide ways for determining the optimal number of selected features and this hampers their utility in practice. Our approach is equipped with a guideline for choosing the number of features, which provides nearly optimal performance in our experiments. Experimental results show that the proposed method outperforms state-of-the-art methods significantly on several real-world datasets.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,995,1003,,,,,,,,,,,,,,,,WOS:000508662100108,0
C,"Durante, D; Dunson, DB",,"Kaski, S; Corander, J",,"Durante, Daniele; Dunson, David B.",,,Bayesian Logistic Gaussian Process Models for Dynamic Networks,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"Time-varying adjacency matrices encoding the presence or absence of a relation among entities are available in many research fields. Motivated by an application to studying dynamic networks among sports teams, we propose a Bayesian nonparametric model. The proposed approach uses a logistic mapping from the probability matrix, encoding link probabilities between each team, to an embedded latent relational space. Within this latent space, we incorporate a dictionary of Gaussian process (GP) latent trajectories characterizing changes over time in each team, while allowing learning of the number of latent dimensions through a specially tailored prior for the GP covariance. The model is provably flexible and borrows strength across the network and over time. We provide simulation experiments and an application to the Italian soccer Championship.",,,,,"Durante, Daniele/O-8277-2017","Durante, Daniele/0000-0002-8595-6719",,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,194,201,,,,,,,,,,,,,,,,WOS:000508355800022,0
C,"Kaser, T; Schwing, AG; Hazan, T; Gross, M",,"Kaski, S; Corander, J",,"Kaeser, Tanja; Schwing, Alexander G.; Hazan, Tamir; Gross, Markus",,,Computational Education using Latent Structured Prediction,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"Computational education offers an important add-on to conventional teaching. To provide optimal learning conditions, accurate representation of students' current skills and adaptation to newly acquired knowledge are essential. To obtain sufficient representational power we investigate suitability of general graphical models and discuss adaptation by learning parameters of a log-linear distribution. For interpretability we propose to constrain the parameter space a-priori by leveraging domain knowledge. We show the benefits of general graphical models and of regularizing the parameter space by evaluation of our models on data collected from a computational education software for children having difficulties in learning mathematics.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,540,548,,,,,,,,,,,,,,,,WOS:000508355800060,0
C,"Mohan, K; Pearl, J",,"Kaski, S; Corander, J",,"Mohan, Karthika; Pearl, Judea",,,On the Testability of Models with Missing Data,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"Graphical models that depict the process by which data are lost are helpful in recovering information from missing data. We address the question of whether any such model can be submitted to a statistical test given that the data available are corrupted by missingness. We present sufficient conditions for testability in missing data applications and note the impediments for testability when data are contaminated by missing entries. Our results strengthen the available tests for MCAR and MAR and further provide tests in the category of MNAR. Furthermore, we provide sufficient conditions to detect the existence of dependence between a variable and its missingness mechanism. We use our results to show that model sensitivity persists in almost all models typically categorized as MNAR.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,643,650,,,,,,,,,,,,,,,,WOS:000508355800071,0
C,"Oliva, JB; Neiswanger, W; Poczos, B; Schneider, J; Xing, E",,"Kaski, S; Corander, J",,"Oliva, Junier B.; Neiswanger, Willie; Poczos, Barnabas; Schneider, Jeff; Xing, Eric",,,Fast Distribution To Real Regression,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"We study the problem of distribution to real regression, where one aims to regress a mapping f that takes in a distribution input covariate P is an element of I (for a non-parametric family of distributions I) and outputs a real-valued response Y f(P) + epsilon. This setting was recently studied in [15], where the Kernel-Kernel estimator was introduced and shown to have a polynomial rate of convergence, However, evaluating a new prediction with the Kernel-Kernel estimator scales as Omega(N). This causes the difficult situation where a large amount of data may be necessary for a low estimation risk, but the computation cost of estimation becomes infeasible when the data-set is too large. To this end, we propose the Double-Basis estimator, which looks to alleviate this big data problem in two ways: first, the Double-Basis estimator is shown to have a computation complexity that is independent of the number of of instances N when evaluating new predictions after training; secondly, the Double-Basis estimator is shown to have a fast rate of convergence for a general class of mappings f is an element of F.",,,,,,"Oliva, Junier/0000-0002-2601-5652",,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,706,714,,,,,,,,,,,,,,,,WOS:000508355800078,0
C,"Yogatama, D; Mann, G",,"Kaski, S; Corander, J",,"Yogatama, Dani; Mann, Gideon",,,Efficient Transfer Learning Method for Automatic Hyperparameter Tuning,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"We propose a fast and effective algorithm for automatic hyperparameter tuning that can generalize across datasets. Our method is an instance of sequential model-based optimization (SMBO) that transfers information by constructing a common response surface for all datasets, similar to Bardenet et al. (2013). The time complexity of reconstructing the response surface at every SMBO iteration in our method is linear in the number of trials (significantly less than previous work with comparable performance), allowing the method to realistically scale to many more datasets. Specifically, we use deviations from the per-dataset mean as the response values. We empirically show the superiority of our method on a large number of synthetic and real-world datasets for tuning hyperparameters of logistic regression and ensembles of classifiers.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,1077,1085,,,,,,,,,,,,,,,,WOS:000508355800119,0
C,"Cohen, S; Luise, G; Terenin, A; Amos, B; Deisenroth, MP",,"Banerjee, A; Fukumizu, K",,"Cohen, Samuel; Luise, Giulia; Terenin, Alexander; Amos, Brandon; Deisenroth, Marc Peter",,,Aligning Time Series on Incomparable Spaces,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Dynamic time warping (DTW) is a useful method for aligning, comparing and combining time series, but it requires them to live in comparable spaces. In this work, we consider a setting in which time series live on different spaces without a sensible ground metric, causing DTW to become ill-defined. To alleviate this, we propose Gromov dynamic time warping (GDTW), a distance between time series on potentially incomparable spaces that avoids the comparability requirement by instead considering intra-relational geometry. We demonstrate its effectiveness at aligning, combining and comparing time series living on incomparable spaces. We further propose a smoothed version of GDTW as a differentiable loss and assess its properties in a variety of settings, including barycentric averaging, generative modeling and imitation learning.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801029,0
C,"Curth, A; van der Schaar, M",,"Banerjee, A; Fukumizu, K",,"Curth, Alicia; van der Schaar, Mihaela",,,Nonparametric Estimation of Heterogeneous Treatment Effects: From Theory to Learning Algorithms,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"The need to evaluate treatment effectiveness is ubiquitous in most of empirical science, and interest in flexibly investigating effect heterogeneity is growing rapidly. To do so, a multitude of model-agnostic, nonparametric meta-learners have been proposed in recent years. Such learners decompose the treatment effect estimation problem into separate sub-problems, each solvable using standard supervised learning methods. Choosing between different meta-learners in a data-driven manner is difficult, as it requires access to counterfactual information. Therefore, with the ultimate goal of building better understanding of the conditions under which some learners can be expected to perform better than others a priori, we theoretically analyze four broad meta-learning strategies which rely on plug-in estimation and pseudo-outcome regression. We highlight how this theoretical reasoning can be used to guide principled algorithm design and translate our analyses into practice by considering a variety of neural network architectures as base-learners for the discussed metalearning strategies. In a simulation study, we showcase the relative strengths of the learners under different data-generating processes.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802021,0
C,"De Souza, DA; Mesquita, D; Mattos, CL; Gomes, JP",,"Banerjee, A; Fukumizu, K",,"de Souza, Daniel Augusto; Mesquita, Diego; Mattos, Cesar Lincoln; Gomes, Joao Paulo",,,Learning GPLVM with arbitrary kernels using the unscented transformation,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Gaussian Process Latent Variable Model (GPLVM) is a flexible framework to handle uncertain inputs in Gaussian Processes (GPs) and incorporate GPs as components of larger graphical models. Nonetheless, the standard GPLVM variational inference approach is tractable only for a narrow family of kernel functions. The most popular implementations of GPLVM circumvent this limitation using quadrature methods, which may become a computational bottleneck even for relatively low dimensions. For instance, the widely employed Gauss-Hermite quadrature has exponential complexity on the number of dimensions. In this work, we propose using the unscented transformation instead. Overall, this method presents comparable, if not better, performance than off-the-shelf solutions to GPLVM, and its computational complexity scales only linearly on dimension. In contrast to Monte Carlo methods, our approach is deterministic and works well with quasi-Newton methods, such as the Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm. We illustrate the applicability of our method with experiments on dimensionality reduction and multistep-ahead prediction with uncertainty propagation.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,451,+,,,,,,,,,,,,,,,,WOS:000659893800051,0
C,"Fisher, MA; Nolan, TH; Graham, MM; Prangle, D; Oates, CJ",,"Banerjee, A; Fukumizu, K",,"Fisher, Matthew A.; Nolan, Tui H.; Graham, Matthew M.; Prangle, Dennis; Oates, Chris J.",,,Measure Transport with Kernel Stein Discrepancy,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Measure transport underpins several recent algorithms for posterior approximation in the Bayesian context, wherein a transport map is sought to minimise the Kullback-Leibler divergence (KLD) from the posterior to the approximation. The KLD is a strong mode of convergence, requiring absolute continuity of measures and placing restrictions on which transport maps can be permitted. Here we propose to minimise a kernel Stein discrepancy (KSD) instead, requiring only that the set of transport maps is dense in an L-2 sense and demonstrating how this condition can be validated. The consistency of the associated posterior approximation is established and empirical results suggest that KSD is a competitive and more flexible alternative to KLD for measure transport.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801031,0
C,"Gan, K; Li, AA; Lipton, ZC; Tayur, S",,"Banerjee, A; Fukumizu, K",,"Gan, Kyra; Li, Andrew A.; Lipton, Zachary C.; Tayur, Sridhar",,,Causal Inference with Selectively Deconfounded Data,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Given only data generated by a standard confounding graph with unobserved confounder, the Average Treatment Effect (ATE) is not identifiable. To estimate the ATE, a practitioner must then either (a) collect deconfounded data; (b) run a clinical trial; or (c) elucidate further properties of the causal graph that might render the ATE identifiable. In this paper, we consider the benefit of incorporating a large confounded observational dataset (confounder unobserved) alongside a small deconfounded observational dataset (confounder revealed) when estimating the ATE. Our theoretical results suggest that the inclusion of confounded data can significantly reduce the quantity of deconfounded data required to estimate the ATE to within a desired accuracy level. Moreover, in some cases-say, genetics-we could imagine retrospectively selecting samples to deconfound. We demonstrate that by actively selecting these samples based upon the (already observed) treatment and outcome, we can reduce sample complexity further. Our theoretical and empirical results establish that the worst-case relative performance of our approach (vs. a natural benchmark) is bounded while our best-case gains are unbounded. Finally, we demonstrate the benefits of selective deconfounding using a large real-world dataset related to genetic mutation in cancer.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803037,0
C,"Green, A; Balakrishnan, S; Tibshirani, RJ",,"Banerjee, A; Fukumizu, K",,"Green, Alden; Balakrishnan, Sivaraman; Tibshirani, Ryan J.",,,Minimax Optimal Regression over Sobolev Spaces via Laplacian Regularization on Neighborhood Graphs,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"In this paper we study the statistical properties of Laplacian smoothing, a graph-based approach to nonparametric regression. Under standard regularity conditions, we establish upper bounds on the error of the Laplacian smoothing estimator (f) over cap, and a goodness-of-fit test also based on (f) over cap. These upper bounds match the minimax optimal estimation and testing rates of convergence over the first-order Sobolev class H-1 (X), for X subset of R-d and 1 <= d < 4; in the estimation problem, for d = 4, they are optimal modulo a log n factor. Additionally, we prove that Laplacian smoothing is manifold-adaptive: if X subset of R-d is an m-dimensional manifold with m < d, then the error rate of Laplacian smoothing (in either estimation or testing) depends only on m, in the same way it would if X were a full-dimensional set in R-m.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803016,0
C,"Horii, S",,"Banerjee, A; Fukumizu, K",,"Horii, Shunsuke",,,Bayesian Model Averaging for Causality Estimation and its Approximation based on Gaussian Scale Mixture Distributions,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"In the estimation of the causal effect under linear Structural Causal Models (SCMs), it is common practice to first identify the causal structure, estimate the probability distributions, and then calculate the causal effect. However, if the goal is to estimate the causal effect, it is not necessary to fix a single causal structure or probability distributions. In this paper, we first show from a Bayesian perspective that it is Bayes optimal to weight (average) the causal effects estimated under each model rather than estimating the causal effect under a fixed single model. This idea is also known as Bayesian model averaging. Although the Bayesian model averaging is optimal, as the number of candidate models increases, the weighting calculations become computationally hard. We develop an approximation to the Bayes optimal estimator by using Gaussian scale mixture distributions.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801020,0
C,"Karimi, MR; Gurel, NM; Karlas, B; Rausch, J; Zhang, C; Krause, A",,"Banerjee, A; Fukumizu, K",,"Karimi, Mohammad Reza; Guerel, Nezihe Merve; Karlas, Bojan; Rausch, Johannes; Zhang, Ce; Krause, Andreas",,,Online Active Model Selection for Pre-trained Classifiers,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Given k pre-trained classifiers and a stream of unlabeled data examples, how can we actively decide when to query a label so that we can distinguish the best model from the rest while making a small number of queries? Answering this question has a profound impact on a range of practical scenarios. In this work, we design an online selective sampling approach that actively selects informative examples to label and outputs the best model with high probability at any round. Our algorithm can be used for online prediction tasks for both adversarial and stochastic streams. We establish several theoretical guarantees for our algorithm and extensively demonstrate its effectiveness in our experimental studies.",,,,,,"Zhang, Ce/0000-0002-8105-7505",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,307,+,,,,,,,,,,,,,,,,WOS:000659893800035,0
C,"Kwon, Y; Rivas, MA; Zou, J",,"Banerjee, A; Fukumizu, K",,"Kwon, Yongchan; Rivas, Manuel A.; Zou, James",,,Efficient Computation and Analysis of Distributional Shapley Values,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Distributional data Shapley value (DShapley) has recently been proposed as a principled framework to quantify the contribution of individual datum in machine learning. DShapley develops the foundational game theory concept of Shapley values into a statistical framework and can be applied to identify data points that are useful (or harmful) to a learning algorithm. Estimating DShapley is computationally expensive, however, and this can be a major challenge to using it in practice. Moreover, there has been little mathematical analyses of how this value depends on data characteristics. In this paper, we derive the first analytic expressions for DShapley for the canonical problems of linear regression, binary classification, and non-parametric density estimation. These analytic forms provide new algorithms to estimate DShapley that are several orders of magnitude faster than previous state-of-the-art methods. Furthermore, our formulas are directly interpretable and provide quantitative insights into how the value varies for different types of data. We demonstrate the practical efficacy of our approach on multiple real and synthetic datasets.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801002,0
C,"Li, XY; Li, P",,"Banerjee, A; Fukumizu, K",,"Li, Xiaoyun; Li, Ping",,,One-Sketch-for-All: Non-linear Random Features from Compressed Linear Measurements,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"RFF (random Fourier features) is a popular technique for approximating the commonly used Gaussian kernel. Due to the crucial tuning parameter gamma in the Gaussian kernel, the design of effective quantization schemes for RFF appears to be challenging. Intuitively one would expect that a different quantizer is needed for a different gamma value (and we need to store a different set of quantized data for each gamma). Interestingly, the recent work (Li and Li, 2021) showed that only one Lloyd-Max (LM) quantizer is needed by showing that the marginal distribution of RFF is free of the tuning parameter gamma. On the other hand, Li and Li (2021) still required to store a different set of quantized data for each gamma value. In this paper, we adopt the one-sketch-forall paradigm for quantizing RFFs. Basically, we only store one set of quantized linear sketches after applying random projections on the original data. From the same set of quantized data, we construct RFFs to approximate Gaussian kernels for any tuning parameter gamma. Compared with Li and Li (2021), our proposed one-sketch-for-all scheme would inevitably lose some accuracy as one should expect. Nevertheless, our proposed method still performs noticeably better than other quantization algorithms such as stochastic rounding. We provide statistical analysis on properties of the proposed quantization method, and conduct experiments to empirically illustrate its effectiveness.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803021,0
C,"Oymak, S; Gulcu, TC",,"Banerjee, A; Fukumizu, K",,"Oymak, Samet; Gulcu, Talha Cihad",,,A Theoretical Characterization of Semi-supervised Learning with Self-training for Gaussian Mixture Models,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Self-training is a classical approach in semi-supervised learning which is successfully applied to a variety of machine learning problems. Self-training algorithms generate pseudo-labels for the unlabeled examples and progressively refine these pseudo-labels which hopefully coincides with the actual labels. This work provides theoretical insights into self-training algorithms with a focus on linear classifiers. First, we provide a sample complexity analysis for Gaussian mixture models with two components. This is established by sharp non-asymptotic characterization of the self-training iterations which captures the evolution of the model accuracy in terms of a fixed-point iteration. Our analysis reveals the provable benefits of rejecting samples with low confidence and demonstrates how self-training iterations can gracefully improve the model accuracy. Secondly, we study a generalized GMM where the component means follow a distribution. We demonstrate that ridge regularization and class margin (i.e. separation between the component means) is crucial for the success and lack of regularization may prevent self-training from identifying the core features in the data.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804035,0
C,"Royer, M; Chazal, F; Levrard, C; Yuhei, U; Yuichi, I",,"Banerjee, A; Fukumizu, K",,"Royer, Martin; Chazal, Frederic; Levrard, Clement; Yuhei, Umeda; Yuichi, Ike",,,ATOL: Measure Vectorization for Automatic Topologically-Oriented Learning,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Robust topological information commonly comes in the form of a set of persistence diagrams, finite measures that are in nature uneasy to affix to generic machine learning frameworks. We introduce a fast, learnt, unsupervised vectorization method for measures in Euclidean spaces and use it for reflecting underlying changes in topological behaviour in machine learning contexts. The algorithm is simple and efficiently discriminates important space regions where meaningful differences to the mean measure arise. It is proven to be able to separate clusters of persistence diagrams. We showcase the strength and robustness of our approach on a number of applications, from emulous and modern graph collections where the method reaches state-of-the-art performance to a geometric synthetic dynamical orbits problem. The proposed methodology comes with a single high level tuning parameter: the total measure encoding budget. We provide a completely open access software.",,,,,,"Ike, Yuichi/0000-0002-8907-8319",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801025,0
C,"Ruan, YC; Zhang, XX; Liang, SC; Joe, CRLEW",,"Banerjee, A; Fukumizu, K",,"Ruan, Yichen; Zhang, Xiaoxi; Liang, Shu-Che; Joe-Wong, Carlee",,,Towards Flexible Device Participation in Federated Learning,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Traditional federated learning algorithms impose strict requirements on the participation rates of devices, which limit the potential reach of federated learning. This paper extends the current learning paradigm to include devices that may become inactive, compute incomplete updates, and depart or arrive in the middle of training. We derive analytical results to illustrate how allowing more flexible device participation can affect the learning convergence when data is not independently and identically distributed (non-IID). We then propose a new federated aggregation scheme that converges even when devices may be inactive or return incomplete updates. We also study how the learning process can adapt to early departures or late arrivals, and analyze their impacts on the convergence.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804013,0
C,"Turner, P; Liu, JB; Rigollet, P",,"Banerjee, A; Fukumizu, K",,"Turner, Paxton; Liu, Jingbo; Rigollet, Philippe",,,A Statistical Perspective on Coreset Density Estimation,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Coresets have emerged as a powerful tool to summarize data by selecting a small subset of the original observations while retaining most of its information. This approach has led to significant computational speedups but the performance of statistical procedures run on coresets is largely unexplored. In this work, we develop a statistical framework to study coresets and focus on the canonical task of nonparameteric density estimation. Our contributions are twofold. First, we establish the minimax rate of estimation achievable by coreset-based estimators. Second, we show that the practical coreset kernel density estimators are near-minimax optimal over a large class of Holder-smooth densities.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803006,0
C,"Wildner, C; Koeppl, H",,"Banerjee, A; Fukumizu, K",,"Wildner, Christian; Koeppl, Heinz",,,Moment-Based Variational Inference for Stochastic Differential Equations,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Existing deterministic variational inference approaches for diffusion processes use simple proposals and target the marginal density of the posterior. We construct the variational process as a controlled version of the prior process and approximate the posterior by a set of moment functions. In combination with moment closure, the smoothing problem is reduced to a deterministic optimal control problem. Exploiting the path-wise Fisher information, we propose an optimization procedure that corresponds to a natural gradient descent in the variational parameters. Our approach allows for richer variational approximations that extend to state-dependent diffusion terms. The classical Gaussian process approximation is recovered as a special case.",,,,,,"Wildner, Christian/0000-0003-2845-7507",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802033,0
C,"Xu, C; Alaa, AM; Bica, I; Ershoff, BD; Cannesson, M; van der Schaar, M",,"Banerjee, A; Fukumizu, K",,"Xu, Can; Alaa, Ahmed M.; Bica, Ioana; Ershoff, Brent D.; Cannesson, Maxime; van der Schaar, Mihaela",,,Learning Matching Representations for Individualized Organ Transplantation Allocation,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Organ transplantation is often the last resort for treating end-stage illness, but the probability of a successful transplantation depends greatly on compatibility between donors and recipients. Current medical practice relies on coarse rules for donor-recipient matching, but is short of domain knowledge regarding the complex factors underlying organ compatibility. In this paper, we formulate the problem of learning data-driven rules for organ matching using observational data for organ allocations and transplant outcomes. This problem departs from the standard supervised learning setup in that it involves matching the two feature spaces (i.e., donors and recipients), and requires estimating transplant outcomes under counterfactual matches not observed in the data. To address these problems, we propose a model based on representation learning to predict donor-recipient compatibility; our model learns representations that cluster donor features, and applies donor-invariant transformations to recipient features to predict outcomes for a given donor-recipient feature instance. Experiments on semi-synthetic and real-world datasets show that our model outperforms state-of-art allocation methods and policies executed by human experts.",,,,,,"Xu, Can/0000-0002-5103-8688",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802057,0
C,"Yang, JQ",,"Banerjee, A; Fukumizu, K",,"Yang, Jiaqi",,,Fully Gap-Dependent Bounds for Multinomial Logit Bandit,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We study the multinomial logit (MNL) bandit problem, where at each time step, the seller offers an assortment of size at most K from a pool of N items, and the buyer purchases an item from the assortment according to a MNL choice model. The objective is to learn the model parameters and maximize the expected revenue. We present (i) an algorithm that identifies the optimal assortment S* within (O) over tilde(Sigma(N)(i=1) Delta(-2)(i)) time steps with high probability, and (ii) an algorithm that incurs O(Sigma(i is not an element of S*) K Delta(-1)(i) log T) regret in T time steps. To our knowledge, our algorithms are the first to achieve gap-dependent bounds that fully depends on the suboptimality gaps of all items. Our technical contributions include an algorithmic framework that relates the MNL-bandit problem to a variant of the top-K arm identification problem in multiarmed bandits, a generalized epoch-based offering procedure, and a layer-based adaptive estimation procedure.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,199,207,,,,,,,,,,,,,,,,WOS:000659893800023,0
C,"Zhou, F; Li, P; Zhou, ZX",,"Banerjee, A; Fukumizu, K",,"Zhou, Fan; Li, Ping; Zhou, Zhixin",,,Principal Subspace Estimation Under Information Diffusion,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Let A = L-0 + S-0, where L-0 is an element of R-dxd is low rank and S-0 is a perturbation matrix. We study the principal subspace estimation of L0 through observations y(j) = f (A)xj, j = 1, ... , n, where f : R -> R is an unknown polynomial and x(j) 's are i.i.d. random input signals. Such models are widely used in graph signal processing to model information diffusion dynamics over networks with applications in network topology inference and data analysis. We develop an estimation procedure based on nuclear norm penalization, and establish upper bounds on the principal subspace estimation error when A is the adjacency matrix of a random graph generated by L-0. Our theory shows that when the signal strength is strong enough, the exact rank of L-0 can be recovered. By applying our results to blind community detection, we show that consistency of spectral clustering can be achieved for some popular stochastic block models. Together with the experimental results, our theory show that there is a fundamental limit of using the principal components obtained from diffused graph signals which is commonly adapted in current practice. Finally, under some structured perturbation S-0, we build the connection between this model with spiked covariance model and develop a new estimation procedure. We show that such estimators can be optimal under the minimax paradigm.",,,,,"Zhou, Zhixin/GSD-7016-2022","ZHOU, Zhixin/0000-0003-3737-9248",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803083,0
C,"Ahmadian, S; Chatziafratis, V; Epasto, A; Lee, E; Mahdian, M; Makarychev, K; Yaroslavtsev, G",,"Chiappa, S; Calandra, R",,"Ahmadian, Sara; Chatziafratis, Vaggos; Epasto, Alessandro; Lee, Euiwoong; Mahdian, Mohammad; Makarychev, Konstantin; Yaroslavtsev, Grigory",,,Bisect and Conquer: Hierarchical Clustering via Max-Uncut Bisection,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Hierarchical Clustering is an unsupervised data analysis method which has been widely used for decades. Despite its popularity, it had an underdeveloped analytical foundation and to address this, Dasgupta recently introduced an optimization viewpoint of hierarchical clustering with pair-wise similarity information that spurred a line of work shedding light on old algorithms (e.g., Average-Linkage), but also designing new algorithms. Here, for the maximization dual of Dasgupta's objective (introduced by Moseley-Wang), we present polynomial-time 0:4246 approximation algorithms that use MAX-UNCUT Bisection as a subroutine. The previous best worst-case approximation factor in polynomial time was 0:336, improving only slightly over Average-Linkage which achieves 1/3. Finally, we complement our positive results by providing APX-hardness (even for 0-1 similarities), under the Small Set Expansion hypothesis.",,,,,"Makarychev, Konstantin/P-6054-2017",,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3121,3131,,,,,,,,,,,,,,,,WOS:000559931300052,0
C,"Carriere, M; Chazal, F; Ike, Y; Lacombe, T; Royer, M; Umeda, Y",,"Chiappa, S; Calandra, R",,"Carriere, Mathieu; Chazal, Frederic; Ike, Yuichi; Lacombe, Theo; Royer, Martin; Umeda, Yuhei",,,PersLay: A Neural Network Layer for Persistence Diagrams and New Graph Topological Signatures,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Persistence diagrams, the most common descriptors of Topological Data Analysis, encode topological properties of data and have already proved pivotal in many different applications of data science. However, since the metric space of persistence diagrams is not Hilbert, they end up being difficult inputs for most Machine Learning techniques. To address this concern, several vectorization methods have been put forward that embed persistence diagrams into either finite-dimensional Euclidean space or implicit infinite dimensional Hilbert space with kernels. In this work, we focus on persistence diagrams built on top of graphs. Relying on extended persistence theory and the so-called heat kernel signature, we show how graphs can be encoded by (extended) persistence diagrams in a provably stable way. We then propose a general and versatile framework for learning vectorizations of persistence diagrams, which encompasses most of the vectorization techniques used in the literature. We finally showcase the experimental strength of our setup by achieving competitive scores on classification tasks on real-life graph datasets.",,,,,,"Ike, Yuichi/0000-0002-8907-8319",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2786,2795,,,,,,,,,,,,,,,,WOS:000559931300045,0
C,"Chatterji, NS; Diakonikolas, J; Jordan, MI; Bartlett, PL",,"Chiappa, S; Calandra, R",,"Chatterji, Niladri S.; Diakonikolas, Jelena; Jordan, Michael I.; Bartlett, Peter L.",,,Langevin Monte Carlo without smoothness,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Langevin Monte Carlo (LMC) is an iterative algorithm used to generate samples from a distribution that is known only up to a normalizing constant. The nonasymptotic dependence of its mixing time on the dimension and target accuracy is understood mainly in the setting of smooth (gradient-Lipschitz) log-densities, a serious limitation for applications in machine learning. In this paper, we remove this limitation, providing polynomial-time convergence guarantees for a variant of LMC in the setting of nonsmooth log-concave distributions. At a high level, our results follow by leveraging the implicit smoothing of the log-density that comes from a small Gaussian perturbation that we add to the iterates of the algorithm and controlling the bias and variance that are induced by this perturbation.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1716,1725,,,,,,,,,,,,,,,,WOS:000559931300049,0
C,"Efremenko, K; Kontorovich, A; Noivirt, M",,"Chiappa, S; Calandra, R",,"Efremenko, Klim; Kontorovich, Aryeh; Noivirt, Moshe",,,Fast and Bayes-consistent nearest neighbors,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Research on nearest-neighbor methods tends to focus somewhat dichotomously either on the statistical or the computational aspects - either on, say, Bayes consistency and rates of convergence or on techniques for speeding up the proximity search. This paper aims at bridging these realms: to reap the advantages of fast evaluation time while maintaining Bayes consistency, and further without sacrificing too much in the risk decay rate. We combine the locality-sensitive hashing (LSH) technique with a novel missing-mass argument to obtain a fast and Bayes-consistent classifier. Our algorithm's prediction runtime compares favorably against state of the art approximate NN methods, while maintaining Bayes-consistency and attaining rates comparable to minimax. On samples of size n in R-d, our pre-processing phase has runtime O(dn log n), while the evaluation phase has runtime O(d log n) per query point.",,,,,,"Efremenko, Klim/0000-0003-3280-3927",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1276,1285,,,,,,,,,,,,,,,,WOS:000559931300083,0
C,"Fallah, A; Mokhtari, A; Ozdaglar, A",,"Chiappa, S; Calandra, R",,"Fallah, Alireza; Mokhtari, Aryan; Ozdaglar, Asuman",,,On the Convergence Theory of Gradient-Based Model-Agnostic Meta-Learning Algorithms,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We study the convergence of a class of gradient-based Model-Agnostic Meta-Learning (MAML) methods and characterize their overall complexity as well as their best achievable accuracy in terms of gradient norm for nonconvex loss functions. We start with the MAML method and its first-order approximation (FO-MAML) and high-light the challenges that emerge in their analysis. By overcoming these challenges not only we provide the first theoretical guarantees for MAML and FO-MAML in nonconvex settings, but also we answer some of the unanswered questions for the implementation of these algorithms including how to choose their learning rate and the batch size for both tasks and datasets corresponding to tasks. In particular, we show that MAML can find an epsilon-first-order stationary point (epsilon-FOSP) for any positive epsilon after at most O(1/epsilon(2)) iterations at the expense of requiring second-order information. We also show that FO-MAML which ignores the second-order information required in the update of MAML cannot achieve any small desired level of accuracy, i.e., FO-MAML cannot find an epsilon-FOSP for any epsilon > 0. We further propose a new-variant of the MAML algorithm called Hessian-free MAML which preserves all theoretical guarantees of MAML, without requiring access to second-order information.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1082,1091,,,,,,,,,,,,,,,,WOS:000559931300087,0
C,"Fogliato, R; G'Sell, M; Chouldechova, A",,"Chiappa, S; Calandra, R",,"Fogliato, Riccardo; G'Sell, Max; Chouldechova, Alexandra",,,Fairness Evaluation in Presence of Biased Noisy Labels,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Risk assessment tools are widely used around the country to inform decision making within the criminal justice system. Recently, considerable attention has been devoted to the question of whether such tools may suffer from racial bias. In this type of assessment, a fundamental issue is that the training and evaluation of the model is based on a variable (arrest) that may represent a noisy version of an unobserved outcome of more central interest (offense). We propose a sensitivity analysis framework for assessing how assumptions on the noise across groups affect the predictive bias properties of the risk assessment model as a predictor of reoffense. Our experimental results on two real world criminal justice data sets demonstrate how even small biases in the observed labels may call into question the conclusions of an analysis based on the noisy outcome.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2325,2335,,,,,,,,,,,,,,,,WOS:000559931301004,0
C,"Futoma, J; Hughes, MC; Doshi-Velez, F",,"Chiappa, S; Calandra, R",,"Futoma, Joseph; Hughes, Michael C.; Doshi-Velez, Finale",,,POPCORN: Partially Observed Prediction Constrained Reinforcement Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Many medical decision-making tasks can be framed as partially observed Markov decision processes (POMDPs). However, prevailing two-stage approaches that first learn a POMDP and then solve it often fail because the model that best fits the data may not be well suited for planning. We introduce a new optimization objective that (a) produces both high-performing policies and high-quality generative models, even when some observations are irrelevant for planning, and (b) does so in batch off-policy settings that are typical in healthcare, when only retrospective data is available. We demonstrate our approach on synthetic examples and a challenging medical decision-making problem.",,,,,,"Hughes, Michael/0000-0003-4859-7400",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3578,3587,,,,,,,,,,,,,,,,WOS:000559931301009,0
C,"Grant, JA; Leslie, DS",,"Chiappa, S; Calandra, R",,"Grant, James A.; Leslie, David S.",,,On Thompson Sampling for Smoother-than-Lipschitz Bandits,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Thompson Sampling is a well established approach to bandit and reinforcement learning problems. However its use in continuum armed bandit problems has received relatively little attention. We provide the first bounds on the regret of Thompson Sampling for continuum armed bandits under weak conditions on the function class containing the true function and sub-exponential observation noise. Our bounds are realised by analysis of the eluder dimension, a recently proposed measure of the complexity of a function class, which has been demonstrated to be useful in bounding the Bayesian regret of Thompson Sampling for simpler bandit problems under sub-Gaussian observation noise. We derive a new bound on the eluder dimension for classes of functions with Lipschitz derivatives, and generalise previous analyses in multiple regards.",,,,,,"Leslie, David Stuart/0000-0001-5253-7676",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2612,2621,,,,,,,,,,,,,,,,WOS:000559931301025,0
C,"Han, J; Ding, F; Liu, XL; Torresani, L; Peng, J; Liu, Q",,"Chiappa, S; Calandra, R",,"Han, Jun; Ding, Fan; Liu, Xianglong; Torresani, Lorenzo; Peng, Jian; Liu, Qiang",,,Stein Variational Inference for Discrete Distributions,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Gradient-based approximate inference methods, such as Stein variational gradient descent (SVGD) (Liu & Wang, 2016), provide simple and general purpose inference engines for differentiable continuous distributions. However, existing forms of SVGD cannot be directly applied to discrete distributions. In this work, we fill this gap by proposing a simple yet general framework that transforms discrete distributions to equivalent piecewise continuous distributions, on which the gradient-free SVGD is applied to perform efficient approximate inference. The empirical results show that our method outperforms traditional algorithms such as Gibbs sampling and discontinuous Hamiltonian Monte Carlo on various challenging benchmarks of discrete graphical models. We demonstrate that our method provides a promising tool for learning ensembles of binarized neural network (BNN), outperforming other widely used ensemble methods on learning binarized AlexNet on CIFAR-10 dataset. In addition, such transform can be straightforwardly employed in gradient-free kernelized Stein discrepancy to perform goodness-of-fit (GOF) test on discrete distributions. Our proposed method outperforms existing GOF test methods for intractable discrete distributions.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,4563,4571,,,,,,,,,,,,,,,,WOS:000559931301034,0
C,"Ilyas, A; Zampetakis, M; Daskalakis, C",,"Chiappa, S; Calandra, R",,"Ilyas, Andrew; Zampetakis, Manolis; Daskalakis, Constantinos",,,A Theoretical and Practical Framework for Regression and Classification from Truncated Samples,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Machine learning and statistics are invaluable for extracting insights from data. A key assumption of most methods, however, is that they have access to independent samples from the distribution of relevant data. As such, these methods often perform poorly in the face of biased data which breaks this assumption. In this work, we consider the classical challenge of bias due to truncation, wherein samples falling outside of an observation window cannot be observed. We present a general framework for regression and classification from samples that are truncated according to the value of the dependent variable. The framework argues that stochastic gradient descent (SGD) can be efficiently executed on the population log-likelihood of the truncated sample. Our framework is broadly applicable, and we provide end-to-end guarantees for the well-studied problems of truncated logistic and probit regression, where we argue that the true model parameters can be identified computationally and statistically efficiently from truncated data, extending recent work on truncated linear regression. We also provide experiments to illustrate the practicality of our framework on synthetic and real data.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,4463,4472,,,,,,,,,,,,,,,,WOS:000559931301051,0
C,"Lu, Q; Karanikolas, GV; Shen, YN; Giannakis, GB",,"Chiappa, S; Calandra, R",,"Lu, Qin; Karanikolas, Georgios V.; Shen, Yanning; Giannakis, Georgios B.",,,Ensemble Gaussian Processes with Spectral Features for Online Interactive Learning with Scalability,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Combining benefits of kernels with Bayesian models, Gaussian process (GP) based approaches have well-documented merits not only in learning over a rich class of nonlinear functions, but also quantifying the associated uncertainty. While most GP approaches rely on a single preselected prior, the present work employs a weighted ensemble of GP priors, each having a unique covariance (kernel) belonging to a prescribed kernel dictionary - which leads to a richer space of learning functions. Leveraging kernel approximants formed by spectral features for scalability, an online interactive ensemble (OI-E) GP framework is developed to jointly learn the sought function, and for the first time select interactively the EGP kernel on-the-fly. Performance of OI-EGP is benchmarked by the best fixed function estimator via regret analysis. Furthermore, the novel OI-EGP is adapted to accommodate dynamic learning functions. Synthetic and real data tests demonstrate the e.ectiveness of the proposed schemes.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1910,1919,,,,,,,,,,,,,,,,WOS:000559931302022,0
C,"Modi, A; Jiang, N; Tewari, A; Singh, S",,"Chiappa, S; Calandra, R",,"Modi, Aditya; Jiang, Nan; Tewari, Ambuj; Singh, Satinder",,,Sample Complexity of Reinforcement Learning using Linearly Combined Model Ensembles,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Reinforcement learning (RL) methods have been shown to be capable of learning intelligent behavior in rich domains. However, this has largely been done in simulated domains without adequate focus on the process of building the simulator. In this paper, we consider a setting where we have access to an ensemble of pre-trained and possibly inaccurate simulators (models). We approximate the real environment using a state-dependent linear combination of the ensemble, where the coefficients are determined by the given state features and some unknown parameters. Our proposed algorithm provably learns a near-optimal policy with a sample complexity polynomial in the number of unknown parameters, and incurs no dependence on the size of the state (or action) space. As an extension, we also consider the more challenging problem of model selection, where the state features are unknown and can be chosen from a large candidate set. We provide exponential lower bounds that illustrate the fundamental hardness of this problem, and develop a provably efficient algorithm under additional natural assumptions.",,,,,"Modi, Aditya/GPK-8582-2022","Modi, Aditya/0000-0002-6959-0593",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2010,2019,,,,,,,,,,,,,,,,WOS:000559931302046,0
C,"Piponi, D; Hoffman, MD; Sountsov, P",,"Chiappa, S; Calandra, R",,"Piponi, Dan; Hoffman, Matthew D.; Sountsov, Pavel",,,Hamiltonian Monte Carlo Swindles,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Hamiltonian Monte Carlo (HMC) is a powerful Markov chain Monte Carlo (MCMC) algorithm for estimating expectations with respect to continuous un-normalized probability distributions. MCMC estimators typically have higher variance than classical Monte Carlo with i.i.d. samples due to autocorrelations; most MCMC research tries to reduce these autocorrelations. In this work, we explore a complementary approach to variance reduction based on two classical Monte Carlo swindles: first, running an auxiliary coupled chain targeting a tractable approximation to the target distribution, and using the auxiliary samples as control variates; and second, generating anti-correlated (antithetic) samples by running two chains with flipped randomness. Both ideas have been explored previously in the context of Gibbs samplers and random-walk Metropolis algorithms, but we argue that they are ripe for adaptation to HMC in light of recent coupling results from the HMC theory literature. For many posterior distributions, we find that these swindles generate effective sample sizes orders of magnitude larger than plain HMC, as well as being more efficient than analogous swindles for Metropolis-adjusted Langevin algorithm and random-walk Metropolis.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3774,3782,,,,,,,,,,,,,,,,WOS:000559931302074,0
C,"Rad, KR; Zhou, WD; Maleki, A",,"Chiappa, S; Calandra, R",,"Rad, Kamiar Rahnama; Zhou, Wenda; Maleki, Arian",,,Error bounds in estimating the out-of-sample prediction error using leave-one-out cross validation in high-dimensions,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We study the problem of out-of-sample risk estimation in the high dimensional regime where both the sample size n and number of features p are large, and n/p can be less than one. Extensive empirical evidence confirms the accuracy of leave-one-out cross validation (LO) for out-of-sample risk estimation. Yet, a unifying theoretical evaluation of the accuracy of LO in high-dimensional problems has remained an open problem. This paper aims to fill this gap for penalized regression in the generalized linear family. With minor assumptions about the data generating process, and without any sparsity assumptions on the regression coefficients, our theoretical analysis obtains finite sample upper bounds on the expected squared error of LO in estimating the out-of-sample error. Our bounds show that the error goes to zero as n, p -> infinity, even when the dimension p of the feature vectors is comparable with or greater than the sample size n. One technical advantage of the theory is that it can be used to clarify and connect some results from the recent literature on scalable approximate LO.",,,,,"Zhou, Wenda/AFF-2109-2022","Zhou, Wenda/0000-0001-5549-7884",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,4067,4076,,,,,,,,,,,,,,,,WOS:000559931302084,0
C,"Radiya-Dixit, E; Wang, X",,"Chiappa, S; Calandra, R",,"Radiya-Dixit, Evani; Wang, Xin",,,How fine can fine-tuning be? Learning efficient language models,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"State-of-the-art performance on language understanding tasks is now achieved with increasingly large networks; the current record holder has billions of parameters. Given a language model pre-trained on massive unlabeled text corpora, only very light supervised fine-tuning is needed to learn a task: the number of fine-tuning steps is typically five orders of magnitude lower than the total parameter count. Does this mean that fine-tuning only introduces small differences from the pre-trained model in the parameter space? If so, can one avoid storing and computing an entire model for each task? In this work, we address these questions by using Bidirectional Encoder Representations from Transformers (BERT) as an example. As expected, we find that the fine-tuned models are close in parameter space to the pre-trained one, with the closeness varying from layer to layer. We show that it suffices to fine-tune only the most critical layers. Further, we find that there are surprisingly many good solutions in the set of sparsified versions of the pre-trained model. As a result, fine-tuning of huge language models can be achieved by simply setting a certain number of entries in certain layers of the pre-trained parameters to zero, saving both task-specific parameter storage and computational cost.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2435,2442,,,,,,,,,,,,,,,,WOS:000559931302085,0
C,"Sharoff, P; Mehta, NA; Ganti, R",,"Chiappa, S; Calandra, R",,"Sharoff, P.; Mehta, Nishant A.; Ganti, Ravi",,,A Farewell to Arms: Sequential Reward Maximization on a Budget with a Giving Up Option,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We consider a sequential decision-making problem where an agent can take one action at a time and each action has a stochastic temporal extent, i.e., a new action cannot be taken until the previous one is finished. Upon completion, the chosen action yields a stochastic reward. The agent seeks to maximize its cumulative reward over a finite time budget, with the option of giving up on a current action - hence forfeiting any reward in order to choose another action. We cast this problem as a variant of the stochastic multi-armed bandits problem with stochastic consumption of resource. For this problem, we first establish that the optimal arm is the one that maximizes the ratio of the expected reward of the arm to the expected waiting time before the agent sees the reward due to pulling that arm. Using a novel upper confidence bound on this ratio, we then introduce an upper confidence based-algorithm, Wait-UCB, for which we establish logarithmic, problem-dependent regret bound which has an improved dependence on problem parameters compared to previous works. Simulations on various problem configurations comparing Wait-UCB against the state-of-the-art algorithms are also presented.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303012,0
C,"Wang, ZY; Cheng, SY; Li, YR; Zhu, J; Zhang, B",,"Chiappa, S; Calandra, R",,"Wang, Ziyu; Cheng, Shuyu; Li, Yueru; Zhu, Jun; Zhang, Bo",,,A Wasserstein Minimum Velocity Approach to Learning Unnormalized Models,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Score matching provides an effective approach to learning flexible unnormalized models, but its scalability is limited by the need to evaluate a second-order derivative. In this paper, we present a scalable approximation to a general family of learning objectives including score matching, by observing a new connection between these objectives and Wasserstein gradient flows. We present applications with promise in learning neural density estimators on manifolds, and training implicit variational and Wasserstein auto-encoders with a manifold-valued prior.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303068,0
C,"Wei, D; Ramamurthy, KN; Calmon, FP",,"Chiappa, S; Calandra, R",,"Wei, Dennis; Ramamurthy, Karthikeyan Natesan; Calmon, Flavio P.",,,Optimized Score Transformation for Fair Classification,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"This paper considers fair probabilistic classification where the outputs of primary interest are predicted probabilities, commonly referred to as scores. We formulate the problem of transforming scores to satisfy fairness constraints while minimizing the loss in utility. The formulation can be applied either to post-process classifier outputs or to pre-process training data, thus allowing maximum freedom in selecting a classification algorithm. We derive a closed-form expression for the optimal transformed scores and a convex optimization problem for the transformation parameters. In the population limit, the transformed score function is the fairness-constrained minimizer of cross-entropy with respect to the optimal unconstrained scores. In the finite sample setting, we propose to approach this solution using a combination of standard probabilistic classifiers and ADMM. Comprehensive experiments comparing to 10 existing methods show that the proposed FairScoreTransformer has advantages for score-based metrics such as Brier score and AUC while remaining competitive for binary label-based metrics such as accuracy.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303075,0
C,"Ambrogioni, L; Guclu, U; Berezutskaya, J; van den Borne, E; Gucluturk, Y; Hinne, M; Maris, E; van Gerven, M",,"Chaudhuri, K; Sugiyama, M",,"Ambrogioni, L.; Guclu, U.; Berezutskaya, J.; van den Borne, E.; Gucluturk, Y.; Hinne, M.; Maris, E.; van Gerven, M.",,,Forward Amortized Inference for Likelihood-Free Variational Marginalization,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"In this paper, we introduce a new form of amortized variational inference by using the forward KL divergence in a joint-contrastive variational loss. The resulting forward amortized variational inference is a likelihood-free method as its gradient can be sampled without bias and without requiring any evaluation of either the model joint distribution or its derivatives. We prove that our new variational loss is optimized by the exact posterior marginals in the fully factorized mean-field approximation, a property that is not shared with the more conventional reverse KL inference. Furthermore, we show that forward amortized inference can be easily marginalized over large families of latent variables in order to obtain a marginalized variational posterior. We consider two examples of variational marginalization. In our first example we train a Bayesian forecaster for predicting a simplified chaotic model of atmospheric convection. In the second example we train an amortized variational approximation of a Bayesian optimal classifier by marginalizing over the model space. The result is a powerful meta-classification network that can solve arbitrary classification problems without further training.",,,,,"Berezutskaya, Julia/ABG-8555-2021; Hinne, Max/ABD-1486-2021; Gl, Umut/AAX-6105-2020","Berezutskaya, Julia/0000-0002-2005-8758; Hinne, Max/0000-0002-9279-6725; Gl, Umut/0000-0003-4753-159X",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,777,786,,,,,,,,,,,,,,,,WOS:000509687900081,0
C,"Bollapragada, R; Scieur, D; d'Aspremont, A",,"Chaudhuri, K; Sugiyama, M",,"Bollapragada, Raghu; Scieur, Damien; d'Aspremont, Alexandre",,,Nonlinear Acceleration of Primal-Dual Algorithms,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We study convergence acceleration schemes for multi-step optimization algorithms. The extrapolated solution is written as a nonlinear average of the iterates produced by the original optimization algorithm. Our analysis of Regularized Nonlinear Acceleration, aka Anderson acceleration, does not need the underlying fixed-point operator to be symmetric, hence handles e.g. algorithms with momentum terms such as Nesterov's accelerated method, or primal-dual methods such as Chambolle-Pock. The weights are computed via a simple linear system and we analyze performance in both online and offline modes. We use Crouzeix's conjecture to show that acceleration is controlled by the solution of a Chebyshev problem on the numerical range of a non-symmetric operator modelling the behavior of iterates near the optimum. Numerical experiments are detailed on image processing and logistic regression problems.",,,,,,"Bollapragada, Vijaya Raghavendra/0000-0001-5692-0832",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,739,747,,,,,,,,,,,,,,,,WOS:000509687900077,0
C,"Cheung, WC; Simchi-Levi, D; Zhu, RH",,"Chaudhuri, K; Sugiyama, M",,"Cheung, Wang Chi; Simchi-Levi, David; Zhu, Ruihao",,,Learning to Optimize under Non-Stationarity,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We introduce algorithms that achieve state-of-the-art dynamic regret bounds for non-stationary linear stochastic bandit setting. It captures natural applications such as dynamic pricing and ads allocation in a changing environment. We show how the difficulty posed by the non-Stationarity can be overcome by a novel marriage between stochastic and adversarial bandits learning algorithms. Defining d, B-T, and T as the problem dimension, the variation budget, and the total time horizon, respectively, our main contributions are the tuned Sliding Window UCB (SW-UCB) algorithm with optimal (O) over tilde (d(2/3)(B-T+1)T-1/3(2/3)) dynamic regret, and the tuning free bandit-over-bandit (BOB) framework built on top of the SW-UCB algorithm with best (O) over tilde (d(2/3)(B-T + 1)T-1/4(3/4)) dynamic regret.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901013,0
C,"Inoue, H",,"Chaudhuri, K; Sugiyama, M",,"Inoue, Hiroshi",,,Adaptive Ensemble Prediction for Deep Neural Networks based on Confidence Level,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Ensembling multiple predictions is a widely used technique for improving the accuracy of various machine learning tasks. One obvious drawback of ensembling is its higher execution cost during inference. In this paper, we first describe our insights on the relationship between the probability of prediction and the effect of ensembling with current deep neural networks; ensembling does not help mispredictions for inputs predicted with a high probability even when there is a non-negligible number of mispredicted inputs. This finding motivated us to develop a way to adaptively control the ensembling. If the prediction for an input reaches a high enough probability, i.e., the output from the softmax function, on the basis of the confidence level, we stop ensembling for this input to avoid wasting computation power. We evaluated the adaptive ensembling by using various datasets and showed that it reduces the computation cost significantly while achieving accuracy similar to that of static ensembling using a predefined number of local predictions. We also show that our statistically rigorous confidence-level-based early-exit condition reduces the burden of task-dependent threshold tuning better compared with naive early exit based on a pre-defined threshold in addition to yielding a better accuracy with the same cost.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901034,0
C,"Lapanowski, AF; Gaynanova, I",,"Chaudhuri, K; Sugiyama, M",,"Lapanowski, Alexander F.; Gaynanova, Irina",,,Sparse Feature Selection in Kernel Discriminant Analysis via Optimal Scoring,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We consider the two-group classification problem and propose a kernel classifier based on the optimal scoring framework. Unlike previous approaches, we provide theoretical guarantees on the expected risk consistency of the method. We also allow for feature selection by imposing structured sparsity using weighted kernels. We propose fully-automated methods for selection of all tuning parameters, and in particular adapt kernel shrinkage ideas for ridge parameter selection. Numerical studies demonstrate the superior classification performance of the proposed approach compared to existing non-parametric classifiers.",,,,,,"Gaynanova, Irina/0000-0002-4116-0268",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901078,0
C,"Mondelli, M; Montanari, A",,"Chaudhuri, K; Sugiyama, M",,"Mondelli, Marco; Montanari, Andrea",,,On the Connection Between Learning Two-Layer Neural Networks and Tensor Decomposition,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We establish connections between the problem of learning a two-layer neural network and tensor decomposition. We consider a model with feature vectors x E I d, r hidden units with weights {wi}1<z<r and output y E i.e., y = i_1 Q(wT x), with activation functions given by low-degree polynomials. In particular, if Q(x) = ao + aix + a3x3, we prove that no polynomial-time algorithm can outperform the trivial predictor that assigns to each example the response variable E(y), when d3/2 << r << d2. Our conclusion holds for a `natural data distribution', namely standard Gaussian feature vectors x, and output distributed according to a two-layer neural network with random isotropic weights, and under a certain complexity-theoretic assumption on tensor decomposition. Roughly speaking, we assume that no polynomial-time algorithm can substantially outperform current methods for tensor decomposition based on the sum-of-squares hierarchy. We also prove generalizations of this statement for higher degree polynomial activations, and non-random weight vectors. Remarkably, several existing algorithms for learning two-layer networks with rigorous guarantees are based on tensor decomposition. Our results support the idea that this is indeed the core computational difficulty in learning such networks, under the stated generative model for the data. As a side result, we show that under this model learning the network requires accurate learning of its weights, a property that does not hold in a more general setting.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901010,0
C,"Sadeghi, K; Rinaldo, A",,"Chaudhuri, K; Sugiyama, M",,"Sadeghi, Kayvan; Rinaldo, Alessandro",,,Markov Properties of Discrete Determinantal Point Processes,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Determinantal point processes (DPPs) are probabilistic models for repulsion. When used to represent the occurrence of random subsets of a finite base set, DPPs allow to model global negative associations in a mathematically elegant and direct way. Discrete DPPs have become popular and computationally tractable models for solving several machine learning tasks that require the selection of diverse objects, and have been successfully applied in numerous real-life problems. Despite their popularity, the statistical properties of such models have not been adequately explored. In this note, we derive the Markov properties of discrete DPPs and show how they can be expressed using graphical models.",,,,,,"Sadeghi, Kayvan/0000-0001-7314-744X",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901037,0
C,"Titsias, MK; Ruiz, FJR",,"Chaudhuri, K; Sugiyama, M",,"Titsias, Michalis K.; Ruiz, Francisco J. R.",,,Unbiased Implicit Variational Inference,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We develop unbiased implicit variational inference (UIVI), a method that expands the applicability of variational inference by defining an expressive variational family. UIVI considers an implicit variational distribution obtained in a hierarchical manner using a simple reparameterizable distribution whose variational parameters are defined by arbitrarily flexible deep neural networks. Unlike previous works, UIVI directly optimizes the evidence lower bound (ELBO) rather than an approximation to the ELBO. We demonstrate UIVI on several models, including Bayesian multinomial logistic regression and variational autoencoders, and show that UIVI achieves both tighter ELBO and better predictive performance than existing approaches at a similar computational cost.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,167,176,,,,,,,,,,,,,,,,WOS:000509687900018,0
C,"Zhang, X; Yu, YD; Wang, LX; Gu, QQ",,"Chaudhuri, K; Sugiyama, M",,"Zhang, Xiao; Yu, Yaodong; Wang, Lingxiao; Gu, Quanquan",,,Learning One-hidden-layer ReLU Networks via Gradient Descent,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We study the problem of learning one-hidden-layer neural networks with Rectified Linear Unit (ReLU) activation function, where the inputs are sampled from standard Gaussian distribution and the outputs are generated from a noisy teacher network. We analyze the performance of gradient descent for training such kind of neural networks based on empirical risk minimization, and provide algorithm-dependent guarantees. In particular, we prove that tensor initialization followed by gradient descent can converge to the ground-truth parameters at a linear rate up to some statistical error. To the best of our knowledge, this is the first work characterizing the recovery guarantee for practical learning of one-hidden-layer ReLU networks with multiple neurons. Numerical experiments verify our theoretical findings.",,,,,"Yu, Yaodong/AAN-1642-2021","Yu, Yaodong/0000-0003-0540-8526; Wang, Lingxiao/0000-0002-8798-692X",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901059,0
C,"Zhu, Y; Yu, ZQ; Cheng, G",,"Chaudhuri, K; Sugiyama, M",,"Zhu, Ying; Yu, Zhuqing; Cheng, Guang",,,High Dimensional Inference in Partially Linear Models,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We propose two semiparametric versions of the debiased Lasso procedure for the model Y-i = X-i beta(0)+g(0)(Z(i))+epsilon(i), where the parameter vector of interest beta(0) is high dimensional but sparse (exactly or approximately) and g(0) is an unknown nuisance function. Both versions are shown to have the same asymptotic normal distribution and do not require the minimal signal condition for statistical inference of any component in beta(0). We further develop a simultaneous hypothesis testing procedure based on multiplier bootstrap. Our testing method takes into account of the dependence structure within the debiased estimates, and allows the number of tested components to be exponentially high.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902083,0
C,"Foulds, JR",,"Storkey, A; PerezCruz, F",,"Foulds, James R.",,,Mixed Membership Word Embeddings for Computational Social Science,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Word embeddings improve the performance of NLP systems by revealing the hidden structural relationships between words. Despite their success in many applications, word embeddings have seen very little use in computational social science NLP tasks, presumably due to their reliance on big data, and to a lack of interpretability. I propose a probabilistic model-based word embedding method which can recover interpretable embeddings, without big data. The key insight is to leverage mixed membership modeling, in which global representations are shared, but individual entities (i.e. dictionary words) are free to use these representations to uniquely differing degrees. I show how to train the model using a combination of state-of-the-art training techniques for word embeddings and topic models. The experimental results show an improvement in predictive language modeling of up to 63% in MRR over the skip-gram, and demonstrate that the representations are beneficial for supervised learning. I illustrate the interpretability of the models with computational social science case studies on State of the Union addresses and NIPS articles.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300010,0
C,"Katariya, S; Jain, L; Sengupta, N; Evans, J; Nowak, R",,"Storkey, A; PerezCruz, F",,"Katariya, Sumeet; Jain, Lalit; Sengupta, Nandana; Evans, James; Nowak, Robert",,,Adaptive Sampling for Coarse Ranking,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"We consider the problem of active coarse ranking, where the goal is to sort items according to their means into clusters of prespecified sizes, by adaptively sampling from their reward distributions. This setting is useful in many social science applications involving human raters and the approximate rank of every item is desired. Approximate or coarse ranking can significantly reduce the number of ratings required in comparison to the number needed to find an exact ranking. We propose a computationally efficient PAC algorithm LUCBRank for coarse ranking, and derive an upper bound on its sample complexity. We also derive a nearly matching distribution-dependent lower bound. Experiments on synthetic as well as real-world data show that LUCBRank performs better than state-of-the-art baseline methods, even when these methods have the advantage of knowing the underlying parametric model.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300192,0
C,"Katz-Samuels, J; Scott, C",,"Storkey, A; PerezCruz, F",,"Katz-Samuels, Julian; Scott, Clayton",,,Nonparametric Preference Completion,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"We consider the task of collaborative preference completion: given a pool of items, a pool of users and a partially observed item-user rating matrix, the goal is to recover the personalized ranking of each user over all of the items. Our approach is nonparametric: we assume that each item i and each user u have unobserved features x(i) and y(u), and that the associated rating is given by g(i),(f (x(i), y(u))) where f is Lipschitz and gu is a monotonic transformation that depends on the user. We propose a k-nearest neighbors-like algorithm and prove that it is consistent. To the best of our knowledge, this is the first consistency result for the collaborative preference completion problem in a nonparametric setting. Finally, we demonstrate the performance of our algorithm with experiments on the Netflix and Movielens datasets.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300067,0
C,"Kazemi, E; Chen, L; Dasgupta, S; Karbasi, A",,"Storkey, A; PerezCruz, F",,"Kazemi, Ehsan; Chen, Lin; Dasgupta, Sanjoy; Karbasi, Amin",,,Comparison Based Learning from Weak Oracles,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"There is increasing interest in learning algorithms that involve interaction between human and machine. Comparison-based queries are among the most natural ways to get feedback from humans. A challenge in designing comparison-based interactive learning algorithms is coping with noisy answers. The most common fix is to submit a query several times, but this is not applicable in many situations due to its prohibitive cost and due to the unrealistic assumption of independent noise in different repetitions of the same query. In this paper, we introduce a new weak oracle model, where a non-malicious user responds to a pairwise comparison query only when she is quite sure about the answer. This model is able to mimic the behavior of a human in noise-prone regions. We also consider the application of this weak oracle model to the problem of content search (a variant of the nearest neighbor search problem) through comparisons. More specifically, we aim at devising efficient algorithms to locate a target object in a database equipped with a dissimilarity metric via invocation of the weak comparison oracle. We propose two algorithms termed WorcsI and Worcs-II (Weak-Oracle Comparison-based Search), which provably locate the target object in a number of comparisons close to the entropy of the target distribution. While Worcs-I provides better theoretical guarantees, Worcs-II is applicable to more technically challenging scenarios where the algorithm has limited access to the ranking dissimilarity between objects. A series of experiments validate the performance of our proposed algorithms.",,,,,"Chen, Lin/CAH-1961-2022; Kazemi, Ehsan/G-7735-2015","Chen, Lin/0000-0003-0349-6577; Kazemi, Ehsan/0000-0001-8427-054X",,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300193,0
C,"Pansari, P; Russell, C; Kumar, MP",,"Storkey, A; PerezCruz, F",,"Pansari, Pankaj; Russell, Chris; Kumar, M. Pawan",,,Worst-case Optimal Submodular Extensions for Marginal Estimation,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Submodular extensions of an energy function can be used to efficiently compute approximate marginals via variational inference. The accuracy of the marginals depends crucially on the quality of the submodular extension. To identify the best possible extension, we show an equivalence between the submodular extensions of the energy and the objective functions of linear programming (LP) relaxations for the corresponding MAP estimation problem. This allows us to (i) establish the worst-case optimality of the submodular extension for Potts model used in the literature; (ii) identify the worst-case optimal submodular extension for the more general class of metric labeling; and (iii) efficiently compute the marginals for the widely used dense CRF model with the help of a recently proposed Gaussian filtering method. Using synthetic and real data, we show that our approach provides comparable upper bounds on the log-partition function to those obtained using tree-reweighted message passing (TRW) in cases where the latter is computationally feasible. Importantly, unlike TRW, our approach provides the first practical algorithm to compute an upper bound on the dense CRF model.",,,,,,"Russell, Chris/0000-0003-1665-1759",,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300035,0
C,"Saeedi, A; Hoffman, MD; DiVerdi, SJ; Ghandeharioun, A; Johnson, MJ; Adams, RP",,"Storkey, A; PerezCruz, F",,"Saeedi, Ardavan; Hoffman, Matthew D.; DiVerdi, Stephen J.; Ghandeharioun, Asma; Johnson, Matthew J.; Adams, Ryan P.",,,Multimodal Prediction and Personalization of Photo Edits with Deep Generative Models,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Professional-grade software applications are powerful but complicated-expert users can achieve impressive results, but novices often struggle to complete even basic tasks. Photo editing is a prime example: after loading a photo, the user is confronted with an array of cryptic sliders like clarity, temp, and highlights. An automatically generated suggestion could help, but there is no single correct edit for a given image-different experts may make very different aesthetic decisions when faced with the same image, and a single expert may make different choices depending on the intended use of the image (or on a whim). We therefore want a system that can propose multiple diverse, high-quality edits while also learning from and adapting to a user's aesthetic preferences. In this work, we develop a statistical model that meets these objectives. Our model builds on recent advances in neural network generative modeling and scalable inference, and uses hierarchical structure to learn editing patterns across many diverse users. Empirically, we find that our model outperforms other approaches on this challenging multimodal prediction task.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300137,0
C,"Uziel, G; El-Yaniv, R",,"Storkey, A; PerezCruz, F",,"Uziel, Guy; El-Yaniv, Ran",,,Growth-Optimal Portfolio Selection under CVaR Constraints,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Online portfolio selection research has so far focused mainly on minimizing regret defined in terms of wealth growth. Practical financial decision making, however, is deeply concerned with both wealth and risk. We consider online learning of portfolios of stocks whose prices are governed by arbitrary (unknown) stationary and ergodic processes, where the goal is to maximize wealth while keeping the conditional value at risk (CVaR) below a desired threshold. We characterize the asymptomatically optimal risk-adjusted performance and present an investment strategy whose portfolios are guaranteed to achieve the asymptotic optimal solution while fulfilling the desired risk constraint. We also numerically demonstrate and validate the viability of our method on standard datasets.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300006,0
C,"Wang, Z; Gehring, C; Kohli, P; Jegelka, S",,"Storkey, A; PerezCruz, F",,"Wang, Zi; Gehring, Clement; Kohli, Pushmeet; Jegelka, Stefanie",,,Batched Large-scale Bayesian Optimization in High-dimensional Spaces,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Bayesian optimization (BO) has become an effective approach for black-box function optimization problems when function evaluations are expensive and the optimum can be achieved within a relatively small number of queries. However, many cases, such as the ones with high-dimensional inputs, may require a much larger number of observations for optimization. Despite an abundance of observations thanks to parallel experiments, current BO techniques have been limited to merely a few thousand observations. In this paper, we propose ensemble Bayesian optimization (EBO) to address three current challenges in BO simultaneously: (1) large-scale observations; (2) high dimensional input spaces; and (3) selections of batch queries that balance quality and diversity. The key idea of EBO is to operate on an ensemble of additive Gaussian process models, each of which possesses a randomized strategy to divide and conquer. We show unprecedented, previously impossible results of scaling up BO to tens of thousands of observations within minutes of computation.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300079,0
C,"Yu, SQ; Drton, M; Shojaie, A",,"Storkey, A; PerezCruz, F",,"Yu, Shiqing; Drton, Mathias; Shojaie, Ali",,,Graphical Models for Non-Negative Data Using Generalized Score Matching,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"A common challenge in estimating parameters of probability density functions is the intractability of the normalizing constant. While in such cases maximum likelihood estimation may be implemented using numerical integration, the approach becomes computationally intensive. In contrast, the score matching method of Hyvarinen (2005) avoids direct calculation of the normalizing constant and yields closed-form estimates for exponential families of continuous distributions over Rm. Hyvarinen (2007) extended the approach to distributions supported on the non-negative orthant R-+(m). In this paper, we give a generalized form of score matching for non-negative data that improves estimation efficiency. We also generalize the regularized score matching method of Lin et al. (2016) for non-negative Gaussian graphical models, with improved theoretical guarantees.",,,,,,"Yu, Shiqing/0000-0002-2719-7558",,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300186,0
C,"Cheng, H; Yu, YL; Zhang, XH; Xing, E; Schuurmans, D",,"Gretton, A; Robert, CC",,"Cheng, Hao; Yu, Yaoliang; Zhang, Xinhua; Xing, Eric; Schuurmans, Dale",,,Scalable and Sound Low-Rank Tensor Learning,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Many real-world data arise naturally as tensors. Equipped with a low rank prior, learning algorithms can benefit from exploiting the rich dependency encoded in a tensor. Despite its prevalence in low-rank matrix learning, trace norm ceases to be tractable in tensors and therefore most existing works resort to matrix unfolding. Although some theoretical guarantees are available, these approaches may lose valuable structure information and are not scalable in general. To address this problem, we propose directly optimizing the tensor trace norm by approximating its dual spectral norm, and we show that the approximation bounds can be efficiently converted to the original problem via the generalized conditional gradient algorithm. The resulting approach is scalable to large datasets, and matches state-of-the-art recovery guarantees. Experimental results on tensor completion and multitask learning confirm the superiority of the proposed method.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,1114,1123,,,,,,,,,,,,,,,,WOS:000508662100121,0
C,"Csaji, BC",,"Gretton, A; Robert, CC",,"Csaji, Balazs Csanad",,,Score Permutation Based Finite Sample Inference for Generalized AutoRegressive Conditional Heteroskedasticity (GARCH) Models,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"A standard model of (conditional) heteroscedasticity, i.e., the phenomenon that the variance of a process changes over time, is the Generalized AutoRegressive Conditional Heteroskedasticity (GARCH) model, which is especially important for economics and finance. GARCH models are typically estimated by the Quasi-Maximum Likelihood (QML) method, which works under mild statistical assumptions. Here, we suggest a finite sample approach, called ScoPe, to construct distribution-free confidence regions around the QML estimate, which have exact coverage probabilities, despite no additional assumptions about moments are made. ScoPe is inspired by the recently developed Sign-Perturbed Sums (SPS) method, which however cannot be applied in the GARCH case. ScoPe works by perturbing the score function using randomly permuted residuals. This produces alternative samples which lead to exact confidence regions. Experiments on simulated and stock market data are also presented, and ScoPe is compared with the asymptotic theory and bootstrap approaches.",,,,,"Csanad, Csaji Balazs/AAB-5771-2020",,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,296,304,,,,,,,,,,,,,,,,WOS:000508662100033,0
C,"Heinemann, U; Livni, R; Eban, E; Elidan, G; Globerson, A",,"Gretton, A; Robert, CC",,"Heinemann, Uri; Livni, Roi; Eban, Elad; Elidan, Gal; Globerson, Amir",,,Improper Deep Kernels,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Neural networks have recently re-emerged as a powerful hypothesis class, yielding impressive classification accuracy in multiple domains. However, their training is a non-convex optimization problem which poses theoretical and practical challenges. Here we address this difficulty by turning to improper learning of neural nets. In other words, we learn a classifier that is not a neural net but is competitive with the best neural net model given a sufficient number of training examples. Our approach relies on a novel kernel construction scheme in which the kernel is a result of integration over the set of all possible instantiation of neural models. It turns out that the corresponding integral can be evaluated in closed-form via a simple recursion. Thus we translate the non-convex learning problem of a neural net to an SVM with an appropriate kernel. We also provide sample complexity results which depend on the stability of the optimal neural net.",,,,,,"Elidan, Gal/0000-0001-5365-599X; Globerson, Amir/0000-0003-2557-1742",,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,1159,1167,,,,,,,,,,,,,,,,WOS:000508662100126,0
C,"Lee, CY; Gallagher, PW; Tu, ZW",,"Gretton, A; Robert, CC",,"Lee, Chen-Yu; Gallagher, Patrick W.; Tu, Zhuowen",,,"Generalizing Pooling Functions in Convolutional Neural Networks: Mixed, Gated, and Tree","ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"We seek to improve deep neural networks by generalizing the pooling operations that play a central role in current architectures. We pursue a careful exploration of approaches to allow pooling to learn and to adapt to complex and variable patterns. The two primary directions lie in (1) learning a pooling function via (two strategies of) combining of max and average pooling, and (2) learning a pooling function in the form of a tree-structured fusion of pooling filters that are themselves learned. In our experiments every generalized pooling operation we explore improves performance when used in place of average or max pooling. We experimentally demonstrate that the proposed pooling operations provide a boost in invariance properties relative to conventional pooling and set the state of the art on several widely adopted benchmark datasets; they are also easy to implement, and can be applied within various deep neural network architectures. These benefits come with only a light increase in computational overhead during training and a very modest increase in the number of model parameters.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,464,472,,,,,,,,,,,,,,,,WOS:000508662100051,0
C,"Mandt, S; McInerney, J; Abrol, F; Ranganath, R; Blei, D",,"Gretton, A; Robert, CC",,"Mandt, Stephan; McInerney, James; Abrol, Farhan; Ranganath, Rajesh; Blei, David",,,Variational Tempering,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Variational inference (VI) combined with data subsampling enables approximate posterior inference over large data sets, but suffers from poor local optima. We first formulate a deterministic annealing approach for the generic class of conditionally conjugate exponential family models. This approach uses a decreasing temperature parameter which deterministically deforms the objective during the course of the optimization. A well-known drawback to this annealing approach is the choice of the cooling schedule. We therefore introduce variational tempering, a variational algorithm that introduces a temperature latent variable to the model. In contrast to related work in the Markov chain Monte Carlo literature, this algorithm results in adaptive annealing schedules. Lastly, we develop local variational tempering, which assigns a latent temperature to each data point; this allows for dynamic annealing that varies across data. Compared to the traditional VI, all proposed approaches find improved predictive likelihoods on held-out data.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,704,712,,,,,,,,,,,,,,,,WOS:000508662100077,0
C,"Toulis, P; Tran, D; Airoldi, EM",,"Gretton, A; Robert, CC",,"Toulis, Panos; Tran, Dustin; Airoldi, Edoardo M.",,,Towards stability and optimality in stochastic gradient descent,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Iterative procedures for parameter estimation based on stochastic gradient descent (SGD) allow the estimation to scale to massive data sets. However, they typically suffer from numerical instability, while estimators based on SGD are statistically inefficient as they do not use all the information in the data set. To address these two issues we propose an iterative estimation procedure termed averaged implicit SGD (AI-SGD). For statistical efficiency AI-SGD employs averaging of the iterates, which achieves the Cramer-Rao bound under strong convexity, i.e., it is asymptotically an optimal unbiased estimator of the true parameter value. For numerical stability AI-SGD employs an implicit update at each iteration, which is similar to updates performed by proximal operators in optimization. In practice, AI-SGD achieves competitive performance with state-of-the-art procedures. Furthermore, it is more stable than averaging procedures that do not employ proximal updates, and is simple to implement as it requires fewer tunable hyperparameters than procedures that do employ proximal updates.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,1290,1298,,,,,,,,,,,,,,,,WOS:000508662100140,0
C,"Le, T; Nguyen, V; Nguyen, TD; Phung, D",,"Gretton, A; Robert, CC",,Trung Le; Vu Nguyen; Tu Dinh Nguyen; Dinh Phung,,,Nonparametric Budgeted Stochastic Gradient Descent,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"One of the most challenging problems in kernel online learning is to bound the model size. Budgeted kernel online learning addresses this issue by bounding the model size to a predefined budget. However, determining an appropriate value for such predefined budget is arduous. In this paper, we propose the Nonparametric Budgeted Stochastic Gradient Descent that allows the model size to automatically grow with data in a principled way. We provide theoretical analysis to show that our framework is guaranteed to converge for a large collection of loss functions (e.g. Hinge, Logistic, L2, L1, and epsilon-insensitive) which enables the proposed algorithm to perform both classification and regression tasks without hurting the ideal convergence rate O (1/T) of the standard Stochastic Gradient Descent. We validate our algorithm on the real-world datasets to consolidate the theoretical claims.",,,,,"Nguyen, Vu/HGV-1806-2022; Nguyen, Vu/AAQ-5062-2020","Nguyen, Vu/0000-0002-0294-4561; Phung, Dinh/0000-0002-9977-8247",,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,564,572,,,,,,,,,,,,,,,,WOS:000508662100062,0
C,"Yu, JQ; Blaschko, M",,"Gretton, A; Robert, CC",,"Yu, Jiaqian; Blaschko, Matthew",,,A Convex Surrogate Operator for General Non-Modular Loss Functions,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Empirical risk minimization frequently employs convex surrogates to underlying discrete loss functions in order to achieve computational tractability during optimization. However, classical convex surrogates can only tightly bound modular loss functions, submodular functions or supermodular functions separately while maintaining polynomial time computation. In this work, a novel generic convex surrogate for general non-modular loss functions is introduced, which provides for the first time a tractable solution for loss functions that are neither supermodular nor submodular. This convex surrogate is based on a submodular-supermodular decomposition for which the existence and uniqueness is proven in this paper. It takes the sum of two convex surrogates that separately bound the supermodular component and the submodular component using slack-rescaling and the Lovasz hinge, respectively. It is further proven that this surrogate is convex, piecewise linear, an extension of the loss function, and for which subgradient computation is polynomial time. Empirical results are reported on a non-submodular loss based on the Sorensen-Dice difference function, and a real-world face track dataset with tens of thousands of frames, demonstrating the improved performance, efficiency, and scalability of the novel convex surrogate.",,,,,,"Blaschko, Matthew/0000-0002-2640-181X",,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,1032,1041,,,,,,,,,,,,,,,,WOS:000508662100112,0
C,"Zaheer, M; Wick, M; Tristan, JB; Smola, A; Steele, GL",,"Gretton, A; Robert, CC",,"Zaheer, Manzil; Wick, Michael; Tristan, Jean-Baptiste; Smola, Alex; Steele, Guy L., Jr.",,,Exponential Stochastic Cellular Automata for Massively Parallel Inference,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"We propose an embarrassingly parallel, memory efficient inference algorithm for latent variable models in which the complete data likelihood is in the exponential family. The algorithm is a stochastic cellular automaton and converges to a valid maximum a posteriori fixed point. Applied to latent Dirichlet allocation we find that our algorithm is over an order or magnitude faster than the fastest current approaches. A simple C++/MPI implementation on a 20-node Amazon EC2 cluster samples at more than 1 billion tokens per second. We process 3 billion documents and achieve predictive power competitive with collapsed Gibbs sampling and variational inference.",,,,,"Zaheer, Manzil/ABG-6249-2021",,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,966,975,,,,,,,,,,,,,,,,WOS:000508662100105,0
C,"Davies, V; Reeve, R; Harvey, W; Maree, FF; Husmeier, D",,"Kaski, S; Corander, J",,"Davies, Vinny; Reeve, Richard; Harvey, William; Maree, Francois F.; Husmeier, Dirk",,,Sparse Bayesian Variable Selection for the Identification of Antigenic Variability in the Foot-and-Mouth Disease Virus,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"Vaccines created from closely related viruses are vital for offering protection against newly emerging strains. For Foot-and-Mouth disease virus (FMDV), where multiple serotypes co-circulate, testing large numbers of vaccines can be infeasible. Therefore the development of an in silico predictor of cross-protection between strains is important to help optimise vaccine choice. Here we describe a novel sparse Bayesian variable selection model using spike and slab priors which is able to predict antigenic variability and identify sites which are important for the neutralisation of the virus. We are able to identify multiple residues which are known to be key indicators of antigenic variability. Many of these were not identified previously using Frequentist mixed-effects models and still cannot be found when an l(1) penalty is used. We further explore how the Markov chain Monte Carlo (MCMC) proposal method for the inclusion of variables can offer significant reductions in computational requirements, both for spike and slab priors in general, and our hierarchical Bayesian model in particular.",,,,,"Reeve, Richard/K-9918-2015; Maree, Francois/AAR-7418-2020","Reeve, Richard/0000-0003-2589-8091; Maree, Francois Frederick/0000-0002-8777-250X; Davies, Vinny/0000-0003-1896-8936",,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,149,158,,,,,,,,,,,,,,,,WOS:000508355800017,0
C,"Gane, A; Hazan, T; Jaakkola, T",,"Kaski, S; Corander, J",,"Gane, Andreea; Hazan, Tamir; Jaakkola, Tommi",,,Learning with Maximum A-Posteriori Perturbation Models,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"Perturbation models are families of distributions induced from perturbations. They combine randomization of the parameters with maximization to draw unbiased samples. Unlike Gibbs' distributions, a perturbation model defined on the basis of low order statistics still gives rise to high order dependencies. In this paper, we analyze, extend and seek to estimate such dependencies from data. In particular, we shift the modelling focus from the parameters of the Gibbs' distribution used as a base model to the space of perturbations. We estimate dependent perturbations over the parameters using a hardEM approach, cast in the form of inverse convex programs. Each inverse program confines the randomization to the parameter polytope responsible for generating the observed answer. We illustrate the method on several computer vision problems.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,247,256,,,,,,,,,,,,,,,,WOS:000508355800028,0
C,"Honorio, J; Jaakkola, TS",,"Kaski, S; Corander, J",,"Honorio, Jean; Jaakkola, Tommi S.",,,Tight Bounds for the Expected Risk of Linear Classifiers and PAC-Bayes Finite-Sample Guarantees,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"We analyze the expected risk of linear classifiers for a fixed weight vector in the minimax setting. That is, we analyze the worst-case risk among all data distributions with a given mean and covariance. We provide a simpler proof of the tight polynomial-tail bound for general random variables. For sub-Gaussian random variables, we derive a novel tight exponential-tail bound. We also provide new PAC-Bayes finite-sample guarantees when training data is available. Our minimax generalization bounds are dimensionality-independent and O(root 1/m) for m samples.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,384,392,,,,,,,,,,,,,,,,WOS:000508355800043,0
C,"Kanagawa, M; Fukumizu, K",,"Kaski, S; Corander, J",,"Kanagawa, Motonobu; Fukumizu, Kenji",,,Recovering Distributions from Gaussian RKHS Embeddings,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"Recent advances of kernel methods have yielded a framework for nonparametric statistical inference called RKHS embeddings, in which all probability distributions are represented as elements in a reproducing kernel Hilbert space, namely kernel means. In this paper, we consider the recovery of the information of a distribution from an estimate of the kernel mean, when a Gaussian kernel is used. To this end, we theoretically analyze the properties of a consistent estimator of a kernel mean, which is represented as a weighted sum of feature vectors. First, we prove that the weighted average of a function in a Besov space, whose weights and samples are given by the kernel mean estimator, converges to the expectation of the function. As corollaries, we show that the moments and the probability measures on intervals can be recovered from an estimate of the kernel mean. We also prove that a consistent estimator of the density of a distribution can be defined using a kernel mean estimator. This result confirms that we can in fact completely recover the information of distributions from RKHS embeddings.",,,,,,"Fukumizu, Kenji/0000-0002-3488-2625; Kanagawa, Motonobu/0000-0002-3948-8053",,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,457,465,,,,,,,,,,,,,,,,WOS:000508355800051,0
C,"Noh, YK; Sugiyama, M; Liu, S; du Plessis, MC; Park, FC; Lee, DD",,"Kaski, S; Corander, J",,"Noh, Yung-Kyun; Sugiyama, Masashi; Liu, Song; du Plessis, Marthinus C.; Park, Frank Chongwoo; Lee, Daniel D.",,,Bias Reduction and Metric Learning for Nearest-Neighbor Estimation of Kullback-Leibler Divergence,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"Asymptotically unbiased nearest-neighbor estimators for KL divergence have recently been proposed and demonstrated in a number of applications. With small sample sizes, however, these nonparametric methods typically suffer from high estimation bias due to the non-local statistics of empirical nearest-neighbor information. In this paper, we show that this non-local bias can be mitigated by changing the distance metric, and we propose a method for learning an optimal Mahalanobis-type metric based on global information provided by approximate parametric models of the underlying densities. In both simulations and experiments, we demonstrate that this interplay between parametric models and nonparametric estimation methods significantly improves the accuracy of the nearest-neighbor KL divergence estimator.",,,,,"Sugiyama, Masashi/AEO-1176-2022","Sugiyama, Masashi/0000-0001-6658-6743; Lee, Daniel/0000-0003-4239-8777",,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,669,677,,,,,,,,,,,,,,,,WOS:000508355800074,0
C,"Parviainen, P; Farahani, HS; Lagergren, J",,"Kaski, S; Corander, J",,"Parviainen, Pekka; Farahani, Hossein Shahrabi; Lagergren, Jens",,,Learning Bounded Tree-width Bayesian Networks using Integer Linear Programming,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"In many applications one wants to compute conditional probabilities given a Bayesian network. This inference problem is NP-hard in general but becomes tractable when the network has low tree-width. Since the inference problem is common in many application areas, we provide a practical algorithm for learning bounded tree-width Bayesian networks. We cast this problem as an integer linear program (ILP). The program can be solved by an anytime algorithm which provides upper bounds to assess the quality of the found solutions. A key component of our program is a novel integer linear formulation for bounding tree-width of a graph. Our tests clearly indicate that our approach works in practice, as our implementation was able to find an optimal or nearly optimal network for most of the data sets.",,,,,,"Lagergren, Jens/0000-0002-4552-0240; Farahani, Hossein/0000-0002-9503-1875",,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,751,759,,,,,,,,,,,,,,,,WOS:000508355800083,0
C,"Raykar, VC; Agrawal, P",,"Kaski, S; Corander, J",,"Raykar, Vikas C.; Agrawal, Priyanka",,,Sequential crowdsourced labeling as an epsilon-greedy exploration in a Markov Decision Process,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"Crowdsourcing marketplaces are widely used for curating large annotated datasets by collecting labels from multiple annotators. In such scenarios one has to balance the tradeoff between the accuracy of the collected labels, the cost of acquiring these labels, and the time taken to finish the labeling task. With the goal of reducing the labeling cost, we introduce the notion of sequential crowdsourced labeling, where instead of asking for all the labels in one shot we acquire labels from annotators sequentially one at a time. We model it as an epsilon-greedy exploration in a Markov Decision Process with a Bayesian decision theoretic utility function that incorporates accuracy, cost and time. Experimental results confirm that the proposed sequential labeling procedure can achieve similar accuracy at roughly half the labeling cost and at any stage in the labeling process the algorithm achieves a higher accuracy compared to randomly asking for the next label.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,832,840,,,,,,,,,,,,,,,,WOS:000508355800092,0
C,"Shrivastava, A; Li, P",,"Kaski, S; Corander, J",,"Shrivastava, Anshumali; Li, Ping",,,In Defense of MinHash Over SimHash,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"MinHash and SimHash are the two widely adopted Locality Sensitive Hashing (LSH) algorithms for large-scale data processing applications. Deciding which LSH to use for a particular problem at hand is an important question, which has no clear answer in the existing literature. In this study, we provide a theoretical answer (validated by experiments) that MinHash virtually always outperforms SimHash when the data are binary, as common in practice such as search. The collision probability of MinHash is a function of resemblance similarity (R,), while the collision probability of SimHash is a function of cosine similarity (S). To provide a common basis for comparison, we evaluate retrieval results in terms of S for both MinHash and SimHash. This evaluation is valid as we can prove that MinHash is a valid LSH with respect to S, by using a general inequality S-2 < R <= S/2-S. Our worst case analysis can show that MinHash significantly outperforms SimHash in high similarity region. Interestingly, our intensive experiments reveal that MinHash is also substantially better than SimHash even in datasets where most of the data points are not too similar to each other. This is partly because, in practical data, often R >= s/z-s holds where z is only slightly larger than 2 (e.g., z < 2.1). Our restricted worst case analysis by assuming s/z-s <= R <= S/2-S shows that MinHash indeed significantly outperforms SimHash even in low similarity region. We believe the results in this paper will provide valuable guidelines for search in practice, especially when the data are sparse.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,886,894,,,,,,,,,,,,,,,,WOS:000508355800098,0
C,"Wang, J; Trapeznikov, K; Saligrama, V",,"Kaski, S; Corander, J",,"Wang, Joseph; Trapeznikov, Kirill; Saligrama, Venkatesh",,,An LP for Sequential Learning Under Budgets,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"We present a convex framework to learn sequential decisions and apply it to the problem of learning under a budget. We consider the structure proposed in [1], where sensor measurements are acquired in a sequence. The goal after acquiring each new measurement is to make a decision whether to stop and classify or to pay the cost of using the next sensor in the sequence. We introduce a novel formulation of an empirical risk objective for the multi stage sequential decision problem. This objective naturally lends itself to a non-convex multilinear formulation. Nevertheless, we derive a novel perspective that leads to a tight convex objective. This is accomplished by expressing the empirical risk in terms of linear superposition of indicator functions. We then derive an LP formulation by utilizing hinge loss surrogates. Our LP achieves or exceeds the empirical performance of the non-convex alternating algorithm that requires a large number of random initializations. Consequently, the LP has the advantage of guaranteed convergence, global optimality, repeatability and computation efficiency.",,,,,,"Saligrama, Venkatesh/0000-0002-0675-2268",,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,987,995,,,,,,,,,,,,,,,,WOS:000508355800109,0
C,"Ashutosh, K; Nair, J; Kagrecha, A; Jagannathan, K",,"Banerjee, A; Fukumizu, K",,"Ashutosh, Kumar; Nair, Jayakrishnan; Kagrecha, Anmol; Jagannathan, Krishna",,,Bandit algorithms: Letting go of logarithmic regret for statistical robustness,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We study regret minimization in a stochastic multi-armed bandit setting, and establish a fundamental trade-off between the regret suffered under an algorithm, and its statistical robustness. Considering broad classes of underlying arms' distributions, we show that bandit learning algorithms with logarithmic regret are always inconsistent and that consistent learning algorithms always suffer a super-logarithmic regret. This result highlights the inevitable statistical fragility of all 'logarithmic regret' bandit algorithms available in the literature - for instance, if a UCB algorithm designed for sigma-subGaussian distributions is used in a subGaussian setting with a mismatched variance parameter, the learning performance could be inconsistent. Next, we show a positive result: statistically robust and consistent learning performance is attainable if we allow the regret to be slightly worse than logarithmic. Specifically, we propose three classes of distribution oblivious algorithms that achieve an asymptotic regret that is arbitrarily close to logarithmic.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,622,+,,,,,,,,,,,,,,,,WOS:000659893800070,0
C,"Jiang, HM; Chen, ZH; Shi, YY; Dai, B; Zhao, T",,"Banerjee, A; Fukumizu, K",,"Jiang, Haoming; Chen, Zhehui; Shi, Yuyang; Dai, Bo; Zhao, Tuo",,,Learning to Defend by Learning to Attack,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Adversarial training provides a principled approach for training robust neural networks. From an optimization perspective, adversarial training is essentially solving a bilevel optimization problem. The leader problem is trying to learn a robust classifier, while the follower maximization is trying to generate adversarial samples. Unfortunately, such a bilevel problem is difficult to solve due to its highly complicated structure. This work proposes a new adversarial training method based on a generic learning-to-learn (L2L) framework. Specifically, instead of applying existing hand-designed algorithms for the inner problem, we learn an optimizer, which is parametrized as a convolutional neural network. At the same time, a robust classifier is learned to defense the adversarial attack generated by the learned optimizer. Experiments over CIFAR-10 and CIFAR-100 datasets demonstrate that L2L outperforms existing adversarial training methods in both classification accuracy and computational efficiency. Moreover, our L2L framework can be extended to generative adversarial imitation learning and stabilize the training.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,577,+,,,,,,,,,,,,,,,,WOS:000659893800065,0
C,"Klusowski, JM",,"Banerjee, A; Fukumizu, K",,"Klusowski, Jason M.",,,Sharp Analysis of a Simple Model for Random Forests,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Random forests have become an important tool for improving accuracy in regression and classification problems since their inception by Leo Breiman in 2001. In this paper, we revisit a historically important random forest model, called centered random forests, originally proposed by Breiman in 2004 and later studied by Gerard Biau in 2012, where a feature is selected at random and the splits occurs at the midpoint of the node along the chosen feature. If the regression function is d-dimensional and Lipschitz, we show that, given access to n observations, the mean-squared prediction error is O((n(log n)(d-1)/2) = 1/d log 2+1). This positively answers an outstanding question of Biau about whether the rate of convergence for this random forest model could be improved beyond O(n=1/d(4/3)log2+1). Furthermore, by a refined analysis of the approximation and estimation errors for linear models, we show that our new rate cannot be improved in general. Finally, we generalize our analysis and improve current prediction error bounds for another random forest model, called median random forests, in which each tree is constructed from subsampled data and the splits are performed at the empirical median along a chosen feature.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,757,+,,,,,,,,,,,,,,,,WOS:000659893800085,0
C,"Loizou, N; Vaswani, S; Laradji, I; Lacoste-Julien, S",,"Banerjee, A; Fukumizu, K",,"Loizou, Nicolas; Vaswani, Sharan; Laradji, Issam; Lacoste-Julien, Simon",,,Stochastic Polyak Step-size for SGD: An Adaptive Learning Rate for Fast Convergence,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We propose a stochastic variant of the classical Polyak step-size (Polyak, 1987) commonly used in the subgradient method. Although computing the Polyak step-size requires knowledge of the optimal function values, this information is readily available for typical modern machine learning applications. Consequently, the proposed stochastic Polyak step-size (SPS) is an attractive choice for setting the learning rate for stochastic gradient descent (SGD). We provide theoretical convergence guarantees for SGD equipped with SPS in different settings, including strongly convex, convex and non-convex functions. Furthermore, our analysis results in novel convergence guarantees for SGD with a constant step-size. We show that SPS is particularly effective when training over-parameterized models capable of interpolating the training data. In this setting, we prove that SPS enables SGD to converge to the true solution at a fast rate without requiring the knowledge of any problem-dependent constants or additional computational overhead. We experimentally validate our theoretical results via extensive experiments on synthetic and real datasets. We demonstrate the strong performance of SGD with SPS compared to state-of-the-art optimization methods when training over-parameterized models.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801059,0
C,"Lueckmann, JM; Boelts, J; Greenberg, DS; Goncalves, PJ; Macke, JH",,"Banerjee, A; Fukumizu, K",,"Lueckmann, Jan-Matthis; Boelts, Jan; Greenberg, David S.; Goncalves, Pedro J.; Macke, Jakob H.",,,Benchmarking Simulation-Based Inference,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Recent advances in probabilistic modelling have led to a large number of simulation-based inference algorithms which do not require numerical evaluation of likelihoods. However, a public benchmark with appropriate performance metrics for such 'likelihood-free' algorithms has been lacking. This has made it difficult to compare algorithms and identify their strengths and weaknesses. We set out to fill this gap: We provide a benchmark with inference tasks and suitable performance metrics, with an initial selection of algorithms including recent approaches employing neural networks and classical Approximate Bayesian Computation methods. We found that the choice of performance metric is critical, that even state-of-the-art algorithms have substantial room for improvement, and that sequential estimation improves sample efficiency. Neural network-based approaches generally exhibit better performance, but there is no uniformly best algorithm. We provide practical advice and highlight the potential of the benchmark to diagnose problems and improve algorithms. The results can be explored interactively on a companion website. All code is open source, making it possible to contribute further benchmark tasks and inference algorithms.",,,,,,"Greenberg, David/0000-0002-8515-0459; Boelts, Jan/0000-0003-4979-7092; Goncalves, Pedro/0000-0002-6987-4836",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,343,+,,,,,,,,,,,,,,,,WOS:000659893800039,0
C,"Mohapatra, J; Ko, CY; Weng, TW; Chen, PY; Liu, SJ; Daniel, L",,"Banerjee, A; Fukumizu, K",,"Mohapatra, Jeet; Ko, Ching-Yun; Weng, Tsui-Wei; Chen, Pin-Yu; Liu, Sijia; Daniel, Luca",,,Hidden Cost of Randomized Smoothing,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"The fragility of modern machine learning models has drawn a considerable amount of attention from both academia and the public. While immense interests were in either crafting adversarial attacks as a way to measure the robustness of neural networks or devising worst-case analytical robustness verification with guarantees, few methods could enjoy both scalability and robustness guarantees at the same time. As an alternative to these attempts, randomized smoothing adopts a different prediction rule that enables statistical robustness arguments which easily scale to large networks. However, in this paper, we point out the side effects of current randomized smoothing workflows. Specifically, we articulate and prove two major points: 1) the decision boundaries of smoothed classifiers will shrink, resulting in disparity in class-wise accuracy; 2) applying noise augmentation in the training process does not necessarily resolve the shrinking issue due to the inconsistent learning objectives.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804082,0
C,"Omidvar, H; Akhlaghi, V; Franceschetti, M; Gupta, RK",,"Banerjee, A; Fukumizu, K",,"Omidvar, Hamed; Akhlaghi, Vahideh; Franceschetti, Massimo; Gupta, Rajesh K.",,,Associative Convolutional Layers,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We provide a general and easy to implement method for reducing the number of parameters of Convolutional Neural Networks (CNNs) during the training and inference phases. We introduce a simple trainable auxiliary neural network which can generate approximate versions of \slices of the sets of convolutional filters of any CNN architecture from a low dimensional\code space. These slices are then concatenated to form the sets of filters in the CNN architecture. The auxiliary neural network, which we call\Convolutional Slice Generator (CSG), is unique to the network and provides the association among its convolutional layers. We apply our method to various CNN architectures including ResNet, DenseNet, MobileNet and ShueNet. Experiments on CIFAR-10 and ImageNet-1000, without any hyper-parameter tuning, show that our approach reduces the network parameters by approximately 2x while the reduction in accuracy is confined to within one percent and sometimes the accuracy even improves after compression. Interestingly, through our experiments, we show that even when the CSG takes random binary values for its weights that are not learned, still acceptable performances are achieved. To show that our approach generalizes to other tasks, we apply it to an image segmentation architecture, Deeplab V3, on the Pascal VOC 2012 dataset. Results show that without any parameter tuning, there is approximate to 2.3x parameter reduction and the mean Intersection over Union (mIoU) drops by approximate to 3%. Finally, we provide comparisons with several related methods showing the superiority of our method in terms of accuracy.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803073,0
C,"Seddik, ME; Louart, C; Couillet, R; Tamaazousti, M",,"Banerjee, A; Fukumizu, K",,"Seddik, Mohamed El Amine; Louart, Cosme; Couillet, Romain; Tamaazousti, Mohamed",,,The Unexpected Deterministic and Universal Behavior of Large Softmax Classifiers,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"This paper provides a large dimensional analysis of the Softmax classifier. We discover and prove that, when the classifier is trained on data satisfying loose statistical modeling assumptions, its weights become deterministic and solely depend on the data statistical means and covariances. As a striking consequence, despite the implicit and non-linear nature of the underlying optimization problem, the performance of the Softmax classifier is the same as if performed on a mere Gaussian mixture model, thereby disrupting the intuition that non-linearities inherently extract advanced statistical features from the data. Our findings are theoretically as well as numerically sustained on CNN representations of images produced by GANs.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801030,0
C,"Simpson, F; Boukouvalas, A; Cadek, V; Sarkans, E; Durrande, N",,"Banerjee, A; Fukumizu, K",,"Simpson, Fergus; Boukouvalas, Alexis; Cadek, Vaclav; Sarkans, Elvijs; Durrande, Nicolas",,,The Minecraft Kernel: Modelling correlated Gaussian Processes in the Fourier domain,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"In the univariate setting, using the kernel spectral representation is an appealing approach for generating stationary covariance functions. However, performing the same task for multiple-output Gaussian processes is substantially more challenging. We demonstrate that current approaches to modelling cross-covariances with a spectral mixture kernel possess a critical blind spot. For a given pair of processes, the cross-covariance is not reproducible across the full range of permitted correlations, aside from the special case where their spectral densities are of identical shape. We present a solution to this issue by replacing the conventional Gaussian components of a spectral mixture with block components of finite bandwidth (i.e. rectangular step functions). The proposed family of kernel represents the first multi-output generalisation of the spectral mixture kernel that can approximate any stationary multi-output kernel to arbitrary precision.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802036,0
C,"Singh, S",,"Banerjee, A; Fukumizu, K",,"Singh, Shashank",,,Continuum-Armed Bandits: A Function Space Perspective,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Continuum-armed bandits (a.k.a., black-box or 0th-order optimization) involves optimizing an unknown objective function given an oracle that evaluates the function at a query point, with the goal of using as few query points as possible. In the most well-studied case, the objective function is assumed to be Lipschitz continuous and minimax rates of simple and cumulative regrets are known in both noiseless and noisy settings. This paper studies continuum-armed bandits under more general smoothness conditions, namely Besov smoothness conditions, on the objective function. In both noiseless and noisy conditions, we derive minimax rates under simple and cumulative regrets. Our results show that minimax rates over objective functions in a Besov space are identical to minimax rates over objective functions in the smallest Holder space into which the Besov space embeds.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803018,0
C,"Voevodski, K",,"Banerjee, A; Fukumizu, K",,"Voevodski, Konstantin",,,Large Scale K-Median Clustering for Stable Clustering Instances,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,We study the problem of computing a good k-median clustering in a parallel computing environment. We design an efficient algorithm that gives a constant-factor approximation to the optimal solution for stable clustering instances. The notion of stability that we consider is resilience to perturbations of the distances between the points. Our computational experiments show that our algorithm works well in practice - we are able to find better clusterings than Lloyd's algorithm and a centralized coreset construction using samples of the same size.,,,,,,"Voevodski, Konstantin/0000-0002-7518-8242",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803048,0
C,"Yang, ZH; Lei, YW; Lyu, SW; Ying, YM",,"Banerjee, A; Fukumizu, K",,"Yang, Zhenhuan; Lei, Yunwen; Lyu, Siwei; Ying, Yiming",,,Stability and Differential Privacy of Stochastic Gradient Descent for Pairwise Learning with Non-Smooth Loss,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Pairwise learning has recently received increasing attention since it subsumes many important machine learning tasks (e.g. AUC maximization and metric learning) into a unifying framework. In this paper, we give the first-ever-known stability and generalization analysis of stochastic gradient descent (SGD) for pairwise learning with non-smooth loss functions, which are widely used (e.g. Ranking SVM with the hinge loss). We introduce a novel decomposition in its stability analysis to decouple the pairwisely dependent random variables, and derive generalization bounds which are consistent with the setting of pointwise learning. Furthermore, we apply our stability analysis to develop di.erentially private SGD for pairwise learning, for which our utility bounds match with the state-of-the-art output perturbation method (Huai et al., 2020) with smooth losses. Finally, we illustrate the results using specific examples of AUC maximization and similarity metric learning. As a byproduct, we provide an affirmative solution to an open question on the advantage of the nuclear-norm constraint over the Frobenius-norm constraint in similarity metric learning.",,,,,"Ying, Yiming/AGD-7246-2022; Lei, Yunwen/V-2782-2018","Ying, Yiming/0000-0001-7345-6672; Lei, Yunwen/0000-0002-5383-467X",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802045,0
C,"Zhang, XY; Xiao, TH; Jia, HL; Cheng, MM; Yang, MH",,"Banerjee, A; Fukumizu, K",,"Zhang, Xin-Yu; Xiao, Taihong; Jia, Haolin; Cheng, Ming-Ming; Yang, Ming-Hsuan",,,Semi-Supervised Learning with Meta-Gradient,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"In this work, we propose a simple yet effective meta-learning algorithm in semi-supervised learning. We notice that most existing consistency-based approaches suffer from over-fitting and limited model generalization ability, especially when training with only a small number of labeled data. To alleviate this issue, we propose a learn-to-generalize regularization term by utilizing the label information and optimize the problem in a meta-learning fashion. Specifically, we seek the pseudo labels of the unlabeled data so that the model can generalize well on the labeled data, which is formulated as a nested optimization problem. We address this problem using the meta-gradient that bridges between the pseudo label and the regularization term. In addition, we introduce a simple first-order approximation to avoid computing higher-order derivatives and provide theoretic convergence analysis. Extensive evaluations on the SVHN, CIFAR, and ImageNet datasets demonstrate that the proposed algorithm performs favorably against state-of-the-art methods.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,73,+,,,,,,,,,,,,,,,,WOS:000659893800009,0
C,"Bernstein, DI; Saeed, B; Squires, C; Uhler, C",,"Chiappa, S; Calandra, R",,"Bernstein, Daniel Irving; Saeed, Basil; Squires, Chandler; Uhler, Caroline",,,Ordering-Based Causal Structure Learning in the Presence of Latent Variables,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We consider the task of learning a causal graph in the presence of latent confounders given i.i.d. samples from the model. While current algorithms for causal structure discovery in the presence of latent confounders are constraint-based, we here propose a hybrid approach. We prove that under assumptions weaker than faithfulness, any sparsest independence map (IMAP) of the distribution belongs to the Markov equivalence class of the true model. This motivates the Sparsest Poset formulation - that posets can be mapped to minimal IMAPs of the true model such that the sparsest of these IMAPs is Markov equivalent to the true model. Motivated by this result, we propose a greedy algorithm over the space of posets for causal structure discovery in the presence of latent confounders and compare its performance to the current state-of-the-art algorithms FCI and FCI+ on synthetic data.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,4098,4107,,,,,,,,,,,,,,,,WOS:000559931300031,0
C,"Cucuringu, MH; Li, H; Sun, H; Zanetti, L",,"Chiappa, S; Calandra, R",,"Cucuringu, Mihai; Li, Huan; Sun, He; Zanetti, Luca",,,Hermitian matrices for clustering directed graphs: insights and applications,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Graph clustering is a basic technique in machine learning, and has widespread applications in different domains. While spectral techniques have been successfully applied for clustering undirected graphs, the performance of spectral clustering algorithms for directed graphs (digraphs) is not in general satisfactory: these algorithms usually require symmetrising the matrix representing a digraph, and typical objective functions for undirected graph clustering do not capture cluster-structures in which the information given by the direction of the edges is crucial. To overcome these downsides, we propose a spectral clustering algorithm based on a complex-valued matrix representation of digraphs. We analyse its theoretical performance on a Stochastic Block Model for digraphs in which the cluster-structure is given not only by variations in edge densities, but also by the direction of the edges. The significance of our work is highlighted on a data set pertaining to internal migration in the United States: while previous spectral clustering algorithms for digraphs can only reveal that people are more likely to move between counties that are geographically close, our approach is able to cluster together counties with a similar socio-economical profile even when they are geographically distant, and illustrates how people tend to move from rural to more urbanised areas.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,983,991,,,,,,,,,,,,,,,,WOS:000559931300066,0
C,"Djolonga, J; Lucic, M; Cuturi, M; Bachem, O; Bousquet, O; Gelly, S",,"Chiappa, S; Calandra, R",,"Djolonga, Josip; Lucic, Mario; Cuturi, Marco; Bachem, Olivier; Bousquet, Olivier; Gelly, Sylvain",,,Precision-Recall Curves Using Information Divergence Frontiers,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Despite the tremendous progress in the estimation of generative models, the development of tools for diagnosing their failures and assessing their performance has advanced at a much slower pace. Recent developments have investigated metrics that quantify which parts of the true distribution is modeled well, and, on the contrary, what the model fails to capture, akin to precision and recall in information retrieval. In this paper, we present a general evaluation framework for generative models that measures the trade-off between precision and recall using Renyi divergences. Our framework provides a novel perspective on existing techniques and extends them to more general domains. As a key advantage, this formulation encompasses both continuous and discrete models and allows for the design of efficient algorithms that do not have to quantize the data. We further analyze the biases of the approximations used in practice.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2550,2558,,,,,,,,,,,,,,,,WOS:000559931300076,0
C,"Ishibashi, H; Hino, H",,"Chiappa, S; Calandra, R",,"Ishibashi, Hideaki; Hino, Hideitsu",,,Stopping criterion for active learning based on deterministic generalization bounds,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Active learning is a framework in which the learning machine can select the samples to be used for training. This technique is promising, particularly when the cost of data acquisition and labeling is high. In active learning, determining the timing at which learning should be stopped is a critical issue. In this study, we propose a criterion for automatically stopping active learning. The proposed stopping criterion is based on the difference in the expected generalization errors and hypothesis testing. We derive a novel upper bound for the difference in expected generalization errors before and after obtaining a new training datum based on PAC-Bayesian theory. Unlike ordinary PAC-Bayesian bounds, though, the proposed bound is deterministic; hence, there is no uncontrollable trade-off between the confidence and tightness of the inequality. We combine the upper bound with a statistical test to derive a stopping criterion for active learning. We demonstrate the effectiveness of the proposed method via experiments with both artificial and real datasets.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,386,396,,,,,,,,,,,,,,,,WOS:000559931301054,0
C,"Ito, S",,"Chiappa, S; Calandra, R",,"Ito, Shinji",,,An Optimal Algorithm for Bandit Convex Optimization with Strongly-Convex and Smooth Loss,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We consider non-stochastic bandit convex optimization with strongly-convex and smooth loss functions. For this problem, Hazan and Levy have proposed an algorithm with a regret bound of (O) over tilde (d(3/2) root T) given access to an O(d)-self-concordant barrier over the feasible region, where d and T stand for the dimensionality of the feasible region and the number of rounds, respectively. However, there are no known efficient ways for constructing self-concordant barriers for general convex sets, and a (O) over tilde(root d) gap has remained between the upper and lower bounds, as the known regret lower bound is Omega(d root T). Our study resolves these two issues by introducing an algorithm that achieves an optimal regret bound of (O) over tilde (d root T) under a mild assumption, without self-concordant barriers. More precisely, the algorithm requires only a membership oracle for the feasible region, and it achieves an optimal regret bound of (O) over tilde (d root T) under the assumption that the optimal solution is an interior of the feasible region. Even without this assumption, our algorithm achieves (O) over tilde (d(3/2) root T)-regret.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2229,2238,,,,,,,,,,,,,,,,WOS:000559931301055,0
C,"Liu, Y; Helmbold, DP",,"Chiappa, S; Calandra, R",,"Liu, Yang; Helmbold, David P.",,,Online Learning Using Only Peer Prediction,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"This paper considers a variant of the classical online learning problem with expert predictions. Our model's differences and challenges are due to lacking any direct feedback on the loss each expert incurs at each time step t. We propose an approach that uses peer prediction and identify conditions where it succeeds. Our techniques revolve around a carefully designed peer score function s() that scores experts' predictions based on the peer consensus. We show a sufficient condition, that we call peer calibration, under which standard online learning algorithms using loss feedback computed by the carefully crafted s() have bounded regret with respect to the unrevealed ground truth values. We then demonstrate how suitable s() functions can be derived for different assumptions and models.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2032,2041,,,,,,,,,,,,,,,,WOS:000559931302017,0
C,"Meehan, C; Chaudhuri, K; Dasgupta, S",,"Chiappa, S; Calandra, R",,"Meehan, Casey; Chaudhuri, Kamalika; Dasgupta, Sanjoy",,,A Non-Parametric Test to Detect Data-Copying in Generative Models,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Detecting overfitting in generative models is an important challenge in machine learning. In this work, we formalize a form of overfitting that we call data-copying where the generative model memorizes and outputs training samples or small variations thereof. We provide a three sample test for detecting data-copying that uses the training set, a separate sample from the target distribution, and a generated sample from the model, and study the performance of our test on several canonical models and datasets.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3546,3555,,,,,,,,,,,,,,,,WOS:000559931302036,0
C,"Paty, FP; d'Aspremont, A; Cuturi, M",,"Chiappa, S; Calandra, R",,"Paty, Francois-Pierre; d'Aspremont, Alexandre; Cuturi, Marco",,,Regularity as Regularization: Smooth and Strongly Convex Brenier Potentials in Optimal Transport,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Estimating Wasserstein distances between two high-dimensional densities suffers from the curse of dimensionality: one needs an exponential (wrt dimension) number of samples to ensure that the distance between two empirical measures is comparable to the distance between the original densities. Therefore, optimal transport (OT) can only be used in machine learning if it is substantially regularized. On the other hand, one of the greatest achievements of the OT literature in recent years lies in regularity theory: Caffarelli (2000) showed that the OT map between two well behaved measures is Lipschitz, or equivalently when considering 2-Wasserstein distances, that Brenier convex potentials (whose gradient yields an optimal map) are smooth. We propose in this work to draw inspiration from this theory and use regularity as a regularization tool. We give algorithms operating on two discrete measures that can recover nearly optimal transport maps with small distortion, or equivalently, nearly optimal Brenier potentials that are strongly convex and smooth. The problem boils down to solving alternatively a convex QCQP and a discrete OT problem, granting access to the values and gradients of the Brenier potential not only on sampled points, but also out of sample at the cost of solving a simpler QCQP for each evaluation. We propose algorithms to estimate and evaluate transport maps with desired regularity properties, benchmark their statistical performance, apply them to domain adaptation and visualize their action on a color transfer task.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1222,1231,,,,,,,,,,,,,,,,WOS:000559931302067,0
C,"Saemundsson, S; Terenin, A; Hofmann, K; Deisenroth, MP",,"Chiappa, S; Calandra, R",,"Saemundsson, Steindor; Terenin, Alexander; Hofmann, Katja; Deisenroth, Marc Peter",,,Variational Integrator Networks for Physically Structured Embeddings,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Learning workable representations of dynamical systems is becoming an increasingly important problem in a number of application areas. By leveraging recent work connecting deep neural networks to systems of differential equations, we propose variational integrator networks, a class of neural network architectures designed to preserve the geometric structure of physical systems. This class of network architectures facilitates accurate long-term prediction, interpretability, and data-efficient learning, while still remaining highly flexible and capable of modeling complex behavior. We demonstrate that they can accurately learn dynamical systems from both noisy observations in phase space and from image pixels within which the unknown dynamics are embedded.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3078,3086,,,,,,,,,,,,,,,,WOS:000559931302100,0
C,"Sessa, PG; Bogunovic, I; Kamgarpour, M; Krause, A",,"Chiappa, S; Calandra, R",,"Sessa, Pier Giuseppe; Bogunovic, Ilija; Kamgarpour, Maryam; Krause, Andreas",,,Mixed Strategies for Robust Optimization of Unknown Objectives,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We consider robust optimization problems, where the goal is to optimize an unknown objective function against the worst-case realization of an uncertain parameter. For this setting, we design a novel sample-efficient algorithm GP-MRO, which sequentially learns about the unknown objective from noisy point evaluations. GP-MRO seeks to discover a robust and randomized mixed strategy, that maximizes the worst-case expected objective value. To achieve this, it combines techniques from online learning with nonparametric confidence bounds from Gaussian processes. Our theoretical results characterize the number of samples required by GP-MRO to discover a robust near-optimal mixed strategy for different GP kernels of interest. We experimentally demonstrate the performance of our algorithm on synthetic datasets and on human-assisted trajectory planning tasks for autonomous vehicles. In our simulations, we show that robust deterministic strategies can be overly conservative, while the mixed strategies found by GP-MRO significantly improve the overall performance.",,,,,,"Krause, Andreas/0000-0001-7260-9673",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303007,0
C,"Seznec, J; Menard, P; Lazaric, A; Valko, M",,"Chiappa, S; Calandra, R",,"Seznec, Julien; Menard, Pierre; Lazaric, Alessandro; Valko, Michal",,,A single algorithm for both restless and rested rotting bandits,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"In many application domains (e.g., recommender systems, intelligent tutoring systems), the rewards associated to the actions tend to decrease over time. This decay is either caused by the actions executed in the past (e.g., a user may get bored when songs of the same genre are recommended over and over) or by an external factor (e.g., content becomes outdated). These two situations can be modeled as specific instances of the rested and restless bandit settings, where arms are rotting (i.e., their value decrease over time). These problems were thought to be significantly different, since Levine el al, (2017) showed that state-of-the-art algorithms for restless bandit perform poorly in the rested rotting setting. In this paper, we introduce a novel algorithm, Rotting Adaptive Window UCB (RAW-UCB), that achieves near-optimal regret in both rotting rested and restless bandit, without any prior knowledge of the setting (rested or restless) and the type of non-stationarity (e.g., piece-wise constant, bounded variation). This is in striking contrast with previous negative results showing that no algorithm can achieve similar results as soon as rewards are allowed to increase. We confirm our theoretical findings on a number of synthetic and dataset-based experiments.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303008,0
C,"Warrington, A; Naderiparizi, S; Wood, F",,"Chiappa, S; Calandra, R",,"Warrington, Andrew; Naderiparizi, Saeid; Wood, Frank",,,Coping With Simulators That Don't Always Return,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Deterministic models are approximations of reality that are easy to interpret and often easier to build than stochastic alternatives. Unfortunately, as nature is capricious, observational data can never be fully explained by deterministic models in practice. Observation and process noise need to be added to adapt deterministic models to behave stochastically, such that they are capable of explaining and extrapolating from noisy data. We investigate and address computational inefficiencies that arise from adding process noise to deterministic simulators that fail to return for certain inputs; a property we describe as brittle. We show how to train a conditional normalizing flow to propose perturbations such that the simulator succeeds with high probability, increasing computational efficiency.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303073,0
C,"Zhao, Q; Ye, Z; Chen, C; Wang, YS",,"Chiappa, S; Calandra, R",,"Zhao, Qi; Ye, Ze; Chen, Chao; Wang, Yusu",,,Persistence Enhanced Graph Neural Network,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Local structural information can increase the adaptability of graph convolutional networks to large graphs with heterogeneous topology. Existing methods only use relatively simple topological information, such as node degrees. We present a novel approach leveraging advanced topological information, i.e., persistent homology, which measures the information flow efficiency at different parts of the graph. To fully exploit such structural information in real world graphs, we propose a new network architecture which learns to use persistent homology information to reweight messages passed between graph nodes during convolution. For node classification tasks, our network outperforms existing ones on a broad spectrum of graph benchmarks.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2896,2905,,,,,,,,,,,,,,,,WOS:000559931304019,0
C,"Zhou, DR; Cao, Y; Gu, QQ",,"Chiappa, S; Calandra, R",,"Zhou, Dongruo; Cao, Yuan; Gu, Quanquan",,,Accelerated Factored Gradient Descent for Low-Rank Matrix Factorization,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We study the low-rank matrix estimation problem, where the objective function L(M) is defined over the space of positive semidefinite matrices with rank less than or equal to r. A fast approach to solve this problem is matrix factorization, which reparameterizes M as the product of two smaller matrix such that M = UU inverted perpendicular and then performs gradient descent on U directly, a.k.a., factored gradient descent. Since the resulting problem is nonconvex, whether Nesterov's acceleration scheme can be adapted to it remains a long-standing question. In this paper, we answer this question affirmatively by proposing a novel and practical accelerated factored gradient descent method motivated by Nesterov's accelerated gradient descent. The proposed method enjoys better iteration complexity and computational complexity than the state-of-the-art algorithms in a wide regime. The key idea of our algorithm is to restrict all its iterates onto a special convex set, which enables the acceleration. Experimental results demonstrate the faster convergence of our algorithm and corroborate our theory.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,4430,4439,,,,,,,,,,,,,,,,WOS:000559931304023,0
C,"Atan, O; Zame, WR; van der Schaar, M",,"Chaudhuri, K; Sugiyama, M",,"Atan, Onur; Zame, William R.; van der Schaar, Mihaela",,,Sequential Patient Recruitment and Allocation for Adaptive Clinical Trials,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Randomized Controlled Trials (RCTs) are the gold standard for comparing the effectiveness of a new treatment to the current one (the control). Most RCTs allocate the patients to the treatment group and the control group by uniform randomization. We show that this procedure can be highly suboptimal (in terms of learning) if - as is often the case - patients can be recruited in cohorts (rather than all at once), the effects on each cohort can be observed before recruiting the next cohort, and the effects are heterogeneous across identifiable subgroups of patients. We formulate the patient allocation problem as a finite stage Markov Decision Process in which the objective is to minimize a given weighted combination of type-I and type-II errors. Because finding the exact solution to this Markov Decision Process is computationally intractable, we propose an algorithm Knowledge Gradient for Randomized Controlled Trials (RCT-KG) - that yields an approximate solution. Our experiment on a synthetic dataset with Bernoulli outcomes shows that for a given size of trial our method achieves significant reduction in error, and to achieve a prescribed level of confidence (in identifying whether the treatment is superior to the control), our method requires many fewer patients.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901097,0
C,"Bauer, M; Mnih, A",,"Chaudhuri, K; Sugiyama, M",,"Bauer, Matthias; Mnih, Andriy",,,Resampled Priors for Variational Autoencoders,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We propose Learned Accept/Reject Sampling (LABS), a method for constructing richer priors using rejection sampling with a learned acceptance function. This work is motivated by recent analyses of the VAE objective, which pointed out that commonly used simple priors can lead to underfitting. As the distribution induced by LARS involves an intractable normalizing constant, we show how to estimate it and its gradients efficiently. We demonstrate that LARS priors improve VAE performance on several standard datasets both when they are learned jointly with the rest of the model and when they are fitted to a pretrained model. Finally, we show that LARS can be combined with existing methods for defining flexible priors for an additional boost in performance.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,66,75,,,,,,,,,,,,,,,,WOS:000509687900008,0
C,"Cardoso, AR; Xu, H",,"Chaudhuri, K; Sugiyama, M",,"Cardoso, Adrian Rivera; Xu, Huan",,,Risk-Averse Stochastic Convex Bandit,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Motivated by applications in clinical trials and finance, we study the problem of online convex optimization (with bandit feedback) where the decision maker is risk-averse. We provide two algorithms to solve this problem. The first one is a descent-type algorithm which is easy to implement. The second algorithm, which combines the ellipsoid method and a center point device, achieves (almost) optimal regret bounds with respect to the number of rounds. To the best of our knowledge this is the first attempt to address risk-aversion in the online convex bandit problem.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,39,47,,,,,,,,,,,,,,,,WOS:000509687900005,0
C,"Huyuk, A; Tekin, C",,"Chaudhuri, K; Sugiyama, M",,"Huyuk, Alihan; Tekin, Cem",,,Analysis of Thompson Sampling for Combinatorial Multi-armed Bandit with Probabilistically Triggered Arms,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We analyze the regret of combinatorial Thompson sampling (CTS) for the combinatorial multi-armed bandit with probabilistically triggered arms under the semi-bandit feedback setting. We assume that the learner has access to an exact optimization oracle but does not know the expected base arm outcomes beforehand. When the expected reward function is Lipschitz continuous in the expected base arm outcomes, we derive O(  11ogT (pi0z)) regret bound for CTS, where m denotes the number of base arms, pi denotes the minimum non-zero triggering probability of base arm i and / denotes the minimum suboptimality gap of base arm i. We also compare CTS with combinatorial upper confidence bound (CUCB) via numerical experiments on a cascading bandit problem.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901038,0
C,"Lang, H; Sontag, D; Vijayaraghavan, A",,"Chaudhuri, K; Sugiyama, M",,"Lang, Hunter; Sontag, David; Vijayaraghavan, Aravindan",,,Block Stability for MAP Inference,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Recent work (Lang et al., 2018) has shown that some popular approximate MAP inference algorithms perform very well when the input instance is stable. The simplest stability condition assumes that the MAP solution does not change at all when some of the pairwise potentials are adversarially perturbed. Unfortunately, this strong condition does not seem to hold in practice. We introduce a significantly more relaxed condition that only requires portions of an input instance to be stable. Under this block stability condition, we prove that the pairwise LP relaxation is persistent on the stable blocks. We complement our theoretical results with an evaluation of real-world examples from computer vision, and we find that these instances have large stable regions.",,,,,"Vijayaraghavan, Aravindan/I-2257-2015",,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,216,225,,,,,,,,,,,,,,,,WOS:000509687900023,0
C,"Lee, J; James, LF; Choi, S; Caron, F",,"Chaudhuri, K; Sugiyama, M",,"Lee, Juho; James, Lancelot F.; Choi, Seungjin; Caron, Francois",,,A Bayesian model for sparse graphs with flexible degree distribution and overlapping community structure,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We consider a non-projective class of inhomogeneous random graph models with interpretable parameters and a number of interesting asymptotic properties. Using the results of Bollobas et al. (2007), we show that i) the class of models is sparse and ii) depending on the choice of the parameters, the model is either scale-free, with power-law exponent greater than 2, or with an asymptotic degree distribution which is power-law with exponential cut-off. We propose an extension of the model that can accommodate an overlapping community structure. Scalable posterior inference can be performed due to the specific choice of the link probability. We present experiments on five different real-world networks with up to 100,000 nodes and edges, showing that the model can provide a good fit to the degree distribution and recovers well the latent community structure.",,,,,"Lee, Juho/AAA-2901-2022",,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,758,767,,,,,,,,,,,,,,,,WOS:000509687900079,0
C,"Lessard, L; Zhang, XZ; Zhu, XJ",,"Chaudhuri, K; Sugiyama, M",,"Lessard, Laurent; Zhang, Xuezhou; Zhu, Xiaojin",,,An Optimal Control Approach to Sequential Machine Teaching,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Given a sequential learning algorithm and a target model, sequential machine teaching aims to find the shortest training sequence to drive the learning algorithm to the target model. We present the first principled way to find such shortest training sequences. Our key insight is to formulate sequential machine teaching as a time-optimal control problem. This allows us to solve sequential teaching by leveraging key theoretical and computational tools developed over the past 60 years in the optimal control community. Specifically, we study the Pontryagin Maximum Principle, which yields a necessary condition for optimality of a training sequence. We present analytic, structural, and numerical implications of this approach on a case study with a least-squares loss function and gradient descent learner. We compute and visualize the optimal training sequences for this problem, and find that they can vastly outperform the best available heuristics for generating training sequences.",,,,,"Zhang, Xuezhou/ABD-8993-2021",,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902056,0
C,"Li, CY; Bai, K; Li, JQ; Wang, GY; Chen, CY; Carin, L",,"Chaudhuri, K; Sugiyama, M",,"Li, Chunyuan; Bai, Ke; Li, Jianqiao; Wang, Guoyin; Chen, Changyou; Carin, Lawrence",,,Adversarial Learning of a Sampler Based on an Unnormalized Distribution,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We investigate adversarial learning in the case when only an unnormalized form of the density can be accessed, rather than samples. With insights so garnered, adversarial learning is extended to the case for which one has access to an unnormalized form u(x) of the target density function, but no samples. Further, new concepts in GAN regularization are developed, based on learning from samples or from u(x). The proposed method is compared to alternative approaches, with encouraging results demonstrated across a range of applications, including deep soft Q-learning.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687903037,0
C,"Matsuda, T; Hyvarinen, A",,"Chaudhuri, K; Sugiyama, M",,"Matsuda, Takeru; Hyvarinen, Aapo",,,Estimation of Non-Normalized Mixture Models,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We develop a general method for estimating a finite mixture of non-normalized models. A non-normalized model is defined to be a parametric distribution with an intractable normalization constant. Existing methods for estimating non-normalized models without computing the normalization constant are not applicable to mixture models because they contain more than one intractable normalization constant. The proposed method is derived by extending noise contrastive estimation (NCE), which estimates non-normalized models by discriminating between the observed data and some artificially generated noise. In particular, the proposed method provides a probabilistically principled clustering method that is able to utilize a deep representation. Applications to clustering of natural images and neuroimaging data give promising results.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902062,0
C,"Okuno, A; Shimodaira, H",,"Chaudhuri, K; Sugiyama, M",,"Okuno, Akifumi; Shimodaira, Hidetoshi",,,Robust Graph Embedding with Noisy Link Weights,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,We propose beta-graph embedding for robustly learning feature vectors from data vectors and noisy link weights. A newly introduced empirical moment beta-score reduces the influence of contamination and robustly measures the difference between the underlying correct expected weights of links and the specified generative model. The proposed method is computationally tractable; we employ a minibatch-based efficient stochastic algorithm and prove that this algorithm locally minimizes the empirical moment beta-score. We conduct numerical experiments on synthetic and real-world datasets.,,,,,"Shimodaira, Hidetoshi/B-9127-2008","Shimodaira, Hidetoshi/0000-0002-3371-7724",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,664,673,,,,,,,,,,,,,,,,WOS:000509687900069,0
C,"Pedregosa, F; Fatras, K; Casotto, M",,"Chaudhuri, K; Sugiyama, M",,"Pedregosa, Fabian; Fatras, Kilian; Casotto, Mattia",,,Proximal Splitting Meets Variance Reduction,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Despite the raise to fame of stochastic variance reduced methods like SAGA and ProxSvRG, their use in non-smooth optimization is still limited to a few simple cases. Existing methods require to compute the proximal operator of the non-smooth term at each iteration, which, for complex penalties like the total variation, overlapping group lasso or trend filtering, is an iterative process that becomes unfeasible for moderately large problems. In this work we propose and analyze VR-Tos, a variance-reduced method to solve problems with an arbitrary number of non-smooth terms. Like other variance reduced methods, it only requires to evaluate one gradient per iteration and converges with a constant step size, and so is ideally suited for large scale applications. Unlike existing variance reduced methods, it admits multiple non-smooth terms whose proximal operator only needs to be evaluated once per iteration. We provide a convergence rate analysis for the proposed methods that achieves the same asymptotic rate as their full gradient variants and illustrate its computational advantage on 4 different large scale datasets.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,1,10,,,,,,,,,,,,,,,,WOS:000509687900001,0
C,"Porrello, A; Abati, D; Calderara, S; Cucchiara, R",,"Chaudhuri, K; Sugiyama, M",,"Porrello, Angelo; Abati, Davide; Calderara, Simone; Cucchiara, Rita",,,Classifying Signals on Irregular Domains via Convolutional Cluster Pooling,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We present a, rued and hierarchical approach for supervised classification of signals spanning over a fixed graph, reflecting shared properties of the dataset, lb this end, we introduce a Convolutional Cluster Pooling layer exploiting a. multi-scale clustering in order to highlight, at different resolutions, locally connected regions on the input g posal generalises well established neural models such as Convolutional Neural Networks (CNNs) on irregular and complex domains by means of the exploitation of the ti sharing propert v in a graph-oriented a, tune. In this work, such property is basecl on the centrality of each vertex within its soft -assigned cluster. Extensive experiments on NTU RGB+L), CIEAR-10 and 20NEWS demonstrate the effectiveness of the proposed technique in capturing both local and global patterns in graph-structured data, out of different domains.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901045,0
C,"Wenk, P; Gorbach, NS; Gotovos, A; Krause, A; Bauer, S; Buhmann, JM",,"Chaudhuri, K; Sugiyama, M",,"Wenk, Philippe; Gorbach, Nico S.; Gotovos, Alkis; Krause, Andreas; Bauer, Stefan; Buhmann, Joachim M.",,,Fast Gaussian process based gradient matching for parameter identification in systems of nonlinear ODES,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Parameter identification and comparison of dynamical systems is a challenging task in many fields. Bayesian approaches based on Gaussian process regression over time-series data have been successfully applied to infer the parameters of a dynamical system without explicitly solving it. While the benefits in computational cost are well established, the theoretical foundation has been criticized in the past. We offer a novel interpretation which leads to a better understanding and improvements in state-of-the-art performance in terms of accuracy, robustness and a decrease in run time due to a more efficient setup for general nonlinear dynamical systems.",,,,,"Buhmann, Joachim/AAU-4760-2020; Gotovos, Alkis/AAS-9761-2021","Krause, Andreas/0000-0001-7260-9673",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901041,0
C,"Yu, C; Karlas, B; Zhong, J; Zhang, C; Liu, J",,"Chaudhuri, K; Sugiyama, M",,"Yu, Chen; Karlas, Bojan; Zhong, Jie; Zhang, Ce; Liu, Ji",,,"AutoML from Service Provider's Perspective: Multi-device, Multi-tenant Model Selection with GP-EI","22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"AutoML has become a popular service that is provided by most leading cloud service providers today. In this paper, we focus on the AutoMI. problem from the service provider's perspective, motivated by the following practical consideration: When an AutoML service needs to serve multiple as users with epic devices at the same time, how can we allocate these devices to users in an efficient way? We focus on GP-EI, one of the most popular algorithms for automatic model select ion and hyperparameter tuning, used by systems such as Google Vizer. The technical contribution of this paper is the first multi device, multi-tenant algorithm for GE-EI that is aware of multiple computation devices and multiple users sharing the same set of computation devices. Theoretically, given N users and M devices, we obtain a regret bound of O((MIU(T, K) + M) N-2/M), where MIU(T,K) refers to the maximal incremental uncertainty up to time T for the covariance matrix K. Empirically, we evaluate our algorithm on two applications of automatic model selection, and show that our algorithm significantly outperforms the strategy of serving users independently. Moreover, when multiple computation devices are available, we achieve near-linear speedup when the number of users is much larger than the number of devices.",,,,,,"Zhang, Ce/0000-0002-8105-7505",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902090,0
C,"Yu, Y; Wu, JX; Huang, JZ",,"Chaudhuri, K; Sugiyama, M",,"Yu, Yue; Wu, Jiaxiang; Huang, Junzhou",,,Exploring Fast and Communication-Efficient Algorithms in Large-scale Distributed Networks,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"The communication overhead has become a significant bottleneck in data-parallel network with the increasing of model size and data samples. In this work, we propose a new algorithm LPC-SVRG with quantized gradients and its acceleration ALPC-SVRG to effectively reduce the communication complexity while maintaining the same convergence as the unquantized algorithms. Specifically, we formulate the heuristic gradient clipping technique within the quantization scheme and show that unbiased quantization methods in related works [3, 33, 38] are special cases of ours. We introduce double sampling in the accelerated algorithm ALPC-SVRG to fully combine the gradients of full-precision and low-precision, and then achieve acceleration with fewer communication overhead. Our analysis focuses on the nonsmooth composite problem, which makes our algorithms more general. The experiments on linear models and deep neural networks validate the effectiveness of our algorithms.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,674,683,,,,,,,,,,,,,,,,WOS:000509687900070,0
C,"Zhao, RB; Haskell, WB; Tan, VYF",,"Chaudhuri, K; Sugiyama, M",,"Zhao, Renbo; Haskell, William B.; Tan, Vincent Y. F.",,,An Optimal Algorithm for Stochastic Three-Composite Optimization,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We develop an optimal primal-dual first order algorithm for a class of stochastic three-composite convex minimization problems. The convergence rate of our method not only improves upon the existing methods, but also matches a lower bound derived for all first-order methods that solve this problem. We extend our proposed algorithm to solve a composite stochastic program with any finite number of nonsmooth functions. In addition, we generalize an optimal stochastic alternating direction method of multipliers (SADMM) algorithm proposed for the two-composite case to solve this problem, and establish its connection to our optimal primal-dual algorithm. We perform extensive numerical experiments on a variety of machine learning applications to demonstrate the superiority of our method via-a-vis the state-of-the-art.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,428,437,,,,,,,,,,,,,,,,WOS:000509687900045,0
C,"Zhou, KW; Cheng, J; Ding, QH; Li, DL; Shang, FH; Luo, ZQ",,"Chaudhuri, K; Sugiyama, M",,"Zhou, Kaiwen; Cheng, James; Ding, Qinghua; Li, Danli; Shang, Fanhua; Luo, Zhi-Quan",,,Direct Acceleration of SAGA using Sampled Negative Momentum,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Variance reduction is a simple and effective technique that accelerates convex (or non-convex) stochastic optimization. Among existing variance reduction methods, SVRG and SAGA adopt unbiased gradient estimators and are the most popular variance reduction methods in recent years. Although various accelerated variants of SVRG (e.g., Katyusha and Acc-Prox-SVRG) have been proposed, the direct acceleration of SAGA still remains unknown. In this paper, we propose a directly accelerated variant of SAGA using a novel Sampled Negative Momentum (SSNM), which achieves the best known oracle complexity for strongly convex problems (with known strong convexity parameter). Consequently, our work fills the void of directly accelerated SAGA.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901067,0
C,"Arbel, M; Gretton, A",,"Storkey, A; PerezCruz, F",,"Arbel, Michael; Gretton, Arthur",,,Kernel Conditional Exponential Family,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"A nonparametric family of conditional distributions is introduced, which generalizes conditional exponential families using functional parameters in a suitable RKHS. An algorithm is provided for learning the generalized natural parameter, and consistency of the estimator is established in the well specified case. In experiments, the new method generally outperforms a competing approach with consistency guarantees, and is competitive with a deep conditional density model on datasets that exhibit abrupt transitions and heteroscedasticity.",,,,,"Arbel, Michael/AAJ-5905-2020",,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300140,0
C,"Cheng, DH; Ruchansky, N; Liu, Y",,"Storkey, A; PerezCruz, F",,"Cheng, Dehua; Ruchansky, Natali; Liu, Yan",,,Matrix completability analysis via graph k-connectivity,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"The problem of low-rank matrix completion is continually attracting attention for its applicability to many real-world problems. Still, the large size, extreme sparsity, and non-uniformity of these matrices pose a challenge. In this paper, we make the observation that even when the observed matrix is not suitable for accurate completion there may be portions of the data where completion is still possible. We propose the CompleteID algorithm, which exploits the non-uniformity of the observation, to analyze the completability of the input instead of blindly applying completion. Balancing statistical accuracy with computational efficiency, we relate completability to edge-connectivity of the graph associated with the input partially-observed matrix. We develop the MaxKCD algorithm for finding maximally k-edge-connected components efficiently. Experiments across datasets from a variety of applications demonstrate not only the success of CompleteID but also the importance of completability analysis.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300042,0
C,"Gu, B; Huo, ZY; Huang, H",,"Storkey, A; PerezCruz, F",,"Gu, Bin; Huo, Zhouyuan; Huang, Heng",,,Asynchronous Doubly Stochastic Group Regularized Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Group regularized learning problems (such as group Lasso) are important in machine learning. The asynchronous parallel stochastic optimization algorithms have received huge attentions recently as handling large scale problems. However, existing asynchronous stochastic algorithms for solving the group regularized learning problems are not scalable enough simultaneously in sample size and feature dimensionality. To address this challenging problem, in this paper, we propose a novel asynchronous doubly stochastic proximal gradient algorithm with variance reduction (AsyDSPG+). To the best of our knowledge, AsyDSPG+ is the first asynchronous doubly stochastic proximal gradient algorithm, which can scale well with the large sample size and high feature dimensionality simultaneously. More importantly, we provide a comprehensive convergence guarantee to AsyDSPG+. The experimental results on various large-scale real-world datasets not only confirm the fast convergence of our new method, but also show that AsyDSPG+ scales better than the existing algorithms with the sample size and dimension simultaneously.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300187,0
C,"Gupta, S; Shilton, A; Rana, S; Venkatesh, S",,"Storkey, A; PerezCruz, F",,"Gupta, Sunil; Shilton, Alistair; Rana, Santu; Venkatesh, Svetha",,,Exploiting Strategy-Space Diversity for Batch Bayesian Optimisation,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"This paper proposes a novel approach to batch Bayesian optimisation using a multi-objective optimisation framework with exploitation and exploration forming two objectives. The key advantage of this approach is that it uses a suite of strategies to balance exploration and exploitation and thus can efficiently handle the optimisation of a variety of functions with small to large number of local extrema. Another advantage is that it automatically determines the batch size within a specified budget avoiding unnecessary function evaluations. Theoretical analysis shows that the regret not only reduces sub-linearly but also by an additional reduction factor determined by the batch size. We demonstrate the efficiency of our algorithm by optimising a variety of benchmark functions, performing hyperparameter tuning of support vector regression and classification, and finally heat treatment process of an Al-Sc alloy. Comparisons with recent baseline algorithms confirm the usefulness of our algorithm.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300057,0
C,"Hashimoto, TB; Yadlowsky, S; Duchi, JC",,"Storkey, A; PerezCruz, F",,"Hashimoto, Tatsunori B.; Yadlowsky, Steve; Duchi, John C.",,,Derivative free optimization via repeated classification,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"We develop an algorithm for minimizing a function using n batched function value measurements at each of T rounds by using classifiers to identify a function's sublevel set. We show that sufficiently accurate classifiers can achieve linear convergence rates, and show that the convergence rate is tied to the difficulty of active learning sublevel sets. Further, we show that the bootstrap is a computationally efficient approximation to the necessary classification scheme. The end result is a computationally efficient derivative-free algorithm requiring no tuning that consistently outperforms other approaches on simulations, standard benchmarks, real-world DNA binding optimization, and airfoil design problems whenever batched function queries are natural.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300212,0
C,"Kremer, J; Sha, F; Igel, C",,"Storkey, A; PerezCruz, F",,"Kremer, Jan; Sha, Fei; Igel, Christian",,,Robust Active Label Correction,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Active label correction addresses the problem of learning from input data for which noisy labels are available (e.g., from imprecise measurements or crowd-sourcing) and each true label can be obtained at a significant cost (e.g., through additional measurements or human experts). To minimize these costs, we are interested in identifying training patterns for which knowing the true labels maximally improves the learning performance. We approximate the true label noise by a model that learns the aspects of the noise that are class-conditional (i.e., independent of the input given the observed label). To select labels for correction, we adopt the active learning strategy of maximizing the expected model change. We consider the change in regularized empirical risk functionals that use different pointwise loss functions for patterns with noisy and true labels, respectively. Different loss functions for the noisy data lead to different active label correction algorithms. If loss functions consider the label noise rates, these rates are estimated during learning, where importance weighting compensates for the sampling bias. We show empirically that viewing the true label as a latent variable and computing the maximum likelihood estimate of the model parameters performs well across all considered problems. A maximum a posteriori estimate of the model parameters was beneficial in most test cases. An image classification experiment using convolutional neural networks demonstrates that the class-conditional noise model, which can be learned efficiently, can guide re-labeling in real-world applications.",,,,,,"Kremer, Jan/0000-0002-1348-7095",,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300033,0
C,"Sokolovska, N; Chevaleyre, Y; Zucker, JD",,"Storkey, A; PerezCruz, F",,"Sokolovska, Nataliya; Chevaleyre, Yann; Zucker, Jean-Daniel",,,A Provable Algorithm for Learning Interpretable Scoring Systems,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Score learning aims at taking advantage of supervised learning to produce interpretable models which facilitate decision making. Scoring systems are simple classification models that let users quickly perform stratification. Ideally, a scoring system is based on simple arithmetic operations, is sparse, and can be easily explained by human experts. In this contribution, we introduce an original methodology to simultaneously learn interpretable binning mapped to a class variable, and the weights associated with these bins contributing to the score. We develop and show the theoretical guarantees for the proposed method. We demonstrate by numerical experiments on benchmark data sets that our approach is competitive compared to the state-of-the-art methods. We illustrate by a real medical problem of type 2 diabetes remission prediction that a scoring system learned automatically purely from data is comparable to one manually constructed by clinicians.",,,,,"ZUCKER, Jean-Daniel/K-3008-2016","ZUCKER, Jean-Daniel/0000-0002-5597-7922",,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300060,0
C,"Sutherland, DJ; Strathmann, H; Arbel, M; Gretton, A",,"Storkey, A; PerezCruz, F",,"Sutherland, Dougal J.; Strathmann, Heiko; Arbel, Michael; Gretton, Arthur",,,Efficient and principled score estimation with Nystrom kernel exponential families,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"We propose a fast method with statistical guarantees for learning an exponential family density model where the natural parameter is in a reproducing kernel Hilbert space, and may be infinite-dimensional. The model is learned by fitting the derivative of the log density, the score, thus avoiding the need to compute a normalization constant. Our approach improves the computational efficiency of an earlier solution by using a low-rank, Nystrom-like solution. The new solution retains the consistency and convergence rates of the full-rank solution (exactly in Fisher distance, and nearly in other distances), with guarantees on the degree of cost and storage reduction. We evaluate the method in experiments on density estimation and in the construction of an adaptive Hamiltonian Monte Carlo sampler. Compared to an existing score learning approach using a denoising autoencoder, our estimator is empirically more data-efficient when estimating the score, runs faster, and has fewer parameters (which can be tuned in a principled and interpretable way), in addition to providing statistical guarantees.",,,,,"Arbel, Michael/AAJ-5905-2020","Gretton, Arthur/0000-0003-3169-7624; Sutherland, Danica J./0000-0002-1525-3532",,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300069,0
C,"Dicker, LH; Erdogdu, MA",,"Gretton, A; Robert, CC",,"Dicker, Lee H.; Erdogdu, Murat A.",,,Maximum Likelihood for Variance Estimation in High-Dimensional Linear Models,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"We study maximum likelihood estimators (MLEs) for the residual variance, the signal-to-noise ratio, and other variance parameters in high-dimensional linear models. These parameters are essential in many statistical applications involving regression diagnostics, inference, tuning parameter selection for high-dimensional regression, and other applications, including genetics. The estimators that we study are not new, and have been widely used for variance component estimation in linear random-effects models. However, our analysis is new and it implies that the MLEs, which were devised for random-effects models, may also perform very well in high-dimensional linear models with fixed-effects, which are more commonly studied in some areas of high-dimensional statistics. The MLEs are shown to be consistent and asymptotically normal in fixed-effects models with random design, in asymptotic settings where the number of predictors (p) is proportional to the number of observations (n). Moreover, the estimators' asymptotic variance can be given explicitly in terms moments of the Marcenko-Pastur distribution. A variety of analytical and empirical results show that the MLEs outperform other, previously proposed estimators for variance parameters in high-dimensional linear models with fixed-effects. More broadly, the results in this paper illustrate a strategy for drawing connections between fixed- and random-effects models in high dimensions, which may be useful in other applications.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,159,167,,,,,,,,,,,,,,,,WOS:000508662100018,0
C,"Gabillon, V; Lazaric, A; Ghavamzadeh, M; Ortner, R; Bartlett, P",,"Gretton, A; Robert, CC",,"Gabillon, Victor; Lazaric, Alessandro; Ghavamzadeh, Mohammad; Ortner, Ronald; Bartlett, Peter",,,Improved Learning Complexity in Combinatorial Pure Exploration Bandits,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"We study the problem of combinatorial pure exploration in the stochastic multi-armed bandit problem. We first construct a new measure of complexity that provably characterizes the learning performance of the algorithms we propose for the fixed confidence and the fixed budget setting. We show that this complexity is never higher than the one in existing work and illustrate a number of configurations in which it can be significantly smaller. While in general this improvement comes at the cost of increased computational complexity, we provide a series of examples, including a planning problem, where this extra cost is not significant.",,,,,,"Ortner, Ronald/0000-0001-6033-2208",,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,1004,1012,,,,,,,,,,,,,,,,WOS:000508662100109,0
C,"Hsu, LK; Achim, T; Ermon, S",,"Gretton, A; Robert, CC",,"Hsu, Lun-Kai; Achim, Tudor; Ermon, Stefano",,,Tight Variational Bounds via Random Projections and I-Projections,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Information projections are the key building block of variational inference algorithms and are used to approximate a target probabilistic model by projecting it onto a family of tractable distributions. In general, there is no guarantee on the quality of the approximation obtained. To overcome this issue, we introduce a new class of random projections to reduce the dimensionality and hence the complexity of the original model. In the spirit of random projections, the projection preserves (with high probability) key properties of the target distribution. We show that information projections can be combined with random projections to obtain provable guarantees on the quality of the approximation obtained, regardless of the complexity of the original model. We demonstrate empirically that augmenting mean field with a random projection step dramatically improves partition function and marginal probability estimates, both on synthetic and real world data.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,1087,1095,,,,,,,,,,,,,,,,WOS:000508662100118,0
C,"Li, CL; Lin, HT; Lu, CJ",,"Gretton, A; Robert, CC",,"Li, Chun-Liang; Lin, Hsuan-Tien; Lu, Chi-Jen",,,Rivalry of Two Families of Algorithms for Memory-Restricted Streaming PCA,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"We study the problem of recovering the subspace spanned by the first k principal components of d-dimensional data under the streaming setting, with a memory bound of O(kd). Two families of algorithms are known for this problem. The first family is based on the framework of stochastic gradient descent. Nevertheless, the convergence rate of the family can be seriously affected by the learning rate of the descent steps and deserves more serious study. The second family is based on the power method over blocks of data, but setting the block size for its existing algorithms is not an easy task. In this paper, we analyze the convergence rate of a representative algorithm with decayed learning rate (Oja and Karhunen, 1985) in the first family for the general k > 1 case. Moreover, we propose a novel algorithm for the second family that sets the block sizes automatically and dynamically with faster convergence rate. We then conduct empirical studies that fairly compare the two families on realworld data. The studies reveal the advantages and disadvantages of these two families.",,,,,"Lin, Hsuan-Tien/AAE-4359-2020; Lu, Chi-Jen/AAQ-3728-2021",,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,473,481,,,,,,,,,,,,,,,,WOS:000508662100052,0
C,"Ravanbakhsh, S; Poczos, B; Schneider, J; Schuurmans, D; Greiner, R",,"Gretton, A; Robert, CC",,"Ravanbakhsh, Siamak; Poczos, Barnabas; Schneider, Jeff; Schuurmans, Dale; Greiner, Russell",,,Stochastic Neural Networks with Monotonic Activation Functions,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"We propose a Laplace approximation that creates a stochastic unit from any smooth monotonic activation function, using only Gaussian noise. This paper investigates the application of this stochastic approximation in training a family of Restricted Boltzmann Machines (RBM) that are closely linked to Bregman divergences. This family, that we call exponential family RBM (Exp-RBM), is a subset of the exponential family Harmoniums that expresses family members through a choice of smooth monotonic non-linearity for each neuron. Using contrastive divergence along with our Gaussian approximation, we show that Exp-RBM can learn useful representations using novel stochastic units.",,,,,"Greiner, Russell/AAQ-4502-2020","Greiner, Russell/0000-0001-8327-934X",,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,809,818,,,,,,,,,,,,,,,,WOS:000508662100088,0
C,"Salah, A; Rogovschi, N; Nadif, M",,"Gretton, A; Robert, CC",,"Salah, Aghiles; Rogovschi, Nicoleta; Nadif, Mohamed",,,Model-based Co-clustering for High Dimensional Sparse Data,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"We propose a novel model based on the von Mises-Fisher (vMF) distribution for co-clustering high dimensional sparse matrices. While existing vMF-based models are only suitable for clustering along one dimension, our model acts simultaneously on both dimensions of a data matrix. Thereby it has the advantage of exploiting the inherent duality between rows and columns. Setting our model under the maximum likelihood (ML) approach and the classification ML (CML) approach, we derive two novel, hard and soft, co-clustering algorithms. Empirical results on numerous synthetic and real-world text datasets, demonstrate the effectiveness of our approach, for modelling high dimensional sparse data and co-clustering. Furthermore, thanks to our formulation, that performs an implicitly adaptive dimensionality reduction at each stage, our model alleviates the problem of high concentration parameters kappa's, a well known difficulty in the classical vMF-based models.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,866,874,,,,,,,,,,,,,,,,WOS:000508662100094,0
C,"Saul, AD; Hensman, J; Vehtari, A; Lawrence, ND",,"Gretton, A; Robert, CC",,"Saul, Alan D.; Hensman, James; Vehtari, Aki; Lawrence, Neil D.",,,Chained Gaussian Processes,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Gaussian process models are flexible, Bayesian non-parametric approaches to regression. Properties of multivariate Gaussians mean that they can be combined linearly in the manner of additive models and via a link function (like in generalized linear models) to handle non-Gaussian data. However, the link function formalism is restrictive, link functions are always invertible and must convert a parameter of interest to an linear combination of the underlying processes. There are many likelihoods and models where a non-linear combination is more appropriate. We term these more general models Chained Gaussian Processes: the transformation of the GPs to the likelihood parameters will not generally be invertible, and that implies that linearisation would only be possible with multiple (localized) links, i.e a chain. We develop an approximate inference procedure for Chained GPs that is scalable and applicable to any factorized likelihood. We demonstrate the approximation on a range of likelihood functions.",,,,,,"Vehtari, Aki/0000-0003-2164-9469",,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,1431,1440,,,,,,,,,,,,,,,,WOS:000508662100155,0
C,"Shahriari, B; Bouchard-Cote, A; de Freitas, N",,"Gretton, A; Robert, CC",,"Shahriari, Bobak; Bouchard-Cote, Alexandre; de Freitas, Nando",,,Unbounded Bayesian Optimization via Regularization,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Bayesian optimization has recently emerged as a powerful and flexible tool in machine learning for hyperparameter tuning and more generally for the efficient global optimization of expensive black box functions. The established practice requires a user-defined bounded domain, which is assumed to contain the global optimizer. However, when little is known about the probed objective function, it can be difficult to prescribe such a domain. In this work, we modify the standard Bayesian optimization framework in a principled way to allow for unconstrained exploration of the search space. We introduce a new alternative method and compare it to a volume doubling baseline on two common synthetic benchmarking test functions. Finally, we apply our proposed methods on the task of tuning the stochastic gradient descent optimizer for both a multi-layered perceptron and a convolutional neural network on the MNIST dataset.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,1168,1176,,,,,,,,,,,,,,,,WOS:000508662100127,0
C,"Sra, S; Yu, AW; Li, M; Smola, AJ",,"Gretton, A; Robert, CC",,"Sra, Suvrit; Yu, Adams Wei; Li, Mu; Smola, Alexander J.",,,AdaDelay: Delay Adaptive Distributed Stochastic Optimization,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"We develop distributed stochastic convex optimization algorithms under a delayed gradient model in which server nodes update parameters and worker nodes compute stochastic (sub)gradients. Our setup is motivated by the behavior of real-world distributed computation systems; in particular, we analyze a setting wherein worker nodes can be differently slow at different times. In contrast to existing approaches, we do not impose a worst-case bound on the delays experienced but rather allow the updates to be sensitive to the actual delays experienced. This sensitivity allows use of larger stepsizes, which can help speed up initial convergence without having to wait too long for slower machines; the global convergence rate is still preserved. We experiment with different delay patterns, and obtain noticeable improvements for large-scale real datasets with billions of examples and features.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,957,965,,,,,,,,,,,,,,,,WOS:000508662100104,0
C,"Blanchard, G; Scott, C",,"Kaski, S; Corander, J",,"Blanchard, Gilles; Scott, Clayton",,,Decontamination of Mutually Contaminated Models,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"A variety of machine learning problems are characterized by data sets that are drawn from multiple different convex combinations of a fixed set of base distributions. We call this a mutual contamination model. In such problems, it is often of interest to recover these base distributions, or otherwise discern their properties. This work focuses on the problem of classification with multiclass label noise, in a general setting where the noise proportions are unknown and the true class distributions are nonseparable and potentially quite complex. We develop a procedure for decontamination of the contaminated models from data, which then facilitates the design of a consistent discrimination rule. Our approach relies on a novel method for estimating the error when projecting one distribution onto a convex combination of others, where the projection is with respect to a statistical distance known as the separation distance. Under sufficient conditions on the amount of noise and purity of the base distributions, this projection procedure successfully recovers the underlying class distributions. Connections to novelty detection, topic modeling, and other learning problems are also discussed.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,1,9,,,,,,,,,,,,,,,,WOS:000508355800001,0
C,"Kumar, A; Beutel, A; Ho, Q; Xing, EP",,"Kaski, S; Corander, J",,"Kumar, Abhimanu; Beutel, Alex; Ho, Qirong; Xing, Eric P.",,,Fugue: Slow-Worker-Agnostic Distributed Learning for Big Models on Big Data,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"We present a scheme for fast, distributed learning on big (i.e. high-dimensional) models applied to big datasets. Unlike algorithms that focus on distributed learning in either the big data or big model setting (but not both), our scheme partitions both the data and model variables simultaneously. This not only leads to faster learning on distributed clusters, but also enables machine learning applications where both data and model are too large to fit within the memory of a single machine. Furthermore, our scheme allows worker machines to perform additional updates while waiting for slow workers to finish, which provides users with a tunable synchronization strategy that can be set based on learning needs and cluster conditions. We prove the correctness of such strategies, as well as provide bounds on the variance of the model variables under our scheme. Finally, we present empirical results for latent space models such as topic models, which demonstrate that our method scales well with large data and model sizes, while beating learning strategies that fail to take both data and model partitioning into account.",,,,,"Kumar, Abhimanu/F-9191-2016","Kumar, Abhimanu/0000-0003-4529-7883",,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,531,539,,,,,,,,,,,,,,,,WOS:000508355800059,0
C,"London, B; Huang, B; Taskar, B; Getoor, L",,"Kaski, S; Corander, J",,"London, Ben; Huang, Bert; Taskar, Ben; Getoor, Lise",,,PAC-Bayesian Collective Stability,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"Recent results have shown that the generalization error of structured predictors decreases with both the number of examples and the size of each example, provided the data distribution has weak dependence and the predictor exhibits a smoothness property called collective stability. These results use an especially strong definition of collective stability that must hold uniformly over all inputs and all hypotheses in the class. We investigate whether weaker definitions of collective stability suffice. Using the PAC-Bayes framework, which is particularly amenable to our new definitions, we prove that generalization is indeed possible when uniform collective stability happens with high probability over draws of predictors (and inputs). We then derive a generalization bound for a class of structured predictors with variably convex inference, which suggests a novel learning objective that optimizes collective stability.",,,,,"Huang, Bert/E-2576-2016","Huang, Bert/0000-0002-8548-7246",,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,585,594,,,,,,,,,,,,,,,,WOS:000508355800065,0
C,"Neiswanger, W; Wood, F; Xing, E",,"Kaski, S; Corander, J",,"Neiswanger, Willie; Wood, Frank; Xing, Eric",,,The Dependent Dirichlet Process Mixture of Objects for Detection-free Tracking and Object Modeling,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"This paper explores how to find, track, and learn models of arbitrary objects in a video without a predefined method for object detection. We present a model that localizes objects via unsupervised tracking while learning a representation of each object, avoiding the need for pre-built detectors. Our model uses a dependent Dirichlet process mixture to capture the uncertainty in the number and appearance of objects and requires only spatial and color video data that can be efficiently extracted via frame differencing. We give two inference algorithms for use in both online and offline settings, and use them to perform accurate detection-free tracking on multiple real videos. We demonstrate our method in difficult detection scenarios involving occlusions and appearance shifts, on videos containing a large number of objects, and on a recent human-tracking benchmark where we show performance comparable to state of the art detector-based methods.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,660,668,,,,,,,,,,,,,,,,WOS:000508355800073,0
C,"Sarkhel, S; Venugopal, D; Singla, P; Gogate, V",,"Kaski, S; Corander, J",,"Sarkhel, Somdeb; Venugopal, Deepak; Singla, Parag; Gogate, Vibhav",,,Lifted MAP Inference for Markov Logic Networks,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"In this paper, we present a new approach for lifted MAP inference in Markov Logic Networks (MLNs). Our approach is based on the following key result that we prove in the paper: if an MLN has no shared terms then MAP inference over it can be reduced to MAP inference over a Markov network having the following properties: (i) the number of random variables in the Markov network is equal to the number of first-order atoms in the MLN; and (ii) the domain size of each variable in the Markov network is equal to the number of groundings of the corresponding first-order atom. We show that inference over this Markov network is exponentially more efficient than ground inference, namely inference over the Markov network obtained by grounding all first-order atoms in the MLN. We improve this result further by showing that if non-shared MLNs contain no self joins, namely every atom appears at most once in each of its formulas, then all variables in the corresponding Markov network need only be bi-valued. Our approach is quite general and can be easily applied to an arbitrary MLN by simply grounding all of its shared terms. The key feature of our approach is that because we reduce lifted inference to propositional inference, we can use any propositional MAP inference algorithm for performing lifted MAP inference. Within our approach, we experimented with two propositional MAP inference algorithms: Gurobi and MaxWalkSAT. Our experiments on several benchmark MLNs clearly demonstrate our approach is superior to ground MAP inference in terms of scalability and solution quality.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,859,867,,,,,,,,,,,,,,,,WOS:000508355800095,0
C,"Steorts, RC; Hall, R; Fienberg, SE",,"Kaski, S; Corander, J",,"Steorts, Rebecca C.; Hall, Rob; Fienberg, Stephen E.",,,SMERED: A Bayesian Approach to Graphical Record Linkage and De-duplication,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"We propose a novel unsupervised approach for linking records across arbitrarily many files, while simultaneously detecting duplicate records within files. Our key innovation is to represent the pattern of links between records as a bipartite graph, in which records are directly linked to latent true individuals, and only indirectly linked to other records. This flexible new representation of the linkage structure naturally allows us to estimate the attributes of the unique observable people in the population, calculate k-way posterior probabilities of matches across records, and propagate the uncertainty of record linkage into later analyses. Our linkage structure lends itself to an efficient, linear-time, hybrid Markov chain Monte Carlo algorithm, which overcomes many obstacles encountered by previously proposed methods of record linkage, despite the high dimensional parameter space. We assess our results on real and simulated data.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,922,930,,,,,,,,,,,,,,,,WOS:000508355800102,0
C,"Arvanitidis, G; Hauberg, S; Scholkopf, B",,"Banerjee, A; Fukumizu, K",,"Arvanitidis, Georgios; Hauberg, Soren; Schoelkopf, Bernhard",,,Geometrically Enriched Latent Spaces,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"A common assumption in generative models is that the generator immerses the latent space into a Euclidean ambient space. Instead, we consider the ambient space to be a Riemannian manifold, which allows for encoding domain knowledge through the associated Riemannian metric. Shortest paths can then be defined accordingly in the latent space to both follow the learned manifold and respect the ambient geometry. Through careful design of the ambient metric we can ensure that shortest paths are well-behaved even for deterministic generators that otherwise would exhibit a misleading bias. Experimentally we show that our approach improves the interpretability and the functionality of learned representations both using stochastic and deterministic generators.",,,,,"; Hauberg, Soren/L-2104-2016","Arvanitidis, Georgios/0000-0002-0377-2976; Hauberg, Soren/0000-0001-7223-877X",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,631,+,,,,,,,,,,,,,,,,WOS:000659893800071,0
C,"Assaad, S; Mehta, N; Zeng, SX; Henao, R; Tao, CY; Li, F; Datta, S; Carin, L",,"Banerjee, A; Fukumizu, K",,"Assaad, Serge; Mehta, Nikhil; Zeng, Shuxi; Henao, Ricardo; Tao, Chenyang; Li, Fan; Datta, Shounak; Carin, Lawrence",,,Counterfactual Representation Learning with Balancing Weights,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"A key to causal inference with observational data is achieving balance in predictive features associated with each treatment type. Recent literature has explored representation learning to achieve this goal. In this work, we discuss the pitfalls of these strategies - such as a steep trade-off between achieving balance and predictive power - and present a remedy via the integration of balancing weights in causal learning. Specifically, we theoretically link balance to the quality of propensity estimation, emphasize the importance of identifying a proper target population, and elaborate on the complementary roles of feature balancing and weight adjustments. Using these concepts, we then develop an algorithm for flexible, scalable and accurate estimation of causal effects. Finally, we show how the learned weighted representations may serve to facilitate alternative causal learning procedures with appealing statistical features. We conduct an extensive set of experiments on both synthetic examples and standard benchmarks, and report encouraging results relative to state-of-the-art baselines.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802039,0
C,"Bachoc, F; Cesari, T; Gerchinovitz, S",,"Banerjee, A; Fukumizu, K",,"Bachoc, Francois; Cesari, Tommaso; Gerchinovitz, Sebastien",,,The Sample Complexity of Level Set Approximation,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We study the problem of approximating the level set of an unknown function by sequentially querying its values. We introduce a family of algorithms called Bisect and Approximate through which we reduce the level set approximation problem to a local function approximation problem. We then show how this approach leads to rate-optimal sample complexity guarantees for Holder functions, and we investigate how such rates improve when additional smoothness or other structural assumptions hold true.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,424,+,,,,,,,,,,,,,,,,WOS:000659893800048,0
C,"Bi, YJ; Lavaei, J",,"Banerjee, A; Fukumizu, K",,"Bi, Yingjie; Lavaei, Javad",,,On the Absence of Spurious Local Minima in Nonlinear Low-Rank Matrix Recovery Problems,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"The restricted isometry property (RIP) is a well-known condition that guarantees the absence of spurious local minima in low-rank matrix recovery problems with linear measurements. In this paper, we introduce a novel property named bound difference property (BDP) to study low-rank matrix recovery problems with nonlinear measurements. Using RIP and BDP jointly, we propose a new criterion to certify the nonexistence of spurious local minima in the rank-1 case, and prove that it leads to a much stronger theoretical guarantee than the existing bounds on RIP.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,379,+,,,,,,,,,,,,,,,,WOS:000659893800043,0
C,"Dziugaite, GK; Hsu, K; Gharbieh, W; Arpino, G; Roy, DM",,"Banerjee, A; Fukumizu, K",,"Dziugaite, Gintare Karolina; Hsu, Kyle; Gharbieh, Waseem; Arpino, Gabriel; Roy, Daniel M.",,,On the role of data in PAC-Bayes bounds,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"The dominant term in PAC-Bayes bounds is often the Kullback-Leibler divergence between the posterior and prior. For so-called linear PAC-Bayes risk bounds based on the empirical risk of a fixed posterior kernel, it is possible to minimize the expected value of the bound by choosing the prior to be the expected posterior, which we call the oracle prior on the account that it is distribution dependent. In this work, we show that the bound based on the oracle prior can be suboptimal: In some cases, a stronger bound is obtained by using a data-dependent oracle prior, i.e., a conditional expectation of the posterior, given a subset of the training data that is then excluded from the empirical risk term. While using data to learn a prior is a known heuristic, its essential role in optimal bounds is new. In fact, we show that using data can mean the difference between vacuous and nonvacuous bounds. We apply this new principle in the setting of nonconvex learning, simulating data-dependent oracle priors on MNIST and Fashion MNIST with and without held-out data, and demonstrating new nonvacuous bounds in both cases.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,604,+,,,,,,,,,,,,,,,,WOS:000659893800068,0
C,"Eriksson, D; Poloczek, M",,"Banerjee, A; Fukumizu, K",,"Eriksson, David; Poloczek, Matthias",,,Scalable Constrained Bayesian Optimization,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"The global optimization of a high-dimensional black-box function under black-box constraints is a pervasive task in machine learning, control, and engineering. These problems are challenging since the feasible set is typically non-convex and hard to find, in addition to the curses of dimensionality and the heterogeneity of the underlying functions. In particular, these characteristics dramatically impact the performance of Bayesian optimization methods, that otherwise have become the de facto standard for sample-efficient optimization in unconstrained settings, leaving practitioners with evolutionary strategies or heuristics. We propose the scalable constrained Bayesian optimization (SCBO) algorithm that overcomes the above challenges and pushes the applicability of Bayesian optimization far beyond the state-of-the-art. A comprehensive experimental evaluation demonstrates that SCBO achieves excellent results on a variety of benchmarks. To this end, we propose two new control problems that we expect to be of independent value for the scientific community.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,730,+,,,,,,,,,,,,,,,,WOS:000659893800082,0
C,"Fan, TH; Ramadge, PJ",,"Banerjee, A; Fukumizu, K",,"Fan, Ting-Han; Ramadge, Peter J.",,,A Contraction Approach to Model-based Reinforcement Learning,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Despite its experimental success, Model-based Reinforcement Learning still lacks a complete theoretical understanding. To this end, we analyze the error in the cumulative reward using a contraction approach. We consider both stochastic and deterministic state transitions for continuous (non-discrete) state and action spaces. This approach doesn't require strong assumptions and can recover the typical quadratic error to the horizon. We prove that branched rollouts can reduce this error and are essential for deterministic transitions to have a Bellman contraction. Our analysis of policy mismatch error also applies to Imitation Learning. In this case, we show that GAN-type learning has an advantage over Behavioral Cloning when its discriminator is well-trained.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,325,+,,,,,,,,,,,,,,,,WOS:000659893800037,0
C,"Gottlieb, LA; Kaufman, E; Kontorovich, A; Nivasch, G; Pele, O",,"Banerjee, A; Fukumizu, K",,"Gottlieb, Lee-Ad; Kaufman, Eran; Kontorovich, Aryeh; Nivasch, Gabriel; Pele, Ofir",,,Nested Barycentric Coordinate System as an Explicit Feature Map,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We introduce a new embedding technique based on a barycentric coordinate system. We show that our embedding can be used to transform the problem of polytope approximation into one of finding a linear classifier in a higher dimensional (but nevertheless quite sparse) representation. In effect, this embedding maps a piecewise linear function into an everywhere-linear function, and allows us to invoke well-known algorithms for the latter problem to solve the former. We demonstrate that our embedding has applications to the problems of approximating separating polytopes - in fact, it can approximate any convex body and unions of convex bodies - as well as to classification by separating polytopes and piecewise linear regression.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,766,+,,,,,,,,,,,,,,,,WOS:000659893800086,0
C,"Gurbuzbalaban, M; Hu, YH",,"Banerjee, A; Fukumizu, K",,"Gurbuzbalaban, Mert; Hu, Yuanhan",,,Fractional moment-preserving initialization schemes for training deep neural networks,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"A traditional approach to initialization in deep neural networks (DNNs) is to sample the network weights randomly for preserving the second moment of layer outputs. On the other hand, recent results show that training with SGD can result in heavy-tailedness in the distribution of the network weights with a potentially infinite variance. This suggests that the traditional approach to initialization may be restrictive as SGD updates do not necessarily preserve the finiteness of the variance of layer outputs. Motivated by this, we develop initialization schemes for fully connected feed-forward networks that can provably preserve any given moment of order s is an element of (0, 2] over the layers for a class of activations including ReLU, Leaky ReLU, Randomized Leaky ReLU and linear activations. These generalized schemes recover traditional initialization schemes in the limit s -> 2 and serve as part of a principled theory for initialization. For all these schemes, we show that the network output admits a finite almost sure limit as the number of layers grows, and the limit is heavy-tailed in some settings. We also prove that the logarithm of the norm of the network outputs, if properly scaled, will converge to a Gaussian distribution with an explicit mean and variance we can compute depending on the activation used, the value of s chosen and the network width, where log-normality serves as a further justification of why the norm of the network output can be heavy-tailed in DNNs. We also prove that our initialization scheme avoids small network output values more frequently compared to traditional approaches. Our results extend if dropout is used and the proposed initialization strategy does not have an extra cost during the training procedure. Finally, we discuss extensions of our results to convolutional neural networks and show through numerical experiments that our initialization can improve the initial stages of training.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802068,0
C,"Holland, MJ; Haress, E",,"Banerjee, A; Fukumizu, K",,"Holland, Matthew J.; Haress, El Mehdi",,,Learning with risk-averse feedback under potentially heavy tails,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We study learning algorithms that seek to minimize the conditional value-at-risk (CVaR), when all the learner knows is that the losses (and gradients) incurred may be heavy-tailed. We begin by studying a general-purpose estimator of CVaR for potentially heavy-tailed random variables, which is easy to implement in practice, and requires nothing more than finite variance and a distribution function that does not change too fast or slow around just the quantile of interest. With this estimator in hand, we then derive a new learning algorithm which robustly chooses among candidates produced by stochastic gradient-driven sub-processes, obtain excess CVaR bounds, and finally complement the theory with a regression application.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801013,0
C,"Jaini, P; Nielsen, D; Welling, M",,"Banerjee, A; Fukumizu, K",,"Jaini, Priyank; Nielsen, Didrik; Welling, Max",,,Sampling in Combinatorial Spaces with SurVAE Flow Augmented MCMC,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Hybrid Monte Carlo is a powerful Markov Chain Monte Carlo method for sampling from complex continuous distributions. However, a major limitation of HMC is its inability to be applied to discrete domains due to the lack of gradient signal. In this work, we introduce a new approach based on augmenting Monte Carlo methods with SurVAE Flows to sample from discrete distributions using a combination of neural transport methods like normalizing flows and variational dequantization, and the Metropolis-Hastings rule. Our method first learns a continuous embedding of the discrete space using a surjective map and subsequently learns a bijective transformation from the continuous space to an approximately Gaussian distributed latent variable. Sampling proceeds by simulating MCMC chains in the latent space and mapping these samples to the target discrete space via the learned transformations. We demonstrate the efficacy of our algorithm on a range of examples from statistics, computational physics and machine learning, and observe improvements compared to alternative algorithms.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804007,0
C,"Parmas, P; Sugiyama, M",,"Banerjee, A; Fukumizu, K",,"Parmas, Paavo; Sugiyama, Masashi",,,A unified view of likelihood ratio and reparameterization gradients,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Reparameterization (RP) and likelihood ratio (LR) gradient estimators are used to estimate gradients of expectations throughout machine learning and reinforcement learning; however, they are usually explained as simple mathematical tricks, with no insight into their nature. We use a first principles approach to explain that LR and RP are alternative methods of keeping track of the movement of probability mass, and the two are connected via the divergence theorem. Moreover, we show that the space of all possible estimators combining LR and RP can be completely parameterized by a flow field u(x) and importance sampling distribution q(x). We prove that there cannot exist a single-sample estimator of this type outside our characterized space, thus, clarifying where we should be searching for better Monte Carlo gradient estimators.",,,,,"Sugiyama, Masashi/AEO-1176-2022","Sugiyama, Masashi/0000-0001-6658-6743",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804087,0
C,"Subbaswamy, A; Adams, R; Saria, S",,"Banerjee, A; Fukumizu, K",,"Subbaswamy, Adarsh; Adams, Roy; Saria, Suchi",,,Evaluating Model Robustness and Stability to Dataset Shift,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"As the use of machine learning in high impact domains becomes widespread, the importance of evaluating safety has increased. An important aspect of this is evaluating how robust a model is to changes in setting or population, which typically requires applying the model to multiple, independent datasets. Since the cost of collecting such datasets is often prohibitive, in this paper, we propose a framework for analyzing this type of stability using the available data. We use the original evaluation data to determine distributions under which the algorithm performs poorly, and estimate the algorithm's performance on the worst-case distribution. We consider shifts in user defined conditional distributions, allowing some distributions to shift while keeping other portions of the data distribution fixed. For example, in a healthcare context, this allows us to consider shifts in clinical practice while keeping the patient population fixed. To address the challenges associated with estimation in complex, high-dimensional distributions, we derive a debiased estimator which maintains root N-consistency even when machine learning methods with slower convergence rates are used to estimate the nuisance parameters. In experiments on a real medical risk prediction task, we show this estimator can be used to analyze stability and accounts for realistic shifts that could not previously be expressed. The proposed framework allows practitioners to proactively evaluate the safety of their models without requiring additional data collection.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803017,0
C,"Wilde, H; Jewson, J; Vollmer, S; Holmes, C",,"Banerjee, A; Fukumizu, K",,"Wilde, Harrison; Jewson, Jack; Vollmer, Sebastian; Holmes, Chris",,,Foundations of Bayesian Learning from Synthetic Data,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"There is significant growth and interest in the use of synthetic data as an enabler for machine learning in environments where the release of real data is restricted due to privacy or availability constraints. Despite a large number of methods for synthetic data generation, there are comparatively few results on the statistical properties of models learnt on synthetic data, and fewer still for situations where a researcher wishes to augment real data with another party's synthesised data. We use a Bayesian paradigm to characterise the updating of model parameters when learning in these settings, demonstrating that caution should be taken when applying conventional learning algorithms without appropriate consideration of the synthetic data generating process and learning task at hand. Recent results from general Bayesian updating support a novel and robust approach to Bayesian synthetic-learning founded on decision theory that outperforms standard approaches across repeated experiments on supervised learning and inference problems.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,541,+,,,,,,,,,,,,,,,,WOS:000659893800061,0
C,"Yildiz, I; Dy, J; Erdogmus, D; Ostmo, S; Campbell, JP; Chiang, MF; Ioannidis, S",,"Banerjee, A; Fukumizu, K",,"Yildiz, Ilkay; Dy, Jennifer; Erdogmus, Deniz; Ostmo, Susan; Campbell, J. Peter; Chiang, Michael F.; Ioannidis, Stratis",,,Deep Spectral Ranking,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Learning from ranking observations arises in many domains, and siamese deep neural networks have shown excellent inference performance in this setting. However, SGD does not scale well, as an epoch grows exponentially with the ranking observation size. We show that a spectral algorithm can be combined with deep learning methods to significantly accelerate training. We combine a spectral estimate of Plackett-Luce ranking scores with a deep model via the Alternating Directions Method of Multipliers with a Kullback-Leibler proximal penalty. Compared to a state-of-the-art siamese network, our algorithms are up to 175 times faster and attain better predictions by up to 26% Top-1 Accuracy and 6% Kendall-Tau correlation over five real-life ranking datasets.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,361,+,,,,,,,,,,,,,,,,WOS:000659893800041,0
C,"Zhang, YL; Bu, ZQ",,"Banerjee, A; Fukumizu, K",,"Zhang, Yiliang; Bu, Zhiqi",,,Efficient Designs of SLOPE Penalty Sequences in Finite Dimension,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"In linear regression, SLOPE is a new convex analysis method that generalizes the Lasso via the sorted l(1) penalty: larger fitted co-efficients are penalized more heavily. This magnitude-dependent regularization requires an input of penalty sequence A, instead of a scalar penalty as in the Lasso case, thus making the design extremely expensive in computation. In this paper, we propose two efficient algorithms to design the possibly high-dimensional SLOPE penalty, in order to minimize the mean squared error. For Gaussian data matrices, we propose a first order Projected Gradient Descent (PGD) under the Approximate Message Passing regime. For general data matrices, we present a zero-th order Coordinate Descent (CD) to design a sub-class of SLOPE, referred to as the k-level SLOPE. Our CD allows a useful tradeoff between the accuracy and the computation speed. We demonstrate the performance of SLOPE with our designs via extensive experiments on synthetic data and real-world datasets.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803091,0
C,"Bao, H; Sugiyama, M",,"Chiappa, S; Calandra, R",,"Bao, Han; Sugiyama, Masashi",,,Calibrated Surrogate Maximization of Linear-fractional Utility in Binary Classification,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Complex classification performance metrics such as the F-beta-measure and Jaccard index are often used, in order to handle class-imbalanced cases such as information retrieval and image segmentation. These performance metrics are not decomposable, that is, they cannot be expressed in a per-example manner, which hinders a straightforward application of M-estimation widely used in supervised learning. In this paper, we consider linear-fractional metrics, which are a family of classification performance metrics that encompasses many standard ones such as the F-beta-measure and Jaccard index, and propose methods to directly maximize performances under those metrics. A clue to tackle their direct optimization is a calibrated surrogate utility, which is a tractable lower bound of the true utility function representing a given metric. We characterize sufficient conditions which make the surrogate maximization coincide with the maximization of the true utility. Simulation results on benchmark datasets validate the effectiveness of our calibrated surrogate maximization especially if the sample sizes are extremely small.",,,,,"Sugiyama, Masashi/AEO-1176-2022","Sugiyama, Masashi/0000-0001-6658-6743",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2337,2346,,,,,,,,,,,,,,,,WOS:000559931300024,0
C,"Bastani, O",,"Chiappa, S; Calandra, R",,"Bastani, Osbert",,,Sample Complexity of Estimating the Policy Gradient for Nearly Deterministic Dynamical Systems,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Reinforcement learning is a promising approach to learning robotics controllers. It has recently been shown that algorithms based on finite-difference estimates of the policy gradient are competitive with algorithms based on the policy gradient theorem. We propose a theoretical framework for understanding this phenomenon. Our key insight is that many dynamical systems (especially those of interest in robotics control tasks) are nearly deterministic-i.e., they can be modeled as a deterministic system with a small stochastic perturbation. We show that for such systems, finite-difference estimates of the policy gradient can have substantially lower variance than estimates based on the policy gradient theorem. Finally, we empirically evaluate our insights in an experiment on the inverted pendulum.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3858,3868,,,,,,,,,,,,,,,,WOS:000559931300026,0
C,"Frogner, C; Poggio, T",,"Chiappa, S; Calandra, R",,"Frogner, Charlie; Poggio, Tomaso",,,Approximate Inference with Wasserstein Gradient Flows,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We present a novel approximate inference method for diffusion processes, based on the Wasserstein gradient flow formulation of the diffusion. In this formulation, the time-dependent density of the diffusion is derived as the limit of implicit Euler steps that follow the gradients of a particular free energy functional. Existing methods for computing Wasserstein gradient flows rely on discretization of the domain of the diffusion, prohibiting their application to domains in more than several dimensions. We propose instead a discretization-free inference method that computes the Wasserstein gradient flow directly in a space of continuous functions. We characterize approximation properties of the proposed method and evaluate it on a nonlinear filtering task, finding performance comparable to the state-of-the-art for filtering diffusions.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2581,2589,,,,,,,,,,,,,,,,WOS:000559931301007,0
C,"Garreau, D; von Luxburg, U",,"Chiappa, S; Calandra, R",,"Garreau, Damien; von Luxburg, Ulrike",,,Explaining the Explainer: A First Theoretical Analysis of LIME,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Machine learning is used more and more often for sensitive applications, sometimes replacing humans in critical decision-making processes. As such, interpretability of these algorithms is a pressing need. One popular algorithm to provide interpretability is LIME (Local Interpretable Model-Agnostic Explanation). In this paper, we provide the first theoretical analysis of LIME. We derive closed-form expressions for the coefficients of the interpretable model when the function to explain is linear. The good news is that these coefficients are proportional to the gradient of the function to explain: LIME indeed discovers meaningful features. However, our analysis also reveals that poor choices of parameters can lead LIME to miss important features.",,,,,,"Garreau, Damien/0000-0002-7855-2847",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1287,1295,,,,,,,,,,,,,,,,WOS:000559931301015,0
C,"Hao, BT; Zhang, AR; Cheng, G",,"Chiappa, S; Calandra, R",,"Hao, Botao; Zhang, Anru; Cheng, Guang",,,Sparse and Low-rank Tensor Estimation via Cubic Sketchings,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"In this paper, we propose a general framework for sparse and low-rank tensor estimation from cubic sketchings. A two-stage non-convex implementation is developed based on sparse tensor decomposition and thresholded gradient descent, which ensures exact recovery in the noiseless case and stable recovery in the noisy case with high probability. The non-asymptotic analysis sheds light on an interplay between optimization error and statistical error. The proposed procedure is shown to be rate-optimal under certain conditions. As a technical by-product, novel high-order concentration inequalities are derived for studying high-moment sub-Gaussian tensors. An interesting tensor formulation illustrates the potential application to high-order interaction pursuit in high-dimensional linear regression.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1319,1329,,,,,,,,,,,,,,,,WOS:000559931301035,0
C,"Jahani, M; He, X; Ma, CX; Mokhtari, A; Mudigere, D; Ribeiro, A; Takac, M",,"Chiappa, S; Calandra, R",,"Jahani, Majid; He, Xi; Ma, Chenxin; Mokhtari, Aryan; Mudigere, Dheevatsa; Ribeiro, Alejandro; Takac, Martin",,,Efficient Distributed Hessian Free Algorithm for Large-scale Empirical Risk Minimization via Accumulating Sample Strategy,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"In this paper, we propose a Distributed Accumulated Newton Conjugate gradiEnt (DANCE) method in which sample size is gradually increasing to quickly obtain a solution whose empirical loss is under satisfactory statistical accuracy. Our proposed method is multistage in which the solution of a stage serves as a warm start for the next stage which contains more samples (including the samples in the previous stage). The proposed multistage algorithm reduces the number of passes over data to achieve the statistical accuracy of the full training set. Moreover, our algorithm in nature is easy to be distributed and shares the strong scaling property indicating that acceleration is always expected by using more computing nodes. Various iteration complexity results regarding descent direction computation, communication efficiency and stopping criteria are analyzed under convex setting. Our numerical results illustrate that the proposed method outperforms other comparable methods for solving learning problems including neural networks.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2634,2643,,,,,,,,,,,,,,,,WOS:000559931301057,0
C,"Janati, H; Cuturi, M; Gramfort, A",,"Chiappa, S; Calandra, R",,"Janati, Hicham; Cuturi, Marco; Gramfort, Alexandre",,,Spatio-Temporal Alignments: Optimal transport through space and time,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Comparing data defined over space and time is notoriously hard. It involves quantifying both spatial and temporal variability while taking into account the chronological structure of the data. Dynamic Time Warping (DTW) computes a minimal cost alignment between time series that preserves the chronological order but is inherently blind to spatio-temporal shifts. In this paper, we propose Spatio-Temporal Alignments (STA), a new differentiable formulation of DTW that captures spatial and temporal variability. Spatial differences between time samples are captured using regularized Optimal transport. While temporal alignment cost exploits a smooth variant of DTW called soft-DTW. We show how smoothing DTW leads to alignment costs that increase quadratically with time shifts. The costs are expressed using an unbalanced Wasserstein distance to cope with observations that are not probabilities. Experiments on handwritten letters and brain imaging data confirm our theoretical findings and illustrate the effectiveness of STA as a dissimilarity for spatio-temporal data.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1695,1703,,,,,,,,,,,,,,,,WOS:000559931301058,0
C,"Kuzelka, O; Wang, YY",,"Chiappa, S; Calandra, R",,"Kuzelka, Ondrej; Wang, Yuyi",,,Domain-Liftability of Relational Marginal Polytopes,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We study computational aspects of relational marginal polytopes which are statistical relational learning counterparts of marginal polytopes, well-known from probabilistic graphical models. Here, given some first-order logic formula, we can define its relational marginal statistic to be the fraction of groundings that make this formula true in a given possible world. For a list of first-order logic formulas, the relational marginal polytope is the set of all points that correspond to expected values of the relational marginal statistics that are realizable. In this paper we study the following two problems: (i) Do domain-liftability results for the partition functions of Markov logic networks (MLNs) carry over to the problem of relational marginal polytope construction? (ii) Is the relational marginal polytope containment problem hard under some plausible complexity-theoretic assumptions? Our positive results have consequences for lifted weight learning of MLNs. In particular, we show that weight learning of MLNs is domain-liftable whenever the computation of the partition function of the respective MLNs is domain-liftable (this result has not been rigorously proven before).",,,,,"Krejcar, Ondrej/A-8639-2008; Kuzelka, Ondrej/T-3922-2017","Krejcar, Ondrej/0000-0002-5992-2574; Kuzelka, Ondrej/0000-0002-6523-9114",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2284,2291,,,,,,,,,,,,,,,,WOS:000559931301090,0
C,"Li, XC; Wong, TKL; Chen, RTQ; Duvenaud, D",,"Chiappa, S; Calandra, R",,"Li, Xuechen; Wong, Ting-Kam Leonard; Chen, Ricky T. Q.; Duvenaud, David",,,Scalable Gradients for Stochastic Differential Equations,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"The adjoint sensitivity method scalably computes gradients of solutions to ordinary differential equations. We generalize this method to stochastic differential equations, allowing time-efficient and constant-memory computation of gradients with high-order adaptive solvers. Specifically, we derive a stochastic differential equation whose solution is the gradient, a memory-efficient algorithm for caching noise, and conditions under which numerical solutions converge. In addition, we combine our method with gradient-based stochastic variational inference for latent stochastic differential equations. We use our method to fit stochastic dynamics defined by neural networks, achieving competitive performance on a 50-dimensional motion capture dataset.",,,,,"Chen, Ricky Tian Qi/AAS-3168-2021",,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3870,3881,,,,,,,,,,,,,,,,WOS:000559931302009,0
C,"Liu, L; Shen, YY; Li, TY; Caramanis, C",,"Chiappa, S; Calandra, R",,"Liu, Liu; Shen, Yanyao; Li, Tianyang; Caramanis, Constantine",,,High Dimensional Robust Sparse Regression,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We provide a novel {and to the best of our knowledge, the first {algorithm for high dimensional sparse regression with constant fraction of corruptions in explanatory and/or response variables. Our algorithm recovers the true sparse parameters with sub-linear sample complexity, in the presence of a constant fraction of arbitrary corruptions. Our main contribution is a robust variant of Iterative Hard Thresholding. Using this, we provide accurate estimators: when the covariance matrix in sparse regression is identity, our error guarantee is near information-theoretically optimal. We then deal with robust sparse regression with unknown structured covariance matrix. We propose a filtering algorithm which consists of a novel randomized outlier removal technique for robust sparse mean estimation that may be of interest in its own right: the filtering algorithm is flexible enough to deal with unknown covariance. Also, it is orderwise more efficient computationally than the ellipsoid algorithm. Using sub-linear sample complexity, our algorithm achieves the best known (and first) error guarantee. We demonstrate the effectiveness on large-scale sparse regression problems with arbitrary corruptions.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,411,420,,,,,,,,,,,,,,,,WOS:000559931302015,0
C,"Lu, CK; Yang, SCH; Hao, XR; Shafto, P",,"Chiappa, S; Calandra, R",,"Lu, Chi-Ken; Yang, Scott Cheng-Hsin; Hao, Xiaoran; Shafto, Patrick",,,Interpretable Deep Gaussian Processes with Moments,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Deep Gaussian Processes (DGPs) combine the expressiveness of Deep Neural Networks (DNNs) with quantified uncertainty of Gaussian Processes (GPs). Expressive power and intractable inference both result from the non-Gaussian distribution over composition functions. We propose interpretable DGP based on approximating DGP as a GP by calculating the exact moments, which additionally identify the heavy-tailed nature of some DGP distributions. Consequently, our approach admits interpretation as both NNs with specified activation functions and as a variational approximation to DGP. We identify the expressivity parameter of DGP and find non-local and non-stationary correlation from DGP composition. We provide general recipes for deriving the effective kernels for DGP of two, three, or infinitely many layers, composed of homogeneous or heterogeneous kernels. Results illustrate the expressiveness of our effective kernels through samples from the prior and inference on simulated and real data and demonstrate advantages of interpretability by analysis of analytic forms, and draw relations and equivalences across kernels.",,,,,"Yang, Scott Cheng-Hsin/GMW-8090-2022; Lu, Chi-Ken/AAR-3629-2020","Lu, Chi-Ken/0000-0003-1329-6069",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,613,622,,,,,,,,,,,,,,,,WOS:000559931302020,0
C,"Park, TE; Moon, T",,"Chiappa, S; Calandra, R",,"Park, Tae-Eon; Moon, Taesup",,,Unsupervised Neural Universal Denoiser for Finite-Input General-Output Noisy Channel,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We devise a novel neural network-based universal denoiser for the finite-input, general-output (FIGO) channel. Based on the assumption of known noisy channel densities, which is realistic in many practical scenarios, we train the network such that it can denoise as well as the best sliding window denoiser for any given underlying clean source data. Our algorithm, dubbed as Generalized CUDE (Gen-CUDE), enjoys several desirable properties; it can be trained in an unsupervised manner (solely based on the noisy observation data), has much smaller computational complexity compared to the previously developed universal denoiser for the same setting, and has much tighter upper bound on the denoising performance, which is obtained by a theoretical analysis. In our experiments, we show such tighter upper bound is also realized in practice by showing that Gen-CUDE achieves much better denoising results compared to other strong baselines for both synthetic and real underlying clean sequences.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,331,339,,,,,,,,,,,,,,,,WOS:000559931302065,0
C,"Schreuder, N; Brunel, VE; Dalalyan, A",,"Chiappa, S; Calandra, R",,"Schreuder, Nicolas; Brunel, Victor-Emmanuel; Dalalyan, Arnak",,,A nonasymptotic law of iterated logarithm for general M-estimators,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"M-estimators are ubiquitous in machine learning and statistical learning theory. They are used both for defining prediction strategies and for evaluating their precision. In this paper, we propose the first non-asymptotic any-time deviation bounds for general M-estimators, where any-time means that the bound holds with a prescribed probability for every sample size. These bounds are non-asymptotic versions of the law of iterated logarithm. They are established under general assumptions such as Lipschitz continuity of the loss function and (local) curvature of the population risk. These conditions are satisfied for most examples used in machine learning, including those ensuring robustness to outliers and to heavy-tailed distributions. As an example of application, we consider the problem of best arm identification in a stochastic multi-armed bandit setting. We show that the established bound can be converted into a new algorithm, with provably optimal theoretical guarantees. Numerical experiments illustrating the validity of the algorithm are reported.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1331,1340,,,,,,,,,,,,,,,,WOS:000559931300068,0
C,"Shi, JX; Titsias, MK; Mnih, A",,"Chiappa, S; Calandra, R",,"Shi, Jiaxin; Titsias, Michalis K.; Mnih, Andriy",,,Sparse Orthogonal Variational Inference for Gaussian Processes,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We introduce a new interpretation of sparse variational approximations for Gaussian processes using inducing points, which can lead to more scalable algorithms than previous methods. It is based on decomposing a Gaussian process as a sum of two independent processes: one spanned by a finite basis of inducing points and the other capturing the remaining variation. We show that this formulation recovers existing approximations and at the same time allows to obtain tighter lower bounds on the marginal likelihood and new stochastic variational inference algorithms. We demonstrate the efficiency of these algorithms in several Gaussian process models ranging from standard regression to multi-class classification using (deep) convolutional Gaussian processes and report state-of-the-art results on CIFAR-10 among purely GP-based models.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303018,0
C,"Shilton, A; Gupta, S; Rana, S; Vellanki, P; Park, L; Li, C; Venkatesh, S; Dorin, T; Sutti, A; Rubin, D; Slezak, T; Vahid, A; Height, M",,"Chiappa, S; Calandra, R",,"Shilton, Alistair; Gupta, Sunil; Rana, Santu; Vellanki, Pratibha; Park, Laurence; Li, Cheng; Venkatesh, Svetha; Dorin, Thomas; Sutti, Alessandra; Rubin, David; Slezak, Teo; Vahid, Alireza; Height, Murray",,,Accelerated Bayesian Optimization through Weight-Prior Tuning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Bayesian optimization (BO) is a widely-used method for optimizing expensive (to evaluate) problems. At the core of most BO methods is the modeling of the objective function using a Gaussian Process (GP) whose covariance is selected from a set of standard covariance functions. From a weight-space view, this models the objective as a linear function in a feature space implied by the given covariance K, with an arbitrary Gaussian weight prior w similar to N(0, I). In many practical applications there is data available that has a similar (covariance) structure to the objective, but which, having different form, cannot be used directly in standard transfer learning. In this paper we show how such auxiliary data may be used to construct a GP covariance corresponding to a more appropriate weight prior for the objective function. Building on this, we show that we may accelerate BO by modeling the objective function using this (learned) weight prior, which we demonstrate on both test functions and a practical application to short-polymer fibre manufacture.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303019,0
C,"Tao, M; Ohsawa, T",,"Chiappa, S; Calandra, R",,"Tao, Molei; Ohsawa, Tomoki",,,"Variational Optimization on Lie Groups, with Examples of Leading (Generalized) Eigenvalue Problems","INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"The article considers smooth optimization of functions on Lie groups. By generalizing NAG variational principle in vector space (Wibisono et al., 2016) to Lie groups, continuous Lie-NAG dynamics which are guaranteed to converge to local optimum are obtained. They correspond to momentum versions of gradient flow on Lie groups. A particular case of SO(n) is then studied in details, with objective functions corresponding to leading Generalized EigenValue problems: the Lie-NAG dynamics are first made explicit in coordinates, and then discretized in structure preserving fashions, resulting in optimization algorithms with faithful energy behavior (due to conformal symplecticity) and exactly remaining on the Lie group. Stochastic gradient versions are also investigated. Numerical experiments on both synthetic data and practical problem (LDA for MNIST) demonstrate the effectiveness of the proposed methods as optimization algorithms (not as a classification method).",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303038,0
C,"Vieillard, N; Scherrer, B; Pietquin, O; Geist, M",,"Chiappa, S; Calandra, R",,"Vieillard, Nino; Scherrer, Bruno; Pietquin, Olivier; Geist, Matthieu",,,Momentum in Reinforcement Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We adapt the optimization's concept of momentum to reinforcement learning. Seeing the state-action value functions as an analog to the gradients in optimization, we interpret momentum as an average of consecutive q-functions. We derive Momentum Value Iteration (MoVI), a variation of Value iteration that incorporates this momentum idea. Our analysis shows that this allows MoVI to average errors over successive iterations. We show that the proposed approach can be readily extended to deep learning. Specifically,we propose a simple improvement on DQN based on MoVI, and experiment it on Atari games.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303055,0
C,"Weilbach, C; Beronov, B; Harvey, W; Wood, F",,"Chiappa, S; Calandra, R",,"Weilbach, Christian; Beronov, Boyan; Harvey, William; Wood, Frank",,,Structured Conditional Continuous Normalizing Flows for Efficient Amortized Inference in Graphical Models,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We exploit minimally faithful inversion of graphical model structures to specify sparse continuous normalizing flows (CNFs) for amortized inference. We find that the sparsity of this factorization can be exploited to reduce the numbers of parameters in the neural network, adaptive integration steps of the flow, and consequently FLOPs at both training and inference time without decreasing performance in comparison to unconstrained flows. By expressing the structure inversion as a compilation pass in a probabilistic programming language, we are able to apply it in a novel way to models as complex as convolutional neural networks. Furthermore, we extend the training objective for CNFs in the context of inference amortization to the symmetric Kullback-Leibler divergence, and demonstrate its theoretical and practical advantages.",,,,,"Beronov, Boyan/AAC-1242-2020","Beronov, Boyan/0000-0002-0900-752X",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303076,0
C,"Zhang, RQ; Cooper, AF; De Sa, C",,"Chiappa, S; Calandra, R",,"Zhang, Ruqi; Cooper, A. Feder; De Sa, Christopher",,,AMAGOLD: Amortized Metropolis Adjustment for Efficient Stochastic Gradient MCMC,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Stochastic gradient Hamiltonian Monte Carlo (SGHMC) is an efficient method for sampling from continuous distributions. It is a faster alternative to HMC: instead of using the whole dataset at each iteration, SGHMC uses only a subsample. This improves performance, but introduces bias that can cause SGHMC to converge to the wrong distribution. One can prevent this using a step size that decays to zero, but such a step size schedule can drastically slow down convergence. To address this tension, we propose a novel second-order SGMCMC algorithm-AMAGOLD-that infrequently uses Metropolis-Hastings (M-H) corrections to remove bias. The infrequency of corrections amortizes their cost. We prove AMAGOLD converges to the target distribution with a fixed, rather than a diminishing, step size, and that its convergence rate is at most a constant factor slower than a full-batch baseline. We empirically demonstrate AMAGOLD's effectiveness on synthetic distributions, Bayesian logistic regression, and Bayesian neural networks.",,,,,,"Zhang, Ruqi/0000-0002-4340-0528",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2142,2151,,,,,,,,,,,,,,,,WOS:000559931304011,0
C,"Amari, S; Karakida, R; Oizumi, M",,"Chaudhuri, K; Sugiyama, M",,"Amari, Shun-ichi; Karakida, Ryo; Oizumi, Masafumi",,,Fisher Information and Natural Gradient Learning in Random Deep Networks,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"The parameter space of a deep neural network is a Riemannian manifold, where the metric is defined by the Fisher information matrix. The natural gradient method uses the steepest descent direction in a Riemannian manifold, but it requires inversion of the Fisher matrix, however, which is practically difficult. The present paper uses statistical neurodynamical method to reveal the properties of the Fisher information matrix in a net of random connections. We prove that the Fisher information matrix is unit-wise block diagonal supplemented by small order terms of off-block-diagonal elements. We further prove that the Fisher information matrix of a single unit has a simple reduced form, a sum of a diagonal matrix and a rank 2 matrix of weight-bias correlations. We obtain the inverse of Fisher information explicitly. We then have an explicit form of the approximate natural gradient, without relying on the matrix inversion.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,694,702,,,,,,,,,,,,,,,,WOS:000509687900072,0
C,"Gaboardi, M; Rogers, R; Sheffet, O",,"Chaudhuri, K; Sugiyama, M",,"Gaboardi, Marco; Rogers, Ryan; Sheffet, Or",,,Locally Private Mean Estimation: Z-test and Tight Confidence Intervals,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"This work provides tight upper- and lower-bounds for the problem of mean estimation under differential privacy in the local-model, when the input is composed of n i.i.d. drawn samples from a Gaussian. Our algorithms result in a (1 - beta)-confidence interval for the underlying distribution's mean mu of length O (sigma root log(n/beta)log(1/beta)/epsilon root n). In addition, our algorithms leverage on binary search using local differential privacy for quantile estimation, a result which may be of separate interest. Moreover, our algorithms have a matching lower-bound, where we prove that any one-shot (each individual is presented with a single query) local differentially private algorithm must return an interval of length Omega(sigma root log(1/beta)/epsilon root n).",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902061,0
C,"Gao, SY; Brekelmans, R; Ver Steeg, G; Galstyan, A",,"Chaudhuri, K; Sugiyama, M",,"Gao, Shuyang; Brekelmans, Rob; Ver Steeg, Greg; Galstyan, Aram",,,Auto-Encoding Total Correlation Explanation,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Advances in unsupervised learning enable reconstruction and generation of samples from complex distributions, but this success is marred by the inscrutability of the representations learned. We propose an informationtheoretic approach to characterizing disentanglement and dependence in representation learning using multivariate mutual information, also called total correlation. The principle of Total Cor-relation Ex-planation (CorEx) has motivated successful unsupervised learning applications across a variety of domains but under some restrictive assumptions. Here we relax those restrictions by introducing a flexible variational lower bound to CorEx. Surprisingly, we find this lower bound is equivalent to the one in variational autoencoders (VAE) under certain conditions. This information-theoretic view of VAE deepens our understanding of hierarchical VAE and motivates a new algorithm, AnchorVAE, that makes latent codes more interpretable through information maximization and enables generation of richer and more realistic samples.",,,,,,"Galstyan, Aram/0000-0003-4215-0886; Ver Steeg, Greg/0000-0002-0793-141X",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901021,0
C,"Gasthaus, J; Benidis, K; Wang, YY; Rangapuram, SS; Salinas, D; Flunkert, V; Januschowski, T",,"Chaudhuri, K; Sugiyama, M",,"Gasthaus, Jan; Benidis, Konstantinos; Wang, Yuyang; Rangapuram, Syama S.; Salinas, David; Flunkert, Valentin; Januschowski, Tim",,,Probabilistic Forecasting with Spline Quantile Function RNNs,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"In this paper, we propose a flexible method for probabilistic modeling with conditional quantile functions using monotonic regression splines. The shape of the spline is parameterized by a neural network whose parameters are learned by minimizing the continuous ranked probability score. Within this framework, we propose a method for probabilistic time series forecasting, which combines the modeling capacity of recurrent neural networks with the flexibility of a splinebased representation of the output distribution. Unlike methods based on parametric probability density functions and maximum likelihood estimation, the proposed method can flexibly adapt to different output distributions without manual intervention. We empirically demonstrate the effectiveness of the approach on synthetic and real-world data sets.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901098,0
C,"Liu, YR; Xie, DL; Wang, X",,"Chaudhuri, K; Sugiyama, M",,"Liu, Yingru; Xie, Dongliang; Wang, Xin",,,Generalized Boltzmann Machine with Deep Neural Structure,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Restricted Boltzmann Machine (RBM) is an essential component in many machine learning applications. As a probabilistic graphical model, RBM posits a shallow structure, which makes it less capable of modeling real-world applications. In this paper, to bridge the gap between RBM and artificial neural network, we propose an energy-based probabilistic model that is more flexible on modeling continuous data. By introducing the pairwise inverse autoregressive flow into RBM, we propose two generalized continuous RBMs which contain deep neural network structure to more flexibly track the practical data distribution while still keeping the inference tractable. In addition, we extend the generalized RBM structures into sequential setting to better model the stochastic process of time series. Performance improvements on probabilistic modeling and representation learning are demonstrated by the experiments on diverse datasets.",,,,,"Liu, Yingru/AGG-3200-2022","Liu, Yingru/0000-0002-3784-4886",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,926,934,,,,,,,,,,,,,,,,WOS:000509687900096,0
C,"Middleton, L; Deligiannidis, G; Doucet, A; Jacob, PE",,"Chaudhuri, K; Sugiyama, M",,"Middleton, Lawrence; Deligiannidis, George; Doucet, Arnaud; Jacob, Pierre E.",,,Unbiased Smoothing using Particle Independent Metropolis Hastings,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We consider the approximation of expectations with respect to the distribution of a latent Markov process given noisy measurements. This is known as the smoothing problem and is often approached with particle and Markov chain Monte Carlo (MCMC) methods. These methods provide consistent but biased estimators when run for a finite time. We propose a simple way of coupling two MCMC chains built using Particle Independent Metropolis Hastings (PIMH) to produce unbiased smoothing estimators. Unbiased estimators are appealing in the context of parallel computing, and facilitate the construction of confidence intervals. The proposed scheme only requires access to off-the-shelf Particle Filters (PF) and is thus easier to implement than recently proposed unbiased smoothers. The approach is demonstrated on a Levy-driven stochastic volatility model and a stochastic kinetic model.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902044,0
C,"Panda, R; Pensia, A; Mehta, N; Zhou, MY; Rai, P",,"Chaudhuri, K; Sugiyama, M",,"Panda, Rajat; Pensia, Ankit; Mehta, Nikhil; Zhou, Mingyuan; Rai, Piyush",,,Deep Topic Models for Multi-label Learning,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We present a probabilistic framework for multi-label learning based on a deep generative model for the binary label vector associated with each observation. Our generative model learns deep multi-layer latent embeddings of the binary label vector, which are conditioned on the input features of the observation. The model also has an interesting interpretation in terms of a deep topic model, with each label vector representing a bag-of-words document, with the input features being its meta-data. In addition to capturing the structural properties of the label space (e.g., a near-low-rank label matrix), the model also offers a clean, geometric interpretation. In particular, the nonlinear classification boundaries learned by the model can be seen as the union of multiple convex polytopes. Our model admits a simple and scalable inference via efficient Gibbs sampling or EM algorithm. We compare our model with state-of-the-art baselines for multi-label learning on benchmark data sets, and also report some interesting qualitative results.",,,,,"Zhou, Mingyuan/AAE-8717-2021",,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902092,0
C,"Roberts, DA; Gallagher, M; Taimre, T",,"Chaudhuri, K; Sugiyama, M",,"Roberts, David A.; Gallagher, Marcus; Taimre, Thomas",,,Reversible Jump Probabilistic Programming,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"In this paper we present a method for automatically deriving a Reversible Jump Markov chain Monte Carlo sampler from probabilistic programs that specify the target and proposal distributions. The main challenge in automatically deriving such an inference procedure, in comparison to deriving a generic Metropolis Hastings sampler, is in calculating the Jacobian adjustment to the proposal acceptance ratio. To achieve this, our approach relies on the interaction of several different components, including automatic differentiation, transformation inversion, and optimised code generation. We also present Stochaskell, a new probabilistic programming language embedded in Haskell, which provides an implementation of our method.",,,,,,"Roberts, David/0000-0002-6140-0636",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,634,643,,,,,,,,,,,,,,,,WOS:000509687900066,0
C,"Schmit, S; Shah, V; Johari, R",,"Chaudhuri, K; Sugiyama, M",,"Schmit, Sven; Shah, Virag; Johari, Ramesh",,,Optimal Testing in the Experiment-rich Regime,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Motivated by the widespread adoption of large-scale A/B testing in industry, we propose a new experimentation framework for the setting where potential experiments are abundant (i.e., many hypotheses are available to test), and observations are costly; we refer to this as the experiment-rich regime. Such scenarios require the experimenter to internalize the opportunity cost of assigning a sample to a particular experiment. We fully characterize the optimal policy and give an algorithm to compute it. Furthermore, we develop a simple heuristic that also provides intuition for the optimal policy. We use simulations based on real data to compare both the optimal algorithm and the heuristic to other natural alternative experimental design frameworks. In particular, we discuss the paradox of power: high-powered classical tests can lead to highly inefficient sampling in the experiment-rich regime.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,626,633,,,,,,,,,,,,,,,,WOS:000509687900065,0
C,"Sessa, PG; Kamgarpour, M; Krause, A",,"Chaudhuri, K; Sugiyama, M",,"Sessa, Pier Giuseppe; Kamgarpour, Maryam; Krause, Andreas",,,Bounding Inefficiency of Equilibria in Continuous Actions Games using Submodularity and Curvature,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Games with continuous strategy sets arise in several machine learning problems (e.g. adversarial learning). For such games, simple no-regret learning algorithms exist in several cases and ensure convergence to coarse correlated equilibria (CCE). The efficiency of such equilibria with respect to a social function, however, is not well understood. In this paper, we define the class of valid utility games with continuous strategies and provide efficiency bounds for their CCEs. Our bounds rely on the social function being a monotone DR-submodular function. We further refine our bounds based on the curvature of the social function. Furthermore, we extend our efficiency bounds to a class of non-submodular functions that satisfy approximate submodularity properties. Finally, we show that valid utility games with continuous strategies can be designed to maximize monotone DR-submodular functions subject to disjoint constraints with approximation guarantees. The approximation guarantees we derive are based on the efficiency of the equilibria of such games and can improve the existing ones in the literature. We illustrate and validate our results on a budget allocation game and a sensor coverage problem.",,,,,,"Krause, Andreas/0000-0001-7260-9673",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902007,0
C,"Shekhar, S; Javidi, T",,"Chaudhuri, K; Sugiyama, M",,"Shekhar, Shubhanshu; Javidi, Tara",,,Multiscale Gaussian Process Level Set Estimation,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"In this paper, the problem of estimating the level set of a black-box function from noisy and expensive evaluation queries is considered. A new algorithm for this problem in the Bayesian framework with a Gaussian Process (GP) prior is proposed. The proposed algorithm employs a hierarchical sequence of partitions to explore different regions of the search space at varying levels of detail depending upon their proximity to the level set boundary. It is shown that this approach results in the algorithm having a low complexity implementation whose computational cost is significantly smaller than the existing algorithms for higher dimensional search space X. Furthermore, high probability bounds on a measure of discrepancy between the estimated level set and the true level set for the the proposed algorithm are obtained, which are shown to be strictly better than the existing guarantees for a large class of GPs. In the process, a tighter characterization of the information gain of the proposed algorithm is obtained which takes into account the structured nature of the evaluation points. This approach improves upon the existing technique of bounding the information gain with maximum information gain.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687903035,0
C,"Zhu, SY; Chen, B; Yang, PF; Chen, ZT",,"Chaudhuri, K; Sugiyama, M",,"Zhu, Shengyu; Chen, Biao; Yang, Pengfei; Chen, Zhitang",,,Universal Hypothesis Testing with Kernels: Asymptotically Optimal Tests for Goodness of Fit,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We characterize the asymptotic performance of nonparametric goodness of fit testing. The exponential decay rate of the type-II error probability is used as the asymptotic performance metric, and a test is optimal if it achieves the maximum rate subject to a constant level constraint on the type-I error probability. We show that two classes of Maximum Mean Discrepancy (MMD) based tests attain this optimality on R-d, while the quadratic time Kernel Stein Discrepancy (KSD) based tests achieve the maximum exponential decay rate under a relaxed level constraint. Under the same performance metric, we proceed to show that the quadratic-time MMD based two-sample tests are also optimal for general two-sample problems, provided that kernels are bounded continuous and characteristic. Key to our approach are Sanov's theorem from large deviation theory and the weak metrizable properties of the MMD and KSD.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901061,0
C,"Ambrogioni, L; Maris, E",,"Storkey, A; PerezCruz, F",,"Ambrogioni, Luca; Maris, Eric",,,Integral Transforms from Finite Data: An Application of Gaussian Process Regression to Fourier Analysis,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Computing accurate estimates of the Fourier transform of analog signals from discrete data points is important in many fields of science and engineering. The conventional approach of performing the discrete Fourier transform of the data implicitly assumes periodicity and bandlimitedness of the signal. In this paper, we use Gaussian process regression to estimate the Fourier transform (or any other integral transform) without making these assumptions. This is possible because the posterior expectation of Gaussian process regression maps a finite set of samples to a function defined on the whole real line, expressed as a linear combination of covariance functions. We estimate the covariance function from the data using an appropriately designed gradient ascent method that constrains the solution to a linear combination of tractable kernel functions. This procedure results in a posterior expectation of the analog signal whose Fourier transform can be obtained analytically by exploiting linearity. Our simulations show that the new method leads to sharper and more precise estimation of the spectral density both in noise-free and noise-corrupted signals. We further validate the method in two real-world applications: the analysis of the yearly fluctuation in atmospheric CO2 level and the analysis of the spectral content of brain signals.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300024,0
C,"Cormode, G; Hickey, C",,"Storkey, A; PerezCruz, F",,"Cormode, Graham; Hickey, Christopher",,,Cheap Checking for Cloud Computing: Statistical Analysis via Annotated Data Streams,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"As the popularity of outsourced computation increases, questions of accuracy and trust between the client and the cloud computing services become ever more relevant. Our work aims to provide fast and practical methods to verify analysis of large data sets, where the client's computation and memory costs are kept to a minimum. Our verification protocols are based on defining proofs which are easy to create and check. These add only a small overhead to reporting the result of the computation itself. We build up a series of protocols for elementary statistical methods, to create more complex protocols for Ordinary Least Squares, Principal Component Analysis and Linear Discriminant Analysis, and show them to be very efficient in practice.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300138,0
C,"Dutta, S; Joshi, G; Ghosh, S; Dube, P; Nagpurkar, P",,"Storkey, A; PerezCruz, F",,"Dutta, Sanghamitra; Joshi, Gauri; Ghosh, Soumyadip; Dube, Parijat; Nagpurkar, Priya",,,Slow and Stale Gradients Can Win the Race: Error-Runtime Trade-offs in Distributed SGD,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Distributed Stochastic Gradient Descent (SGD) when run in a synchronous manner, suffers from delays in waiting for the slowest learners (stragglers). Asynchronous methods can alleviate stragglers, but cause gradient staleness that can adversely affect convergence. In this work we present the first theoretical characterization of the speed-up offered by asynchronous methods by analyzing the trade-off between the error in the trained model and the actual training runtime (wall-clock time). The novelty in our work is that our runtime analysis considers random straggler delays, which helps us design and compare distributed SGD algorithms that strike a balance between stragglers and staleness. We also present a new convergence analysis of asynchronous SGD variants without bounded or exponential delay assumptions, and a novel learning rate schedule to compensate for gradient staleness.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300085,0
C,"Nalisnick, E; Smyth, P",,"Storkey, A; PerezCruz, F",,"Nalisnick, Eric; Smyth, Padhraic",,,Learning Priors for Invariance,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Informative priors are often difficult, if not impossible, to elicit for modern large-scale Bayesian models. Yet, often, some prior knowledge is known, and this information is incorporated via engineering tricks or methods less principled than a Bayesian prior. However, employing these tricks is difficult to reconcile with principled probabilistic inference. For instance, in the case of data set augmentation, the posterior is conditioned on artificial data and not on what is actually observed. In this paper, we address the problem of how to specify an informative prior when the problem of interest is known to exhibit invariance properties. The proposed method is akin to posterior variational inference: we choose a parametric family and optimize to find the member of the family that makes the model robust to a given transformation. We demonstrate the method's utility for dropout and rotation transformations, showing that the use of these priors results in performance competitive to that of non-Bayesian methods. Furthermore, our approach does not depend on the data being labeled and thus can be used in semi-supervised settings.",,,,,,"Smyth, Padhraic/0000-0001-9971-8378",,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300039,0
C,"Piironen, J; Vehtari, A",,"Storkey, A; PerezCruz, F",,"Piironen, Juho; Vehtari, Aki",,,Iterative Supervised Principal Components,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"In high-dimensional prediction problems, where the number of features may greatly exceed the number of training instances, fully Bayesian approach with a sparsifying prior is known to produce good results but is computationally challenging. To alleviate this computational burden, we propose to use a preprocessing step where we first apply a dimension reduction to the original data to reduce the number of features to something that is computationally conveniently handled by Bayesian methods. To do this, we propose a new dimension reduction technique, called iterative supervised principal components (ISPCs), which combines variable screening and dimension reduction and can be considered as an extension to the existing technique of supervised principal components (SPCs). Our empirical evaluations confirm that, although not foolproof, the proposed approach provides very good results on several microarray benchmark datasets with very affordable computation time, and it can also be very useful for visualizing high-dimensional data.",,,,,"Piironen, Juho/GSJ-3745-2022","Piironen, Juho/0000-0002-0784-8835; Vehtari, Aki/0000-0003-2164-9469",,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300012,0
C,"Auer, P; Chiang, CK; Ortner, R; Drugan, MM",,"Gretton, A; Robert, CC",,"Auer, Peter; Chiang, Chao-Kai; Ortner, Ronald; Drugan, Madalina M.",,,Pareto Front Identification from Stochastic Bandit Feedback,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"We consider the problem of identifying the Pareto front for multiple objectives from a finite set of operating points. Sampling an operating point gives a random vector where each coordinate corresponds to the value of one of the objectives. The Pareto front is the set of operating points that are not dominated by any other operating point in respect to all objectives (considering the mean of their objective values). We propose a confidence bound algorithm to approximate the Pareto front, and prove problem specific lower and upper bounds, showing that the sample complexity is characterized by some natural geometric properties of the operating points. Experiments confirm the reliability of our algorithm. For the problem of finding a sparse cover of the Pareto front, we propose an asymmetric covering algorithm of independent interest.",,,,,"Auer, Peter/AAC-1314-2019","Auer, Peter/0000-0001-8385-9635; Ortner, Ronald/0000-0001-6033-2208",,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,939,947,,,,,,,,,,,,,,,,WOS:000508662100102,0
C,"Ghashami, M; Perry, DJ; Phillips, JM",,"Gretton, A; Robert, CC",,"Ghashami, Mina; Perry, Daniel J.; Phillips, Jeff M.",,,Streaming Kernel Principal Component Analysis,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Kernel principal component analysis (KPCA) provides a concise set of basis vectors which capture nonlinear structures within large data sets, and is a central tool in data analysis and learning. To allow for nonlinear relations, typically a full n. n kernel matrix is constructed over n data points, but this requires too much space and time for large values of n. Techniques such as the Nystrom method and random feature maps can help towards this goal, but they do not explicitly maintain the basis vectors in a stream and take more space than desired. We propose a new approach for streaming KPCA which maintains a small set of basis elements in a stream, requiring space only logarithmic in n, and also improves the dependence on the error parameter. Our technique combines together random feature maps with recent advances in matrix sketching, it has guaranteed spectral norm error bounds with respect to the original kernel matrix, and it compares favorably in practice to state-of-the-art approaches.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,1365,1374,,,,,,,,,,,,,,,,WOS:000508662100148,0
C,"Que, QC; Belkin, M",,"Gretton, A; Robert, CC",,"Que, Qichao; Belkin, Mikhail",,,Back to the Future: Radial Basis Function Networks Revisited,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Radial Basis Function (RBF) networks are a classical family of algorithms for supervised learning. The most popular approach for training RBF networks has relied on kernel methods using regularization based on a norm in a Reproducing Kernel Hilbert Space (RKHS), which is a principled and empirically successful framework. In this paper we aim to revisit some of the older approaches to training the RBF networks from a more modern perspective. Specifically, we analyze two common regularization procedures, one based on the square norm of the coefficients in the network and another one using centers obtained by k-means clustering. We show that both of these RBF methods can be recast as certain data-dependent kernels. We provide a theoretical analysis of these methods as well as a number of experimental results, pointing out very competitive experimental performance as well as certain advantages over the standard kernel methods in terms of both flexibility (incorporating of unlabeled data) and computational complexity. Finally, our results shed light on some impressive recent successes of using soft k-means features for image recognition and other tasks.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,1375,1383,,,,,,,,,,,,,,,,WOS:000508662100149,0
C,"Andrade-Pacheco, R; Hensman, J; Zwiessele, M; Lawrence, ND",,"Kaski, S; Corander, J",,"Andrade-Pacheco, Ricardo; Hensman, James; Zwiessele, Max; Lawrence, Neil D.",,,Hybrid Discriminative-Generative Approach with Gaussian Processes,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"Machine learning practitioners are often faced with a choice between a discriminative and a generative approach to modelling. Here, we present a model based on a hybrid approach that breaks down some of the barriers between the discriminative and generative points of view, allowing continuous dimensionality reduction of hybrid discrete-continuous data, discriminative classification with missing inputs and manifold learning informed by class labels.",,,,,,"Hensman, James/0000-0002-4989-3589",,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,47,56,,,,,,,,,,,,,,,,WOS:000508355800006,0
C,"Hu, CW; Ryu, E; Carlson, D; Wang, YJ; Carin, L",,"Kaski, S; Corander, J",,"Hu, Changwei; Ryu, Eunsu; Carlson, David; Wang, Yingjian; Carin, Lawrence",,,Latent Gaussian Models for Topic Modeling,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"A new approach is proposed for topic modeling, in which the latent matrix factorization employs Gaussian priors, rather than the Dirichlet-class priors widely used in such models. The use of a latent-Gaussian model permits simple and efficient approximate Bayesian posterior inference, via the Laplace approximation. On multiple datasets, the proposed approach is demonstrated to yield results as accurate as state-of-the-art approaches based on Dirichlet constructions, at a small fraction of the computation. The framework is general enough to jointly model text and binary data, here demonstrated to produce accurate and fast results for joint analysis of voting rolls and the associated legislative text. Further, it is demonstrated how the technique may be scaled up to massive data, with encouraging performance relative to alternative methods.",,,,,,"Carin, Lawrence/0000-0001-6277-7948; Carlson, David/0000-0003-1005-6385",,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,393,401,,,,,,,,,,,,,,,,WOS:000508355800044,0
C,"Kulesza, A; Rao, NR; Singh, S",,"Kaski, S; Corander, J",,"Kulesza, Alex; Rao, N. Raj; Singh, Satinder",,,Low-Rank Spectral Learning,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"Spectral learning methods have recently been proposed as alternatives to slow, non-convex optimization algorithms like EM for a variety of probabilistic models in which hidden information must be inferred by the learner. These methods are typically controlled by a rank hyperparameter that sets the complexity of the model; when the model rank matches the true rank of the process generating the data, the resulting predictions are provably consistent and admit finite sample convergence bounds. However, in practice we usually do not know the true rank, and, in any event, from a computational and statistical standpoint it is likely to be prohibitively large. It is therefore of great practical interest to understand the behavior of low-rank spectral learning, where the model rank is less than the true rank. Counterintuitively, we show that even when the singular values omitted by lowering the rank are arbitrarily small, the resulting prediction errors can in fact be arbitrarily large. We identify two distinct possible causes for this bad behavior, and illustrate them with simple examples. We then show that these two causes are essentially complete: assuming that they do not occur, we can prove that the prediction error is bounded in terms of the magnitudes of the omitted singular values. We argue that the assumptions necessary for this result are relatively realistic, making low-rank spectral learning a viable option for many applications.",,,,,,"Nadakuditi, Raj Rao/0000-0002-5506-1631",,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,522,530,,,,,,,,,,,,,,,,WOS:000508355800058,0
C,"Sasaki, H; Gutmann, MU; Shouno, H; Hyvarinen, A",,"Kaski, S; Corander, J",,"Sasaki, Hiroaki; Gutmann, Michael U.; Shouno, Hayaru; Hyvarinen, Aapo",,,Estimating Dependency Structures for non-Gaussian Components with Linear and Energy Correlations,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"The statistical dependencies which independent component analysis (ICA) cannot remove often provide rich information beyond the ICA components. It would be very useful to estimate the dependency structure from data. However, most models have concentrated on higher-order correlations such as energy correlations, neglecting linear correlations. Linear correlations might be a strong and informative form of a dependency for some real data sets, but they are usually completely removed by ICA and related methods, and not analyzed at all. In this paper, we propose a probabilistic model of non-Gaussian components which are allowed to have both linear and energy correlations. The dependency structure of the components is explicitly parametrized by a parameter matrix, which defines an undirected graphical model over the latent components. Furthermore, the estimation of the parameter matrix is shown to be particularly simple because using score matching, the objective function is a quadratic form. Using artificial data, we demonstrate that the proposed method is able to estimate non-Gaussian components and their dependency structures, as it is designed to do. When applied to natural images and outputs of simulated complex cells in the primary visual cortex, novel dependencies between the estimated features are discovered.",,,,,"Sasaki, Hiroaki/GXG-5024-2022","Hyvarinen, Aapo/0000-0002-5806-4432",,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,868,876,,,,,,,,,,,,,,,,WOS:000508355800096,0
C,"Shah, A; Wilson, AG; Ghahramani, Z",,"Kaski, S; Corander, J",,"Shah, Amar; Wilson, Andrew Gordon; Ghahramani, Zoubin",,,Student-t Processes as Alternatives to Gaussian Processes,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"We investigate the Student-t process as an alternative to the Gaussian process as a non-parametric prior over functions. We derive closed form expressions for the marginal likelihood and predictive distribution of a Student-t process, by integrating away an inverse Wishart process prior over the co-variance kernel of a Gaussian process model. We show surprising equivalences between different hierarchical Gaussian process models leading to Student-t processes, and derive a new sampling scheme for the inverse Wishart process, which helps elucidate these equivalences. Overall, we show that a Student-t process can retain the attractive properties of a Gaussian process a nonparametric representation, analytic marginal and predictive distributions, and easy model selection through covariance kernels but has enhanced flexibility, and predictive covariances that, unlike a Gaussian process, explicitly depend on the values of training observations. We verify empirically that a Student-t process is especially useful in situations where there are changes in covariance structure, or in applications such as Bayesian optimization, where accurate predictive covariances are critical for good performance. These advantages come at no additional computational cost over Gaussian processes.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,877,885,,,,,,,,,,,,,,,,WOS:000508355800097,0
C,"Wood, F; van de Meent, JW; Mansinghka, V",,"Kaski, S; Corander, J",,"Wood, Frank; van de Meent, Jan Willem; Mansinghka, Vikash",,,A New Approach to Probabilistic Programming Inference,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"We introduce and demonstrate a new approach to inference in expressive probabilistic programming languages based on particle Markov chain Monte Carlo. Our approach is simple to implement and easy to parallelize. It applies to Turing-complete probabilistic programming languages and supports accurate inference in models that make use of complex control flow, including stochastic recursion. It also includes primitives from Bayesian nonparametric statistics. Our experiments show that this approach can be more efficient than previously introduced single-site Metropolis-Hastings methods.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,1024,1032,,,,,,,,,,,,,,,,WOS:000508355800113,0
C,"Abeille, M; Lazaric, A",,"Singh, A; Zhu, J",,"Abeille, Marc; Lazaric, Alessandro",,,Linear Thompson Sampling Revisited,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"We derive an alternative proof for the regret of Thompson sampling (TS) in the stochastic linear bandit setting. While we obtain a regret bound of order O (d(3/2)root T) as in previous results, the proof sheds new light on the functioning of the TS. We leverage on the structure of the problem to show how the regret is related to the sensitivity (i.e., the gradient) of the objective function and how selecting optimal arms associated to optimistic parameters does control it. Thus we show that TS can be seen as a generic randomized algorithm where the sampling distribution is designed to have a fixed probability of being optimistic, at the cost of an additional root d regret factor compared to a UCB-like approach. Furthermore, we show that our proof can be readily applied to regularized linear optimization and generalized linear model problems.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,176,184,,,,,,,,,,,,,,,,WOS:000509368500020,0
C,"Alquier, P; Mai, TT; Pontil, M",,"Singh, A; Zhu, J",,"Alquier, Pierre; The Tien Mai; Pontil, Massimiliano",,,Regret Bounds for Lifelong Learning,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"We consider the problem of transfer learning in an online setting. Di.erent tasks are presented sequentially and processed by a within-task algorithm. We propose a lifelong learning strategy which refines the underlying data representation used by the within-task algorithm, thereby transferring information from one task to the next. We show that when the within-task algorithm comes with some regret bound, our strategy inherits this good property. Our bounds are in expectation for a general loss function, and uniform for a convex loss. We discuss applications to dictionary learning and finite set of predictors. In the latter case, we improve previous O(1/root m) bounds to O(1/m), where m is the per task sample size.",,,,,"Mai, The Tien/AAE-9213-2021","Mai, The Tien/0000-0002-3514-9636; Alquier, Pierre/0000-0003-4249-7337",,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,261,269,,,,,,,,,,,,,,,,WOS:000509368500029,0
C,"Farias, VF; Li, AA",,"Singh, A; Zhu, J",,"Farias, Vivek F.; Li, Andrew A.",,,Optimal Recovery of Tensor Slices,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"We consider the problem of large scale matrix recovery given side information in the form of additional matrices of conforming dimension. This is a parsimonious model that captures a number of interesting problems including context and location aware recommendations, personalized 'tag' learning, demand learning with side information, etc. Viewing the matrix we seek to recover and the side information we have as slices of a tensor, we consider the problem of Slice Recovery, which is to recover specific slices of a tensor from noisy observations of the tensor. We provide an efficient algorithm to recover slices of structurally 'simple' tensors given noisy observations of the tensor's entries; our definition of simplicity subsumes low-rank tensors for a variety of definitions of tensor rank. Our algorithm is practical for large datasets and provides a significant performance improvement over state of the art incumbent approaches to tensor recovery. We establish theoretical recovery guarantees that under reasonable assumptions are minimax optimal for slice recovery. These guarantees also imply the first minimax optimal guarantees for recovering tensors of low Tucker rank and general noise. Experiments on data from a music streaming service demonstrate the performance and scalability of our algorithm.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,1394,1402,,,,,,,,,,,,,,,,WOS:000509368500149,0
C,"Lee, BJ; Lee, J; Kim, KE",,"Singh, A; Zhu, J",,"Lee, Byung-Jun; Lee, Jongmin; Kim, Kee-Eung",,,Hierarchically-partitioned Gaussian Process Approximation,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"The Gaussian process (GP) is a simple yet powerful probabilistic framework for various machine learning tasks. However, exact algorithms for learning and prediction are prohibitive to be applied to large datasets due to inherent computational complexity. To overcome this main limitation, various techniques have been proposed, and in particular, local GP algorithms that scales truly linearly with respect to the dataset size. In this paper, we introduce a hierarchical model based on local GP for large-scale datasets, which stacks inducing points over inducing points in layers. By using different kernels in each layer, the overall model becomes multi-scale and is able to capture both long- and short-range dependencies. We demonstrate the effectiveness of our model by speed-accuracy performance on challenging real-world datasets.",,,,,"Lee, Byung-Jun/AAL-3784-2021","Lee, Byung-Jun/0000-0002-0684-607X",,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,822,831,,,,,,,,,,,,,,,,WOS:000509368500088,0
C,"Piironen, J; Vehtari, A",,"Singh, A; Zhu, J",,"Piironen, Juho; Vehtari, Aki",,,On the Hyperprior Choice for the Global Shrinkage Parameter in the Horseshoe Prior,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"The horseshoe prior has proven to be a noteworthy alternative for sparse Bayesian estimation, but as shown in this paper, the results can be sensitive to the prior choice for the global shrinkage hyperparameter. We argue that the previous default choices are dubious due to their tendency to favor solutions with more unshrunk coefficients than we typically expect a priori. This can lead to bad results if this parameter is not strongly identified by data. We derive the relationship between the global parameter and the effective number of nonzeros in the coefficient vector, and show an easy and intuitive way of setting up the prior for the global parameter based on our prior beliefs about the number of nonzero coefficients in the model. The results on real world data show that one can benefit greatly - in terms of improved parameter estimates, prediction accuracy, and reduced computation time - from transforming even a crude guess for the number of nonzero coefficients into the prior for the global parameter using our framework.",,,,,"Piironen, Juho/GSJ-3745-2022",,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,905,913,,,,,,,,,,,,,,,,WOS:000509368500097,0
C,"Chen, S; Banerjee, A",,"Lebanon, G; Vishwanathan, SVN",,"Chen, Sheng; Banerjee, Arindam",,,One-bit Compressed Sensing with the k-Support Norm,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"In one-bit compressed sensing (1-bit CS), one attempts to estimate a structured parameter (signal) only using the sign of suitable linear measurements. In this paper, we investigate 1-bit CS problems for sparse signals using the recently proposed k-support norm. We show that the new estimator has a closed-form solution, so no optimization is needed. We establish consistency and recovery guarantees of the estimator for both Gaussian and sub-Gaussian random measurements. For Gaussian measurements, our estimator is comparable to the best known in the literature, along with guarantees on support recovery. For sub-Gaussian measurements, our estimator has an irreducible error which, unlike existing results, can be controlled by scaling the measurement vectors. In both cases, our analysis covers the setting of model misspecification, i.e., when the true sparsity is unknown. Experimental results illustrate several strengths of the new estimator.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,138,146,,,,,,,,,,,,,,,,WOS:000508399700016,0
C,"Adhikary, S; Srinivasan, S; Miller, J; Rabusseau, G; Boots, B",,"Banerjee, A; Fukumizu, K",,"Adhikary, Sandesh; Srinivasan, Siddarth; Miller, Jacob; Rabusseau, Guillaume; Boots, Byron",,,"Quantum Tensor Networks, Stochastic Processes, and Weighted Automata",24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Modeling joint probability distributions over sequences has been studied from many perspectives. The physics community developed matrix product states, a tensor-train decomposition for probabilistic modeling, motivated by the need to tractably model many-body systems. But similar models have also been studied in the stochastic processes and weighted automata literature, with little work on how these bodies of work relate to each other. We address this gap by showing how stationary or uniform versions of popular quantum tensor network models have equivalent representations in the stochastic processes and weighted automata literature, in the limit of infinitely long sequences. We demonstrate several equivalence results between models used in these three communities: (i) uniform variants of matrix product states, Born machines and locally purified states from the quantum tensor networks literature, (ii) predictive state representations, hidden Markov models, norm-observable operator models and hidden quantum Markov models from the stochastic process literature, and (iii) stochastic weighted automata, probabilistic automata and quadratic automata from the formal languages literature. Such connections may open the door for results and methods developed in one area to be applied in another.",,,,,"s, s/HIK-1178-2022",,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802051,0
C,"Bu, ZQ; Xu, SY; Chen, K",,"Banerjee, A; Fukumizu, K",,"Bu, Zhiqi; Xu, Shiyun; Chen, Kan",,,A Dynamical View on Optimization Algorithms of Overparameterized Neural Networks,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"When equipped with efficient optimization algorithms, the over-parameterized neural networks have demonstrated high level of performance even though the loss function is nonconvex and non-smooth. While many works have been focusing on understanding the loss dynamics by training neural networks with the gradient descent (GD), in this work, we consider a broad class of optimization algorithms that are commonly used in practice. For example, we show from a dynamical system perspective that the Heavy Ball (HB) method can converge to global minimum on mean squared error (MSE) at a linear rate (similar to GD); however, the Nesterov accelerated gradient descent (NAG) may only converge to global minimum sublinearly. Our results rely on the connection between neural tangent kernel (NTK) and finitelywide over-parameterized neural networks with ReLU activation, which leads to analyzing the limiting ordinary differential equations (ODE) for optimization algorithms. We show that, optimizing the non-convex loss over the weights corresponds to optimizing some strongly convex loss over the prediction error. As a consequence, we can leverage the classical convex optimization theory to understand the convergence behavior of neural networks. We believe our approach can also be extended to other optimization algorithms and network architectures.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803081,0
C,"Dolera, E; Favaro, S; Peluchetti, S",,"Banerjee, A; Fukumizu, K",,"Dolera, Emanuele; Favaro, Stefano; Peluchetti, Stefano",,,A Bayesian nonparametric approach to count-min sketch under power-law data streams,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"The count-min sketch (CMS) is a randomized data structure that provides estimates of tokens' frequencies in a large data stream using a compressed representation of the data by random hashing. In this paper, we rely on a recent Bayesian nonparametric (BNP) view on the CMS to develop a novel learning-augmented CMS under power-law data streams. We assume that tokens in the stream are drawn from an unknown discrete distribution, which is endowed with a normalized inverse Gaussian process (NIGP) prior. Then, using distributional properties of the NIGP, we compute the posterior distribution of a token's frequency in the stream, given the hashed data, and in turn corresponding BNP estimates. Applications to synthetic and real data show that our approach achieves a remarkable performance in the estimation of low-frequency tokens. This is known to be a desirable feature in the context of natural language processing, where it is indeed common in the context of the power-law behaviour of the data.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,226,+,,,,,,,,,,,,,,,,WOS:000659893800026,0
C,"Ghalebikesabi, S; Cornish, R; Kelly, LJ; Holmes, C",,"Banerjee, A; Fukumizu, K",,"Ghalebikesabi, Sahra; Cornish, Rob; Kelly, Luke J.; Holmes, Chris",,,Deep Generative Missingness Pattern-Set Mixture Models,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We propose a variational autoencoder architecture to model both ignorable and nonignorable missing data using pattern-set mixtures as proposed by Little (1993). Our model explicitly learns to cluster the missing data into missingness pattern sets based on the observed data and missingness masks. Underpinning our approach is the assumption that the data distribution under missingness is probabilistically semi-supervised by samples from the observed data distribution. Our setup trades off the characteristics of ignorable and nonignorable missingness and can thus be applied to data of both types. We evaluate our method on a wide range of data sets with different types of missingness and achieve state-of-the-art imputation performance. Our model outperforms many common imputation algorithms, especially when the amount of missing data is high and the missingness mechanism is nonignorable.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804049,0
C,"Hochlehnert, A; Terenin, A; Saemundsson, S; Deisenroth, MP",,"Banerjee, A; Fukumizu, K",,"Hochlehnert, Andreas; Terenin, Alexander; Saemundsson, Steindor; Deisenroth, Marc Peter",,,Learning Contact Dynamics using Physically Structured Neural Networks,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Learning physically structured representations of dynamical systems that include contact between different objects is an important problem for learning-based approaches in robotics. Black-box neural networks can learn to approximately represent discontinuous dynamics, but they typically require large quantities of data and often suffer from pathological behaviour when forecasting for longer time horizons. In this work, we use connections between deep neural networks and differential equations to design a family of deep network architectures for representing contact dynamics between objects. We show that these networks can learn discontinuous contact events in a data-efficient manner from noisy observations in settings that are traditionally difficult for black-box approaches and recent physics inspired neural networks. Our results indicate that an idealised form of touch feedback-which is heavily relied upon by biological systems-is a key component of making this learning problem tractable. Together with the inductive biases introduced through the network architectures, our techniques enable accurate learning of contact dynamics from observations.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802059,0
C,"Husain, H; Ciosek, K; Tomioka, R",,"Banerjee, A; Fukumizu, K",,"Husain, Hisham; Ciosek, Kamil; Tomioka, Ryota",,,Regularized Policies are Reward Robust,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Entropic regularization of policies in Reinforcement Learning (RL) is a commonly used heuristic to ensure that the learned policy explores the state-space sufficiently before overfitting to a local optimal policy. The primary motivation for using entropy is for exploration and disambiguating optimal policies; however, the theoretical effects are not entirely understood. In this work, we study the more general regularized RL objective and using Fenchel duality; we derive the dual problem which takes the form of an adversarial reward problem. In particular, we find that the optimal policy found by a regularized objective is precisely an optimal policy of a reinforcement learning problem under a worst-case adversarial reward. Our result allows us to reinterpret the popular entropic regularization scheme as a form of robustification. Furthermore, due to the generality of our results, we apply to other existing regularization schemes. Our results thus give insights into the effects of regularization of policies and deepen our understanding of exploration through robust rewards at large.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,64,72,,,,,,,,,,,,,,,,WOS:000659893800008,0
C,"Immer, A; Korzepa, M; Bauer, M",,"Banerjee, A; Fukumizu, K",,"Immer, Alexander; Korzepa, Maciej; Bauer, Matthias",,,Improving predictions of Bayesian neural nets via local linearization,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"The generalized Gauss-Newton (GGN) approximation is often used to make practical Bayesian deep learning approaches scalable by replacing a second order derivative with a product of first order derivatives. In this paper we argue that the GGN approximation should be understood as a local linearization of the underlying Bayesian neural network (BNN), which turns the BNN into a generalized linear model (GLM). Because we use this linearized model for posterior inference, we should also predict using this modified model instead of the original one. We refer to this modified predictive as GLM predictive and show that it effectively resolves common underfitting problems of the Laplace approximation. It extends previous results in this vein to general likelihoods and has an equivalent Gaussian process formulation, which enables alternative inference schemes for BNNs in function space. We demonstrate the effectiveness of our approach on several standard classification datasets and on out-of-distribution detection. We provide an implementation at https://github.com/AlexImmer/BNN-predictions.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,703,+,,,,,,,,,,,,,,,,WOS:000659893800079,0
C,"Koskela, A; Jalko, J; Prediger, L; Honkela, A",,"Banerjee, A; Fukumizu, K",,"Koskela, Antti; Jalko, Joonas; Prediger, Lukas; Honkela, Antti",,,Tight Differential Privacy for Discrete-Valued Mechanisms and for the Subsampled Gaussian Mechanism Using FFT,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We propose a numerical accountant for evaluating the tight (epsilon, delta)-privacy loss for algorithms with discrete one dimensional output. The method is based on the privacy loss distribution formalism and it uses the recently introduced fast Fourier transform based accounting technique. We carry out an error analysis of the method in terms of moment bounds of the privacy loss distribution which leads to rigorous lower and upper bounds for the true (epsilon, delta)-values. As an application, we present a novel approach to accurate privacy accounting of the subsampled Gaussian mechanism. This completes the previously proposed analysis by giving strict lower and upper bounds for the privacy parameters. We demonstrate the performance of the accountant on the binomial mechanism and show that our approach allows decreasing noise variance up to 75 percent at equal privacy compared to existing bounds in the literature. We also illustrate how to compute tight bounds for the exponential mechanism applied to counting queries.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804008,0
C,"Krishnamurthy, SK; Hadad, V; Athey, S",,"Banerjee, A; Fukumizu, K",,"Krishnamurthy, Sanath Kumar; Hadad, Vitor; Athey, Susan",,,Tractable contextual bandits beyond realizability,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Tractable contextual bandit algorithms often rely on the realizability assumption i.e., that the true expected reward model belongs to a known class, such as linear functions. In this work, we present a tractable bandit algorithm that is not sensitive to the realizability assumption and computationally reduces to solving a constrained regression problem in every epoch. When realizability does not hold, our algorithm ensures the same guarantees on regret achieved by realizability-based algorithms under realizability, up to an additive term that accounts for the misspecification error. This extra term is proportional to T times a function of the mean squared error between the best model in the class and the true model, where T is the total number of time-steps. Our work sheds light on the bias-variance trade-off for tractable contextual bandits. This trade-off is not captured by algorithms that assume realizability, since under this assumption there exists an estimator in the class that attains zero bias.",,,,,,"Athey, Susan/0000-0001-6934-562X",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801072,0
C,"Maddox, WJ; Tang, S; Moreno, PG; Wilson, AG; Damianou, A",,"Banerjee, A; Fukumizu, K",,"Maddox, Wesley J.; Tang, Shuai; Moreno, Pablo Garcia; Wilson, Andrew Gordon; Damianou, Andreas",,,Fast Adaptation with Linearized Neural Networks,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"The inductive biases of trained neural networks are difficult to understand and, consequently, to adapt to new settings. We study the inductive biases of linearizations of neural networks, which we show to be surprisingly good summaries of the full network functions. Inspired by this finding, we propose a technique for embedding these inductive biases into Gaussian processes through a kernel designed from the Jacobian of the network. In this setting, domain adaptation takes the form of interpretable posterior inference, with accompanying uncertainty estimation. This inference is analytic and free of local optima issues found in standard techniques such as fine-tuning neural network weights to a new task. We develop significant computational speed-ups based on matrix multiplies, including a novel implementation for scalable Fisher vector products. Our experiments on both image classification and regression demonstrate the promise and convenience of this framework for transfer learning, compared to neural network fine-tuning. Code is available at https://github.com/amzn/xfer/tree/master/finite_ntk.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803031,0
C,"Mucke, N",,"Banerjee, A; Fukumizu, K",,"Muecke, Nicole",,,Stochastic Gradient Descent Meets Distribution Regression,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Stochastic gradient descent (SGD) provides a simple and efficient way to solve a broad range of machine learning problems. Here, we focus on distribution regression (DR), involving two stages of sampling: Firstly, we regress from probability measures to realvalued responses. Secondly, we sample bags from these distributions for utilizing them to solve the overall regression problem. Recently, DR has been tackled by applying kernel ridge regression and the learning properties of this approach are well understood. However, nothing is known about the learning properties of SGD for two stage sampling problems. We fill this gap and provide theoretical guarantees for the performance of SGD for DR. Our bounds are optimal in a mini-max sense under standard assumptions.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802058,0
C,"Tanielian, U; Sangnier, M; Biau, G",,"Banerjee, A; Fukumizu, K",,"Tanielian, U.; Sangnier, M.; Biau, G.",,,Approximating Lipschitz continuous functions with GroupSort neural networks,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Recent advances in adversarial attacks and Wasserstein GANs have advocated for use of neural networks with restricted Lipschitz constants. Motivated by these observations, we study the recently introduced GroupSort neural networks, with constraints on the weights, and make a theoretical step towards a better understanding of their expressive power. We show in particular how these networks can represent any Lipschitz continuous piecewise linear functions. We also prove that they are well-suited for approximating Lipschitz continuous functions and exhibit upper bounds on both the depth and size. To conclude, the efficiency of GroupSort networks compared with more standard ReLU networks is illustrated in a set of synthetic experiments.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,442,+,,,,,,,,,,,,,,,,WOS:000659893800050,0
C,"Theisen, R; Klusowski, JM; Mahoney, MW",,"Banerjee, A; Fukumizu, K",,"Theisen, Ryan; Klusowski, Jason M.; Mahoney, Michael W.",,,Good Classifiers are Abundant in the Interpolating Regime,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Within the machine learning community, the widely-used uniform convergence framework has been used to answer the question of how complex, over-parameterized models can generalize well to new data. This approach bounds the test error of the worst-case model one could have fit to the data, but it has fundamental limitations. Inspired by the statistical mechanics approach to learning, we formally define and develop a methodology to compute precisely the full distribution of test errors among interpolating classifiers from several model classes. We apply our method to compute this distribution for several real and synthetic datasets, with both linear and random feature classification models. We find that test errors tend to concentrate around a small typical value epsilon*, which deviates substantially from the test error of the worst-case interpolating model on the same datasets, indicating that bad classifiers are extremely rare. We provide theoretical results in a simple setting in which we characterize the full asymptotic distribution of test errors, and we show that these indeed concentrate around a value epsilon*, which we also identify exactly. We then formalize a more general conjecture supported by our empirical findings. Our results show that the usual style of analysis in statistical learning theory may not be fine-grained enough to capture the good generalization performance observed in practice, and that approaches based on the statistical mechanics of learning may offer a promising alternative.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804010,0
C,"Wang, F; Pinar, A",,"Banerjee, A; Fukumizu, K",,"Wang, Fulton; Pinar, Ali",,,The Multiple Instance Learning Gaussian Process Probit Model,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"In the Multiple Instance Learning (MIL) scenario, the training data consists of instances grouped into bags. Bag labels specify whether each bag contains at least one positive instance, but instance labels are not observed. Recently, Haussmann et al [10] tackled the MIL instance label prediction task by introducing the Multiple Instance Learning Gaussian Process Logistic (MIL-GP-Logistic) model, an adaptation of the Gaussian Process Logistic Classification model that inherits its uncertainty quantification and flexibility. Notably, they give a fast mean-field variational inference procedure. However, due to their use of the logit link, they do not maximize the variational inference ELBO objective directly, but rather a lower bound on it. This approximation, as we show, hurts predictive performance. In this work, we propose the Multiple Instance Learning Gaussian Process Probit (MIL-GP-Probit) model, an adaptation of the Gaussian Process Probit Classification model to solve the MIL instance label prediction problem. Leveraging the analytical tractability of the probit link, we give a variational inference procedure based on variable augmentation that maximizes the ELBO objective directly. Applying it, we show MIL-GP-Probit is more calibrated than MIL-GP-Logistic on all 20 datasets of the benchmark 20 Newsgroups dataset collection, and achieves higher AUC than MIL-GP-Logistic on an additional 51 out of 59 datasets. Finally, we show how the probit formulation enables principled bag label predictions and a Gibbs sampling scheme. This is the first exact inference scheme for any Bayesian model for the MIL scenario. [GRAPHICS]",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803064,0
C,"Yashima, S; Nitanda, A; Suzuki, T",,"Banerjee, A; Fukumizu, K",,"Yashima, Shingo; Nitanda, Atsushi; Suzuki, Taiji",,,Exponential Convergence Rates of Classification Errors on Learning with SGD and Random Features,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Although kernel methods are widely used in many learning problems, they have poor scalability to large datasets. To address this problem, sketching and stochastic gradient methods are the most commonly used techniques to derive computationally efficient learning algorithms. We consider solving a binary classification problem using random features and stochastic gradient descent, both of which are common and widely used in practical large-scale problems. Although there are plenty of previous works investigating the efficiency of these algorithms in terms of the convergence of the objective loss function, these results suggest that the computational gain comes at expense of the learning accuracy when dealing with general Lipschitz loss functions such as logistic loss. In this study, we analyze the properties of these algorithms in terms of the convergence not of the loss function, but the classification error under the strong low-noise condition, which reflects a realistic property of real-world datasets. We extend previous studies on SGD to a random features setting, examining a novel analysis about the error induced by the approximation of random features in terms of the distance between the generated hypothesis to show that an exponential convergence of the expected classification error is achieved even if random features approximation is applied. We demonstrate that the convergence rate does not depend on the number of features and there is a significant computational benefit in using random features in classification problems under the strong low-noise condition.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802037,0
C,"Zheng, QQ; Chen, SX; Long, Q; Su, WJ",,"Banerjee, A; Fukumizu, K",,"Zheng, Qinqing; Chen, Shuxiao; Long, Qi; Su, Weijie",,,Federated f-Differential Privacy,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Federated learning (FL) is a training paradigm where the clients collaboratively learn models by repeatedly sharing information without compromising much on the privacy of their local sensitive data. In this paper, we introduce federated f-differential privacy, a new notion specifically tailored to the federated setting, based on the framework of Gaussian differential privacy. Federated f-differential privacy operates on record level : it provides the privacy guarantee on each individual record of one client's data against adversaries. We then propose a generic private federated learning framework PriFedSync that accommodates a large family of state-of-the-art FL algorithms, which provably achieves federated f-differential privacy. Finally, we empirically demonstrate the trade-off between privacy guarantee and prediction performance for models trained by PriFedSync in computer vision tasks.",,,,,,"Long, Qi/0000-0003-0660-5230",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,34350418,,,,,WOS:000659893802070,0
C,"Daskalakis, C; Dikkala, N; Panageas, I",,"Chiappa, S; Calandra, R",,"Daskalakis, Constantinos; Dikkala, Nishanth; Panageas, Ioannis",,,Logistic regression with peer-group effects via inference in higher-order Ising models,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Spin glass models, such as the Sherrington-Kirkpatrick, Hopfield and Ising models, are all well-studied members of the exponential family of discrete distributions, and have been influential in a number of application domains where they are used to model correlation phenomena on networks. Conventionally these models have quadratic sufficient statistics and consequently capture correlations arising from pairwise interactions. In this work we study extensions of these to models with higher-order sufficient statistics, modeling behavior on a social network with peer-group effects. In particular, we model binary outcomes on a network as a higher-order spin glass, where the behavior of an individual depends on a linear function of their own vector of covariates and some polynomial function of the behavior of others, capturing peer-group effects. Using a single, high-dimensional sample from such model our goal is to recover the coefficients of the linear function as well as the strength of the peer-group effects. The heart of our result is a novel approach for showing strong concavity of the log pseudo-likelihood of the model, implying statistical error rate of root d/n for the Maximum Pseudo-Likelihood Estimator (MPLE), where d is the dimensionality of the covariate vectors and n is the size of the network (number of nodes). Our model generalizes vanilla logistic regression as well as the models studied in recent works of Chatterjee et al. (2007); Ghosal and Mukherjee (2018); Daskalakis et al. (2019), and our results extend these results to accommodate higher-order interactions.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3653,3662,,,,,,,,,,,,,,,,WOS:000559931300072,0
C,"de Heide, R; Kirichenko, A; Mehta, NA; Grunwald, PD",,"Chiappa, S; Calandra, R",,"de Heide, Rianne; Kirichenko, Alisa; Mehta, Nishant A.; Grunwald, Peter D.",,,Safe-Bayesian Generalized Linear Regression,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We study generalized Bayesian inference under misspecification, i.e. when the model is 'wrong but useful'. Generalized Bayes equips the likelihood with a learning rate eta. We show that for generalized linear models (GLMs), eta-generalized Bayes concentrates around the best approximation of the truth within the model for specific eta not equal 1, even under severely misspecified noise, as long as the tails of the true distribution are exponential. We derive MCMC samplers for generalized Bayesian lasso and logistic regression and give examples of both simulated and real-world data in which generalized Bayes substantially outperforms standard Bayes.",,,,,,"de Heide, Rianne/0000-0002-2842-1099",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2623,2632,,,,,,,,,,,,,,,,WOS:000559931301040,0
C,"Gluch, G; Urbanke, R",,"Chiappa, S; Calandra, R",,"Gluch, Grzegorz; Urbanke, Rudiger",,,Constructing a provably adversarially-robust classifier from a high accuracy one,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Modern machine learning models with very high accuracy have been shown to be vulnerable to small, adversarially chosen perturbations of the input. Given black-box access to a high-accuracy classifier f, we show how to construct a new classifier g that has high accuracy and is also robust to adversarial L2-bounded perturbations. Our algorithm builds upon the framework of randomized smoothing that has been recently shown to outperform all previous defenses against L2-bounded adversaries. Using techniques like random partitions and doubling dimension, we are able to bound the adversarial error of g in terms of the optimum error. In this paper we focus on our conceptual contribution, but we do present two examples to illustrate our framework. We will argue that, under some assumptions, our bounds are optimal for these cases.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3674,3683,,,,,,,,,,,,,,,,WOS:000559931301021,0
C,"Huntsman, S",,"Chiappa, S; Calandra, R",,"Huntsman, Steve",,,Fast Markov chain Monte Carlo algorithms via Lie groups,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"From basic considerations of the Lie group that preserves a target probability measure, we derive the Barker, Metropolis, and ensemble Markov chain Monte Carlo (MCMC) algorithms, as well as variants of waste-recycling Metropolis-Hastings and an altogether new MCMC algorithm. We illustrate these constructions with explicit numerical computations, and we empirically demonstrate on a spin glass that the new algorithm converges more quickly than its siblings.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2841,2850,,,,,,,,,,,,,,,,WOS:000559931301047,0
C,"Ikonomov, B; Gutmann, MU",,"Chiappa, S; Calandra, R",,"Ikonomov, Borislav; Gutmann, Michael U.",,,Robust Optimisation Monte Carlo,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"This paper is on Bayesian inference for parametric statistical models that are defined by a stochastic simulator which specifies how data is generated. Exact sampling is then possible but evaluating the likelihood function is typically prohibitively expensive. Approximate Bayesian Computation (ABC) is a framework to perform approximate inference in such situations. While basic ABC algorithms are widely applicable, they are notoriously slow and much research has focused on increasing their efficiency. Optimisation Monte Carlo (OMC) has recently been proposed as an efficient and embarrassingly parallel method that leverages optimisation to accelerate the inference. In this paper, we demonstrate an important previously unrecognised failure mode of OMC: It generates strongly overconfident approximations by collapsing regions of similar or near-constant likelihood into a single point. We propose an efficient, robust generalisation of OMC that corrects this. It makes fewer assumptions, retains the main benefits of OMC, and can be performed either as post-processing to OMC or as a stand-alone computation. We demonstrate the effectiveness of the proposed Robust OMC on toy examples and tasks in inverse-graphics where we perform Bayesian inference with a complex image renderer.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2819,2828,,,,,,,,,,,,,,,,WOS:000559931301050,0
C,"Jiang, H; Nachum, O",,"Chiappa, S; Calandra, R",,"Jiang, Heinrich; Nachum, Ofir",,,Identifying and Correcting Label Bias in Machine Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Datasets often contain biases which unfairly disadvantage certain groups, and classifiers trained on such datasets can inherit these biases. In this paper, we provide a mathematical formulation of how this bias can arise. We do so by assuming the existence of underlying, unknown, and unbiased labels which are over-written by an agent who intends to provide accurate labels but may have biases against certain groups. Despite the fact that we only observe the biased labels, we are able to show that the bias may nevertheless be corrected by re-weighting the data points without changing the labels. We show, with theoretical guarantees, that training on the re-weighted dataset corresponds to training on the unobserved but unbiased labels, thus leading to an unbiased machine learning classifier. Our procedure is fast and robust and can be used with virtually any learning algorithm. We evaluate on a number of standard machine learning fairness datasets and a variety of fairness notions, finding that our method outperforms standard approaches in achieving fair classification.(1)",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,702,711,,,,,,,,,,,,,,,,WOS:000559931301062,0
C,"Kim, GB; Kim, S",,"Chiappa, S; Calandra, R",,"Kim, Gi Bum; Kim, Seyoung",,,Multi-level Gaussian Graphical Models Conditional on Covariates,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We address the problem of learning the structure of a high-dimensional Gaussian graphical model conditional on covariates, when each sample belongs to groups at multiple levels of hierarchy. The existing statistical methods for learning covariate-conditioned Gaussian graphical models focused on learning the aggregate behavior of inputs and outputs in a single-layer network. We propose a statistical model called multi-level conditional Gaussian graphical models for modeling multi-level output networks influenced by both individual-level and group-level inputs. We describe a decomposition of our model into a product of two components, one for sum variables and the other for difference variables derived from the original variables. This decomposition leads to an efficient learning algorithm for both complete data and incomplete data with randomly missing individual observations, as the expensive repeated computation of the partition function can be avoided. We demonstrate our method on simulated data and real-world data in finance and genomics.",,,,,"Kim, SeYoung/GSE-5296-2022","Kim, SeYoung/0000-0001-9188-868X",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,4216,4224,,,,,,,,,,,,,,,,WOS:000559931301075,0
C,"Mehmood, S; Ochs, P",,"Chiappa, S; Calandra, R",,"Mehmood, Sheheryar; Ochs, Peter",,,Automatic Differentiation of Some First-Order Methods in Parametric Optimization,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We aim at computing the derivative of the solution to a parametric optimization problem with respect to the involved parameters. For a class broader than that of strongly convex functions, this can be achieved by automatic differentiation of iterative minimization algorithms. If the iterative algorithm converges pointwise, then we prove that the derivative sequence also converges pointwise to the derivative of the minimizer with respect to the parameters. Moreover, we provide convergence rates for both sequences. In particular, we prove that the accelerated convergence rate of the Heavy-ball method compared to Gradient Descent also accelerates the derivative computation. An experiment with L2-Regularized Logistic Regression validates the theoretical results.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1584,1593,,,,,,,,,,,,,,,,WOS:000559931302037,0
C,"Meng, SY; Vaswani, S; Laradji, I; Schmidt, M; Lacoste-Julien, S",,"Chiappa, S; Calandra, R",,"Meng, Si Yi; Vaswani, Sharan; Laradji, Issam; Schmidt, Mark; Lacoste-Julien, Simon",,,Fast and Furious Convergence: Stochastic Second-Order Methods under Interpolation,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We consider stochastic second-order methods for minimizing smooth and strongly-convex functions under an interpolation condition satisfied by over-parameterized models. Under this condition, we show that the regularized subsampled Newton method (R-SSN) achieves global linear convergence with an adaptive step-size and a constant batch-size. By growing the batch size for both the subsampled gradient and Hessian, we show that R-SSN can converge at a quadratic rate in a local neighbourhood of the solution. We also show that R-SSN attains local linear convergence for the family of self-concordant functions. Furthermore, we analyze stochastic BFGS algorithms in the interpolation setting and prove their global linear convergence. We empirically evaluate stochastic L-BFGS and a Hessian-free implementation of R-SSN for binary classification on synthetic, linearly-separable datasets and real datasets under a kernel mapping. Our experimental results demonstrate the fast convergence of these methods, both in terms of the number of iterations and wall-clock time.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1375,1385,,,,,,,,,,,,,,,,WOS:000559931302039,0
C,"Molkaraie, M",,"Chiappa, S; Calandra, R",,"Molkaraie, Mehdi",,,"Marginal Densities, Factor Graph Duality, and High-Temperature Series Expansions","INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We prove that the marginal densities of a global probability mass function in a primal normal factor graph and the corresponding marginal densities in the dual normal factor graph are related via local mappings. The mapping depends on the Fourier transform of the local factors of the models. Details of the mapping, including its fixed points, are derived for the Ising model, and then extended to the Potts model. By employing the mapping, we can transform simultaneously all the estimated marginal densities from one domain to the other, which is advantageous if estimating the marginals can be carried out more efficiently in the dual domain. An example of particular significance is the ferromagnetic Ising model in a positive external field, for which there is a rapidly mixing Markov chain (called the subgraphs-world process) to generate configurations in the dual normal factor graph of the model. Our numerical experiments illustrate that the proposed procedure can provide more accurate estimates of marginal densities in various settings.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,256,264,,,,,,,,,,,,,,,,WOS:000559931302048,0
C,"Saxena, V; Gonzalez, JE; Jalden, J",,"Chiappa, S; Calandra, R",,"Saxena, Vidit; Gonzalez, Joseph E.; Jalden, Joakim",,,Thompson Sampling for Linearly Constrained Bandits,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We address multi-armed bandits (MAB) where the objective is to maximize the cumulative reward under a probabilistic linear constraint. For a few real-world instances of this problem, constrained extensions of the well-known Thompson Sampling (TS) heuristic have recently been proposed. However, finite-time analysis of constrained TS is challenging; as a result, only O(root T) bounds on the cumulative reward loss (i.e., the regret) are available. In this paper, we describe LinConTS, a TS-based algorithm for bandits that place a linear constraint on the probability of earning a reward in every round. We show that for LinConTS, the regret as well as the cumulative constraint violations are upper bounded by O(log T) for the suboptimal arms. We develop a proof technique that relies on careful analysis of the dual problem and combine it with recent theoretical work on unconstrained TS. Through numerical experiments on two real-world datasets, we demonstrate that LinConTS outperforms an asymptotically optimal upper confidence bound (UCB) scheme in terms of simultaneously minimizing the regret and the violation.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303005,0
C,"Thomas, V; Pedregosa, F; van Merrienboer, B; Mangazol, PA; Bengio, Y; Le Roux, N",,"Chiappa, S; Calandra, R",,"Thomas, Valentin; Pedregosa, Fabian; van Merrienboer, Bart; Mangazol, Pierre-Antoine; Bengio, Yoshua; Le Roux, Nicolas",,,On the interplay between noise and curvature and its effect on optimization and generalization,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"The speed at which one can minimize an expected loss using stochastic methods depends on two properties: the curvature of the loss and the variance of the gradients. While most previous works focus on one or the other of these properties, we explore how their interaction affects optimization speed. Further, as the ultimate goal is good generalization performance, we clarify how both curvature and noise are relevant to properly estimate the generalization gap. Realizing that the limitations of some existing works stems from a confusion between these matrices, we also clarify the distinction between the Fisher matrix, the Hessian, and the covariance matrix of the gradients.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303040,0
C,"Tosatto, S; Carvalho, J; Abdulsamad, H; Peters, J",,"Chiappa, S; Calandra, R",,"Tosatto, Samuele; Carvalho, Joao; Abdulsamad, Hany; Peters, Jan",,,A Nonparametric Off-Policy Policy Gradient,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Hein orcernent, learning' algorithms still suffer from high sample complexity despite outstanding recent successes. The need for intensive interactions with the environment is especially observed in many widely popular policy gradient algorithms that perform updates using on-policy samples. The price of such inefficiency becomes evident in real world scenarios such as interaction-dri yell robot learning, where the success been rather limited. We address this by building on the general sample eflcienc Of off-policy algorithms. With nottparanwtric regression and density estimation methods we construct a nonparametric Bellman equation in a principled manner, which allows us to obtain closed-form estimates of the value function, and to analytically express the ruff policy gradient. a theoretical analysis of our estimate to show that it is consistent, under mild smoothness assumptions and empirically show that our approach has better sample efficiency than stato-ol'-the-art policy gradient methods.",,,,,"Peters, Jan/P-6027-2019; Peters, Jan R/D-5068-2009","Peters, Jan/0000-0002-5266-8091; Peters, Jan R/0000-0002-5266-8091",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303042,0
C,"Valls, V; Iosifidis, G; Leith, DJ; Tassiulas, L",,"Chiappa, S; Calandra, R",,"Valls, Victor; Iosifidis, George; Leith, Douglas J.; Tassiulas, Leandros",,,Online Convex Optimization with Perturbed Constraints: Optimal Rates against Stronger Benchmarks,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"This paper studies Online Convex Optimization (OCO) problems where the constraints have additive perturbations that (i) vary over time and (ii) are not known at the time to make a decision. Perturbations may not be i.i.d. generated and can be used, for example, to model a time-varying budget or timevarying requests in resource allocation problems. Our goal is to design a policy that obtains sublinear regret and satisfies the constraints in the long-term. To this end, we present an online primal-dual proximal gradient algorithm that has 0(T V T1) regret and 0(T) constraint violation, where e [0,1) is a parameter in the learning rate. The proposed algorithm obtains optimal rates when = 1/2, and can compare against a stronger comparator (the set of fixed decisions in hindsight) than previous work.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303052,0
C,"Wang, XY; Yang, Y",,"Chiappa, S; Calandra, R",,"Wang, Xinyi; Yang, Yi",,,Neural Topic Model with Attention for Supervised Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Topic modeling utilizing neural variational inference has shown promising results recently. Unlike traditional Bayesian topic models, neural topic models use deep neural network to approximate the intractable marginal distribution and thus gain strong generalisation ability. However, neural topic models are unsupervised model. Directly using the document-specific topic proportions in downstream prediction tasks could lead to sub-optimal performance. This paper presents Topic Attention Model (TAM) (1), a supervised neural topic model that integrates with a recurrent neural network. We design a novel way to utilize document-specific topic proportions and global topic vectors learned from neural topic model in the attention mechanism. We also develop backpropagation inference method that allows for joint model optimisation. Experimental results on three public datasets show that TAM not only significantly improves supervised learning tasks, including classification and regression, but also achieves lower perplexity for the document modeling.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303061,0
C,"Wang, ZJ; Wang, CJ; Zhang, H; Duan, ZB; Zhou, MY; Cheny, B",,"Chiappa, S; Calandra, R",,"Wang, Zhengjue; Wang, Chaojie; Zhang, Hao; Duan, Zhibin; Zhou, Mingyuan; Cheny, Bo",,,Learning Dynamic Hierarchical Topic Graph with Graph Convolutional Network for Document Classification,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Constructing a graph with graph convolutional network (GCN) to explore the relational structure of the data has attracted lots of interests in various tasks. However, for document classification, existing graph based methods often focus on the straightforward word-word and word-document relations, ignoring the hierarchical semantics. Besides, the graph construction is often independent from the task-specific GCN learning. To address these constrains, we integrate a probabilistic deep topic model into graph construction, and propose a novel trainable hierarchical topic graph (HTG), including word-level, hierarchical topic-level and document-level nodes, exhibiting semantic variation from fine-grained to coarse. Regarding the document classification as a document-node label generation task, HTG can be dynamically evolved with GCN by performing variational inference, which leads to an end-to-end document classification method, named dynamic HTG (DHTG). Besides achieving state-of-the-art classification results, our model learns an interpretable document graph with meaningful node embeddings and semantic edges.",,,,,"Zhou, Mingyuan/AAE-8717-2021",,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303070,0
C,"Zhang, MR; Shen, ZB; Mokhtari, A; Hassani, H; Karbasi, A",,"Chiappa, S; Calandra, R",,"Zhang, Mingrui; Shen, Zebang; Mokhtari, Aryan; Hassani, Hamed; Karbasi, Amin",,,One Sample Stochastic Frank-Wolfe,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"One of the beauties of the projected gradient descent method lies in its rather simple mechanism and yet stable behavior with inexact, stochastic gradients, which has led to its wide-spread use in many machine learning applications. However, once we replace the projection operator with a simpler linear program, as is done in the Frank-Wolfe method, both simplicity and stability take a serious hit. The aim of this paper is to bring them back without sacrificing the efficiency. In this paper, we propose the first one-sample stochastic Frank-Wolfe algorithm, called 1-SFW, that avoids the need to carefully tune the batch size, step size, learning rate, and other complicated hyper parameters. In particular, 1-SFW achieves the best known convergence rate of O(1/epsilon(2)) for reaching an epsilon-suboptimal solution in the stochastic convex setting, and a (1-1/e)-epsilon approximate solution for a stochastic monotone DR-submodular maximization problem. Moreover, in a general non-convex setting, 1-SFW finds an epsilon-first-order stationary point after at most O(1/epsilon(3)) iterations, achieving the current best known convergence rate. All of this is possible by designing a novel unbiased momentum estimator that governs the stability of the optimization process while using a single sample at each iteration.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,4012,4022,,,,,,,,,,,,,,,,WOS:000559931304015,0
C,"Zhao, SJ; Yeh, C; Ermon, S",,"Chiappa, S; Calandra, R",,"Zhao, Shengjia; Yeh, Christopher; Ermon, Stefano",,,A Framework for Sample Efficient Interval Estimation with Control Variates,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We consider the problem of estimating confidence intervals for the mean of a random variable, where the goal is to produce the smallest possible interval for a given number of samples. While minimax optimal algorithms are known for this problem in the general case, improved performance is possible under additional assumptions. In particular, we design an estimation algorithm to take advantage of side information in the form of a control variate, leveraging order statistics. Under certain conditions on the quality of the control variates, we show improved asymptotic efficiency compared to existing estimation algorithms. Empirically, we demonstrate superior performance on several real world surveying and estimation tasks where we use the output of regression models as the control variates.",,,,,"Yeh, Christopher/HGA-9865-2022","Yeh, Christopher/0000-0002-7624-6168",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,4583,4591,,,,,,,,,,,,,,,,WOS:000559931304020,0
C,"Zimmer, C; Driess, D; Meister, M; Duy, NT",,"Chiappa, S; Calandra, R",,"Zimmer, Christoph; Driess, Danny; Meister, Mona; Duy Nguyen-Tuong",,,Adaptive Discretization for Probabilistic Safety Cost Functions Evaluation,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"In many real-world planning applications, e.g. dynamic design of experiments, autonomous driving and robot manipulation, it is necessary to evaluate candidate movement paths with respect to a safety cost function. Here, the continuous candidate paths need to be discretized first and, subsequently, evaluated on the discretization points. The resulting quality of planned paths, thus, highly depends on the definition of the safety cost functions, and the resolution of the discretization. In this paper, we propose an approach for evaluating continuous candidate paths by employing an adaptive discretization scheme, with a probabilistic cost function learned from observations. The obtained path is then guaranteed to be epsilon-safe, i.e. the remaining risk of still finding an unsafe point on the trajectory is smaller than epsilon. The proposed approach is investigated theoretically, as well as empirically validated on several robotic path planning scenarios.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2098,2107,,,,,,,,,,,,,,,,WOS:000559931304026,0
C,"Arvanitidis, G; Hauberg, S; Hennig, P; Schober, M",,"Chaudhuri, K; Sugiyama, M",,"Arvanitidis, Georgios; Hauberg, Soren; Hennig, Philipp; Schober, Michael",,,Fast and Robust Shortest Paths on Manifolds Learned from Data,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We propose a fast, simple and robust algorithm for computing shortest paths and distances on Riemannian manifolds learned from data. This amounts to solving a system of ordinary differential equations (ODES) subject to boundary conditions. Here standard solvers perform poorly because they require well-behaved Jacobians of the ODE, and usually, manifolds learned from data imply unstable and ill-conditioned Jacobians. Instead, we propose a fixed-point iteration scheme for solving the ODE that avoids Jacobians. This enhances the stability of the solver, while reduces the computational cost. In experiments involving both Riemannian metric learning and deep generative models we demonstrate significant improvements in speed and stability over both general-purpose state-of-the-art solvers as well as over specialized solvers.",,,,,"; Hauberg, Soren/L-2104-2016","Arvanitidis, Georgios/0000-0002-0377-2976; Hauberg, Soren/0000-0001-7223-877X",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901057,0
C,"Barnes, M; Dubrawski, A",,"Chaudhuri, K; Sugiyama, M",,"Barnes, Matt; Dubrawski, Artur",,,On the Interaction Effects Between Prediction and Clustering,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Machine learning systems increasingly depend on pipelines of multiple algorithms to provide high quality and well structured predictions. This paper argues interaction effects between clustering and prediction (e.g. classification, regression) algorithms can cause subtle adverse behaviors during cross-validation that may not be initially apparent. In particular, we focus on the problem of estimating the out-of-cluster (OOC) prediction loss given an approximate clustering with probabilistic error rate p(o). Traditional cross-validation techniques exhibit significant empirical bias in this setting, and the few attempts to estimate and correct for these effects are intractable on larger datasets. Further, no previous work has been able to characterize the conditions under which these empirical effects occur, and if they do, what properties they have. We precisely answer these questions by providing theoretical properties which hold in various settings, and prove that expected out-of-cluster loss behavior rapidly decays with even minor clustering errors. Fortunately, we are able to leverage these same properties to construct hypothesis tests and scalable estimators necessary for correcting the problem. Empirical results on benchmark datasets validate our theoretical results and demonstrate how scaling techniques provide solutions to new classes of problems.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,118,126,,,,,,,,,,,,,,,,WOS:000509687900013,0
C,"Brault, R; Lambert, A; Szabo, Z; Sangnier, M; D'Alche-Buc, F",,"Chaudhuri, K; Sugiyama, M",,"Brault, Romain; Lambert, Alex; Szabo, Zoltan; Sangnier, Maxime; D'Alche-Buc, Florence",,,Infinite Task Learning in RKHSs,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Machine learning has witnessed tremendous success in solving tasks depending on a single hyperparameter. When considering simultaneously a finite number of tasks, multitask learning enables one to account for the similarities of the tasks via appropriate regularizers. A step further consists of learning a continuum of tasks for various loss functions. A promising approach, called Parametric Task Learning, has paved the way in the continuum setting for affine models and piecewise-linear loss functions. In this work, we introduce a novel approach called Infinite Task Learning: its goal is to learn a function whose output is a function over the hyperparameter space. We leverage tools from operator-valued kernels and the associated Vector-Valued Reproducing Kernel Hilbert Space that provide an explicit control over the role of the hyperparameters, and also allows us to consider new type of constraints. We provide generalization guarantees to the suggested scheme and illustrate its efficiency in cost-sensitive classification, quantile regression and density level set estimation.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901035,0
C,"Clertant, M; Sokolovska, N; Chevaleyre, Y; Hanczar, B",,"Chaudhuri, K; Sugiyama, M",,"Clertant, Matthieu; Sokolovska, Nataliya; Chevaleyre, Yann; Hanczar, Blaise",,,Interpretable Cascade Classifiers with Abstention,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"In many prediction tasks such as medical diagnostics, sequential decisions are crucial to provide optimal individual treatment. Budget in real-life applications is always limited, and it can represent any limited resource such as time, money, or side effects of medications. In this contribution, we develop a POMDP-based framework to learn cost-sensitive heterogeneous cascading systems. We provide both the theoretical support for the introduced approach and the intuition behind it. We evaluate our novel method on some standard benchmarks, and we discuss how the learned models can be interpreted by human experts.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902037,0
C,"Deshpande, A; Louis, A; Singh, AV",,"Chaudhuri, K; Sugiyama, M",,"Deshpande, Amit; Louis, Anand; Singh, Apoorv Vikram",,,On Euclidean k-Means Clustering with alpha-Center Proximity,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"k-means clustering is NP-hard in the worst case but previous work has shown efficient algorithms assuming the optimal k-means clusters are stable under additive or multiplicative perturbation of data. This has two caveats. First, we do not know how to efficiently verify this property of optimal solutions that are NP-hard to compute in the first place. Second, the stability assumptions required for polynomial time k-means algorithms are often unreasonable when compared to the ground-truth clusters in real-world data. A consequence of multiplicative perturbation resilience is center proximity, that is, every point is closer to the center of its own cluster than the center of any other cluster, by some multiplicative factor alpha > 1. We study the problem of minimizing the Euclidean k-means objective only over clusterings that satisfy alpha-center proximity. We give a simple algorithm to find the optimal alpha-center-proximal k-means clustering in running time exponential in k and 1/(alpha - 1) but linear in the number of points and the dimension. We define an analogous alpha-center proximity condition for outliers, and give similar algorithmic guarantees for k-means with outliers and alpha-center proximity. On the hardness side we show that for any alpha' > 1, there exists an alpha <= alpha', (alpha > 1), and an epsilon(0) > 0 such that minimizing the k-means objective over clusterings that satisfy alpha-center proximity is NP-hard to approximate within a multiplicative (1 + epsilon(0) ) factor.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902014,0
C,"Ding, Q; Yu, HF; Hsieh, CJ",,"Chaudhuri, K; Sugiyama, M",,"Ding, Qin; Yu, Hsiang-Fu; Hsieh, Cho-Jui",,,A Fast Sampling Algorithm for Maximum Inner Product Search,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Maximum Inner Product Search (MIPS) has been recognized as an important operation for the inference phase of many machine learning algorithms, including matrix factorization, multi-class/multi-label prediction and neural networks. In this paper, we propose Sampling-MIPS, which is the first sampling based algorithm that can be applied to the MIPS problem on a set of general vectors with both positive and negative values. Our Sampling-MIPS algorithm is efficient in terms of both time and sample complexity. In particular, by designing a two-step sampling with alias table, Sampling-MIPS only requires constant time to draw a candidate. In addition, we show that the probability of candidate generation in our algorithm is consistent with the true ranking induced by the value of the corresponding inner products, and derive the sample complexity of Sampling-MIPS to obtain the true candidate. Furthermore, the algorithm can be easily extended to large problems with sparse candidate vectors. Experimental results on real and synthetic datasets show that Sampling-MIPS is consistently better than other previous approaches such as LSH-MIPS, PCA-MIPS and Diamond sampling approach.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687903006,0
C,"Dong, JL; Shi, YM",,"Chaudhuri, K; Sugiyama, M",,"Dong, Jialin; Shi, Yuanming",,,Blind Demixing via Wirtinger Flow with Random Initialization,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"This paper concerns the problem of demixing a series of source signals from the sum of bilinear measurements. This problem spans diverse areas such as communication, imaging processing, machine learning, etc. However, semidefinite programming for blind demixing is prohibitive to large-scale problems due to high computational complexity and storage cost. Although several efficient algorithms have been developed recently that enjoy the benefits of fast convergence rates and even regularization free, they still call for spectral initialization. To find simple initialization approach that works equally well as spectral initialization, we propose to solve blind demixing problem via Wirtinger flow with random initialization, which yields a natural implementation. To reveal the efficiency of this algorithm, we provide the global convergence guarantee concerning randomly initialized Wirtinger flow for blind demixing. Specifically, it shows that with sufficient samples, the iterates of randomly initialized Wirtinger flow can enter a local region that enjoys strong convexity and strong smoothness within a few iterations at the first stage. At the second stage, iterates of randomly initialized Wirtinger flow further converge linearly to the ground truth.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,362,370,,,,,,,,,,,,,,,,WOS:000509687900038,0
C,"Fan, R; Jang, B; Sun, YK; Zhou, SH",,"Chaudhuri, K; Sugiyama, M",,"Fan, Roger; Jang, Byoungwook; Sun, Yuekai; Zhou, Shuheng",,,Precision Matrix Estimation with Noisy and Missing Data,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Estimating conditional dependence graphs and precision matrices are some of the most common problems in modern statistics and machine learning. When data are fully observed, penalized maximum likelihood-type estimators have become standard tools for estimating graphical models under sparsity conditions. Extensions of these methods to more complex settings where data are contaminated with additive or multiplicative noise have been developed in recent years. In these settings, however, the relative performance of different methods is not well understood and algorithmic gaps still exist. In particular, in high-dimensional settings these methods require using non-positive semidefinite matrices as inputs, presenting novel optimization challenges. We develop an alternating direction method of multipliers (ADMM) algorithm for these problems, providing a feasible algorithm to estimate precision matrices with indefinite input and potentially nonconvex penalties. We compare this method with existing alternative solutions and empirically characterize the tradeoffs between them. Finally, we use this method to explore the networks among US senators estimated from voting records data.",,,,,"Zhou, Shuheng/FLN-6143-2022",,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902088,0
C,"Fan, XH; Li, B; Sisson, SA",,"Chaudhuri, K; Sugiyama, M",,"Fan, Xuhui; Li, Bin; Sisson, Scott A.",,,Binary Space Partitioning Forests,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"The Binary Space Partitioning (BSP)-Tree process is proposed to produce flexible 2-D partition structures which are originally used as a Bayesian nonparametric prior for relational modelling. It can hardly be applied to other learning tasks such as regression trees because extending the BSP-Tree process to a higher dimensional space is nontrivial. This paper is the first attempt to extend the BSP-Tree process to a d-dimensional (d > 2) space. We propose to generate a cutting hyperplane, which is assumed to be parallel to d 2 dimensions, to cut each node in the d-dimensional BSP-tree. By designing a subtle strategy to sample two free dimensions from d dimensions, the extended BSP-Tree process can inherit the essential self-consistency property from the original version. Based on the extended BSP-Tree process, an ensemble model, which is named the BSP-Forest, is further developed for regression tasks. Thanks to the retained self-consistency property, we can thus significantly reduce the geometric calculations in the inference stage. Compared to its counterpart, the Mondrian Forest, the BSP-Forest can achieve similar performance with fewer cuts due to its flexibility. The BSP-Forest also outperforms other (Bayesian) regression forests on a number of real-world data sets.",,,,,"Fan, Xu/GSE-2196-2022",,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687903008,0
C,"Harutyunyan, A; Dabney, W; Borsa, D; Heess, N; Munos, R; Precup, D",,"Chaudhuri, K; Sugiyama, M",,"Harutyunyan, Anna; Dabney, Will; Borsa, Diana; Heess, Nicolas; Munos, Remi; Precup, Doina",,,The Termination Critic,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"In this work, we consider the problem of autonomously discovering behavioral abstractions, or options, for reinforcement learning agents. We propose an algorithm that focuses on the termination condition, as opposed to as is common the policy. The termination condition is usually trained to optimize a control objective: an option ought to terminate if another has better value. We offer a different, information-theoretic perspective, and propose that terminations should focus instead on the compressibility of the option's encoding arguably a key reason for using abstractions. To achieve this algorithmically, we leverage the classical options framework, and learn the option transition model as a critic for the termination condition. Using this model, we derive gradients that optimize the desired criteria. We show that the resulting options are non-trivial, intuitively meaningful, and useful for learning and planning.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902029,0
C,"Hendrikx, H; Bach, F; Massoulie, L",,"Chaudhuri, K; Sugiyama, M",,"Hendrikx, Hadrien; Bach, Francis; Massoulie, Laurent",,,Accelerated Decentralized Optimization with Local Updates for Smooth and Strongly Convex Objectives,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"In this paper, we study the problem of minimizing a sum of smooth and strongly convex functions split over the nodes of a network in a decentralized fashion. We propose the algorithm ESDACD, a decentralized accelerated algorithm that only requires local synchrony. Its rate depends on the condition number kappa of the local functions as well as the network topology and delays. Under mild assumptions on the topology of the graph, ESDACD takes a time O((tau(max) + Delta(max)) root kappa/gamma ln(epsilon(-1))) to reach a precision epsilon where gamma is the spectral gap of the graph, tau(max) the maximum communication delay and Delta(max) the maximum computation time. Therefore, it matches the rate of SSDA (Seaman et al., 2017), which is optimal when tau(max) = Omega(Delta(max)). Applying ES-DACD to quadratic local functions leads to an accelerated randomized gossip algorithm of rate O(root theta(gossip)/n) where theta(gossip) is the rate of the standard randomized gossip (Boyd et al., 2006). To the best of our knowledge, it is the first asynchronous gossip algorithm with a provably improved rate of convergence of the second moment of the error. We illustrate these results with experiments in idealized settings.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,897,906,,,,,,,,,,,,,,,,WOS:000509687900093,0
C,"Imaizumi, M; Fukumizu, K",,"Chaudhuri, K; Sugiyama, M",,"Imaizumi, Masaaki; Fukumizu, Kenji",,,Deep Neural Networks Learn Non-Smooth Functions Effectively,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We elucidate a theoretical reason that deep neural networks (DNNs) perform better than other models in some cases from the viewpoint of their statistical properties for non-smooth functions. While DNNs have empirically shown higher performance than other standard methods, understanding its mechanism is still a challenging problem. From an aspect of the statistical theory, it is known many standard methods attain the optimal rate of generalization errors for smooth functions in large sample asymptotics, and thus it has not been straightforward to find theoretical advantages of DNNs. This paper fills this gap by considering learning of a certain class of non-smooth functions, which was not covered by the previous theory. We derive the generalization error of estimators by DNNs with a ReLU activation, and show that convergence rates of the generalization by DNNs are almost optimal to estimate the non-smooth functions, while some of the popular models do not attain the optimal rate. In addition, our theoretical result provides guidelines for selecting an appropriate number of layers and edges of DNNs. We provide numerical experiments to support the theoretical results.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,869,878,,,,,,,,,,,,,,,,WOS:000509687900090,0
C,"Karimireddy, SP; Koloskova, A; Stich, SU; Jaggi, M",,"Chaudhuri, K; Sugiyama, M",,"Karimireddy, Sai Praneeth; Koloskova, Anastasia; Stich, Sebastian U.; Jaggi, Martin",,,Efficient Greedy Coordinate Descent for Composite Problems,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Coordinate descent with random coordinate selection is the current state of the art for many large scale optimization problems. However, greedy selection of the steepest coordinate on smooth problems can yield convergence rates independent of the dimension n, and requiring up to n times fewer iterations. In this paper, we consider greedy updates that are based on subgradients for a class of non-smooth composite problems, which includes L1-regularized problems, SVMs and related applications. For these problems we provide (i) the first linear rates of convergence independent of n, and show that our greedy update rule provides speedups similar to those obtained in the smooth case. This was previously conjectured to be true for a stronger greedy coordinate selection strategy. Furthermore, we show that (ii) our new selection rule can be mapped to instances of maximum inner product search, allowing to leverage standard nearest neighbor algorithms to speed up convergence. We demonstrate the validity of the approach through extensive numerical experiments.",,,,,,"Jaggi, Martin/0000-0003-1579-5558",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902096,0
C,"Li, XY; Orabona, F",,"Chaudhuri, K; Sugiyama, M",,"Li, Xiaoyu; Orabona, Francesco",,,On the Convergence of Stochastic Gradient Descent with Adaptive Stepsizes,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Stochastic gradient descent is the method of choice for large scale optimization of machine learning objective functions. Yet, its performance is greatly variable and heavily depends on the choice of the stepsizes. This has motivated a large body of research on adaptive stepsizes. However, there is currently a gap in our theoretical understanding of these methods, especially in the non-convex setting. In this paper, we start closing this gap: we theoretically analyze in the convex and non-convex settings a generalized version of the AdaGrad stepsizes. We show sufficient conditions for these stepsizes to achieve almost sure asymptotic convergence of the gradients to zero, proving the first guarantee for generalized AdaGrad stepsizes in the non-convex setting. Moreover, we show that these stepsizes allow to automatically adapt to the level of noise of the stochastic gradients in both the convex and non-convex settings, interpolating between O(1/T) and O(1/root T), up to logarithmic terms.",,,,,,"orabona, francesco/0000-0001-8523-6845",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901003,0
C,"Liang, TY; Poggio, T; Rakhlin, A; Stokes, J",,"Chaudhuri, K; Sugiyama, M",,"Liang, Tengyuan; Poggio, Tomaso; Rakhlin, Alexander; Stokes, James",,,"Fisher-Rao Metric, Geometry, and Complexity of Neural Networks","22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We study the relationship between geometry and capacity measures for deep neural networks from an invariance viewpoint. We introduce a new notion of capacity the Fisher-Rao norm that possesses desirable invariance properties and is motivated by Information Geometry. We discover an analytical characterization of the new capacity measure, through which we establish norm-comparison inequalities and further show that the new measure serves as an umbrella for several existing norm-based complexity measures. We discuss upper bounds on the generalization error induced by the proposed measure. Extensive numerical experiments on CIFAR-10 support our theoretical findings. Our theoretical analysis rests on a key structural lemma about partial derivatives of multi-layer rectifier networks.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,888,896,,,,,,,,,,,,,,,,WOS:000509687900092,0
C,"Mark, B; Raskutti, G; Willett, R",,"Chaudhuri, K; Sugiyama, M",,"Mark, Benjamin; Raskutti, Garvesh; Willett, Rebecca",,,Estimating Network Structure from Incomplete Event Data,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Multivariate Bernoulli autoregressive (BAR) processes model time series of events in which the likelihood of current events is determined by the times and locations of past events. These processes can be used to model nonlinear dynamical systems corresponding to criminal activity, responses of patients to different medical treatment plans, opinion dynamics across social networks, epidemic spread, and more. Past work examines this problem under the assumption that the event data is complete, but in many cases only a fraction of events are observed. Incomplete observations pose a significant challenge in this setting because the unobserved events still govern the underlying dynamical system. In this work, we develop a novel approach to estimating the parameters of a BAR process in the presence of unobserved events via an unbiased estimator of the complete data log-likelihood function. We propose a computationally efficient estimation algorithm which approximates this estimator via Taylor series truncation and establish theoretical results for both the statistical error and optimization error of our algorithm. We further justify our approach by testing our method on both simulated data and a real data set consisting of crimes recorded by the city of Chicago.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902060,0
C,"Mittal, H; Bhardwaj, A; Gogate, V; Singla, P",,"Chaudhuri, K; Sugiyama, M",,"Mittal, Happy; Bhardwaj, Ayush; Gogate, Vibhav; Singla, Parag",,,Domain-Size Aware Markov Logic Networks,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Several domains in AI need to represent the relational structure as well as model uncertainty. Markov Logic is a powerful formalism which achieves this by attaching weights to formulas in finite first-order logic. Though Markov Logic Networks (MLNs) have been used for a wide variety of applications, a significant challenge remains that weights do not generalize well when training domain sizes are different from those seen during testing. In particular, it has been observed that marginal probabilities tend to extremes in the limit of increasing domain sizes. As the first contribution of our work, we further characterize the distribution and show that marginal probabilities tend to a constant independent of weights and not always to extremes as was previously observed. As our second contribution, we present a principled solution to this problem by defining Domain-size Aware Markov Logic Networks (DA-MLNs) which can be seen as re-parameterizing the MLNs after taking domain size into consideration. For some simple but representative MLN formulas, we formally prove that probabilities defined by DA-MLNs are well behaved. On a practical side, DA-MLNs allow us to generalize the weights learned over small-sized training data to much larger domains. Experiments on three different benchmark MLNs show that our approach results in significant performance gains compared to existing methods.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687903028,0
C,"Nacson, MS; Lee, JD; Gunasekar, S; Savarese, PHP; Srebro, N; Soudry, D",,"Chaudhuri, K; Sugiyama, M",,"Nacson, Mor Shpigel; Lee, Jason D.; Gunasekar, Suriya; Savarese, Pedro H. P.; Srebro, Nathan; Soudry, Daniel",,,Convergence of Gradient Descent on Separable Data,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We provide a detailed study on the implicit bias of gradient descent when optimizing loss functions with strictly monotone tails, such as the logistic loss, over separable datasets. We look at two basic questions: (a) what are the conditions on the tail of the loss function under which gradient descent converges in the direction of the L-2 maximum-margin separator? (b) how does the rate of margin convergence depend on the tail of the loss function and the choice of the step size? We show that for a large family of super-polynomial tailed losses, gradient descent iterates on linear networks of any depth converge in the direction of L-2 maximum-margin solution, while this does not hold for losses with heavier tails. Within this family, for simple linear models we show that the optimal rates with fixed step size is indeed obtained for the commonly used exponentially tailed losses such as logistic loss. However, with a fixed step size the optimal convergence rate is extremely slow as 1/log (t), as also proved in Soudry et al. (2018a). For linear models with exponential loss, we further prove that the convergence rate could be improved to log (t)/root t by using aggressive step sizes that compensates for the rapidly vanishing gradients. Numerical results suggest this method might be useful for deep networks.",,,,,,"Lee, Jason/0000-0003-0064-7800",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687903049,0
C,"Song, JM; Kalluri, P; Grover, A; Zhao, SJ; Ermon, S",,"Chaudhuri, K; Sugiyama, M",,"Song, Jiaming; Kalluri, Pratyusha; Grover, Aditya; Zhao, Shengjia; Ermon, Stefano",,,Learning Controllable Fair Representations,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Learning data representations that are transferable and are fair with respect to certain protected attributes is crucial to reducing unfair decisions while preserving the utility of the data. We propose an information theoretically motivated objective for learning maximally expressive representations subject to fairness constraints. We demonstrate that a range of existing approaches optimize approximations to the Lagrangian dual of our objective. In contrast to these existing approaches, our objective allows the user to control the fairness of the representations by specifying limits on unfairness. Exploiting duality, we introduce a method that optimizes the model parameters as well as the expressiveness-fairness trade-off. Empirical evidence suggests that our proposed method can balance the trade-off between multiple notions of fairness and achieves higher expressiveness at a lower computational cost.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902022,0
C,"Zhang, HY; Shao, JR; Salakhutdinov, R",,"Chaudhuri, K; Sugiyama, M",,"Zhang, Hongyang; Shao, Junru; Salakhutdinov, Ruslan",,,Deep Neural Networks with Multi-Branch Architectures Are Intrinsically Less Non-Convex,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Several recently proposed architectures of neural networks such as ResNeXt, Inception, Xception, SqueezeNet and Wide ResNet are based on the designing idea of having multiple branches and have demonstrated improved performance in many applications. We show that one cause for such success is due to the fact that the multi-branch architecture is intrinsically less non-convex in terms of duality gap. The duality gap measures the degree of intrinsic non-convexity of an optimization problem: smaller gap in relative value implies lower degree of intrinsic non-convexity. The challenge is to quantitatively measure the duality gap of highly non-convex problems such as deep neural networks. In this work, we provide strong guarantees of this quantity for two classes of network architectures. For the neural networks with arbitrary activation functions, multi-branch architecture and a variant of hinge loss, we show that the duality gap of both population and empirical risks shrinks to zero as the number of branches increases. This result sheds light on better understanding the power of over-parametrization where increasing the number of branches tends to make the loss surface less non-convex. For the neural networks with linear activation function and l(2) loss, we show that the duality gap of empirical risk is zero. Our two results work for arbitrary depths, while the analytical techniques might be of independent interest to non-convex optimization more broadly. Experiments on both synthetic and real-world datasets validate our results.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901015,0
C,"Al Farabi, KM; Sarkhel, S; Venugopal, D",,"Storkey, A; PerezCruz, F",,"Al Farabi, Khan Mohammad; Sarkhel, Somdeb; Venugopal, Deepak",,,Efficient Weight Learning in High-Dimensional Untied MLNs,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Existing techniques for improving scalability of weight learning in Markov Logic Networks (MLNs) are typically effective when the parameters of the MLN are tied, i.e., several ground formulas in the MLN share the same weight. However, to improve accuracy in real-world problems, we typically need to learn separate weights for different groundings of the MLN. In this paper, we present an approach to perform efficient weight learning in MLNs containing high-dimensional, untied formulas. The fundamental idea in our approach is to help the learning algorithm navigate the parameter search-space more efficiently by a) tying together groundings of untied formulas that are likely to have similar weights, and b) setting good initial values for the parameters. To do this, we follow a hierarchical approach, where we first learn the parameters that are to be tied using a non-relational learner. We then use a relational learner to learn the tied-parameter MLN with initial values derived from parameters learned by the non-relational learner. We illustrate the promise of our approach on three different real-world problems and show that our approach yields much more scalable and accurate results compared to existing state-of-the-art relational learning systems.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300171,0
C,"Ghoshal, A; Honorio, J",,"Storkey, A; PerezCruz, F",,"Ghoshal, Asish; Honorio, Jean",,,Learning linear structural equation models in polynomial time and sample complexity,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"The problem of learning structural equation models (SEMs) from observational data is a fundamental problem in causal inference. We develop a new algorithm - which is computationally and statistically efficient and works in the high-dimensional regime - for learning linear SEMs from purely observational data with arbitrary noise distribution. We consider three aspects of the problem: identifiability, computational efficiency, and statistical efficiency. We show that when data is generated from a linear SEM over p nodes and maximum Markov blanket size d, our algorithm recovers the directed acyclic graph (DAG) structure of the SEM under an identifiability condition that is more general than those considered in the literature, and without faithfulness assumptions. In the population setting, our algorithm recovers the DAG structure in O(p(d + log p)) operations. In the finite sample setting, if the estimated precision matrix is sparse, our algorithm has a smoothed complexity of (O) over tilde (p(3) + pd(4)), while if the estimated precision matrix is dense, our algorithm has a smoothed complexity of (O) over tilde (p(5)). For sub-Gaussian and bounded (4m-th, m being a positive integer) moment noise, our algorithm has a sample complexity of O(d(4)/epsilon(2) log(p/root delta)) and O(d(4)/epsilon(2) (p(2)/delta)(1/m)) resp., to achieve epsilon element-wise additive error with respect to the true autoregression matrix with probability at least 1 - delta.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300153,0
C,"Massias, M; Fercoq, O; Gramfort, A; Salmon, J",,"Storkey, A; PerezCruz, F",,"Massias, Mathurin; Fercoq, Olivier; Gramfort, Alexandre; Salmon, Joseph",,,Generalized Concomitant Multi-Task Lasso for Sparse Multimodal Regression,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"In high dimension, it is customary to consider Lasso-type estimators to enforce sparsity. For standard Lasso theory to hold, the regularization parameter should be proportional to the noise level, which is often unknown in practice. A remedy is to consider estimators such as the Concomitant Lasso, which jointly optimize over the regression coefficients and the noise level. However, when data from different sources are pooled to increase sample size, noise levels differ and new dedicated estimators are needed. We provide new statistical and computational solutions to perform heteroscedastic regression, with an emphasis on brain imaging with magneto- and electroencephalography (M/EEG). When instantiated to de-correlated noise, our framework leads to an efficient algorithm whose computational cost is not higher than for the Lasso, but addresses more complex noise structures. Experiments demonstrate improved prediction and support identification with correct estimation of noise levels.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300105,0
C,"Xu, YX; Honorio, J; Wang, X",,"Storkey, A; PerezCruz, F",,"Xu, Yixi; Honorio, Jean; Wang, Xiao",,,On the Statistical Efficiency of Compositional Nonparametric Prediction,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"In this paper, we propose a compositional nonparametric method in which a model is expressed as a labeled binary tree of 2k + 1 nodes, where each node is either a summation, a multiplication, or the application of one of the q basis functions to one of the p covariates. We show that in order to recover a labeled binary tree from a given dataset, the sufficient number of samples is O(k log(pq)+log(k!)), and the necessary number of samples is Omega(k log(pq) - log(k!)). We further propose a greedy algorithm for regression in order to validate our theoretical findings through synthetic experiments.",,,,,,"Wang, Xiao/0000-0001-6576-8019",,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300160,0
C,"Chen, EYJ; Choi, A; Darwiche, A",,"Gretton, A; Robert, CC",,"Chen, Eunice Yuh-Jie; Choi, Arthur; Darwiche, Adnan",,,Enumerating Equivalence Classes of Bayesian Networks using EC Graphs,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"We consider the problem of learning Bayesian network structures from complete data. In particular, we consider the enumeration of their k-best equivalence classes. We propose a new search space for A* search, called the EC graph, that facilitates the enumeration of equivalence classes, by representing the space of completed, partially directed acyclic graphs. We also propose a canonization of this search space, called the EC tree, which further improves the efficiency of enumeration. Empirically, our approach is orders of magnitude more efficient than the state-of-the-art at enumerating equivalence classes.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,591,599,,,,,,,,,,,,,,,,WOS:000508662100065,0
C,"Mattei, PA; Bouveyron, C; Latouche, P",,"Gretton, A; Robert, CC",,"Mattei, Pierre-Alexandre; Bouveyron, Charles; Latouche, Pierre",,,Globally Sparse Probabilistic PCA,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"With the flourishing development of high-dimensional data, sparse versions of principal component analysis (PCA) have imposed themselves as simple, yet powerful ways of selecting relevant features in an unsupervised manner. However, when several sparse principal components are computed, the interpretation of the selected variables may be difficult since each axis has its own sparsity pattern and has to be interpreted separately. To overcome this drawback, we propose a Bayesian procedure that allows to obtain several sparse components with the same sparsity pattern. To this end, using Roweis' probabilistic interpretation of PCA and an isotropic Gaussian prior on the loading matrix, we provide the first exact computation of the marginal likelihood of a Bayesian PCA model. In order to avoid the drawbacks of discrete model selection, we propose a simple relaxation of our framework which allows to find a path of models using a variational expectation-maximization algorithm. The exact marginal likelihood can eventually be maximized over this path, relying on Occam's razor to select the relevant variables. Since the sparsity pattern is common to all components, we call this approach globally sparse probabilistic PCA (GSPPCA). Its usefulness is illustrated on synthetic data sets and on several real unsupervised feature selection problems.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,976,984,,,,,,,,,,,,,,,,WOS:000508662100106,0
C,"Zhang, AN; Gultekin, S; Paisley, O",,"Gretton, A; Robert, CC",,"Zhang, Aonan; Gultekin, San; Paisley, John",,,Stochastic Variational Inference for the HDP-HMM,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"We derive a variational inference algorithm for the HDP-HMM based on the two-level stick breaking construction. This construction has previously been applied to the hierarchical Dirichlet processes (HDP) for mixed membership models, allowing for efficient handling of the coupled weight parameters. However, the same algorithm is not directly applicable to HDP-based infinite hidden Markov models (HDP-HMM) because of extra sequential dependencies in the Markov chain. In this paper we provide a solution to this problem by deriving a variational inference algorithm for the HDP-HMM, as well as its stochastic extension, for which all parameter updates are in closed form. We apply our algorithm to sequential text analysis and audio signal analysis, comparing our results with the beam-sampled iHMM, the parametric HMM, and other variational inference approximations.",,,,,"Zhang, Aonan/AAP-9129-2020",,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,800,808,,,,,,,,,,,,,,,,WOS:000508662100087,0
C,"DuBois, C; Korattikara, A; Welling, M; Smyth, P",,"Kaski, S; Corander, J",,"DuBois, Christopher; Korattikara, Anoop; Welling, Max; Smyth, Padhraic",,,Approximate Slice Sampling for Bayesian Posterior Inference,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"In this paper, we advance the theory of large scale Bayesian posterior inference by introducing a new approximate slice sampler that uses only small mini-batches of data in every iteration. While this introduces a bias in the stationary distribution, the computational savings allow us to draw more samples in a given amount of time and reduce sampling variance. We empirically verify on three different models that the approximate slice sampling algorithm can significantly outperform a traditional slice sampler if we are allowed only a fixed amount of computing time for our simulations.",,,,,,"Smyth, Padhraic/0000-0001-9971-8378",,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,185,193,,,,,,,,,,,,,,,,WOS:000508355800021,0
C,"Goes, J; Zhang, T; Arora, R; Lerman, G",,"Kaski, S; Corander, J",,"Goes, John; Zhang, Teng; Arora, Raman; Lerman, Gilad",,,Robust Stochastic Principal Component Analysis,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"We consider the problem of finding lower dimensional subspaces in the presence of outtiers and noise in the online setting. In particular, we extend previous batch formulations of robust PCA to the stochastic setting with minimal storage requirements and run-time complexity. We introduce three novel stochastic approximation algorithms for robust PCA that are extensions of standard algorithms for PCA the stochastic power method, incremental PCA and online PCA using matrix-exponentiated-gradient (MEG) updates. For robust online PCA we also give a sub-linear convergence guarantee. Our numerical results demonstrate the superiority of the the robust online method over the other robust stochastic methods and the advantage of robust methods over their non-robust counterparts in the presence of outliers in artificial and real scenarios.",,,,,,"Lerman, Gilad/0000-0003-4624-3115",,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,266,274,,,,,,,,,,,,,,,,WOS:000508355800030,0
C,"Hanawal, MK; Szepesvari, C; Saligrama, V",,"Singh, A; Zhu, J",,"Hanawal, Manjesh K.; Szepesvari, Csaba; Saligrama, Venkatesh",,,Unsupervised Sequential Sensor Acquisition,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"In many security and healthcare systems a sequence of sensors/tests are used for detection and diagnosis. Each test outputs a prediction of the latent state, and carries with it inherent costs. Our objective is to learn strategies for selecting tests to optimize accuracy & costs. Unfortunately it is often impossible to acquire in-situ ground truth annotations and we are left with the problem of unsupervised sensor selection (USS). We pose USS as a version of stochastic partial monitoring problem with an unusual reward structure (even noisy annotations are unavailable). Unsurprisingly no learner can achieve sublinear regret without further assumptions. To this end we propose the notion of weak-dominance. This is a condition on the joint probability distribution of test outputs and latent state and says that whenever a test is accurate on an example, a later test in the sequence is likely to be accurate as well. We empirically verify that weak dominance holds on real datasets and prove that it is a maximal condition for achieving sublinear regret. We reduce USS to a special case of multi-armed bandit problem with side information and develop polynomial time algorithms that achieve sublinear regret.",,,,,,"Saligrama, Venkatesh/0000-0002-0675-2268",,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,803,811,,,,,,,,,,,,,,,,WOS:000509368500086,0
C,"Locatello, F; Khanna, R; Tschannen, M; Jaggi, M",,"Singh, A; Zhu, J",,"Locatello, Francesco; Khanna, Rajiv; Tschannen, Michael; Jaggi, Martin",,,A Unified Optimization View on Generalized Matching Pursuit and Frank-Wolfe,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Two of the most fundamental prototypes of greedy optimization are the matching pursuit and Frank-Wolfe algorithms. In this paper, we take a unified view on both classes of methods, leading to the first explicit convergence rates of matching pursuit methods in an optimization sense, for general sets of atoms. We derive sublinear (1/t) convergence for both classes on general smooth objectives, and linear convergence on strongly convex objectives, as well as a clear correspondence of algorithm variants. Our presented algorithms and rates are affine invariant, and do not need any incoherence or sparsity assumptions.",,,,,"Khanna, Rajiv/GPK-2566-2022; Locatello, Francesco/GQY-6025-2022","Khanna, Rajiv/0000-0003-1314-3126; Locatello, Francesco/0000-0002-4850-0683; Jaggi, Martin/0000-0003-1579-5558",,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,860,868,,,,,,,,,,,,,,,,WOS:000509368500092,0
C,"Sun, RX; Archer, E; Paninski, L",,"Singh, A; Zhu, J",,"Sun, Ruoxi; Archer, Evan; Paninski, Liam",,,Scalable variational inference for super resolution microscopy,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Super-resolution microscopy methods have become essential tools in biology, opening up a variety of new questions that were previously inaccessible with standard light microscopy methods. In this paper we develop new Bayesian image processing methods that extend the reach of super-resolution microscopy even further. Our method couples variational inference techniques with a data summarization based on Laplace approximation to ensure computational scalability. Our formulation makes it straightforward to incorporate prior information about the underlying sample to further improve accuracy. The proposed method obtains dramatic resolution improvements over previous methods while retaining computational tractability.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,1057,1065,,,,,,,,,,,,,,,,WOS:000509368500113,0
C,"Acharya, A; Ghosh, J; Zhou, MY",,"Lebanon, G; Vishwanathan, SVN",,"Acharya, Ayan; Ghosh, Joydeep; Zhou, Mingyuan",,,Nonparametric Bayesian Factor Analysis for Dynamic Count Matrices,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"A gamma process dynamic Poisson factor analysis model is proposed to factorize a dynamic count matrix, whose columns are sequentially observed count vectors. The model builds a novel Markov chain that sends the latent gamma random variables at time (t -1) as the shape parameters of those at time t, which are linked to observed or latent counts under the Poisson likelihood. The significant challenge of inferring the gamma shape parameters is fully addressed, using unique data augmentation and marginalization techniques for the negative binomial distribution. The same nonparametric Bayesian model also applies to the factorization of a dynamic binary matrix, via a Bernoulli-Poisson link that connects a binary observation to a latent count, with closed-form conditional posteriors for the latent counts and efficient computation for sparse observations. We apply the model to text and music analysis, with state-of-the-art results.",,,,,"Zhou, Mingyuan/AAE-8717-2021",,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,1,9,,,,,,,,,,,,,,,,WOS:000508399700001,0
C,"Hughes, MC; Kim, DI; Sudderth, EB",,"Lebanon, G; Vishwanathan, SVN",,"Hughes, Michael C.; Kim, Dae Il; Sudderth, Erik B.",,,Reliable and Scalable Variational Inference for the Hierarchical Dirichlet Process,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"We introduce a new variational inference objective for hierarchical Dirichlet process admixture models. Our approach provides novel and scalable algorithms for learning nonparametric topic models of text documents and Gaussian admixture models of image patches. Improving on the point estimates of topic probabilities used in previous work, we define full variational posteriors for all latent variables and optimize parameters via a novel surrogate likelihood bound. We show that this approach has crucial advantages for data-driven learning of the number of topics. Via merge and delete moves that remove redundant or irrelevant topics, we learn compact and interpretable models with less computation. Scaling to millions of documents is possible using stochastic or memoized variational updates.",,,,,"Hughes, Michael C./AAO-7155-2021","Hughes, Michael C./0000-0003-4859-7400; Sudderth, Erik/0000-0002-0595-9726",,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,370,378,,,,,,,,,,,,,,,,WOS:000508399700041,0
C,"Kleindessner, M; von Luxburg, U",,"Lebanon, G; Vishwanathan, SVN",,"Kleindessner, Matthaeus; von Luxburg, Ulrike",,,Dimensionality estimation without distances,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"While existing methods for estimating the intrinsic dimension of datasets require to know distances between data points, we consider a situation where one has considerably less information. Given a sample of points, all we get to see is who are the k nearest neighbors of every point. In other words, we get the adjacency matrix of the directed, unweighted k-nearest neighbor graph on the sample, but do not know any point coordinates or distances between the points. We provide two estimators for this situation, a naive one and a more elaborate one. Both of them can be proved to be statistically consistent. However, further theoretical and experimental evidence shows that the naive estimator performs rather poorly, whereas the elaborate one achieves results comparable to those of estimators based on distance information.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,471,479,,,,,,,,,,,,,,,,WOS:000508399700052,0
C,"Lakshminarayanan, B; Roy, DM; Teh, YW",,"Lebanon, G; Vishwanathan, SVN",,"Lakshminarayanan, Balaji; Roy, Daniel M.; Teh, Yee Whye",,,Particle Gibbs for Bayesian Additive Regression Trees,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"Additive regression trees are flexible non-parametric models and popular off-the-shelf tools for real-world non-linear regression. In application domains, such as bioinformatics, where there is also demand for probabilistic predictions with measures of uncertainty, the Bayesian additive regression trees (BART) model, introduced by Chipman et al. (2010), is increasingly popular. As data sets have grown in size, however, the standard Metropolis-Hastings algorithms used to perform inference in BART are proving inadequate. In particular, these Markov chains make local changes to the trees and suffer from slow mixing when the data are high-dimensional or the best-fitting trees are more than a few layers deep. We present a novel sampler for BART based on the Particle Gibbs (PG) algorithm (Andrieu et al., 2010) and a top-down particle filtering algorithm for Bayesian decision trees (Lakshminarayanan et al., 2013). Rather than making local changes to individual trees, the PG sampler proposes a complete tree to fit the residual. Experiments show that the PG sampler outperforms existing samplers in many settings.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,553,561,,,,,,,,,,,,,,,,WOS:000508399700061,0
C,"Lee, CY; Xie, SN; Gallagher, PW; Zhang, ZY; Tu, ZW",,"Lebanon, G; Vishwanathan, SVN",,"Lee, Chen-Yu; Xie, Saining; Gallagher, Patrick W.; Zhang, Zhengyou; Tu, Zhuowen",,,Deeply-Supervised Nets,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"We propose deeply-supervised nets (DSN), a method that simultaneously minimizes classification error and improves the directness and transparency of the hidden layer learning process. We focus our attention on three aspects of traditional convolutional-neural-network-type (CNN-type) architectures: (1) transparency in the effect intermediate layers have on overall classification; (2) discriminativeness and robustness of learned features, especially in early layers; (3) training effectiveness in the face of vanishing gradients. To combat these issues, we introduce companion objective functions at each hidden layer, in addition to the overall objective function at the output layer (an integrated strategy distinct from layer-wise pre-training). We also analyze our algorithm using techniques extended from stochastic gradient methods. The advantages provided by our method are evident in our experimental results, showing state-of-the-art performance on MNIST, CIFAR-10, CIFAR-100, and SVHN.",,,,,"zhang, zheng/HCH-9684-2022",,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,562,570,,,,,,,,,,,,,,,,WOS:000508399700062,0
C,"Mei, SK; Zhu, XJ",,"Lebanon, G; Vishwanathan, SVN",,"Mei, Shike; Zhu, Xiaojin",,,The Security of Latent Dirichlet Allocation,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"Latent Dirichlet allocation (LDA) is an increasingly popular tool for data analysis in many domains. If LDA output affects decision making (especially when money is involved), there is an incentive for attackers to compromise it. We ask the question: how can an attacker minimally poison the corpus so that LDA produces topics that the attacker wants the LDA user to see? Answering this question is important to characterize such attacks, and to develop defenses in the future. We give a novel bilevel optimization formulation to identify the optimal poisoning attack. We present an efficient solution (up to local optima) using descent method and implicit functions. We demonstrate poisoning attacks on LDA with extensive experiments, and discuss possible defenses.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,681,689,,,,,,,,,,,,,,,,WOS:000508399700075,0
C,"Abbasi-Yadkori, Y; Bartlett, PL; Gabillon, V; Malek, A",,"Singh, A; Zhu, J",,"Abbasi-Yadkori, Yasin; Bartlett, Peter L.; Gabillon, Victor; Malek, Alan",,,Hit-and-Run for Sampling and Planning in Non-Convex Spaces,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"We propose the Hit-and-Run algorithm for planning and sampling problems in non-convex spaces. For sampling, we show the first analysis of the Hit-and-Run algorithm in non-convex spaces and show that it mixes fast as long as certain smoothness conditions are satisfied. In particular, our analysis reveals an intriguing connection between fast mixing and the existence of smooth measure-preserving mappings from a convex space to the non-convex space. For planning, we show advantages of Hit-and-Run compared to state-of-the-art planning methods such as Rapidly-Exploring Random Trees.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,888,895,,,,,,,,,,,,,,,,WOS:000509368500095,0
C,"Arabshahi, F; Anandkumar, A",,"Singh, A; Zhu, J",,"Arabshahi, Forough; Anandkumar, Animashree",,,Spectral Methods for Correlated Topic Models,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"In this paper we propose guaranteed spectral methods for learning a broad range of topic models, which generalize the popular Latent Dirichlet Allocation (LDA). We overcome the limitation of LDA to incorporate arbitrary topic correlations, by assuming that the hidden topic proportions are drawn from a flexible class of Normalized Infinitely Divisible (NID) distributions. NID distributions are generated by normalizing a family of independent Infinitely Divisible (ID) random variables. The Dirichlet distribution is a special case obtained by normalizing a set of Gamma random variables. We prove that this flexible topic model class can be learnt via spectral methods using only moments up to the third order, with (low order) polynomial sample and computational complexity. The proof is based on a key new technique derived here that allows us to diagonalize the moments of the NID distribution through an efficient procedure that requires evaluating only univariate integrals, despite the fact that we are handling high dimensional multivariate moments. In order to assess the performance of our proposed Latent NID topic model, we use two real datasets of articles collected from New York Times and Pubmed. Our experiments yield improved perplexity on both datasets compared with the baseline.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,1439,1447,,,,,,,,,,,,,,,,WOS:000509368500154,0
C,"Bertasius, G; Liu, Q; Torresani, L; Shi, JB",,"Singh, A; Zhu, J",,"Bertasius, Gedas; Liu, Qiang; Torresani, Lorenzo; Shi, Jianbo",,,Local Perturb-and-MAP for Structured Prediction,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Conditional random fields (CRFs) provide a powerful tool for structured prediction, but cast significant challenges in both the learning and inference steps. Approximation techniques are widely used in both steps, which should be considered jointly to guarantee good performance (a.k.a. inferning). Perturb-and-MAP models provide a promising alternative to CRFs, but require global combinatorial optimization and hence they are usable only on specific models. In this work, we present a new Local Perturband-MAP (locPMAP) framework that replaces the global optimization with a local optimization by exploiting our observed connection between locPMAP and the pseudolikelihood of the original CRF model. We test our approach on three different vision tasks and show that our method achieves consistently improved performance over other approximate inference techniques optimized to a pseudolikelihood objective. Additionally, we demonstrate that we can integrate our method in the fully convolutional network framework to increase our model's complexity. Finally, our observed connection between locPMAP and the pseudolikelihood leads to a novel perspective for understanding and using pseudolikelihood.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,585,594,,,,,,,,,,,,,,,,WOS:000509368500063,0
C,"Dick, T; Li, M; Pillutla, VK; White, C; Balcan, MF; Smola, A",,"Singh, A; Zhu, J",,"Dick, Travis; Li, Mu; Pillutla, Venkata Krishna; White, Colin; Balcan, Maria Florina; Smola, Alex",,,Data Driven Resource Allocation for Distributed Learning,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"In distributed machine learning, data is dispatched to multiple machines for processing. Motivated by the fact that similar data points often belong to the same or similar classes, and more generally, classification rules of high accuracy tend to be locally simple but globally complex (Vapnik and Bottou, 1993), we propose data dependent dispatching that takes advantage of such structure. We present an in-depth analysis of this model, providing new algorithms with provable worstcase guarantees, analysis proving existing scalable heuristics perform well in natural non worstcase conditions, and techniques for extending a dispatching rule from a small sample to the entire distribution. We overcome novel technical challenges to satisfy important conditions for accurate distributed learning, including fault tolerance and balancedness. We empirically compare our approach with baselines based on random partitioning, balanced partition trees, and locality sensitive hashing, showing that we achieve significantly higher accuracy on both synthetic and real world image and advertising datasets. We also demonstrate that our technique strongly scales with the available computing power.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,662,671,,,,,,,,,,,,,,,,WOS:000509368500071,0
C,"Moscovich, A; Jaffe, A; Nadler, B",,"Singh, A; Zhu, J",,"Moscovich, Amit; Jaffe, Ariel; Nadler, Boaz",,,Minimax-optimal semi-supervised regression on unknown manifolds,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"We consider semi-supervised regression when the predictor variables are drawn from an unknown manifold. A simple two step approach to this problem is to: (i) estimate the manifold geodesic distance between any pair of points using both the labeled and unlabeled instances; and (ii) apply a k nearest neighbor regressor based on these distance estimates. We prove that given sufficiently many unlabeled points, this simple method of geodesic kNN regression achieves the optimal finite-sample minimax bound on the mean squared error, as if the manifold were known. Furthermore, we show how this approach can be efficiently implemented, requiring only O(k N log N) operations to estimate the regression function at all N labeled and unlabeled points. We illustrate this approach on two datasets with a manifold structure: indoor localization using WiFi fingerprints and facial pose estimation. In both cases, geodesic kNN is more accurate and much faster than the popular Laplacian eigenvector regressor.",,,,,"Moscovich, Amit/Y-4085-2018","Moscovich, Amit/0000-0002-1289-8052; Nadler, Boaz/0000-0002-9777-4576",,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,933,942,,,,,,,,,,,,,,,,WOS:000509368500100,0
C,"Tang, D; Jebara, T",,"Singh, A; Zhu, J",,"Tang, Da; Jebara, Tony",,,Initialization and Coordinate Optimization for Multi-way Matching,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"We consider the problem of consistently matching multiple sets of elements to each other, which is a common task in fields such as computer vision. To solve the underlying NP-hard objective, existing methods often relax or approximate it, but end up with unsatisfying empirical performance due to a misaligned objective. We propose a coordinate update algorithm that directly optimizes the target objective. By using pairwise alignment information to build an undirected graph and initializing the permutation matrices along the edges of its Maximum Spanning Tree, our algorithm successfully avoids bad local optima. Theoretically, with high probability our algorithm guarantees an optimal solution under reasonable noise assumptions. Empirically, our algorithm consistently and significantly outperforms existing methods on several benchmark tasks on real datasets.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,1385,1393,,,,,,,,,,,,,,,,WOS:000509368500148,0
C,"Wang, JL; Lee, JD; Mahdavi, M; Kolar, M; Srebro, N",,"Singh, A; Zhu, J",,"Wang, Jialei; Lee, Jason D.; Mahdavi, Mehrdad; Kolar, Mladen; Srebro, Nathan",,,Sketching Meets Random Projection in the Dual: A Provable Recovery Algorithm for Big and High-dimensional Data,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"We provide a unified optimization view of iterative Hessian sketch (IHS) and iterative dual random projection (IDRP). We establish a primal-dual connection between the Hessian sketch and dual random projection, and show that their iterative extensions are optimization processes with preconditioning. We develop accelerated versions of IHS and IDRP based on this insight together with conjugate gradient descent, and propose a primal-dual sketch method that simultaneously reduces the sample size and dimensionality.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,1150,1158,,,,,,,,,,,,,,,,WOS:000509368500123,0
C,"Chen, XL; Ziebart, BD",,"Lebanon, G; Vishwanathan, SVN",,"Chen, Xiangli; Ziebart, Brian D.",,,Predictive Inverse Optimal Control for Linear-Quadratic-Gaussian Systems,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"Predictive inverse optimal control is a powerful approach for estimating the control policy of an agent from observed control demonstrations. Its usefulness has been established in a number of large-scale sequential decision settings characterized by complete state observability. However, many real decisions are made in situations where the state is not fully known to the agent making decisions. Though extensions of predictive inverse optimal control to partially observable Markov decision processes have been developed, their applicability has been limited by the complexities of inference in those representations. In this work, we extend predictive inverse optimal control to the linearquadratic-Gaussian control setting. We establish close connections between optimal control laws for this setting and the probabilistic predictions under our approach. We demonstrate the effectiveness and benefit in estimating control policies that are influenced by partial observability on both synthetic and real datasets.",,,,,,"Ziebart, Brian/0000-0003-4041-6871",,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,165,173,,,,,,,,,,,,,,,,WOS:000508399700019,0
C,"Klami, A; Tripathi, A; Sirola, J; Vare, L; Roulland, F",,"Lebanon, G; Vishwanathan, SVN",,"Klami, Arto; Tripathi, Abhishek; Sirola, Johannes; Vare, Lauri; Roulland, Frederic",,,Latent feature regression for multivariate count data,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"We consider the problem of regression on multivariate count data and present a Gibbs sampler for a latent feature regression model suitable for both under- and overdispersed response variables. The model learns countvalued latent features conditional on arbitrary covariates, modeling them as negative binomial variables, and maps them into the dependent count-valued observations using a Dirichlet-multinomial distribution. From another viewpoint, the model can be seen as a generalization of a specific topic model for scenarios where we are interested in generating the actual counts of observations and not just their relative frequencies and co-occurrences. The model is demonstrated on a smart traffic application where the task is to predict public transportation volume for unknown locations based on a characterization of the close-by services and venues.",,,,,"Klami, Arto/E-7227-2012","Klami, Arto/0000-0002-7950-1355",,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,462,470,,,,,,,,,,,,,,,,WOS:000508399700051,0
C,"Sra, S; Hosseini, R; Theis, L; Bethge, M",,"Lebanon, G; Vishwanathan, SVN",,"Sra, Suvrit; Hosseini, Reshad; Theis, Lucas; Bethge, Matthias",,,Data modeling with the elliptical gamma distribution,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"We study mixture modeling using the elliptical gamma (EG) distribution, a non-Gaussian distribution that allows heavy and light tail and peak behaviors. We first consider maximum likelihood parameter estimation, a task that turns out to be very challenging: we must handle positive definiteness constraints, and more crucially, we must handle possibly nonconcave log-likelihoods, which makes maximization hard. We overcome these difficulties by developing algorithms based on fixed-point theory; our methods respect the psd constraint, while also efficiently solving the (possibly) nonconcave maximization to global optimality. Subsequently, we focus on mixture modeling using EG distributions: we present a closed-form expression of the KL-divergence between two EG distributions, which we then combine with our ML estimation methods to obtain an efficient split-and-merge expectation maximization algorithm. We illustrate the use of our model and algorithms on a dataset of natural image patches.",,,,,"Hosseini, Reshad/AAD-8561-2021",,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,903,911,,,,,,,,,,,,,,,,WOS:000508399700099,0
C,"Eban, E; Schain, M; Mackey, A; Gordon, A; Saurous, RA; Elidan, G",,"Singh, A; Zhu, J",,"Eban, Elad; Schain, Mariano; Mackey, Alan; Gordon, Ariel; Saurous, Rif A.; Elidan, Gal",,,Scalable Learning of Non-Decomposable Objectives,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Modern retrieval systems are often driven by an underlying machine learning model. The goal of such systems is to identify and possibly rank the few most relevant items for a given query or context. Thus, such systems are typically evaluated using a ranking-based performance metric such as the area under the precision-recall curve, the F-beta score, precision at fixed recall, etc. Obviously, it is desirable to train such systems to optimize the metric of interest. In practice, due to the scalability limitations of existing approaches for optimizing such objectives, large-scale retrieval systems are instead trained to maximize classification accuracy, in the hope that performance as measured via the true objective will also be favorable. In this work we present a unified framework that, using straightforward building block bounds, allows for highly scalable optimization of a wide range of ranking-based objectives. We demonstrate the advantage of our approach on several real-life retrieval problems that are significantly larger than those considered in the literature, while achieving substantial improvement in performance over the accuracy-objective baseline.",,,,,,"Elidan, Gal/0000-0001-5365-599X",,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,832,840,,,,,,,,,,,,,,,,WOS:000509368500089,0
C,"Gidel, G; Jebara, T; Lacoste-Julien, S",,"Singh, A; Zhu, J",,"Gidel, Gauthier; Jebara, Tony; Lacoste-Julien, Simon",,,Frank-Wolfe Algorithms for Saddle Point Problems,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"We extend the Frank-Wolfe (FW) optimization algorithm to solve constrained smooth convex-concave saddle point (SP) problems. Remarkably, the method only requires access to linear minimization oracles. Leveraging recent advances in FW optimization, we provide the first proof of convergence of a FW-type saddle point solver over polytopes, thereby partially answering a 30 year-old conjecture. We also survey other convergence results and highlight gaps in the theoretical underpinnings of FW-style algorithms. Motivating applications without known efficient alternatives are explored through structured prediction with combinatorial penalties as well as games over matching polytopes involving an exponential number of constraints.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,362,371,,,,,,,,,,,,,,,,WOS:000509368500040,0
C,"Hara, S; Katsuki, T; Yanagisawa, H; Ono, T; Okamoto, R; Takeuchi, S",,"Singh, A; Zhu, J",,"Hara, Satoshi; Katsuki, Takayuki; Yanagisawa, Hiroki; Ono, Takafumi; Okamoto, Ryo; Takeuchi, Shigeki",,,Consistent and Efficient Nonparametric Different-Feature Selection,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Two-sample feature selection is a ubiquitous problem in both scientific and engineering studies. We propose a feature selection method to find features that describe a difference in two probability distributions. The proposed method is nonparametric and does not assume any specific parametric models on data distributions. We show that the proposed method is computationally efficient and does not require any extra computation for model selection. Moreover, we prove that the proposed method provides a consistent estimator of features under mild conditions. Our experimental results show that the proposed method outperforms the current method with regard to both accuracy and computation time.",,,,,"OKAMOTO, RYO/A-5272-2012","OKAMOTO, RYO/0000-0002-7172-6533",,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,130,138,,,,,,,,,,,,,,,,WOS:000509368500015,0
C,"Jiang, H; Kpotufe, S",,"Singh, A; Zhu, J",,"Jiang, Heinrich; Kpotufe, Samory",,,Modal-set estimation with an application to clustering,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"We present a procedure that can estimate - with statistical consistency guarantees - any local-maxima of a density, under benign distributional conditions. The procedure estimates all such local maxima, or modal-sets, of any bounded shape or dimension, including usual point-modes. In practice, modal-sets can arise as dense low-dimensional structures in noisy data, and more generally serve to better model the rich variety of locally dense structures in data. The procedure is then shown to be competitive on clustering applications, and moreover is quite stable to a wide range of settings of its tuning parameter.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,1197,1206,,,,,,,,,,,,,,,,WOS:000509368500128,0
C,"Sen, R; Shanmugam, K; Kocaoglu, M; Dimakis, AG; Shakkottai, S",,"Singh, A; Zhu, J",,"Sen, Rajat; Shanmugam, Karthikeyan; Kocaoglu, Murat; Dimakis, Alexandros G.; Shakkottai, Sanjay",,,Contextual Bandits with Latent Confounders: An NMF Approach,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Motivated by online recommendation and advertising systems, we consider a causal model for stochastic contextual bandits with a latent low-dimensional confounder. In our model, there are L observed contexts and K arms of the bandit. The observed context influences the reward obtained through a latent confounder variable with cardinality m (m << L,K). The arm choice and the latent confounder causally determines the reward while the observed context is correlated with the confounder. Under this model, the L x K mean reward matrix U (for each context in [L] and each arm in [K]) factorizes into non-negative factors A (L x m) and W (m x K). This insight enables us to propose an epsilon-greedy NMF-Bandit algorithm that designs a sequence of interventions (selecting specific arms), that achieves a balance between learning this low-dimensional structure and selecting the best arm to minimize regret. Our algorithm achieves a regret of O (Lpoly(m, log K) log T) at time T, as compared to O(LK log T) for conventional contextual bandits, assuming a constant gap between the best arm and the rest for each context. These guarantees are obtained under mild sufficiency conditions on the factors that are weaker versions of the well-known Statistical RIP condition. We further propose a class of generative models that satisfy our sufficient conditions, and derive a lower bound of O (Km log T). These are the first regret guarantees for online matrix completion with bandit feedback, when the rank is greater than one. We further compare the performance of our algorithm with the state of the art, on synthetic and real world data-sets.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,518,527,,,,,,,,,,,,,,,,WOS:000509368500056,0
C,"Zhou, TY; Ouyang, H; Bilmes, J; Chang, Y; Guestrin, C",,"Singh, A; Zhu, J",,"Zhou, Tianyi; Ouyang, Hua; Bilmes, Jeff; Chang, Yi; Guestrin, Carlos",,,Scaling Submodular Maximization via Pruned Submodularity Graphs,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"We propose a new random pruning method (called submodular sparsification (SS)) to reduce the cost of submodular maximization. The pruning is applied via a submodularity graph over the n ground elements, where each directed edge is associated with a pairwise dependency defined by the submodular function. In each step, SS prunes a 1-1/root c (for c>1) fraction of the nodes using weights on edges computed based on only a small number (O(logn)) of randomly sampled nodes. The algorithm requires log root c n steps with a small and highly parallelizable per-step computation. An accuracy-speed tradeoff parameter c, set as c=8, leads to a fast shrink rate root 2/4 and small iteration complexity log(2 root 2)n. Analysis shows that w.h.p., the greedy algorithm on the pruned set of size O(log(2)n) can achieve a guarantee similar to that of processing the original dataset. In news and video summarization tasks, SS is able to substantially reduce both computational costs and memory usage, while maintaining (or even slightly exceeding) the quality of the original (and much more costly) greedy algorithm.",,,,,,"Zhou, Tianyi/0000-0001-5348-0632; Chang, Yi/0000-0003-2697-8093",,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,316,324,,,,,,,,,,,,,,,,WOS:000509368500035,0
C,"Acharyya, S; Ghosh, J",,"Lebanon, G; Vishwanathan, SVN",,"Acharyya, Sreangsu; Ghosh, Joydeep",,,Parameter Estimation of Generalized Linear Models without Assuming their Link Function,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"Canonical generalized linear models (GLM) are specified by a finite dimensional vector and a monotonically increasing function called the link function. Standard parameter estimation techniques hold the link function fixed and optimizes over the parameter vector. We propose a parameter-recovery facilitating, jointly-convex, regularized loss functional that is optimized globally over the vector as well as the link function, with best rates possible under a first order oracle model. This widens the scope of GLMs to cases where the link function is unknown.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,10,18,,,,,,,,,,,,,,,,WOS:000508399700002,0
C,"Burda, Y; Grosse, RB; Salakhutdinov, R",,"Lebanon, G; Vishwanathan, SVN",,"Burda, Yuri; Grosse, Roger B.; Salakhutdinov, Ruslan",,,Accurate and Conservative Estimates of MRF Log-likelihood using Reverse Annealing,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"Markov random fields (MRFs) are difficult to evaluate as generative models because computing the test log-probabilities requires the intractable partition function. Annealed importance sampling (AIS) is widely used to estimate MRF partition functions, and often yields quite accurate results. However, AIS is prone to overestimate the log-likelihood with little indication that anything is wrong. We present the Reverse AIS Estimator (RAISE), a stochastic lower bound on the log-likelihood of an approximation to the original MRF model. RAISE requires only the same MCMC transition operators as standard AIS. Experimental results indicate that RAISE agrees closely with AIS log-probability estimates for RBMs, DBMs, and DBNs, but typically errs on the side of underestimating, rather than over-estimating, the log-likelihood.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,102,110,,,,,,,,,,,,,,,,WOS:000508399700012,0
C,"Kontorovich, A; Weiss, R",,"Lebanon, G; Vishwanathan, SVN",,"Kontorovich, Aryeh; Weiss, Roi",,,A Bayes consistent 1-NN classifier,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"We show that a simple modification of the 1-nearest neighbor classifier yields a strongly Bayes consistent learner. Prior to this work, the only strongly Bayes consistent proximity-based method was the k-nearest neighbor classifier, for k growing appropriately with sample size. We will argue that a margin-regularized 1-NN enjoys considerable statistical and algorithmic advantages over the k-NN classifier. These include user-friendly finite-sample error bounds, as well as time- and memory-efficient learning and test-point evaluation algorithms with a principled speed-accuracy tradeoff. Encouraging empirical results are reported.",,,,,"Kontorovich, Aryeh/AAB-4744-2020","Kontorovich, Aryeh/0000-0001-8038-8671",,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,480,488,,,,,,,,,,,,,,,,WOS:000508399700053,0
C,"Meshi, O; Srebro, N; Hazan, T",,"Lebanon, G; Vishwanathan, SVN",,"Meshi, Ofer; Srebro, Nathan; Hazan, Tamir",,,Efficient Training of Structured SVMs via Soft Constraints,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"Structured output prediction is a powerful framework for jointly predicting interdependent output labels. Learning the parameters of structured predictors is a central task in machine learning applications. However, training the model from data often becomes computationally expensive. Several methods have been proposed to exploit the model structure, or decomposition, in order to obtain efficient training algorithms. In particular, methods based on linear programming relaxation, or dual decomposition, decompose the prediction task into multiple simpler prediction tasks and enforce agreement between overlapping predictions. In this work we observe that relaxing these agreement constraints and replacing them with soft constraints yields a much easier optimization problem. Based on this insight we propose an alternative training objective, analyze its theoretical properties, and derive an algorithm for its optimization. Our method, based on the Frank-Wolfe algorithm, achieves significant speedups over existing state-of-theart methods without hurting prediction accuracy.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,699,707,,,,,,,,,,,,,,,,WOS:000508399700077,0
C,"Neufeld, J; Bowling, M; Schuurmans, D",,"Lebanon, G; Vishwanathan, SVN",,"Neufeld, James; Bowling, Michael; Schuurmans, Dale",,,Variance Reduction via Antithetic Markov Chains,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"We present a Monte Carlo integration method, antithetic Markov chain sampling (AMCS), that incorporates local Markov transitions in an underlying importance sampler. Like sequential Monte Carlo sampling, the proposed method uses a sequence of Markov transitions to guide the sampling toward influential regions of the integrand (modes). However, AMCS differs in the type of transitions that may be used, the number of Markov chains, and the method of chain termination. In particular, from each point sampled from an initial proposal, AMCS collects a sequence of points by simulating two independent, but antithetic Markov chains, which are terminated by a sample-dependent stopping rule. Such an approach provides greater flexibility for targeting influential areas while eliminating the need to fix the length of the Markov chain a priori. We show that the resulting estimator is unbiased and can reduce variance on peaked multimodal integrands that challenge current methods.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,708,716,,,,,,,,,,,,,,,,WOS:000508399700078,0
C,"Zhou, XY; Zhang, JX; Kulis, B",,"Lebanon, G; Vishwanathan, SVN",,"Zhou, Xiangyang; Zhang, Jiaxin; Kulis, Brian",,,Power-Law Graph Cuts,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"Algorithms based on spectral graph cut objectives such as normalized cuts, ratio cuts and ratio association have become popular in recent years because they are widely applicable and simple to implement via standard eigenvector computations. Despite strong performance for a number of clustering tasks, spectral graph cut algorithms still suffer from several limitations: first, they require the number of clusters to be known in advance, but this information is often unknown a priori; second, they tend to produce clusters with uniform sizes. In some cases, the true clusters exhibit a known size distribution; in image segmentation, for instance, human-segmented images tend to yield segment sizes that follow a power-law distribution. In this paper, we propose a general framework of power-law graph cut algorithms that produce clusters whose sizes are power-law distributed, and also does not fix the number of clusters upfront. To achieve our goals, we treat the Pitman-Yor exchangeable partition probability function (EPPF) as a regularizer to graph cut objectives. Because the resulting objectives cannot be solved by relaxing via eigenvectors, we derive a simple iterative algorithm to locally optimize the objectives. Moreover, we show that our proposed algorithm can be viewed as performing MAP inference on a particular Pitman-Yor mixture model. Our experiments on various data sets show the effectiveness of our algorithms.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,1144,1152,,,,,,,,,,,,,,,,WOS:000508399700125,0
C,"Anand, A; Noothigattu, R; Singla, P; Mausam",,"Singh, A; Zhu, J",,"Anand, Ankit; Noothigattu, Ritesh; Singla, Parag; Mausam",,,Non-Count Symmetries in Boolean & Multi-Valued Prob. Graphical Models,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Lifted inference algorithms commonly exploit symmetries in a probabilistic graphical model (PGM) for efficient inference. However, existing algorithms for Boolean-valued domains can identify only those pairs of states as symmetric, in which the number of ones and zeros match exactly (count symmetries). Moreover, algorithms for lifted inference in multi-valued domains also compute a multi-valued extension of count symmetries only. These algorithms miss many symmetries in a domain. In this paper, we present first algorithms to compute non-count symmetries in both Boolean-valued and multi-valued domains. Our methods can also find symmetries between multi-valued variables that have different domain cardinalities. The key insight in the algorithms is that they change the unit of symmetry computation from a variable to a variable-value (VV) pair. Our experiments find that exploiting these symmetries in MCMC can obtain substantial computational gains over existing algorithms.",,,,,,"Mausam, ./0000-0003-4088-4296",,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,1541,1549,,,,,,,,,,,,,,,,WOS:000509368500165,0
C,"De, S; Yadav, A; Jacobs, D; Goldstein, T",,"Singh, A; Zhu, J",,"De, Soham; Yadav, Abhay; Jacobs, David; Goldstein, Tom",,,Automated Inference with Adaptive Batches,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Classical stochastic gradient methods for optimization rely on noisy gradient approximations that become progressively less accurate as iterates approach a solution. The large noise and small signal in the resulting gradients makes it difficult to use them for adaptive stepsize selection and automatic stopping. We propose alternative big batch SGD schemes that adaptively grow the batch size over time to maintain a nearly constant signal-to-noise ratio in the gradient approximation. The resulting methods have similar convergence rates to classical SGD, and do not require convexity of the objective. The high fidelity gradients enable automated learning rate selection and do not require stepsize decay. Big batch methods are thus easily automated and can run with little or no oversight.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,1504,1513,,,,,,,,,,,,,,,,WOS:000509368500161,0
C,"Katariya, S; Kveton, B; Szepesvari, C; Vernade, C; Wen, Z",,"Singh, A; Zhu, J",,"Katariya, Sumeet; Kveton, Branislav; Szepesvari, Csaba; Vernade, Claire; Wen, Zheng",,,Stochastic Rank-1 Bandits Stochastic Rank-1 Bandits,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"We propose stochastic rank-1 bandits, a class of online learning problems where at each step a learning agent chooses a pair of row and column arms, and receives the product of their values as a reward. The main challenge of the problem is that the individual values of the row and column are unobserved. We assume that these values are stochastic and drawn independently. We propose a computationally-efficient algorithm for solving our problem, which we call RanklElim. We derive a O((K + L) (1/Delta) log n) upper bound on its n-step regret, where K is the number of rows, L is the number of columns, and Delta is the minimum of the row and column gaps; under the assumption that the mean row and column rewards are bounded away from zero. To the best of our knowledge, we present the first bandit algorithm that finds the maximum entry of a rank-1 matrix whose regret is linear in K L, 1/Delta, and log n. We also derive a nearly matching lower bound. Finally, we evaluate RanklElim empirically on multiple problems. We observe that it leverages the structure of our problems and can learn nearoptimal solutions even if our modeling assumptions are mildly violated.",,,,,"wen, zheng/HII-3705-2022",,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,392,401,,,,,,,,,,,,,,,,WOS:000509368500043,0
C,"Steorts, RC; Barnes, M; Neiswanger, W",,"Singh, A; Zhu, J",,"Steorts, Rebecca C.; Barnes, Matt; Neiswanger, Willie",,,Performance Bounds for Graphical Record Linkage,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Record linkage involves merging records in large, noisy databases to remove duplicate entities. It has become an important area because of its widespread occurrence in bibliometrics, public health, official statistics production, political science, and beyond. Traditional linkage methods directly linking records to one another are computationally infeasible as the number of records grows. As a result, it is increasingly common for researchers to treat record linkage as a clustering task, in which each latent entity is associated with one or more noisy database records. We critically assess performance bounds using the Kullback-Leibler (KL) divergence under a Bayesian record linkage framework, making connections to Kolchin partition models. We provide an upper bound using the KL divergence and a lower bound on the minimum probability of misclassifying a latent entity. We give insights for when our bounds hold using simulated data and provide practical user guidance.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,298,306,,,,,,,,,,,,,,,,WOS:000509368500033,0
C,"Fetaya, E; Shamir, O; Ullman, S",,"Lebanon, G; Vishwanathan, SVN",,"Fetaya, Ethan; Shamir, Ohad; Ullman, Shimon",,,Graph Approximation and Clustering on a Budget,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"We consider the problem of learning from a similarity matrix (such as spectral clustering and low-dimensional embedding), when computing pairwise similarities are costly, and only a limited number of entries can be observed. We provide a theoretical analysis using standard notions of graph approximation, significantly generalizing previous results, which focused on spectral clustering with two clusters. We also propose a new algorithmic approach based on adaptive sampling, which experimentally matches or improves on previous methods, while being considerably more general and computationally cheaper.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,241,249,,,,,,,,,,,,,,,,WOS:000508399700027,0
C,"Srivastava, S; Cevher, V; Quoc , TD; Dunson, DB",,"Lebanon, G; Vishwanathan, SVN",,"Srivastava, Sanvesh; Cevher, Volkan; Quoc Tran-Dinh; Dunson, David B.",,,WASP: Scalable Bayes via barycenters of subset posteriors,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"The promise of Bayesian methods for big data sets has not fully been realized due to the lack of scalable computational algorithms. For massive data, it is necessary to store and process subsets on different machines in a distributed manner. We propose a simple, general, and highly efficient approach, which first runs a posterior sampling algorithm in parallel on different machines for subsets of a large data set. To combine these subset posteriors, we calculate the Wasserstein barycenter via a highly efficient linear program. The resulting estimate for the Wasserstein posterior (WASP) has an atomic form, facilitating straightforward estimation of posterior summaries of functionals of interest. The WASP approach allows posterior sampling algorithms for smaller data sets to be trivially scaled to huge data. We provide theoretical justification in terms of posterior consistency and algorithm efficiency. Examples are provided in complex settings including Gaussian process regression and nonparametric Bayes mixture models.",,,,,"Tran-Dinh, Quoc/AAX-8950-2020","Tran-Dinh, Quoc/0000-0002-1077-2579",,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,912,920,,,,,,,,,,,,,,,,WOS:000508399700100,0
C,"Tian, J",,"Lebanon, G; Vishwanathan, SVN",,"Tian, Jin",,,Missing at Random in Graphical Models,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"The notion of missing at random (MAR) plays a central role in the theory underlying current methods for handling missing data. However the standard definition of MAR is difficult to interpret in practice. In this paper, we assume the missing data model is represented as a directed acyclic graph that not only encodes the dependencies among the variables but also explicitly portrays the causal mechanisms responsible for the missingness process. We introduce an intuitively appealing notion of MAR in such graphical models, and establish its relation with the standard MAR and a few versions of MAR used in the literature. We address the question of whether MAR is testable, given that data are corrupted by missingness, by proposing a general method for identifying testable implications imposed by the graphical structure on the observed data.",,,,,"Tian, Jin/GZM-3191-2022","Tian, Jin/0000-0001-5313-1600",,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,977,985,,,,,,,,,,,,,,,,WOS:000508399700107,0
C,"Pham, V; El Ghaoui, L",,"Lebanon, G; Vishwanathan, SVN",,"Vu Pham; El Ghaoui, Laurent",,,Robust sketching for multiple square-root LASSO problems,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"Many learning tasks, such as cross-validation, parameter search, or leave-one-out analysis, involve multiple instances of similar problems, each instance sharing a large part of learning data with the others. We introduce a robust framework for solving multiple square-root LASSO problems, based on a sketch of the learning data that uses low-rank approximations. Our approach allows a dramatic reduction in computational effort, in effect reducing the number of observations from m (the number of observations to start with) to k (the number of singular values retained in the low-rank model), while not sacrificing-sometimes even improving-the statistical performance. Theoretical analysis, as well as numerical experiments on both synthetic and real data, illustrate the efficiency of the method in large scale applications.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,753,761,,,,,,,,,,,,,,,,WOS:000508399700083,0
C,"Zhu, YC; Barber, RF",,"Lebanon, G; Vishwanathan, SVN",,"Zhu, Yuancheng; Barber, Rina Foygel",,,The Log-Shift Penalty for Adaptive Estimation of Multiple Gaussian Graphical Models,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"Sparse Gaussian graphical models characterize sparse dependence relationships between random variables in a network. To estimate multiple related Gaussian graphical models on the same set of variables, we formulate a hierarchical model, which leads to an optimization problem with a nonconvex log-shift penalty function. We show that under mild conditions the optimization problem is convex despite the inclusion of a nonconvex penalty, and derive an efficient optimization algorithm. Experiments on both synthetic and real data show that the proposed method is able to achieve good selection and estimation performance simultaneously, because the nonconvexity of the log-shift penalty allows for weak signals to be thresholded to zero without excessive shrinkage on the strong signals.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,1153,1161,,,,,,,,,,,,,,,,WOS:000508399700126,0
C,"Belletti, F; Sparks, ER; Bayen, AM; Gonzalez, JE",,"Singh, A; Zhu, J",,"Belletti, Francois; Sparks, Evan R.; Bayen, Alexandre M.; Gonzalez, Joseph E.",,,Random Projection Design for Scalable Implicit Smoothing of Randomly Observed Stochastic Processes,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Standard methods for multi-variate time series analysis are hampered by sampling at random timestamps, long range dependencies, and the scale of the data. In this paper we present a novel estimator for cross-covariance of randomly observed time series which identifies the dynamics of an unobserved stochastic process. We analyze the statistical properties of our estimator without the assumption that observation timestamps are independent from the process of interest and show that our solution does not suffer from the corresponding issues affecting standard estimators for cross-covariance. We implement and evaluate our statistically sound and scalable approach in the distributed setting using Apache Spark and demonstrate its ability to identify interactions between processes on simulations and financial data with tens of millions of samples.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,700,708,,,,,,,,,,,,,,,,WOS:000509368500075,0
C,"Cohen-Addad, V; Kanade, V",,"Singh, A; Zhu, J",,"Cohen-Addad, Vincent; Kanade, Varun",,,Online Optimization of Smoothed Piecewise Constant Functions,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"We study online optimization of smoothed piecewise constant functions over the domain [0; 1). This is motivated by the problem of adaptively picking parameters of learning algorithms as in the recently introduced framework by Gupta and Roughgarden (2016). Majority of the machine learning literature has focused on Lipschitz-continuous functions or functions with bounded gradients.(1) This is with good reason any learning algorithm suffers linear regret even against piecewise constant functions that are chosen adversarially, arguably the simplest of non-Lipschitz continuous functions. The smoothed setting we consider is inspired by the seminal work of Spielman and Teng (2004) and the recent work of Gupta and Roughgarden (2016) in this setting, the sequence of functions may be chosen by an adversary, however, with some uncertainty in the location of discontinuities. We give algorithms that achieve sublinear regret in the full information and bandit settings.",,,,,,"Kanade, Varun/0000-0002-2300-4819",,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,412,420,,,,,,,,,,,,,,,,WOS:000509368500045,0
C,"Jerfel, G; Basbug, ME; Engelhardt, BE",,"Singh, A; Zhu, J",,"Jerfel, Ghassen; Basbug, Mehmet E.; Engelhardt, Barbara E.",,,Dynamic Collaborative Filtering With Compound Poisson Factorization,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Model-based collaborative filtering (CF) analyzes user-item interactions to infer latent factors that represent user preferences and item characteristics in order to predict future interactions. Most CF approaches assume that these latent factors are static; however, in most CF data, user preferences and item perceptions drift over time. Here, we propose a new conjugate and numerically stable dynamic matrix factorization (DCPF) based on hierarchical Poisson factorization that models the smoothly drifting latent factors using gamma-Markov chains. We propose a conjugate gamma chain construction that is numerically stable within our compound-Poisson framework. We then derive a stochastic variational inference approach to estimate the parameters of our model. We apply our model to time-stamped ratings data sets from Netflix, Yelp, and Last.fm. We empirically demonstrate that DCPF achieves a higher predictive accuracy than state-of-the-art static and dynamic factorization algorithms.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,738,747,,,,,,,,,,,,,,,,WOS:000509368500079,0
C,"Le Falher, G; Cesa-Bianchi, N; Gentile, C; Vitale, F",,"Singh, A; Zhu, J",,"Le Falher, Geraud; Cesa-Bianchi, Nicolo; Gentile, Claudio; Vitale, Fabio",,,On the Troll-Trust Model for Edge Sign Prediction in Social Networks,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"In the problem of edge sign prediction, we are given a directed graph (representing a social network), and our task is to predict the binary labels of the edges (i.e., the positive or negative nature of the social relationships). Many successful heuristics for this problem are based on the troll-trust features, estimating at each node the fraction of outgoing and incoming positive/negative edges. We show that these heuristics can be understood, and rigorously analyzed, as approximators to the Bayes optimal classifier for a simple probabilistic model of the edge labels. We then show that the maximum likelihood estimator for this model approximately corresponds to the predictions of a Label Propagation algorithm run on a transformed version of the original social graph. Extensive experiments on a number of real-world datasets show that this algorithm is competitive against state-of-the-art classifiers in terms of both accuracy and scalability. Finally, we show that troll-trust features can also be used to derive online learning algorithms which have theoretical guarantees even when edges are adversarially labeled.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,402,411,,,,,,,,,,,,,,,,WOS:000509368500044,0
C,"McMahan, HB; Moore, E; Ramage, D; Hampson, S; Arcas, BAY",,"Singh, A; Zhu, J",,"McMahan, H. Brendan; Moore, Eider; Ramage, Daniel; Hampson, Seth; Aguera y Arcas, Blaise",,,Communication-Efficient Learning of Deep Networks from Decentralized Data,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning. We present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10-100x as compared to synchronized stochastic gradient descent.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,1273,1282,,,,,,,,,,,,,,,,WOS:000509368500136,0
C,"Park, S; Jang, Y; Galanis, A; Shin, J; Stefankovic, D; Vigoda, E",,"Singh, A; Zhu, J",,"Park, Sejun; Jang, Yunhun; Galanis, Andreas; Shin, Jinwoo; Stefankovic, Daniel; Vigoda, Eric",,,Rapid Mixing Swendsen-Wang Sampler for Stochastic Partitioned Attractive Models,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"The Gibbs sampler is the most popular Markov chain used for learning and inference problems in Graphical Models (GM). These tasks are computationally intractable in general, and the Gibbs sampler often suffers from slow mixing. In this paper, we study the Swendsen-Wang dynamics which is a more sophisticated Markov chain designed to overcome bottlenecks that impede Gibbs sampler. We prove O(log n) mixing time for attractive binary pairwise GMs (i.e., ferromagnetic Ising models) on stochastic partitioned graphs having n vertices, under some mild conditions including low temperature regions where the Gibbs sampler provably mixes exponentially slow. Our experiments also confirm that the Swendsen-Wang sampler significantly outperforms the Gibbs sampler for learning parameters of attractive GMs.",,,,,,"Stefankovic, Daniel/0000-0002-4849-7955",,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,440,449,,,,,,,,,,,,,,,,WOS:000509368500048,0
C,"Sokolic, J; Giryes, R; Sapiro, G; Rodrigues, MRD",,"Singh, A; Zhu, J",,"Sokolic, Jure; Giryes, Raja; Sapiro, Guillermo; Rodrigues, Miguel R. D.",,,Generalization Error of Invariant Classifiers,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"This paper studies the generalization error of invariant classifiers. In particular, we consider the common scenario where the classification task is invariant to certain transformations of the input, and that the classifier is constructed (or learned) to be invariant to these transformations. Our approach relies on factoring the input space into a product of a base space and a set of transformations. We show that whereas the generalization error of a non-invariant classifier is proportional to the complexity of the input space, the generalization error of an invariant classifier is proportional to the complexity of the base space. We also derive a set of sufficient conditions on the geometry of the base space and the set of transformations that ensure that the complexity of the base space is much smaller than the complexity of the input space. Our analysis applies to general classifiers such as convolutional neural networks. We demonstrate the implications of the developed theory for such classifiers with experiments on the MNIST and CIFAR-10 datasets.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,1094,1103,,,,,,,,,,,,,,,,WOS:000509368500117,0
C,"Tang, C; Monteleoni, C",,"Singh, A; Zhu, J",,"Tang, Cheng; Monteleoni, Claire",,,Convergence rate of stochastic k-means,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"We analyze online (Bottou & Bengio, 1994) and mini-batch (Sculley, 2010) k-means variants. Both scale up the widely used Lloyd's algorithm via stochastic approximation, and have become popular for large-scale clustering and unsupervised feature learning. We show, for the first time, that they have global convergence towards local optima at rate O(1/t) under general conditions. In addition, we show that if the dataset is clusterable, stochastic k-means with suitable initialization converges to an optimal k-means solution at rate O(1/t) with high probability. The k-means objective is non-convex and non-differentiable; we exploit ideas from non-convex gradient-based optimization by providing a novel characterization of the trajectory of the k-means algorithm on its solution space, and circumvent its non-differentiability via geometric insights about the k-means update.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,1495,1503,,,,,,,,,,,,,,,,WOS:000509368500160,0
C,"Wagberg, J; Zachariah, D; Schon, TB; Stoica, P",,"Singh, A; Zhu, J",,"Wagberg, Johan; Zachariah, Dave; Schon, Thomas B.; Stoica, Petre",,,Prediction Performance After Learning in Gaussian Process Regression,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"This paper considers the quantification of the prediction performance in Gaussian process regression. The standard approach is to base the prediction error bars on the theoretical predictive variance, which is a lower bound on the mean square-error (MSE). This approach, however, does not take into account that the statistical model is learned from the data. We show that this omission leads to a systematic underestimation of the prediction errors. Starting from a generalization of the Cramer-Rao bound, we derive a more accurate MSE bound which provides a measure of uncertainty for prediction of Gaussian processes. The improved bound is easily computed and we illustrate it using synthetic and real data examples.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,1264,1272,,,,,,,,,,,,,,,,WOS:000509368500135,0
C,"Zhong, K; Guo, RQ; Kumar, S; Yan, BW; Simcha, D; Dhillon, IS",,"Singh, A; Zhu, J",,"Zhong, Kai; Guo, Ruiqi; Kumar, Sanjiv; Yan, Bowei; Simcha, David; Dhillon, Inderjit S.",,,Fast Classification with Binary Prototypes,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"In this work, we propose a new technique for fast k-nearest neighbor (k-NN) classification in which the original database is represented via a small set of learned binary prototypes. The training phase simultaneously learns a hash function which maps the data points to binary codes, and a set of representative binary prototypes. In the prediction phase, we first hash the query into a binary code and then do the k-NN classification using the binary prototypes as the database. Our approach speeds up k-NN classification in two aspects. First, we compress the database into a smaller set of prototypes such that k-NN search only goes through a smaller set rather than the whole dataset. Second, we reduce the original space to a compact binary embedding, where the Hamming distance between two binary codes is very efficient to compute. We propose a formulation to learn the hash function and prototypes such that the classification error is minimized. We also provide a novel theoretical analysis of the proposed technique in terms of Bayes error consistency. Empirically, our method is much faster than the state-of-the-art k-NN compression methods with comparable accuracy.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,1255,1263,,,,,,,,,,,,,,,,WOS:000509368500134,0
C,"Zimin, A; Lampert, CH",,"Singh, A; Zhu, J",,"Zimin, Alexander; Lampert, Christoph H.",,,Learning Theory for Conditional Risk Minimization,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"In this work we study the learnability of stochastic processes with respect to the conditional risk, i.e. the existence of a learning algorithm that improves its next-step performance with the amount of observed data. We introduce a notion of pairwise discrepancy between conditional distributions at different times steps and show how certain properties of these discrepancies can be used to construct a successful learning algorithm. Our main results are two theorems that establish criteria for learnability for many classes of stochastic processes, including all special cases studied previously in the literature.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,213,222,,,,,,,,,,,,,,,,WOS:000509368500024,0
C,"Anderson, DG; Du, SS; Mahoney, MW; Melgaard, C; Wu, KM; Gu, M",,"Lebanon, G; Vishwanathan, SVN",,"Anderson, David G.; Du, Simon S.; Mahoney, Michael W.; Melgaard, Christopher; Wu, Kunming; Gu, Ming",,,Spectral Gap Error Bounds for Improving CUR Matrix Decomposition and the Nystrom Method,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"The CUR matrix decomposition and the related Nystrom method build low-rank approximations of data matrices by selecting a small number of representative rows and columns of the data. Here, we introduce novel spectral gap error bounds that judiciously exploit the potentially rapid spectrum decay in the input matrix, a most common occurrence in machine learning and data analysis. Our error bounds are much tighter than existing ones for matrices with rapid spectrum decay, and they justify the use of a constant amount of over-sampling relative to the rank parameter k, i.e, when the number of columns/rows is l = k + O(1). We demonstrate our analysis on a novel deterministic algorithm, StableCUR, which additionally eliminates a previously unrecognized source of potential instability in CUR decompositions. While our algorithm accepts any method of row and column selection, we implement it with a recent column selection scheme with strong singular value bounds. Empirical results on various classes of real world data matrices demonstrate that our algorithm is as efficient as, and often outperforms, competing algorithms.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,19,27,,,,,,,,,,,,,,,,WOS:000508399700003,0
C,"Chen, YT; Meng, LJ; Tian, J",,"Lebanon, G; Vishwanathan, SVN",,"Chen, Yetian; Meng, Lingjian; Tian, Jin",,,Exact Bayesian Learning of Ancestor Relations in Bayesian Networks,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"Ancestor relations in Bayesian networks (BNs) encode long-range causal relations among random variables. In this paper, we develop dynamic programming (DP) algorithms to compute the exact posterior probabilities of ancestor relations in Bayesian networks. Previous algorithm by Parviainen and Koivisto (2011) evaluates all possible ancestor relations in time O(n3(n)) and space O(3(n)). However, their algorithm assumes an order-modular prior over DAGs that does not respect Markov equivalence. The resulting posteriors would bias towards DAGs consistent with more linear orders. To adhere to the uniform prior, we develop a new DP algorithm that computes the exact posteriors of all possible ancestor relations in time O(n(2)5(n-1)) and space O(3(n)). We also discuss the extension of our algorithm to computing the posteriors of s similar to -> p similar to -> t relations, i.e., a directed path from s to t via p. We apply our algorithm to a biological data set for discovering protein signaling pathways.",,,,,"Tian, Jin/GZM-3191-2022","Tian, Jin/0000-0001-5313-1600",,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,174,182,,,,,,,,,,,,,,,,WOS:000508399700020,0
C,"Schmidt, M; Babanezhad, R; Ahmed, MO; Defazio, A; Clifton, A; Sarkar, A",,"Lebanon, G; Vishwanathan, SVN",,"Schmidt, Mark; Babanezhad, Reza; Ahmed, Mohamed Osama; Defazio, Aaron; Clifton, Ann; Sarkar, Anoop",,,Non-Uniform Stochastic Average Gradient Method for Training Conditional Random Fields,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"We apply stochastic average gradient (SAG) algorithms for training conditional random fields (CRFs). We describe a practical implementation that uses structure in the CRF gradient to reduce the memory requirement of this linearly-convergent stochastic gradient method, propose a non-uniform sampling scheme that substantially improves practical performance, and analyze the rate of convergence of the SAGA variant under non-uniform sampling. Our experimental results reveal that our method significantly outperforms existing methods in terms of the training objective, and performs as well or better than optimally-tuned stochastic gradient methods in terms of test error.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,819,828,,,,,,,,,,,,,,,,WOS:000508399700090,0
C,"Fu, TF; Zhang, ZH",,"Singh, A; Zhu, J",,"Fu, Tianfan; Zhang, Zhihua",,,CPSG-MCMC: Clustering-Based Preprocessing method for Stochastic Gradient MCMC,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"In recent years, stochastic gradient Markov Chain Monte Carlo (SG-MCMC) methods have been raised to process large-scale dataset by iterative learning from small minibatches. However, the high variance caused by naive subsampling usually slows down the convergence to the desired posterior distribution. In this paper, we propose an effective subsampling strategy to reduce the variance based on a failed attempt to do importance sampling. In particular, before sampling, we partition the dataset with k-means clustering algorithm in a preprocessing step and use the fixed clustering throughout the entire MCMC simulation. Then during simulation, we approximate the gradient of log-posterior via summing the estimated gradient of each cluster. The resulting procedure is surprisingly simple without enhancing the complexity of the original algorithm during the sampling procedure. We apply our Clustering-based Preprocessing strategy on stochastic gradient Langevin dynamics, stochastic gradient Hamilton Monte Carlo and stochastic gradient Riemann Langevin dynamics. Empirically, we provide thorough numerical results to back up the effectiveness and efficiency of our approach.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,841,850,,,,,,,,,,,,,,,,WOS:000509368500090,0
C,"Ionescu, C; Popa, AI; Sminchisescu, C",,"Singh, A; Zhu, J",,"Ionescu, Catalin; Popa, Alin-Ionut; Sminchisescu, Cristian",,,Large-Scale Data-Dependent Kernel Approximation,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Learning a computationally efficient kernel from data is an important machine learning problem. The majority of kernels in the literature do not leverage the geometry of the data, and those that do are computationally infeasible for contemporary datasets. Recent advances in approximation techniques have expanded the applicability of the kernel methodology to scale linearly with the data size. Data-dependent kernels, which could leverage this computational advantage, have however not yet seen the benefit. Here we derive an approximate large-scale learning procedure for data-dependent kernels that is efficient and performs well in practice. We provide a Lemma that can be used to derive the asymptotic convergence of the approximation in the limit of infinite random features, and, under certain conditions, an estimate of the convergence speed. We empirically prove that our construction represents a valid, yet efficient approximation of the data-dependent kernel. For large-scale datasets of millions of datapoints, where the proposed method is now applicable for the first time, we notice a significant performance boost over both baselines consisting of data independent kernels and of kernel approximations, at comparable computational cost.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,19,27,,,,,,,,,,,,,,,,WOS:000509368500003,0
C,"Jiang, CM; Xie, HQ; Bai, ZJ",,"Singh, A; Zhu, J",,"Jiang, Chengming; Xie, Huiqing; Bai, Zhaojun",,,Robust and Efficient Computation of Eigenvectors in a Generalized Spectral Method for Constrained Clustering,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"FAST-GE is a generalized spectral method for constrained clustering [Cucuringu et al., AISTATS 2016]. It incorporates the must-link and cannot-link constraints into two Laplacian matrices and then minimizes a Rayleigh quotient via solving a generalized eigenproblem, and is considered to be simple and scalable. However, there are two unsolved issues. Theoretically, since both Laplacian matrices are positive semi-definite and the corresponding pencil is singular, it is not proven whether the minimum of the Rayleigh quotient exists and is equivalent to an eigenproblem. Computationally, the locally optimal block preconditioned conjugate gradient (LOBPCG) method is not designed for solving the eigenproblem of a singular pencil. In fact, to the best of our knowledge, there is no existing eigensolver that is immediately applicable. In this paper, we provide solutions to these two critical issues. We prove a generalization of Courant-Fischer variational principle for the Laplacian singular pencil. We propose a regularization for the pencil so that LOBPCG is applicable. We demonstrate the robustness and efficiency of proposed solutions for constrained image segmentation. The proposed theoretical and computational solutions can be applied to eigenproblems of positive semi-definite pencils arising in other machine learning algorithms, such as generalized linear discriminant analysis in dimension reduction and multisurface classification via eigenvectors.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,757,766,,,,,,,,,,,,,,,,WOS:000509368500081,0
C,"Klein, A; Falkner, S; Bartels, S; Hennig, P; Hutter, F",,"Singh, A; Zhu, J",,"Klein, Aaron; Falkner, Stefan; Bartels, Simon; Hennig, Philipp; Hutter, Frank",,,Fast Bayesian Optimization of Machine Learning Hyperparameters on Large Datasets,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Bayesian optimization has become a successful tool for hyperparameter optimization of machine learning algorithms, such as support vector machines or deep neural networks. Despite its success, for large datasets, training and validating a single configuration often takes hours, days, or even weeks, which limits the achievable performance. To accelerate hyperparameter optimization, we propose a generative model for the validation error as a function of training set size, which is learned during the optimization process and allows exploration of preliminary configurations on small subsets, by extrapolating to the full dataset. We construct a Bayesian optimization procedure, dubbed FABOLAS, which models loss and training time as a function of dataset size and automatically trades off high information gain about the global optimum against computational cost. Experiments optimizing support vector machines and deep neural networks show that FABOLAS often finds high-quality solutions 10 to 100 times faster than other state-of-the-art Bayesian optimization methods or the recently proposed bandit strategy Hyperband.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,528,536,,,,,,,,,,,,,,,,WOS:000509368500057,0
C,"Kpotufe, S",,"Singh, A; Zhu, J",,"Kpotufe, Samory",,,"Lipschitz Density-Ratios, Structured Data, and Data-driven Tuning","ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Density-ratio estimation (i.e. estimating f = f(Q)/f(P) for two unknown distributions Q and P) has proved useful in many Machine Learning tasks, e.g., risk-calibration in transfer-learning, two-sample tests, and also useful in common techniques such importance sampling and bias correction. While there are many important analyses of this estimation problem, the present paper derives convergence rates in other practical settings that are less understood, namely, extensions of traditional Lipschitz smoothness conditions, and common high-dimensional settings with structured data (e.g. manifold data, sparse data). Various interesting facts, which hold in earlier settings, are shown to extend to these settings. Namely, (1) optimal rates depend only on the smoothness of the ratio f, and not on the densities f(Q), f(P), supporting the belief that plugging in estimates for f(Q), f(P) is suboptimal; (2) optimal rates depend only on the intrinsic dimension of data, i.e. this problem - unlike density estimation - escapes the curse of dimension. We further show that near-optimal rates are attainable by estimators tuned from data alone, i.e. with no prior distributional information. This last fact is of special interest in unsupervised settings such as this one, where only oracle rates seem to be known, i.e., rates which assume critical distributional information usually unavailable in practice.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,1320,1328,,,,,,,,,,,,,,,,WOS:000509368500141,0
C,"Mroueh, Y; Marcheret, E; Goel, V",,"Singh, A; Zhu, J",,"Mroueh, Youssef; Marcheret, Etienne; Goel, Vaibhava",,,Co-Occurring Directions Sketching for Approximate Matrix Multiply,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"We introduce co-occurring directions sketching, a deterministic algorithm for approximate matrix product (AMM), in the streaming model. We show that co-occurring directions achieves a better error bound for AMM than other randomized and deterministic approaches for AMM. Co-occurring directions gives a (1 + epsilon)-approximation of the optimal low rank approximation of a matrix product. Empirically our algorithm outperforms competing methods for AMM, for a small sketch size. We validate empirically our theoretical findings and algorithms.",,,,,,"Goel, Vaibhava/0000-0002-5504-3863",,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,567,575,,,,,,,,,,,,,,,,WOS:000509368500061,0
C,"Scarlett, J; Cevher, V",,"Singh, A; Zhu, J",,"Scarlett, Jonathan; Cevher, Volkan",,,Lower Bounds on Active Learning for Graphical Model Selection,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"We consider the problem of estimating the underlying graph associated with a Markov random field, with the added twist that the decoding algorithm can iteratively choose which subsets of nodes to sample based on the previous samples, resulting in an active learning setting. Considering both Ising and Gaussian models, we provide algorithm-independent lower bounds for high-probability recovery within the class of degree-bounded graphs. Our main results are minimax lower bounds for the active setting that match the best known lower bounds for the passive setting, which in turn are known to be tight in several cases of interest. Our analysis is based on Fano's inequality, along with novel mutual information bounds for the active learning setting, and the application of restricted graph ensembles. While we consider ensembles that are similar or identical to those used in the passive setting, we require different analysis techniques, with a key challenge being bounding a mutual information quantity associated with observed subsets of nodes, as opposed to full observations.",,,,,"Scarlett, Jonathan/AGK-0892-2022",,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,55,64,,,,,,,,,,,,,,,,WOS:000509368500007,0
C,"Tian, L; Gu, QQ",,"Singh, A; Zhu, J",,"Tian, Lu; Gu, Quanquan",,,Communication-efficient Distributed Sparse Linear Discriminant Analysis,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"We propose a communication-efficient distributed estimation method for sparse linear discriminant analysis (LDA) in the high dimensional regime. Our method distributes the data of size N into m machines, and estimates a local sparse LDA estimator on each machine using the data subset of size N/m. After the distributed estimation, our method aggregates the debiased local estimators from m machines, and sparsifies the aggregated estimator. We show that the aggregated estimator attains the same statistical rate as the centralized estimation method, as long as the number of machines m is chosen appropriately. Moreover, we prove that our method can attain the model selection consistency under a milder condition than the centralized method. Experiments on both synthetic and real datasets corroborate our theory.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,1178,1187,,,,,,,,,,,,,,,,WOS:000509368500126,0
C,"Vanhaesebrouck, P; Bellet, A; Tommasi, M",,"Singh, A; Zhu, J",,"Vanhaesebrouck, Paul; Bellet, Aurelien; Tommasi, Marc",,,Decentralized Collaborative Learning of Personalized Models over Networks,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"We consider a set of learning agents in a collaborative peer-to-peer network, where each agent learns a personalized model according to its own learning objective. The question addressed in this paper is: how can agents improve upon their locally trained model by communicating with other agents that have similar objectives? We introduce and analyze two asynchronous gossip algorithms running in a fully decentralized manner. Our first approach, inspired from label propagation, aims to smooth pre-trained local models over the network while accounting for the confidence that each agent has in its initial model. In our second approach, agents jointly learn and propagate their model by making iterative updates based on both their local dataset and the behavior of their neighbors. To optimize this challenging objective, our decentralized algorithm is based on ADMM.",,,,,,"Bellet, Aurelien/0000-0003-3440-1251",,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,509,517,,,,,,,,,,,,,,,,WOS:000509368500055,0
C,"Zhang, J; Yen, IEH; Ravikumar, P; Dhillon, IS",,"Singh, A; Zhu, J",,"Zhang, Jiong; Yen, Ian E. H.; Ravikumar, Pradeep; Dhillon, Inderjit S.",,,Scalable Convex Multiple Sequence Alignment via Entropy-Regularized Dual Decomposition,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Multiple Sequence Alignment (MSA) is one of the fundamental tasks in biological sequence analysis that underlies applications such as phylogenetic trees, profiles, and structure prediction. The task, however, is NP-hard, and the current practice resorts to heuristic and local-search methods. Recently, a convex optimization approach for MSA was proposed based on the concept of atomic norm [23], which demonstrates significant improvement over existing methods in the quality of alignments. However, the convex program is challenging to solve due to the constraint given by the intersection of two atomic-norm balls, for which the existing algorithm can only handle sequences of length up to 50, with an iteration complexity subject to constants of unknown relation to the natural parameters of MSA. In this work, we propose an accelerated dual decomposition algorithm that exploits entropy regularization to induce closed-form solutions for each atomic-norm-constrained subproblem, giving a single-loop algorithm of iteration complexity linear to the problem size (total length of all sequences). The proposed algorithm gives significantly better alignments than existing methods on sequences of length up to hundreds, where the existing convex programming method fails to converge in one day.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,1514,1522,,,,,,,,,,,28871272,,,,,WOS:000509368500162,0
C,"Xu, KS",,"Lebanon, G; Vishwanathan, SVN",,"Xu, Kevin S.",,,Stochastic Block Transition Models for Dynamic Networks,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"There has been great interest in recent years on statistical models for dynamic networks. In this paper, I propose a stochastic block transition model (SBTM) for dynamic networks that is inspired by the well-known stochastic block model (SBM) for static networks and previous dynamic extensions of the SBM. Unlike most existing dynamic network models, it does not make a hidden Markov assumption on the edge-level dynamics, allowing the presence or absence of edges to directly influence future edge probabilities while retaining the interpretability of the SBM. I derive an approximate inference procedure for the SBTM and demonstrate that it is significantly better at reproducing durations of edges in real social network data.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,1079,1087,,,,,,,,,,,,,,,,WOS:000508399700118,0
C,"Chen, LJ; Li, J; Qiao, MD",,"Singh, A; Zhu, J",,"Chen, Lijie; Li, Jian; Qiao, Mingda",,,Nearly Instance Optimal Sample Complexity Bounds for Top-k Arm Selection,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"In the Best-k-Arm problem, we are given n stochastic bandit arms, each associated with an unknown reward distribution. We are required to identify the k arms with the largest means by taking as few samples as possible. In this paper, we make progress towards a complete characterization of the instance-wise sample complexity bounds for the Best-k-Arm problem. On the lower bound side, we obtain a novel complexity term to measure the sample complexity that every Best-k-Arm instance requires. This is derived by an interesting and nontrivial reduction from the Best-1-Arm problem. We also provide an elimination-based algorithm that matches the instance-wise lower bound within doubly-logarithmic factors. The sample complexity of our algorithm strictly dominates the state-of-the-art for Best-k-Arm (module constant factors).",,,,,,"Qiao, Mingda/0000-0002-9182-6152",,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,101,110,,,,,,,,,,,,,,,,WOS:000509368500012,0
C,"Huang, XR; Yen, IEH; Zhang, RH; Huang, QX; Ravikumar, P; Dhillon, IS",,"Singh, A; Zhu, J",,"Huang, Xiangru; Yen, Ian E. H.; Zhang, Ruohan; Huang, Qixing; Ravikumar, Pradeep; Dhillon, Inderjit S.",,,Greedy Direction Method of Multiplier for MAP Inference of Large Output Domain,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Maximum-a-Posteriori (MAP) inference lies at the heart of Graphical Models and Structured Prediction. Despite the intractability of exact MAP inference, approximate methods based on LP relaxations have exhibited superior performance across a wide range of applications. Yet for problems involving large output domains (i.e., the state space for each variable is large), standard LP relaxations can easily give rise to a large number of variables and constraints which are beyond the limit of existing optimization algorithms. In this paper, we introduce an effective MAP inference method for problems with large output domains. The method builds upon alternating minimization of an Augmented Lagrangian that exploits the sparsity of messages through greedy optimization techniques. A key feature of our greedy approach is to introduce variables in an on-demand manner with a pre-built data structure over local factors. This results in a single-loop algorithm of sublinear cost per iteration and O(log(1/epsilon))-type iteration complexity to achieve epsilon sub-optimality. In addition, we introduce a variant of GDMM for binary MAP inference problems with a large number of factors. Empirically, the proposed algorithms demonstrate orders of magnitude speedup over state-of-the-art MAP inference techniques on MAP inference problems including Segmentation, Protein Folding, Graph Matching, and Multilabel prediction with pairwise interaction.",,,,,"Huang, Xiangru/ABG-5316-2021",,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,1550,1559,,,,,,,,,,,28871273,,,,,WOS:000509368500166,0
C,"Lakkaraju, H; Rudin, C",,"Singh, A; Zhu, J",,"Lakkaraju, Himabindu; Rudin, Cynthia",,,Learning Cost-Effective and Interpretable Treatment Regimes,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Decision makers, such as doctors and judges, make crucial decisions such as recommending treatments to patients, and granting bail to defendants on a daily basis. Such decisions typically involve weighing the potential benefits of taking an action against the costs involved. In this work, we aim to automate this task of learning cost-effective, interpretable and actionable treatment regimes. We formulate this as a problem of learning a decision list - a sequence of if-then-else rules - that maps characteristics of subjects (eg., diagnostic test results of patients) to treatments. This yields an end-to-end individualized policy for tests and treatments. We propose a novel objective to construct a decision list which maximizes outcomes for the population, and minimizes overall costs. Since we do not observe the outcomes corresponding to counterfactual scenarios, we use techniques from causal inference literature to infer them. We model the problem of learning the decision list as a Markov Decision Process (MDP) and employ a variant of the Upper Confidence Bound for Trees (UCT) strategy which leverages customized checks for pruning the search space effectively. Experimental results on real world observational data capturing judicial bail decisions and treatment recommendations for asthma patients demonstrate the effectiveness of our approach.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,166,175,,,,,,,,,,,,,,,,WOS:000509368500019,0
C,"Pimentel-Alarcon, D; Nowak, R",,"Singh, A; Zhu, J",,"Pimentel-Alarcon, Daniel; Nowak, Robert",,,Random Consensus Robust PCA,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"This paper presents R2PCA, a random consensus method for robust principal component analysis. R2PCA takes RANSAC's principle of using as little data as possible one step further. It iteratively selects small subsets of the data to identify pieces of the principal components, to then stitch them together. We show that if the principal components are in general position and the errors are sufficiently sparse, R2PCA will exactly recover the principal components with probability 1, in lieu of assumptions on coherence or the distribution of the sparse errors, and even under adversarial settings. R2PCA enjoys many advantages: it works well under noise, its computational complexity scales linearly in the ambient dimension, it is easily parallelizable, and due to its low sample complexity, it can be used in settings where data is so large it cannot even be stored in memory. We complement our theoretical findings with synthetic and real data experiments showing that R2PCA outperforms state-of-the-art methods in a broad range of settings.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,344,352,,,,,,,,,,,,,,,,WOS:000509368500038,0
C,"Finn, R; Kulis, B",,"Lebanon, G; Vishwanathan, SVN",,"Finn, Robert; Kulis, Brian",,,A Sufficient Statistics Construction of Exponential Family Levy Measure Densities for Nonparametric Conjugate Models,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"Conjugate pairs of distributions over infinite dimensional spaces are prominent in machine learning, particularly due to the widespread adoption of Bayesian nonparametric methodologies for a host of models and applications. Much of the existing literature in the learning community focuses on processes possessing some form of computationally tractable conjugacy as is the case for the beta process and the gamma process (and, via normalization, the Dirichlet process). For these processes, conjugacy is proved via statistical machinery tailored to the particular model. We seek to address the problem of obtaining a general construction of prior distributions over infinite dimensional spaces possessing distributional properties amenable to conjugacy. Our result is achieved by generalizing Hjort's construction of the beta process via appropriate utilization of sufficient statistics for exponential families.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,250,258,,,,,,,,,,,,,,,,WOS:000508399700028,0
C,"Jaffe, A; Nadler, B; Kluger, Y",,"Lebanon, G; Vishwanathan, SVN",,"Jaffe, Ariel; Nadler, Boaz; Kluger, Yuval",,,Estimating the accuracies of multiple classifiers without labeled data,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"In various situations one is given only the predictions of multiple classifiers over a large unlabeled test data. This scenario raises the following questions: Without any labeled data and without any a-priori knowledge about the reliability of these different classifiers, is it possible to consistently and computationally efficiently estimate their accuracies? Furthermore, also in a completely unsupervised manner, can one construct a more accurate unsupervised ensemble classifier? In this paper, focusing on the binary case, we present simple, computationally efficient algorithms to solve these questions. Furthermore, under standard classifier independence assumptions, we prove our methods are consistent and study their asymptotic error. Our approach is spectral, based on the fact that the off-diagonal entries of the classifiers' covariance matrix and 3-d tensor are rank-one. We illustrate the competitive performance of our algorithms via extensive experiments on both artificial and real datasets.",,,,,,"Nadler, Boaz/0000-0002-9777-4576",,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,407,415,,,,,,,,,,,,,,,,WOS:000508399700045,0
C,"Lacoste-Julien, S; Lindsten, F; Bach, F",,"Lebanon, G; Vishwanathan, SVN",,"Lacoste-Julien, Simon; Lindsten, Fredrik; Bach, Francis",,,Sequential Kernel Herding: Frank-Wolfe Optimization for Particle Filtering,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"Recently, the Frank-Wolfe optimization algorithm was suggested as a procedure to obtain adaptive quadrature rules for integrals of functions in a reproducing kernel Hilbert space (RKHS) with a potentially faster rate of convergence than Monte Carlo integration (and kernel herding was shown to be a special case of this procedure). In this paper, we propose to replace the random sampling step in a particle filter by Frank-Wolfe optimization. By optimizing the position of the particles, we can obtain better accuracy than random or quasi-Monte Carlo sampling. In applications where the evaluation of the emission probabilities is expensive (such as in robot localization), the additional computational cost to generate the particles through optimization can be justified. Experiments on standard synthetic examples as well as on a robot localization task indicate indeed an improvement of accuracy over random and quasi-Monte Carlo sampling.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,544,552,,,,,,,,,,,,,,,,WOS:000508399700060,0
C,"Li, LH; Munos, R; Szepesvari, C",,"Lebanon, G; Vishwanathan, SVN",,"Li, Lihong; Munos, Remi; Szepesvari, Csaba",,,Toward Minimax Off-policy Value Estimation,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"This paper studies the off-policy evaluation problem, where one aims to estimate the value of a target policy based on a sample of observations collected by another policy. We first consider the single-state, or multi-armed bandit case, establish a finite-time minimax risk lower bound, and analyze the risk of three standard estimators. For the so-called regression estimator, we show that while it is asymptotically optimal, for small sample sizes it may perform suboptimally compared to an ideal oracle up to a multiplicative factor that depends on the number of actions. We also show that the other two popular estimators can be arbitrarily worse than the optimal, even in the limit of infinitely many data points. The performance of the estimators are studied in synthetic and real problems; illustrating the methods strengths and weaknesses. We also discuss the implications of these results for off-policy evaluation problems in contextual bandits and fixed-horizon Markov decision processes.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,608,616,,,,,,,,,,,,,,,,WOS:000508399700067,0
C,"Stoehr, J; Friel, N",,"Lebanon, G; Vishwanathan, SVN",,"Stoehr, Julien; Friel, Nial",,,Calibration of conditional composite likelihood for Bayesian inference on Gibbs random fields,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"Gibbs random fields play an important role in statistics, however, the resulting likelihood is typically unavailable due to an intractable normalizing constant. Composite likelihoods offer a principled means to construct useful approximations. This paper provides a mean to calibrate the posterior distribution resulting from using a composite likelihood and illustrate its performance in several examples.",,,,,,"Friel, Nial/0000-0003-4778-0254",,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,921,929,,,,,,,,,,,,,,,,WOS:000508399700101,0
C,"Straub, J; Chang, JS; Freifeld, O; Fisher, JW",,"Lebanon, G; Vishwanathan, SVN",,"Straub, Julian; Chang, Jason; Freifeld, Oren; Fisher, John W., III",,,A Dirichlet Process Mixture Model for Spherical Data,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"Directional data, naturally represented as points on the unit sphere, appear in many applications. However, unlike the case of Euclidean data, flexible mixture models on the sphere that can capture correlations, handle an unknown number of components and extend readily to high-dimensional data have yet to be suggested. For this purpose we propose a Dirichlet process mixture model of Gaussian distributions in distinct tangent spaces (DP-TGMM) to the sphere. Importantly, the formulation of the proposed model allows the extension of recent advances in efficient inference for Bayesian nonparametric models to the spherical domain. Experiments on synthetic data as well as real-world 3D surface normal and 20-dimensional semantic word vector data confirm the expressiveness and applicability of the DP-TGMM.",,,,,,/0000-0003-2339-1262,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,930,938,,,,,,,,,,,,,,,,WOS:000508399700102,0
C,"Szabo, Z; Gretton, A; Poczos, B; Sriperumbudur, B",,"Lebanon, G; Vishwanathan, SVN",,"Szabo, Zoltan; Gretton, Arthur; Poczos, Barnabas; Sriperumbudur, Bharath",,,Two-stage sampled learning theory on distributions,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"We focus on the distribution regression problem: regressing to a real-valued response from a probability distribution. Although there exist a large number of similarity measures between distributions, very little is known about their generalization performance in specific learning tasks. Learning problems formulated on distributions have an inherent two-stage sampled difficulty: in practice only samples from sampled distributions are observable, and one has to build an estimate on similarities computed between sets of points. To the best of our knowledge, the only existing method with consistency guarantees for distribution regression requires kernel density estimation as an intermediate step (which suffers from slow convergence issues in high dimensions), and the domain of the distributions to be compact Euclidean. In this paper, we provide theoretical guarantees for a remarkably simple algorithmic alternative to solve the distribution regression problem: embed the distributions to a reproducing kernel Hilbert space, and learn a ridge regressor from the embeddings to the outputs. Our main contribution is to prove the consistency of this technique in the two-stage sampled setting under mild conditions (on separable, topological domains endowed with kernels). For a given total number of observations, we derive convergence rates as an explicit function of the problem difficulty. As a special case, we answer a 15-year-old open question: we establish the consistency of the classical set kernel [Haussler, 1999; Gartner et. al, 2002] in regression, and cover more recent kernels on distributions, including those due to [Christmann and Steinwart, 2010].",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,948,957,,,,,,,,,,,,,,,,WOS:000508399700104,0
C,"Huggins, JH; Zou, J",,"Singh, A; Zhu, J",,"Huggins, Jonathan H.; Zou, James",,,Quantifying the accuracy of approximate diffusions and Markov chains,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Markov chains and diffusion processes are indispensable tools in machine learning and statistics that are used for inference, sampling, and modeling. With the growth of large-scale datasets, the computational cost associated with simulating these stochastic processes can be considerable, and many algorithms have been proposed to approximate the underlying Markov chain or diffusion. A fundamental question is how the computational savings trade off against the statistical error incurred due to approximations. This paper develops general results that address this question. We bound the Wasserstein distance between the equilibrium distributions of two diffusions as a function of their mixing rates and the deviation in their drifts. We show that this error bound is tight in simple Gaussian settings. Our general result on continuous diffusions can be discretized to provide insights into the computational{statistical trade-off of Markov chains. As an illustration, we apply our framework to derive finite-sample error bounds of approximate unadjusted Langevin dynamics. We characterize computation-constrained settings where, by using fast-to-compute approximate gradients in the Langevin dynamics, we obtain more accurate samples compared to using the exact gradients. Finally, as an additional application of our approach, we quantify the accuracy of approximate zig-zag sampling. Our theoretical analyses are supported by simulation experiments.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,382,391,,,,,,,,,,,,,,,,WOS:000509368500042,0
C,"Jun, SH; Wong, SWK; Zidek, JV; Bouchard-Cote, A",,"Singh, A; Zhu, J",,"Jun, Seong-Hwan; Wong, Samuel W. K.; Zidek, James V.; Bouchard-Cote, Alexandre",,,Sequential Graph Matching with Sequential Monte Carlo,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"We develop a novel probabilistic model for graph matchings and develop practical inference methods for supervised and unsupervised learning of the parameters of this model. The framework we develop admits joint inference on the parameters and the matchings. Furthermore, our framework generalizes naturally to K-partite hypergraph matchings or set packing problems. The sequential formulation of the graph matching process naturally leads to sequential Monte Carlo algorithms which can be combined with various parameter inference methods. We apply our method to image matching problems, document ranking, and our own novel quadripartite matching problem arising from the field of computational forestry.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,1075,1084,,,,,,,,,,,,,,,,WOS:000509368500115,0
C,"Khanna, R; Ghosh, J; Poldrack, R; Koyejo, O",,"Singh, A; Zhu, J",,"Khanna, Rajiv; Ghosh, Joydeep; Poldrack, Russell; Koyejo, Oluwasanmi",,,Information Projection and Approximate Inference for Structured Sparse Variables,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Approximate inference via information projection has been recently introduced as a general-purpose technique for efficient probabilistic inference given sparse variables. This manuscript goes beyond classical sparsity by proposing efficient algorithms for approximate inference via information projection that are applicable to any structure on the set of variables that admits enumeration using matroid or knapsack constraints. Further, leveraging recent advances in submodular optimization, we provide an efficient greedy algorithm with strong optimization-theoretic guarantees. The class of probabilistic models that can be expressed in this way is quite broad and, as we show, includes group sparse regression, group sparse principal components analysis and sparse collective matrix factorization, among others. Empirical results on simulated data and high dimensional neuroimaging data highlight the superior performance of the information projection approach as compared to established baselines for a range of probabilistic models.",,,,,"Khanna, Rajiv/GPK-2566-2022","Khanna, Rajiv/0000-0003-1314-3126; Poldrack, Russell/0000-0001-6755-0259",,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,1358,1366,,,,,,,,,,,,,,,,WOS:000509368500145,0
C,"Thomann, P; Blaschzyk, I; Meister, M; Steinwart, I",,"Singh, A; Zhu, J",,"Thomann, Philipp; Blaschzyk, Ingrid; Meister, Mona; Steinwart, Ingo",,,Spatial Decompositions for Large Scale SVMs,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Although support vector machines (SVMs) are theoretically well understood, their underlying optimization problem becomes very expensive if, for example, hundreds of thousands of samples and a non-linear kernel are considered. Several approaches have been proposed in the past to address this serious limitation. In this work we investigate a decomposition strategy that learns on small, spatially defined data chunks. Our contributions are two fold: On the theoretical side we establish an oracle inequality for the overall learning method using the hinge loss, and show that the resulting rates match those known for SVMs solving the complete optimization problem with Gaussian kernels. On the practical side we compare our approach to learning SVMs on small, randomly chosen chunks. Here it turns out that for comparable training times our approach is significantly faster during testing and also reduces the test error in most cases significantly. Furthermore, we show that our approach easily scales up to 10 million training samples: including hyper-parameter selection using cross validation, the entire training only takes a few hours on a single machine. Finally, we report an experiment on 32 million training samples. All experiments used liquidSVM (Steinwart and Thomann, 2017).",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,1329,1337,,,,,,,,,,,,,,,,WOS:000509368500142,0
C,"Xu, P; Zhang, TT; Gu, QQ",,"Singh, A; Zhu, J",,"Xu, Pan; Zhang, Tingting; Gu, Quanquan",,,Efficient Algorithm for Sparse Tensor-variate Gaussian Graphical Models via Gradient Descent,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"We study the sparse tensor-variate Gaussian graphical model (STGGM), where each way of the tensor follows a multivariate normal distribution whose precision matrix has sparse structures. In order to estimate the precision matrices, we propose a sparsity constrained maximum likelihood estimator. However, due to the complex structure of the tensor-variate GGMs, the likelihood based estimator is non-convex, which poses great challenges for both computation and theoretical analysis. In order to address these challenges, we propose an efficient alternating gradient descent algorithm to solve this estimator, and prove that, under certain conditions on the initial estimator, our algorithm is guaranteed to linearly converge to the unknown precision matrices up to the optimal statistical error. Experiments on both synthetic data and real world brain imaging data corroborate our theory.",,,,,"Xu, Pan/AAH-3620-2019; X, Pan/GVS-4402-2022","Xu, Pan/0000-0002-2559-8622; Zhang, Tingting/0000-0002-7895-8070",,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,923,932,,,,,,,,,,,,,,,,WOS:000509368500099,0
C,"Khanna, R; Ghosh, J; Poldrack, RA; Koyejo, O",,"Lebanon, G; Vishwanathan, SVN",,"Khanna, Rajiv; Ghosh, Joydeep; Poldrack, Russell A.; Koyejo, Oluwasanmi",,,Sparse Submodular Probabilistic PCA,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"We propose a novel approach for sparse probabilistic principal component analysis, that combines a low rank representation for the latent factors and loadings with a novel sparse variational inference approach for estimating distributions of latent variables subject to sparse support constraints. Inference and parameter estimation for the resulting model is achieved via expectation maximization with a novel variational inference method for the E-step that induces sparsity. We show that this inference problem can be reduced to discrete optimal support selection. The discrete optimization is submodular, hence, greedy selection is guaranteed to achieve 1-1/e fraction of the optimal. Empirical studies indicate effectiveness of the proposed approach for the recovery of a parsimonious decomposition as compared to established baseline methods. We also evaluate our method against state-of-the-art methods on high dimensional fMRI data, and show that the method performs as well as or better than other methods.",,,,,"Khanna, Rajiv/GPK-2566-2022","Khanna, Rajiv/0000-0003-1314-3126; Poldrack, Russell/0000-0001-6755-0259",,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,453,461,,,,,,,,,,,,,,,,WOS:000508399700050,0
C,"Kurras, S",,"Lebanon, G; Vishwanathan, SVN",,"Kurras, Sven",,,Symmetric Iterative Proportional Fitting,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"Iterative Proportional Fitting (IPF) generates from an input matrix W a sequence of matrices that converges, under certain conditions, to a specific limit matrix (W) over cap. This limit is the relative-entropy nearest solution to W among all matrices of prescribed row marginals r and column marginals c. We prove this known fact by a novel strategy that contributes a pure algorithmic intuition. Then we focus on the symmetric setting: W = W-T and r = c. Since IPF inherently generates non-symmetric matrices, we introduce two symmetrized variants of IPF. We prove convergence for both of them. Further, we give a novel characterization for the existence of cW in terms of expansion properties of the undirected weighted graph represented by (W) over cap. Finally, we show how our results contribute to recent work in machine learning.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,526,534,,,,,,,,,,,,,,,,WOS:000508399700058,0
C,"Li, X; Zhao, FP; Guo, YH",,"Lebanon, G; Vishwanathan, SVN",,"Li, Xin; Zhao, Feipeng; Guo, Yuhong",,,Conditional Restricted Boltzmann Machines for Multi-label Learning with Incomplete Labels,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"Standard multi-label learning methods assume fully labeled training data. This assumption however is impractical in many application domains where labels are difficult to collect and missing labels are prevalent. In this paper, we develop a novel conditional restricted Boltzmann machine model to address multi-label learning with incomplete labels. It uses a restricted Boltzmann machine to capture the high-order label dependence relationships in the output space, aiming to enhance the capacity of recovering missing labels and learning high quality multi-label prediction models. Moreover, it also incorporates label co-occurrence information retrieved from auxiliary resources as prior knowledge. We perform model training by maximizing the regularized marginal conditional likelihood of the label vectors given the input features, and develop a Viterbi style EM algorithm to solve the induced optimization problem. The proposed approach is evaluated on four real word multi-label data sets by comparing to a number of state-of-the-art methods. The experimental results show it outperforms all the other comparison methods across the applied data sets.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,635,643,,,,,,,,,,,,,,,,WOS:000508399700070,0
C,"Flaxman, S; Teh, YW; Sejdinovic, D",,"Singh, A; Zhu, J",,"Flaxman, Seth; Teh, Yee Whye; Sejdinovic, Dino",,,Poisson intensity estimation with reproducing kernels,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Despite the fundamental nature of the inhomogeneous Poisson process in the theory and application of stochastic processes, and its attractive generalizations (e.g. Cox process), few tractable nonparametric modeling approaches of intensity functions exist, especially in high dimensional settings. In this paper we develop a new, computationally tractable Reproducing Kernel Hilbert Space (RKHS) formulation for the inhomogeneous Poisson process. We model the square root of the intensity as an RKHS function. The modeling challenge is that the usual representer theorem arguments no longer apply due to the form of the inhomogeneous Poisson process likelihood. However, we prove that the representer theorem does hold in an appropriately transformed RKHS, guaranteeing that the optimization of the penalized likelihood can be cast as a tractable finite-dimensional problem. The resulting approach is simple to implement, and readily scales to high dimensions and large-scale datasets.",,,,,,"Sejdinovic, Dino/0000-0001-5547-9213",,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,270,279,,,,,,,,,,,,,,,,WOS:000509368500030,0
C,"Lattimore, T; Szepesvari, C",,"Singh, A; Zhu, J",,"Lattimore, Tor; Szepesvari, Csaba",,,The End of Optimism? An Asymptotic Analysis of Finite-Armed Linear Bandits,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Stochastic linear bandits are a natural and simple generalisation of finite-armed bandits with numerous practical applications. Current approaches focus on generalising existing techniques for finite-armed bandits, notably the optimism principle and Thompson sampling. Prior analysis has mostly focussed on the worst-case setting. We analyse the asymptotic regret and show matching upper and lower bounds on what is achievable. Surprisingly, our results show that no algorithm based on optimism or Thompson sampling will ever achieve the optimal rate. In fact, they can be arbitrarily far from optimal, even in very simple cases. This is a disturbing result because these techniques are standard tools that are widely used for sequential optimisation, for example, generalised linear bandits and reinforcement learning.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,728,737,,,,,,,,,,,,,,,,WOS:000509368500078,0
C,"Malek, A; Katariya, S; Chow, Y; Ghavamzadeh, M",,"Singh, A; Zhu, J",,"Malek, Alan; Katariya, Sumeet; Chow, Yinlam; Ghavamzadeh, Mohammad",,,Sequential Multiple Hypothesis Testing with Type I Error Control,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"This work studies multiple hypothesis testing in the setting when we obtain data sequentially and may choose when to stop sampling. We summarize the notion of a sequential p-value (one that can be continually updated and still maintain a type I error guarantee) and provide several examples from the literature. This tool allows us to convert fixed-horizon step-up or step-down multiple hypothesis testing procedures (which includes Benjamini-Hochberg, Holm, and Bonferroni) into a sequential version that allows the statistician to reject a hypothesis as soon as the sequential p-value reaches a threshold while maintaining type I error control. We show that if the original procedure has a type I error guarantee in a certain family (including FDR and FWER), then the sequential conversion inherits an analogous guarantee. The conversion also allows for allocating samples in a data-dependent way, and we provide simulated experiments demonstrating an increased number of rejections when compared to the fixed-horizon setting.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,1468,1476,,,,,,,,,,,,,,,,WOS:000509368500157,0
C,"Sankaran, R; Bach, F; Bhattacharyya, C",,"Singh, A; Zhu, J",,"Sankaran, Raman; Bach, Francis; Bhattacharyya, Chiranjib",,,Identifying groups of strongly correlated variables through Smoothed Ordered Weighted l(1)-norms,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"The failure of LASSO to identify groups of correlated predictors in linear regression has sparked significant research interest. Recently, various norms [1, 2] were proposed, which can be best described as instances of ordered weighted l(1) norms (OWL) [3], as an alternative to l(1) regularization used in LASSO. OWL can identify groups of correlated variables but it forces the model to be constant within a group. This artifact induces unnecessary bias in the model estimation. In this paper we take a submodular perspective and show that OWL can be posed as the Lovasz extension of a suitably defined submodular function. The submodular perspective not only explains the group-wise constant behavior of OWL, but also suggests alternatives. The main contribution of this paper is smoothed OWL (SOWL), a new family of norms, which not only identifies the groups but also allows the model to be flexible inside a group. We establish several algorithmic and theoretical properties of SOWL including group identification and model consistency. We also provide algorithmic tools to compute the SOWL norm and its proximal operator, whose computational complexity O(d log d) is significantly better than that of general purpose solvers in O(d(2) log d). In our experiments, SOWL compares favorably with respect to OWL in the regimes of interest.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,1123,1131,,,,,,,,,,,,,,,,WOS:000509368500120,0
C,"Azizyan, M; Singh, A; Wasserman, L",,"Lebanon, G; Vishwanathan, SVN",,"Azizyan, Martin; Singh, Aarti; Wasserman, Larry",,,Efficient Sparse Clustering of High-Dimensional Non-spherical Gaussian Mixtures,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"We consider the problem of clustering data points in high dimensions, i.e., when the number of data points may be much smaller than the number of dimensions. Specifically, we consider a Gaussian mixture model (GMM) with two non-spherical Gaussian components, where the clusters are distinguished by only a few relevant dimensions. The method we propose is a combination of a recent approach for learning parameters of a Gaussian mixture model and sparse linear discriminant analysis (LDA). In addition to cluster assignments, the method returns an estimate of the set of features relevant for clustering. Our results indicate that the sample complexity of clustering depends on the sparsity of the relevant feature set, while only scaling logarithmically with the ambient dimension. Further, we require much milder assumptions than existing work on clustering in high dimensions. In particular, we do not require spherical clusters nor necessitate mean separation along relevant dimensions.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,37,45,,,,,,,,,,,,,,,,WOS:000508399700005,0
C,"Carpentier, A",,"Lebanon, G; Vishwanathan, SVN",,"Carpentier, Alexandra",,,Implementable confidence sets in high dimensional regression,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"We consider the setting of linear regression in high dimension. We focus on the problem of constructing adaptive and honest confidence sets for the sparse parameter theta, i.e. we want to construct a confidence set for theta that contains theta with high probability, and that is as small as possible. The l(2) diameter of a such confidence set should depend on the sparsity S of theta - the larger S, the wider the confidence set. However, in practice, S is unknown. This paper focuses on constructing a confidence set for theta which contains theta with high probability, whose diameter is adaptive to the unknown sparsity S, and which is implementable in practice.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,120,128,,,,,,,,,,,,,,,,WOS:000508399700014,0
C,"Farajtabar, M; Gomez-Rodriguez, M; Du, N; Zamani, M; Zha, HY; Song, L",,"Lebanon, G; Vishwanathan, SVN",,"Farajtabar, Mehrdad; Gomez-Rodriguez, Manuel; Du, Nan; Zamani, Mohammad; Zha, Hongyuan; Song, Le",,,Back to the Past: Source Identification in Diffusion Networks from Partially Observed Cascades,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"When a piece of malicious information becomes rampant in an information diffusion network, can we identify the source node that originally introduced the piece into the network and infer the time when it initiated this? Being able to do so is critical for curtailing the spread of malicious information, and reducing the potential losses incurred. This is a very challenging problem since typically only incomplete traces are observed and we need to unroll the incomplete traces into the past in order to pinpoint the source. In this paper, we tackle this problem by developing a two-stage framework, which first learns a continuous-time diffusion network model based on historical diffusion traces and then identifies the source of an incomplete diffusion trace by maximizing the likelihood of the trace under the learned model. Experiments on both large synthetic and real-world data show that our framework can effectively go back to the past, and pinpoint the source node and its initiation time significantly more accurately than previous state-of-the-arts.",,,,,"Rodriguez, Manuel Gomez/AAB-5005-2021","Gomez Rodriguez, Manuel/0000-0003-3930-1161",,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,232,240,,,,,,,,,,,,,,,,WOS:000508399700026,0
C,"Jadbabaie, A; Rakhlin, A; Shahrampour, S; Sridharan, K",,"Lebanon, G; Vishwanathan, SVN",,"Jadbabaie, Ali; Rakhlin, Alexander; Shahrampour, Shahin; Sridharan, Karthik",,,Online Optimization : Competing with Dynamic Comparators,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"Recent literature on online learning has focused on developing adaptive algorithms that take advantage of a regularity of the sequence of observations, yet retain worst-case performance guarantees. A complementary direction is to develop prediction methods that perform well against complex benchmarks. In this paper, we address these two directions together. We present a fully adaptive method that competes with dynamic benchmarks in which regret guarantee scales with regularity of the sequence of cost functions and comparators. Notably, the regret bound adapts to the smaller complexity measure in the problem environment. Finally, we apply our results to drifting zero-sum, two-player games where both players achieve no regret guarantees against best sequences of actions in hindsight.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,398,406,,,,,,,,,,,,,,,,WOS:000508399700044,0
C,"Scott, C",,"Lebanon, G; Vishwanathan, SVN",,"Scott, Clayton",,,"A Rate of Convergence for Mixture Proportion Estimation, with Application to Learning from Noisy Labels","ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"Mixture proportion estimation (MPE) is a fundamental tool for solving a number of weakly supervised learning problems - supervised learning problems where label information is noisy or missing. Previous work on MPE has established a universally consistent estimator. In this work we establish a rate of convergence for mixture proportion estimation under an appropriate distributional assumption, and argue that this rate of convergence is useful for analyzing weakly supervised learning algorithms that build on MPE. To illustrate this idea, we examine an algorithm for classification in the presence of noisy labels based on surrogate risk minimization, and show that the rate of convergence for MPE enables proof of the algorithm's consistency. Finally, we provide a practical implementation of mixture proportion estimation and demonstrate its efficacy in classification with noisy labels.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,838,846,,,,,,,,,,,,,,,,WOS:000508399700092,0
C,"Ali, A; Khare, K; Oh, SY; Rajaratnam, B",,"Singh, A; Zhu, J",,"Ali, Alnur; Khare, Kshitij; Oh, Sang-Yun; Rajaratnam, Bala",,,Generalized Pseudolikelihood Methods for Inverse Covariance Estimation,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"We introduce PseudoNet, a new pseudolikelihood-based estimator of the inverse covariance matrix, that has a number of useful statistical and computational properties. We show, through detailed experiments with synthetic as well as real-world finance and wind power data, that PseudoNet outperforms related methods in terms of estimation error and support recovery, making it well-suited for use in a downstream application, where obtaining low estimation error can be important. We also show, under regularity conditions, that PseudoNet is consistent. Our proof assumes the existence of accurate estimates of the diagonal entries of the underlying inverse covariance matrix; we additionally provide a two-step method to obtain these estimates, even in a high-dimensional setting, going beyond the proofs for related methods. Unlike other pseudolikelihood-based methods, we also show that PseudoNet does not saturate, i.e., in high dimensions, there is no hard limit on the number of nonzero entries in the PseudoNet estimate. We present a fast algorithm as well as screening rules that make computing the PseudoNet estimate over a range of tuning parameters tractable.",,,,,,"Oh, Sang-Yun/0000-0002-0364-5109",,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,280,288,,,,,,,,,,,,,,,,WOS:000509368500031,0
C,"Fagan, F; Bhandari, J; Cunningham, JP",,"Singh, A; Zhu, J",,"Fagan, Francois; Bhandari, Jalaj; Cunningham, John P.",,,Annular Augmentation Sampling,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"The exponentially large sample space of general binary probabilistic models renders intractable standard operations such as exact marginalization, inference, and normalization. Typically, researchers deal with these distributions via deterministic approximations, the class of belief propagation methods being a prominent example. Comparatively, Markov Chain Monte Carlo methods have been significantly less used in this domain. In this work, we introduce an auxiliary variable MCMC scheme that samples from an annular augmented space, translating to a great circle path around the hypercube of the binary sample space. This annular augmentation sampler explores the sample space more effectively than coordinate-wise samplers and has no tunable parameters, leading to substantial performance gains in estimating quantities of interest in large binary models. We extend the method to incorporate into the sampler any existing mean-field approximation (such as from belief propagation), leading to further performance improvements. Empirically, we consider a range of large Ising models and an application to risk factors for heart disease.",,,,,"Bhandari, Jalaj/AAP-7711-2021",,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,139,147,,,,,,,,,,,,,,,,WOS:000509368500016,0
C,"Leblond, R; Pedregosa, F; Lacoste-Julien, S",,"Singh, A; Zhu, J",,"Leblond, Remi; Pedregosa, Fabian; Lacoste-Julien, Simon",,,Asaga: Asynchronous Parallel Saga,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"We describe Asaga, an asynchronous parallel version of the incremental gradient algorithm Saga that enjoys fast linear convergence rates. Through a novel perspective, we revisit and clarify a subtle but important technical issue present in a large fraction of the recent convergence rate proofs for asynchronous parallel optimization algorithms, and propose a simplification of the recently introduced perturbed iterate framework that resolves it. We thereby prove that Asaga can obtain a theoretical linear speedup on multi-core systems even without sparsity assumptions. We present results of an implementation on a 40-core architecture illustrating the practical speedup as well as the hardware overhead.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,46,54,,,,,,,,,,,,,,,,WOS:000509368500006,0
C,"Xu, Z; Figueiredo, MAT; Goldstein, T",,"Singh, A; Zhu, J",,"Xu, Zheng; Figueiredo, Mario A. T.; Goldstein, Tom",,,Adaptive ADMM with Spectral Penalty Parameter Selection,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"The alternating direction method of multipliers (ADMM) is a versatile tool for solving a wide range of constrained optimization problems. However, its performance is highly sensitive to a penalty parameter, making ADMM often unreliable and hard to automate for a non-expert user. We tackle this weakness of ADMM by proposing a method that adaptively tunes the penalty parameter to achieve fast convergence. The resulting adaptive ADMM (AADMM) algorithm, inspired by the successful Barzilai-Borwein spectral method for gradient descent, yields fast convergence and relative insensitivity to the initial stepsize and problem scaling.",,,,,"Figueiredo, Mario/C-5428-2008","Figueiredo, Mario/0000-0002-0970-7745",,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,718,727,,,,,,,,,,,,,,,,WOS:000509368500077,0
C,"Qian, J; Root, J; Saligrama, V",,"Lebanon, G; Vishwanathan, SVN",,"Qian, Jing; Root, Jonathan; Saligrama, Venkatesh",,,Learning Efficient Anomaly Detectors from K-NN Graphs,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"We propose a non-parametric anomaly detection algorithm for high dimensional data. We score each datapoint by its average K-NN distance, and rank them accordingly. We then train limited complexity models to imitate these scores based on the max-margin learning-to-rank framework. A test-point is declared as an anomaly at alpha-false alarm level if the predicted score is in the alpha-percentile. The resulting anomaly detector is shown to be asymptotically optimal in that for any false alarm rate alpha, its decision region converges to the alpha-percentile minimum volume level set of the unknown underlying density. In addition, we test both the statistical performance and computational efficiency of our algorithm on a number of synthetic and real-data experiments. Our results demonstrate the superiority of our algorithm over existing K-NN based anomaly detection algorithms, with significant computational savings.",,,,,,"Saligrama, Venkatesh/0000-0002-0675-2268",,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,790,799,,,,,,,,,,,,,,,,WOS:000508399700087,0
C,"Shariff, R; Gyorgy, A; Szepesvari, C",,"Lebanon, G; Vishwanathan, SVN",,"Shariff, Roshan; Gyorgy, Andras; Szepesvari, Csaba",,,Exploiting Symmetries to Construct Efficient MCMC Algorithms With an Application to SLAM,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"The Metropolis-Hastings (MH) algorithm is a flexible method to generate samples from a target distribution, a key problem in probabilistic inference. In this paper we propose a variation of the MH algorithm based on group moves, where the next state is obtained by first choosing a random transformation of the state space and then applying this transformation to the current state. This adds much-needed flexibility to the textbook MH algorithm where all measures involved must be given in terms of densities with respect to a common reference measure. Under mild conditions, our main result extends the acceptance probability formula of the textbook algorithm to MH algorithms with group moves. We work out how the new algorithms can be used to exploit a problem's natural symmetries and apply the technique to the simultaneous localization and mapping (SLAM) problem, obtaining the first fully rigorous justification of a previous MCMC-based SLAM method. New experimental results comparing our method to existing state-of-the-art specialized methods on a standard range-only SLAM benchmark problem validate the strength of the approach.",,,,,,"Gyorgy, Andras/0000-0003-0586-4337",,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,866,874,,,,,,,,,,,,,,,,WOS:000508399700095,0
C,"Wang, F; Rudin, C",,"Lebanon, G; Vishwanathan, SVN",,"Wang, Fulton; Rudin, Cynthia",,,Falling Rule Lists,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"Falling rule lists are classification models consisting of an ordered list of if-then rules, where (i) the order of rules determines which example should be classified by each rule, and (ii) the estimated probability of success decreases monotonically down the list. These kinds of rule lists are inspired by healthcare applications where patients would be stratified into risk sets and the highest at-risk patients should be considered first. We provide a Bayesian framework for learning falling rule lists that does not rely on traditional greedy decision tree learning methods.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,1013,1022,,,,,,,,,,,,,,,,WOS:000508399700111,0
C,"Wang, X; Slavakis, K; Lerman, G",,"Lebanon, G; Vishwanathan, SVN",,"Wang, Xu; Slavakis, Konstantinos; Lerman, Gilad",,,Multi-Manifold Modeling in Non-Euclidean spaces,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"This paper advocates a novel framework for segmenting a dataset on a Riemannian manifold M into clusters lying around low-dimensional submanifolds of M. Important examples of M, for which the proposed algorithm is computationally efficient, include the sphere, the set of positive definite matrices, and the Grassmannian. The proposed algorithm constructs a data-affinity matrix by thoroughly exploiting the intrinsic geometry and then applies spectral clustering. Local geometry is encoded by sparse coding and directional information of local tangent spaces and geodesics, which is important in resolving intersecting clusters and establishing the theoretical guarantees for a simplified variant of the algorithm. To avoid complication, these guarantees assume that the underlying submanifolds are geodesic. Extensive validation on synthetic and real data demonstrates the resiliency of the proposed method against deviations from the theoretical (geodesic) model as well as its superior performance over state-of-the-art techniques.",,,,,,"Lerman, Gilad/0000-0003-4624-3115",,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,1023,1032,,,,,,,,,,,,,,,,WOS:000508399700112,0
C,"Chang, YL; Castaldi, PJ; Chen, JX; Silverman, EK; Cho, MH; Dy, JG",,"Singh, A; Zhu, J",,"Chang, Yale; Castaldi, Peter J.; Chen, Junxiang; Silverman, Edwin K.; Cho, Michael H.; Dy, Jennifer G.",,,Clustering from Multiple Uncertain Experts,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Utilizing expert input often improves clustering performance. However in a knowledge discovery problem, ground truth is unknown even to an expert. Thus, instead of one expert, we solicit the opinion from multiple experts. The key question motivating this work is: which experts should be assigned higher weights when there is disagreement on whether to put a pair of samples in the same group? To model the uncertainty in constraints from different experts, we build a probabilistic model for pairwise constraints through jointly modeling each expert's accuracy and the mapping from features to latent cluster assignments. After learning our probabilistic discriminative clustering model and accuracies of different experts, 1) samples that were not annotated by any expert can be clustered using the discriminative clustering model; and 2) experts with higher accuracies are automatically assigned higher weights in determining the latent cluster assignments. Experimental results on UCI benchmark datasets and a real-world disease subtyping dataset demonstrate that our proposed approach outperforms competing alternatives, including semi-crowdsourced clustering, semi-supervised clustering with constraints from majority voting, and consensus clustering.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,28,36,,,,,,,,,,,,,,,,WOS:000509368500004,0
C,"Park, M; Foulds, J; Chaudhuri, K; Welling, M",,"Singh, A; Zhu, J",,"Park, Mijung; Foulds, James; Chaudhuri, Kamalika; Welling, Max",,,DP-EM: Differentially Private Expectation Maximization,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"The iterative nature of the expectation maximization (EM) algorithm presents a challenge for privacy-preserving estimation, as each iteration increases the amount of noise needed. We propose a practical private EM algorithm that overcomes this challenge using two innovations: (1) a novel moment perturbation formulation for differentially private EM (DP-EM), and (2) the use of two recently developed composition methods to bound the privacy cost of multiple EM iterations: the moments accountant (MA) and zero-mean concentrated differential privacy (zCDP). Both MA and zCDP bound the moment generating function of the privacy loss random variable and achieve a refined tail bound, which effectively decrease the amount of additive noise. We present empirical results showing the benefits of our approach, as well as similar performance between these two composition methods in the DP-EM setting for Gaussian mixture models. Our approach can be readily extended to many iterative learning algorithms, opening up various exciting future directions.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,896,904,,,,,,,,,,,,,,,,WOS:000509368500096,0
C,"Sun, SY; Chen, CY; Carin, L",,"Singh, A; Zhu, J",,"Sun, Shengyang; Chen, Changyou; Carin, Lawrence",,,Learning Structured Weight Uncertainty in Bayesian Neural Networks,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Deep neural networks (DNNs) are increasingly popular in modern machine learning. Bayesian learning affords the opportunity to quantify posterior uncertainty on DNN model parameters. Most existing work adopts independent Gaussian priors on the model weights, ignoring possible structural information. In this paper, we consider the matrix variate Gaussian (MVG) distribution to model structured correlations within the weights of a DNN. To make posterior inference feasible, a reparametrization is proposed for the MVG prior, simplifying the complex MVG-based model to an equivalent yet simpler model with independent Gaussian priors on the transformed weights. Consequently, we develop a scalable Bayesian online inference algorithm by adopting the recently proposed probabilistic backpropagation framework. Experiments on several synthetic and real datasets indicate the superiority of our model, achieving competitive performance in terms of model likelihood and predictive root mean square error. Importantly, it also yields faster convergence speed compared to related Bayesian DNN models.",,,,,,"Carin, Lawrence/0000-0001-6277-7948",,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,1283,1292,,,,,,,,,,,,,,,,WOS:000509368500137,0
C,"Xie, B; Liang, YY; Song, L",,"Singh, A; Zhu, J",,"Xie, Bo; Liang, Yingyu; Song, Le",,,Diverse Neural Network Learns True Target Functions,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Neural networks are a powerful class of functions that can be trained with simple gradient descent to achieve state-of-the-art performance on a variety of applications. Despite their practical success, there is a paucity of results that provide theoretical guarantees on why they are so effective. Lying in the center of the problem is the difficulty of analyzing the non-convex loss function with potentially numerous local minima and saddle points. Can neural networks corresponding to the stationary points of the loss function learn the true target function? If yes, what are the key factors contributing to such nice optimization properties? In this paper, we answer these questions by analyzing one-hidden-layer neural networks with ReLU activation, and show that despite the non-convexity, neural networks with diverse units have no spurious local minima. We bypass the non-convexity issue by directly analyzing the first order optimality condition, and show that the loss can be made arbitrarily small if the minimum singular value of the extended feature matrix is large enough. We make novel use of techniques from kernel methods and geometric discrepancy, and identify a new relation linking the smallest singular value to the spectrum of a kernel function associated with the activation function and to the diversity of the units. Our results also suggest a novel regularization function to promote unit diversity for potentially better generalization.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,1216,1224,,,,,,,,,,,,,,,,WOS:000509368500130,0
C,"Zhu, YC; Liu, Z; Sun, SQ",,"Singh, A; Zhu, J",,"Zhu, Yuancheng; Liu, Zhe; Sun, Siqi",,,Learning Nonparametric Forest Graphical Models with Prior Information,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"We present a framework for incorporating prior information into nonparametric estimation of graphical models. To avoid distributional assumptions, we restrict the graph to be a forest and build on the work of forest density estimation (FDE). We reformulate the FDE approach from a Bayesian perspective, and introduce prior distributions on the graphs. As two concrete examples, we apply this framework to estimating scale-free graphs and learning multiple graphs with similar structures. The resulting algorithms are equivalent to finding a maximum spanning tree of a weighted graph with a penalty term on the connectivity pattern of the graph. We solve the optimization problem via a minorize-maximization procedure with Kruskal's algorithm. Simulations show that the proposed methods outperform competing parametric methods, and are robust to the true data distribution. They also lead to improvement in predictive power and interpretability in two real data sets.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,672,680,,,,,,,,,,,,,,,,WOS:000509368500072,0
C,"Goix, N; Sabourin, A; Clemencon, S",,"Lebanon, G; Vishwanathan, SVN",,"Goix, Nicolas; Sabourin, Anne; Clemencon, Stephan",,,On Anomaly Ranking and Excess-Mass Curves,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"Learning how to rank multivariate unlabeled observations depending on their degree of abnormality/novelty is a crucial problem in a wide range of applications. In practice, it generally consists in building a real valued scoring function on the feature space so as to quantify to which extent observations should be considered as abnormal. In the 1d situation, measurements are generally considered as abnormal when they are remote from central measures such as the mean or the median. Anomaly detection then relies on tail analysis of the variable of interest. Extensions to the multivariate setting are far from straightforward and it is precisely the main purpose of this paper to introduce a novel and convenient (functional) criterion for measuring the performance of a scoring function regarding the anomaly ranking task, referred to as the Excess-Mass curve (EM curve). In addition, an adaptive algorithm for building a scoring function based on unlabeled data X-1, ... , X-n, with a nearly optimal EM is proposed and is analyzed from a statistical perspective.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,287,295,,,,,,,,,,,,,,,,WOS:000508399700032,0
C,"Iwata, T; Takeuchi, K",,"Lebanon, G; Vishwanathan, SVN",,"Iwata, Tomoharu; Takeuchi, Koh",,,Cross-domain recommendation without shared users or items by sharing latent vector distributions,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"We propose a cross-domain recommendation method for predicting the ratings of items in different domains, where neither users nor items are shared across domains. The proposed method is based on matrix factorization, which learns a latent vector for each user and each item. Matrix factorization techniques for a single-domain fail in the cross-domain recommendation task because the learned latent vectors are not aligned over different domains. The proposed method assumes that latent vectors in different domains are generated from a common Gaussian distribution with a full covariance matrix. By inferring the mean and covariance of the common Gaussian from given cross-domain rating matrices, the latent factors are aligned, which enables us to predict ratings in different domains. Experiments conducted on rating datasets from a wide variety of domains, e.g., movie, books and electronics, demonstrate that the proposed method achieves higher performance for predicting cross-domain ratings than existing methods.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,379,387,,,,,,,,,,,,,,,,WOS:000508399700042,0
C,"Ashizawa, M; Sasaki, H; Sakai, T; Sugiyama, M",,"Singh, A; Zhu, J",,"Ashizawa, Mina; Sasaki, Hiroaki; Sakai, Tomoya; Sugiyama, Masashi",,,Least-Squares Log-Density Gradient Clustering for Riemannian Manifolds,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Mean shift is a mode-seeking clustering algorithm that has been successfully used in a wide range of applications such as image segmentation and object tracking. To further improve the clustering performance, mean shift has been extended to various directions, including generalization to handle data on Riemannian manifolds and extension to directly estimating the log-density gradient without density estimation. In this paper, we combine these ideas and propose a novel mode-seeking algorithm for Riemannian manifolds with direct log-density gradient estimation. Although the idea of combining the two extensions is rather straightforward, directly estimating the log-density gradient on Riemannian manifolds is mathematically challenging. We will provide a mathematically sound algorithm and demonstrate its usefulness through experiments.",,,,,"Sugiyama, Masashi/AEO-1176-2022; Sasaki, Hiroaki/GXG-5024-2022","Sugiyama, Masashi/0000-0001-6658-6743; ",,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,537,546,,,,,,,,,,,,,,,,WOS:000509368500058,0
C,"Bewsher, JD; Tosi, A; Osborne, MA; Roberts, SJ",,"Singh, A; Zhu, J",,"Bewsher, Justin D.; Tosi, Alessandra; Osborne, Michael A.; Roberts, Stephen J.",,,Distribution of Gaussian Process Arc Lengths,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"We present the first treatment of the arc length of the Gaussian Process (GP) with more than a single output dimension. GPs are commonly used for tasks such as trajectory modelling, where path length is a crucial quantity of interest. Previously, only paths in one dimension have been considered, with no theoretical consideration of higher dimensional problems. We fill the gap in the existing literature by deriving the moments of the arc length for a stationary GP with multiple output dimensions. A new method is used to derive the mean of a one-dimensional GP over a finite interval, by considering the distribution of the arc length integrand. This technique is used to derive an approximate distribution over the arc length of a vector valued GP in R-n by moment matching the distribution. Numerical simulations confirm our theoretical derivations.",,,,,,"Osborne, Michael/0000-0003-1959-012X",,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,1412,1420,,,,,,,,,,,,,,,,WOS:000509368500151,0
C,"Gaillard, P; Wintenberger, O",,"Singh, A; Zhu, J",,"Gaillard, Pierre; Wintenberger, Olivier",,,Sparse Accelerated Exponential Weights,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"We consider the stochastic optimization problem where a convex function is minimized observing recursively the gradients. We introduce SAEW, a new procedure that accelerates exponential weights procedures with the slow rate 1/root T to procedures achieving the fast rate 1/T. Under the strong convexity of the risk, we achieve the optimal rate of convergence for approximating sparse parameters in R-d. The acceleration is achieved by using successive averaging steps in an online fashion. The procedure also produces sparse estimators thanks to additional hard threshold steps.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,75,82,,,,,,,,,,,,,,,,WOS:000509368500009,0
C,"Galliani, P; Dezfouli, A; Bonilla, EV; Quadrianto, N",,"Singh, A; Zhu, J",,"Galliani, Pietro; Dezfouli, Amir; Bonilla, Edwin V.; Quadrianto, Novi",,,Gray-box inference for structured Gaussian process models,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"We develop an automated variational inference method for Bayesian structured prediction problems with Gaussian process (gp) priors and linear-chain likelihoods. Our approach does not need to know the details of the structured likelihood model and can scale up to a large number of observations. Furthermore, we show that the required expected likelihood term and its gradients in the variational objective (ELBO) can be estimated efficiently by using expectations over very low-dimensional Gaussian distributions. Optimization of the ELBO is fully parallelizable over sequences and amenable to stochastic optimization, which we use along with control variate techniques to make our framework useful in practice. Results on a set of natural language processing tasks show that our method can be as good as (and sometimes better than, in particular with respect to expected log-likelihood) hard-coded approaches including SVM-struct and CRFS, and overcomes the scalability limitations of previous inference algorithms based on sampling. Overall, this is a fundamental step to developing automated inference methods for Bayesian structured prediction.",,,,,,"Quadrianto, Novi/0000-0001-8819-306X",,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,353,361,,,,,,,,,,,,,,,,WOS:000509368500039,0
C,"Jun, KS; Orabona, F; Wright, S; Willett, R",,"Singh, A; Zhu, J",,"Jun, Kwang-Sung; Orabona, Francesco; Wright, Stephen; Willett, Rebecca",,,Improved Strongly Adaptive Online Learning using Coin Betting,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"This paper describes a new parameter-free online learning algorithm for changing environments. In comparing against algorithms with the same time complexity as ours, we obtain a strongly adaptive regret bound that is a factor of at least root log(T) better, where T is the time horizon. Empirical results show that our algorithm outperforms state-of-the-art methods in learning with expert advice and metric learning scenarios.",,,,,,"Jun, Kwang-Sung/0000-0001-5483-3161; orabona, francesco/0000-0001-8523-6845",,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,943,951,,,,,,,,,,,,,,,,WOS:000509368500101,0
C,"Li, TY; Yi, XY; Caramanis, C; Ravikumar, P",,"Singh, A; Zhu, J",,"Li, Tianyang; Yi, Xinyang; Caramanis, Constantine; Ravikumar, Pradeep",,,Minimax Gaussian Classification & Clustering,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"We present minimax bounds for classification and clustering error in the setting where covariates are drawn from a mixture of two isotropic Gaussian distributions. Here, we define clustering error in a discriminative fashion, demonstrating fundamental connections between classification (supervised) and clustering (unsupervised). For both classification and clustering, our lower bounds show that without enough samples, the best any classifier or clustering rule can do is close to random guessing. For classification, as part of our upper bound analysis, we show that Fisher's linear discriminant achieves a fast minimax rate Theta(1/n) with enough samples n. For clustering, as part of our upper bound analysis, we show that a clustering rule constructed using principal component analysis achieves the minimax rate with enough samples. We also provide lower and upper bounds for the high-dimensional sparse setting where the dimensionality of the covariates p is potentially larger than the number of samples n, but where the difference between the Gaussian means is sparse.",,,,,,"Caramanis, Constantine/0000-0001-9939-8378",,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,1,9,,,,,,,,,,,,,,,,WOS:000509368500001,0
C,"Liu, Q; Lee, JD",,"Singh, A; Zhu, J",,"Liu, Qiang; Lee, Jason D.",,,Black-Box Importance Sampling,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Importance sampling is widely used in machine learning and statistics, but its power is limited by the restriction of using simple proposals for which the importance weights can be tractably calculated. We address this problem by studying black-box importance sampling methods that calculate importance weights for samples generated from any unknown proposal or black-box mechanism. Our method allows us to use better and richer proposals to solve difficult problems, and (somewhat counter-intuitively) also has the additional benefit of improving the estimation accuracy beyond typical importance sampling. Both theoretical and empirical analyses are provided.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,952,961,,,,,,,,,,,,,,,,WOS:000509368500102,0
C,"Newling, J; Fleuret, F",,"Singh, A; Zhu, J",,"Newling, James; Fleuret, Francois",,,A Sub-Quadratic Exact Medoid Algorithm,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"We present a new algorithm trimed for obtaining the medoid of a set, that is the element of the set which minimises the mean distance to all other elements. The algorithm is shown to have, under certain assumptions, expected run time O(N-3/2) in R-d where N is the set size, making it the first sub-quadratic exact medoid algorithm for d > 1. Experiments show that it performs very well on spatial network data, frequently requiring two orders of magnitude fewer distance calculations than state-of-the-art approximate algorithms. As an application, we show how trimed can be used as a component in an accelerated K-medoids algorithm, and then how it can be relaxed to obtain further computational gains with only a minor loss in cluster quality.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,185,193,,,,,,,,,,,,,,,,WOS:000509368500021,0
C,"Sasaki, H; Kanamori, T; Sugiyama, M",,"Singh, A; Zhu, J",,"Sasaki, Hiroaki; Kanamori, Takafumi; Sugiyama, Masashi",,,Estimating Density Ridges by Direct Estimation of Density-Derivative-Ratios,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Estimation of density ridges has been gathering a great deal of attention since it enables us to reveal lower-dimensional structures hidden in data. Recently, subspace constrained mean shift (SCMS) was proposed as a practical algorithm for density ridge estimation. A key technical ingredient in SCMS is to accurately estimate the ratios of the density derivatives to the density. SCMS takes a three-step approach for this purpose - first estimating the data density, then computing its derivatives, and finally taking their ratios. However, this three-step approach can be unreliable because a good density estimator does not necessarily mean a good density derivative estimator and division by an estimated density could significantly magnify the estimation error. To overcome these problems, we propose a novel method that directly estimates the ratios without going through density estimation and division. Our proposed estimator has an analytic-form solution and it can be computed efficiently. We further establish a non-parametric convergence bound for the proposed ratio estimator. Finally, based on this direct ratio estimator, we develop a practical algorithm for density ridge estimation and experimentally demonstrate its usefulness on a variety of datasets.",,,,,"Sasaki, Hiroaki/GXG-5024-2022; Sugiyama, Masashi/AEO-1176-2022","Sugiyama, Masashi/0000-0001-6658-6743",,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,204,212,,,,,,,,,,,,,,,,WOS:000509368500023,0
C,"Wang, LX; Zhang, X; Gu, QQ",,"Singh, A; Zhu, J",,"Wang, Lingxiao; Zhang, Xiao; Gu, Quanquan",,,A Unified Computational and Statistical Framework for Nonconvex Low-Rank Matrix Estimation,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"We propose a unified framework for estimating low-rank matrices through nonconvex optimization based on gradient descent algorithm. Our framework is quite general and can be applied to both noisy and noiseless observations. In the general case with noisy observations, we show that our algorithm is guaranteed to linearly converge to the unknown low-rank matrix up to a minimax optimal statistical error, provided an appropriate initial estimator. While in the generic noiseless setting, our algorithm converges to the unknown low-rank matrix at a linear rate and enables exact recovery with optimal sample complexity. In addition, we develop a new initialization algorithm to provide the desired initial estimator, which outperforms existing initialization algorithms for nonconvex low-rank matrix estimation. We illustrate the superiority of our framework through three examples: matrix regression, matrix completion, and one-bit matrix completion. We also corroborate our theory through extensive experiments on synthetic data.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,981,990,,,,,,,,,,,,,,,,WOS:000509368500105,0
C,"Bhowmik, A; Ghosh, J; Koyejo, O",,"Lebanon, G; Vishwanathan, SVN",,"Bhowmik, Avradeep; Ghosh, Joydeep; Koyejo, Oluwasanmi",,,Generalized Linear Models for Aggregated Data,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"Databases in domains such as healthcare are routinely released to the public in aggregated form. Unfortunately, naive modeling with aggregated data may significantly diminish the accuracy of inferences at the individual level. This paper addresses the scenario where features are provided at the individual level, but the target variables are only available as histogram aggregates or order statistics. We consider a limiting case of generalized linear modeling when the target variables are only known up to permutation, and explore how this relates to permutation testing; a standard technique for assessing statistical dependency. Based on this relationship, we propose a simple algorithm to estimate the model parameters and individual level inferences via alternating imputation and standard generalized linear model fitting. Our results suggest the effectiveness of the proposed approach when, in the original data, permutation testing accurately ascertains the veracity of the linear relationship. The framework is extended to general histogram data with larger bins - with order statistics such as the median as a limiting case. Our experimental results on simulated data and aggregated healthcare data suggest a diminishing returns property with respect to the granularity of the histogram - when a linear relationship holds in the original data, the targets can be predicted accurately given relatively coarse histograms.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,93,101,,,,,,,,,,,,,,,,WOS:000508399700011,0
C,"Oliva, JB; Neiswanger, W; Poczos, B; Xing, E; Trac, H; Ho, S; Schneider, J",,"Lebanon, G; Vishwanathan, SVN",,"Oliva, Junier B.; Neiswanger, Willie; Poczos, Barnabas; Xing, Eric; Trac, Hy; Ho, Shirley; Schneider, Jeff",,,Fast Function to Function Regression,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"We analyze the problem of regression when both input covariates and output responses are functions from a nonparametric function class. Function to function regression (FFR) covers a large range of interesting applications including timeseries prediction problems, and also more general tasks like studying a mapping between two separate types of distributions. However, previous nonparametric estimators for FFR type problems scale badly computationally with the number of input/output pairs in a data-set. Given the complexity of a mapping between general functions it may be necessary to consider large datasets in order to achieve a low estimation risk. To address this issue, we develop a novel scalable nonparametric estimator, the Triple-Basis Estimator (3BE), which is capable of operating over data-sets with many instances. To the best of our knowledge, the 3BE is the first nonparametric FFR estimator that can scale to massive data-sets. We analyze the 3BE's risk and derive an upper-bound rate. Furthermore, we show an improvement of several orders of magnitude in terms of prediction speed and a reduction in error over previous estimators in various real-world data-sets.",,,,,"Trac, Hy/N-8838-2014","Trac, Hy/0000-0001-6778-3861; Oliva, Junier/0000-0002-2601-5652",,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,717,725,,,,,,,,,,,,,,,,WOS:000508399700079,0
C,"Yang, ZC; Smola, AJ; Song, L; Wilson, AG",,"Lebanon, G; Vishwanathan, SVN",,"Yang, Zichao; Smola, Alexander J.; Song, Le; Wilson, Andrew Gordon",,,A la Carte - Learning Fast Kernels,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"Kernel methods have great promise for learning rich statistical representations of large modern datasets. However, compared to neural networks, kernel methods have been perceived as lacking in scalability and flexibility. We introduce a family of fast, flexible, lightly parametrized and general purpose kernel learning methods, derived from Fast-food basis function expansions. We provide mechanisms to learn the properties of groups of spectral frequencies in these expansions, which require only O(m log d) time and O(m) memory, for m basis functions and d input dimensions. We show that the proposed methods can learn a wide class of kernels, outperforming the alternatives in accuracy, speed, and memory consumption.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,1098,1106,,,,,,,,,,,,,,,,WOS:000508399700120,0
C,"Bahmani, S; Romberg, J",,"Singh, A; Zhu, J",,"Bahmani, Sohail; Romberg, Justin",,,Phase Retrieval Meets Statistical Learning Theory: A Flexible Convex Relaxation,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"We propose a flexible convex relaxation for the phase retrieval problem that operates in the natural domain of the signal. Therefore, we avoid the prohibitive computational cost associated with lifting and semidefinite programming (SDP) in methods such as PhaseLift and compete with recently developed non-convex techniques for phase retrieval. We relax the quadratic equations for phaseless measurements to inequality constraints each of which representing a symmetric slab. Through a simple convex program, our proposed estimator finds an extreme point of the intersection of these slabs that is best aligned with a given anchor vector. We characterize geometric conditions that certify success of the proposed estimator. Furthermore, using classic results in statistical learning theory, we show that for random measurements the geometric certificates hold with high probability at an optimal sample complexity. Phase transition of our estimator is evaluated through simulations. Our numerical experiments also suggest that the proposed method can solve phase retrieval problems with coded diffraction measurements as well.",,,,,,"Bahmani, Sohail/0000-0002-8316-8313",,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,252,260,,,,,,,,,,,,,,,,WOS:000509368500028,0
C,"Chen, YX; Hassani, SH; Krause, A",,"Singh, A; Zhu, J",,"Chen, Yuxin; Hassani, S. Hamed; Krause, Andreas",,,Near-optimal Bayesian Active Learning with Correlated and Noisy Tests,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"We consider the Bayesian active learning and experimental design problem, where the goal is to learn the value of some unknown target variable through a sequence of informative, noisy tests. In contrast to prior work, we focus on the challenging, yet practically relevant setting where test outcomes can be conditionally dependent given the hidden target variable. Under such assumptions, common heuristics, such as greedily performing tests that maximize the reduction in uncertainty of the target, often perform poorly. We propose ECED, a novel, efficient active learning algorithm, and prove strong theoretical guarantees that hold with correlated, noisy tests. Rather than directly optimizing the prediction error, at each step, ECED picks the test that maximizes the gain in a surrogate objective, which takes into account the dependencies between tests. Our analysis relies on an information-theoretic auxiliary function to track the progress of ECED, and utilizes adaptive submodularity to attain the approximation bound. We demonstrate strong empirical performance of ECED on two problem instances, including a Bayesian experimental design task intended to distinguish among economic theories of how people make risky decisions, and an active preference learning task via pairwise comparisons.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,223,231,,,,,,,,,,,,,,,,WOS:000509368500025,0
C,"Ghoshal, A; Honorio, J",,"Singh, A; Zhu, J",,"Ghoshal, Asish; Honorio, Jean",,,Learning Graphical Games from Behavioral Data: Sufficient and Necessary Conditions,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"In this paper we obtain sufficient and necessary conditions on the number of samples required for exact recovery of the pure-strategy Nash equilibria (PSNE) set of a graphical game from noisy observations of joint actions. We consider sparse linear influence games - a parametric class of graphical games with linear payoffs, and represented by directed graphs of n nodes (players) and in-degree of at most k. We show that one can efficiently recover the PSNE set of a linear influence game with Omega (k(2) log n) samples, under very general observation models. On the other hand, we show that.(k log n) samples are necessary for any procedure to recover the PSNE set from observations of joint actions.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,1532,1540,,,,,,,,,,,,,,,,WOS:000509368500164,0
C,"Khan, ME; Lin, W",,"Singh, A; Zhu, J",,"Khan, Mohammad Emtiyaz; Lin, Wu",,,Conjugate-Computation Variational Inference : Converting Variational Inference in Non-Conjugate Models to Inferences in Conjugate Models,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Variational inference is computationally challenging in models that contain both conjugate and non-conjugate terms. Methods specifically designed for conjugate models, even though computationally efficient, find it difficult to deal with non-conjugate terms. On the other hand, stochastic-gradient methods can handle the non-conjugate terms but they usually ignore the conjugate structure of the model which might result in slow convergence. In this paper, we propose a new algorithm called Conjugate-computation Variational Inference (CVI) which brings the best of the two worlds together - it uses conjugate computations for the conjugate terms and employs stochastic gradients for the rest. We derive this algorithm by using a stochastic mirror-descent method in the mean-parameter space, and then expressing each gradient step as a variational inference in a conjugate model. We demonstrate our algorithm's applicability to a large class of models and establish its convergence. Our experimental results show that our method converges much faster than the methods that ignore the conjugate structure of the model.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,878,887,,,,,,,,,,,,,,,,WOS:000509368500094,0
C,"Thomas, A; Clemencon, S; Gramfort, A; Sabourin, A",,"Singh, A; Zhu, J",,"Thomas, Albert; Clemencon, Stephan; Gramfort, Alexandre; Sabourin, Anne",,,Anomaly Detection in Extreme Regions via Empirical MV-sets on the Sphere,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Extreme regions in the feature space are of particular concern for anomaly detection: anomalies are likely to be located in the tails, whereas data scarcity in such regions makes it difficult to distinguish between large normal instances and anomalies. This paper presents an unsupervised algorithm for anomaly detection in extreme regions. We propose a Minimum Volume set (MV-set) approach relying on multivariate extreme value theory. This framework includes a canonical pre-processing step, which addresses the issue of output sensitivity to standardization choices. The resulting data representation on the sphere highlights the dependence structure of the extremal observations. Anomaly detection is then cast as a MV-set estimation problem on the sphere, where volume is measured by the spherical measure and mass refers to the angular measure. An anomaly then corresponds to an unusual observation given that one of its variables is large. A preliminary rate bound analysis is carried out for the learning method we introduce and its computational advantages are discussed and illustrated by numerical experiments.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,1011,1019,,,,,,,,,,,,,,,,WOS:000509368500108,0
C,"Carlson, D; Cevher, V; Carin, L",,"Lebanon, G; Vishwanathan, SVN",,"Carlson, David; Cevher, Volkan; Carin, Lawrence",,,Stochastic Spectral Descent for Restricted Boltzmann Machines,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"Restricted Boltzmann Machines (RBMs) are widely used as building blocks for deep learning models. Learning typically proceeds by using stochastic gradient descent, and the gradients are estimated with sampling methods. However, the gradient estimation is a computational bottleneck, so better use of the gradients will speed up the descent algorithm. To this end, we first derive upper bounds on the RBM cost function, then show that descent methods can have natural advantages by operating in the l(infinity) and Shatten-infinity norm. We introduce a new method called Stochastic Spectral Descent that updates parameters in the normed space. Empirical results show dramatic improvements over stochastic gradient descent, and have only have a fractional increase on the per-iteration cost.",,,,,,"Carin, Lawrence/0000-0001-6277-7948; Carlson, David/0000-0003-1005-6385",,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,111,119,,,,,,,,,,,,,,,,WOS:000508399700013,0
C,"Hashimoto, TB; Sun, Y; Jaakkola, TS",,"Lebanon, G; Vishwanathan, SVN",,"Hashimoto, Tatsunori B.; Sun, Yi; Jaakkola, Tommi S.",,,Metric recovery from directed unweighted graphs,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"We analyze directed, unweighted graphs obtained from x(i) is an element of R-d by connecting vertex i to j iff vertical bar x(i), - x(j) vertical bar < epsilon(x(i)). Examples of such graphs include k-nearest neighbor graphs, where epsilon(x(i)) varies from point to point, and, arguably, many real-world graphs such as copurchasing graphs. We ask whether we can recover the underlying Euclidean metric epsilon(x(i)) and the associated density p(x(i)) given only the directed graph and d. We show that consistent recovery is possible up to isometric scaling when the vertex degree is at least omega(n(2/(2+d)) log(n)(d/(d+2))). Our estimator is based on a careful characterization of a random walk over the directed graph and the associated continuum limit. As an algorithm, it resembles the PageRank centrality metric. We demonstrate empirically that the estimator performs well on simulated examples as well as on real-world co-purchasing graphs even with a small number of points and degree scaling as low as log(n).",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,342,350,,,,,,,,,,,,,,,,WOS:000508399700038,0
C,"Kawahara, Y; Iyer, R; Bilmes, JA",,"Lebanon, G; Vishwanathan, SVN",,"Kawahara, Yoshinobu; Iyer, Rishabh; Bilmes, Jeffery A.",,,On Approximate Non-submodular Minimization via Tree-Structured Supermodularity,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"We address the problem of minimizing non-submodular functions where the supermodularity is restricted to tree-structured pairwise terms. We are motivated by several real world applications, which require submodularity along with structured supermodularity, and this forms a rich class of expressive models, where the non-submodularity is restricted to a tree. While this problem is NP hard (as we show), we develop several practical algorithms to find approximate and near-optimal solutions for this problem, some of which provide lower and others of which provide upper bounds thereby allowing us to compute a tightness gap. We also show that some of our algorithms can be extended to handle more general forms of supermodularity restricted to arbitrary pairwise terms. We compare our algorithms on synthetic data, and also demonstrate the advantage of the formulation on the real world application of image segmentation, where we incorporate structured supermodularity into higher-order submodular energy minimization.",,,,,"KAWAHARA, Yoshinobu/AAM-7540-2020; Kawahara, Yoshinobu/J-2462-2014","KAWAHARA, Yoshinobu/0000-0001-7789-4709; Kawahara, Yoshinobu/0000-0001-7789-4709",,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,444,452,,,,,,,,,,,,,,,,WOS:000508399700049,0
C,"Maibing, SF; Igel, C",,"Lebanon, G; Vishwanathan, SVN",,"Maibing, Soren Frejstrup; Igel, Christian",,,Computational Complexity of Linear Large Margin Classification With Ramp Loss,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"Minimizing the binary classification error with a linear model leads to an NP-hard problem. In practice, surrogate loss functions are used, in particular loss functions leading to large margin classification such as the hinge loss and the ramp loss. The intuitive large margin concept is theoretically supported by generalization bounds linking the expected classification error to the empirical margin error and the complexity of the considered hypotheses class. This article addresses the fundamental question about the computational complexity of determining whether there is a hypotheses class with a hypothesis such that the upper bound on the generalization error is below a certain value. Results of this type are important for model comparison and selection. This paper takes a first step and proves that minimizing a basic margin-bound is NP-hard when considering linear hypotheses and the rho-margin loss function, which generalizes the ramp loss. This result directly implies the hardness of ramp loss minimization.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,259,267,,,,,,,,,,,,,,,,WOS:000508399700029,0
C,"Rashmi, KV; Gilad-Bachrach, R",,"Lebanon, G; Vishwanathan, SVN",,"Rashmi, K., V; Gilad-Bachrach, Ran",,,DART: Dropouts meet Multiple Additive Regression Trees,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"MART (Friedman, 2001, 2002), an ensemble model of boosted regression trees, is known to deliver high prediction accuracy for diverse tasks, and it is widely used in practice. However, it suffers an issue which we call over-specialization, wherein trees added at later iterations tend to impact the prediction of only a few instances, and make negligible contribution towards the remaining instances. This negatively affects the performance of the model on unseen data, and also makes the model over-sensitive to the contributions of the few, initially added tress. We show that the commonly used tool to address this issue, that of shrinkage, alleviates the problem only to a certain extent and the fundamental issue of over-specialization still remains. In this work, we explore a different approach to address the problem that of employing dropouts, a tool that has been recently proposed in the context of learning deep neural networks (Hinton et al., 2012). We propose a novel way of employing dropouts in MART, resulting in the DART algorithm. We evaluate DART on ranking, regression and classification tasks, using large scale, publicly available datasets, and show that DART outperforms MART in each of the tasks, with a significant margin. We also show that DART overcomes the issue of over-specialization to a considerable extent.",,,,,"Gilad-Bachrach, Ran/ABG-6990-2020","Gilad-Bachrach, Ran/0000-0002-4001-8307",,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,489,497,,,,,,,,,,,,,,,,WOS:000508399700054,0
C,"Sgouritsa, E; Janzing, D; Hennig, P; Scholkopf, B",,"Lebanon, G; Vishwanathan, SVN",,"Sgouritsa, Eleni; Janzing, Dominik; Hennig, Philipp; Schoelkopf, Bernhard",,,Inference of Cause and Effect with Unsupervised Inverse Regression,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"We address the problem of causal discovery in the two-variable case given a sample from their joint distribution. The proposed method is based on a known assumption that, if X -> Y (X causes Y), the marginal distribution of the cause, P (X), contains no information about the conditional distribution P (Y vertical bar X). Consequently, estimating P (Y vertical bar X) from P (X) should not be possible. However, estimating P (X vertical bar Y) based on P (Y) may be possible. This paper employs this asymmetry to propose CURE, a causal discovery method which decides upon the causal direction by comparing the accuracy of the estimations of P (Y vertical bar X) and P (X vertical bar Y). To this end, we propose a method for estimating a conditional from samples of the corresponding marginal, which we call unsupervised inverse GP regression. We evaluate CURE on synthetic and real data. On the latter, our method outperforms existing causal inference methods.",,,,,"Scholkopf, Bernhard/A-7570-2013","Scholkopf, Bernhard/0000-0002-8177-0925",,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,847,855,,,,,,,,,,,,,,,,WOS:000508399700093,0
C,"Shah, NB; Balakrishnan, S; Bradley, J; Parekh, A; Ramchandran, K; Wainwright, MJ",,"Lebanon, G; Vishwanathan, SVN",,"Shah, Nihar B.; Balakrishnan, Sivaraman; Bradley, Joseph; Parekh, Abhay; Ramchandran, Kannan; Wainwright, Martin J.",,,Estimation from Pairwise Comparisons: Sharp Minimax Bounds with Topology Dependence,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"Consider the problem of identifying the underlying qualities of a set of items based on measuring noisy comparisons between pairs of items. The Bradley-Terry-Luce (BTL) and Thurstone models are the most widely used parametric models for such pairwise comparison data. Working within a standard minimax framework, this paper provides sharp upper and lower bounds on the optimal error in estimating the underlying qualities under the BTL and the Thurstone models. These bounds are are topology-aware, meaning that they change qualitatively depending on the comparison graph induced by the subset of pairs being compared. Thus, in settings where the subset of pairs may be chosen, our results provide some principled guidelines for making this choice. Finally, we compare these error rates to those under cardinal measurement models and show that the error rates in the ordinal and cardinal settings have identical scalings apart from constant pre-factors. We use this result to investigate the relative merits of cardinal and ordinal measurement schemes.",,,,,,"Wainwright, Martin J./0000-0002-8760-2236",,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,856,865,,,,,,,,,,,,,,,,WOS:000508399700094,0
C,"van de Meent, JW; Yang, H; Mansinghka, V; Wood, F",,"Lebanon, G; Vishwanathan, SVN",,"van de Meent, Jan-Willem; Yang, Hongseok; Mansinghka, Vikash; Wood, Frank",,,Particle Gibbs with Ancestor Sampling for Probabilistic Programs,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"Particle Markov chain Monte Carlo techniques rank among current state-of-the-art methods for probabilistic program inference. A drawback of these techniques is that they rely on importance resampling, which results in degenerate particle trajectories and a low effective sample size for variables sampled early in a program. We here develop a formalism to adapt ancestor resampling, a technique that mitigates particle degeneracy, to the probabilistic programming setting. We present empirical results that demonstrate nontrivial performance gains.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,986,994,,,,,,,,,,,,,,,,WOS:000508399700108,0
C,"Wu, Y; Wipf, D; Yun, JM",,"Lebanon, G; Vishwanathan, SVN",,"Wu, Yi; Wipf, David; Yun, Jeong-Min",,,Understanding and Evaluating Sparse Linear Discriminant Analysis,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"Linear discriminant analysis (LDA) represents a simple yet powerful technique for partitioning a p-dimensional feature vector into one of K classes based on a linear projection learned from N labeled observations. However, it is well-established that in the high-dimensional setting (p > N) the underlying projection estimator degenerates. Moreover, any linear discriminate function involving a large number of features may be difficult to interpret. To ameliorate these issues, two general categories of sparse LDA modifications have been proposed, both to reduce the number of active features and to stabilize the resulting projections. The first, based on optimal scoring, is more straightforward to implement and analyze but has been heavily criticized for its ambiguous connection with the original LDA formulation. In contrast, a second strategy applies sparse penalty functions directly to the original LDA objective but requires additional heuristic trade-off parameters, has unknown global and local minima properties, and requires a greedy sequential optimization procedure. In all cases the choice of sparse regularizer can be important, but no rigorous guidelines have been provided regarding which penalty might be preferable. Against this backdrop, we winnow down the broad space of candidate sparse LDA algorithms and promote a specific selection based on optimal scoring coupled with a particular, complementary sparse regularizer. This overall process ultimately progresses our understanding of sparse LDA in general, while leading to targeted modifications of existing algorithms that produce superior results in practice on three high-dimensional gene data sets.",,,,,,"Wipf, David/0000-0002-2768-4540",,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,1070,1078,,,,,,,,,,,,,,,,WOS:000508399700117,0
C,"Li, P",,"Singh, A; Zhu, J",,"Li, Ping",,,Binary and Multi-Bit Coding for Stable Random Projections,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"The recent work [17] developed a 1-bit compressed sensing (CS) algorithm based on alpha-stable random projections. Although it was shown in [17] that the method is a strong competitor to other existing 1-bit CS algorithms, the procedure requires knowing K, the sparsity, which is the l(0) norm of the signal. Other existing 1-bit CS algorithms require the l(2) norm of the signal. In this paper, we develop an estimation procedure for the l(alpha) norm of the signal, where 0 < alpha <= 2 from binary or multi-bit measurements. We demonstrate that using a simple closed-form estimator with merely 1-bit information does not result in a significant loss of accuracy if the parameter is chosen appropriately. Theoretical tail bounds are also provided. Using 2 or more bits per measurement reduces the variance and importantly, stabilizes the estimate so that the variance is not too sensitive to chosen parameters.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,1430,1438,,,,,,,,,,,,,,,,WOS:000509368500153,0
C,"Lu, XY; Perrone, V; Hasenclever, L; Teh, YW; Vollmer, SJ",,"Singh, A; Zhu, J",,"Lu, Xiaoyu; Perrone, Valerio; Hasenclever, Leonard; Teh, Yee Whye; Vollmer, Sebastian J.",,,Relativistic Monte Carlo,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Hamiltonian Monte Carlo (HMC) is a popular Markov chain Monte Carlo (MCMC) algorithm that generates proposals for a Metropolis-Hastings algorithm by simulating the dynamics of a Hamiltonian system. However, HMC is sensitive to large time discretizations and performs poorly if there is a mismatch between the spatial geometry of the target distribution and the scales of the momentum distribution. In particular the mass matrix of HMC is hard to tune well. In order to alleviate these problems we propose relativistic Hamiltonian Monte Carlo, a version of HMC based on relativistic dynamics that introduces a maximum velocity on particles. We also derive stochastic gradient versions of the algorithm and show that the resulting algorithms bear interesting relationships to gradient clipping, RMSprop, Adagrad and Adam, popular optimisation methods in deep learning. Based on this, we develop relativistic stochastic gradient descent by taking the zero-temperature limit of relativistic stochastic gradient Hamiltonian Monte Carlo. In experiments we show that the relativistic algorithms perform better than classical Newtonian variants and Adam.",,,,,"lu, xiaoyu/GWU-6357-2022",,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,1236,1245,,,,,,,,,,,,,,,,WOS:000509368500132,0
C,"Le, TA; Baydin, AG; Wood, F",,"Singh, A; Zhu, J",,"Tuan Anh Le; Baydin, Atilim Gunes; Wood, Frank",,,Inference Compilation and Universal Probabilistic Programming,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"We introduce a method for using deep neural networks to amortize the cost of inference in models from the family induced by universal probabilistic programming languages, establishing a framework that combines the strengths of probabilistic programming and deep learning methods. We call what we do compilation of inference because our method transforms a denotational specification of an inference problem in the form of a probabilistic program written in a universal programming language into a trained neural network denoted in a neural network specification language. When at test time this neural network is fed observational data and executed, it performs approximate inference in the original model specified by the probabilistic program. Our training objective and learning procedure are designed to allow the trained neural network to be used as a proposal distribution in a sequential importance sampling inference engine. We illustrate our method on mixture models and Captcha solving and show significant speedups in the efficiency of inference.",,,,,"Baydin, Atilim Gunes/M-7029-2014","Baydin, Atilim Gunes/0000-0001-9854-8100",,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,1338,1348,,,,,,,,,,,,,,,,WOS:000509368500143,0
C,"Wei, C; Murray, I",,"Singh, A; Zhu, J",,"Wei, Colin; Murray, Iain",,,Markov Chain Truncation for Doubly-Intractable Inference,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Computing partition functions, the normalizing constants of probability distributions, is often hard. Variants of importance sampling give unbiased estimates of a normalizer Z, however, unbiased estimates of the reciprocal 1/Z are harder to obtain. Unbiased estimates of 1/Z allow Markov chain Monte Carlo sampling of doubly-intractable distributions, such as the parameter posterior for Markov Random Fields or Exponential Random Graphs. We demonstrate how to construct unbiased estimates for 1/Z given access to black-box importance sampling estimators for Z. We adapt recent work on random series truncation and Markov chain coupling, producing estimators with lower variance and a higher percentage of positive estimates than before. Our debiasing algorithms are simple to implement, and have some theoretical and empirical advantages over existing methods.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,776,784,,,,,,,,,,,,,,,,WOS:000509368500083,0
C,"Hoffman, MD; Blei, DM",,"Lebanon, G; Vishwanathan, SVN",,"Hoffman, Matthew D.; Blei, David M.",,,Structured Stochastic Variational Inference,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"Stochastic variational inference makes it possible to approximate posterior distributions induced by large datasets quickly using stochastic optimization. The algorithm relies on the use of fully factorized variational distributions. However, this mean-field independence approximation limits the fidelity of the posterior approximation, and introduces local optima. We show how to relax the mean-field approximation to allow arbitrary dependencies between global parameters and local hidden variables, producing better parameter estimates by reducing bias, sensitivity to local optima, and sensitivity to hyperparameters.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,361,369,,,,,,,,,,,,,,,,WOS:000508399700040,0
C,"Kuleshov, V; Chaganty, AT; Liang, P",,"Lebanon, G; Vishwanathan, SVN",,"Kuleshov, Volodymyr; Chaganty, Arun Tejasvi; Liang, Percy",,,Tensor Factorization via Matrix Factorization,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"Tensor factorization arises in many machine learning applications, such as knowledge base modeling and parameter estimation in latent variable models. However, numerical methods for tensor factorization have not reached the level of maturity of matrix factorization methods. In this paper, we propose a new algorithm for CP tensor factorization that uses random projections to reduce the problem to simultaneous matrix diagonalization. Our method is conceptually simple and also applies to non-orthogonal and asymmetric tensors of arbitrary order. We prove that a small number random projections essentially preserves the spectral information in the tensor, allowing us to remove the dependence on the eigengap that plagued earlier tensor-to-matrix reductions. Experimentally, our method outperforms existing tensor factorization methods on both simulated data and two real datasets.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,507,516,,,,,,,,,,,,,,,,WOS:000508399700056,0
C,"Li, P; Zhang, CH",,"Lebanon, G; Vishwanathan, SVN",,"Li, Ping; Zhang, Cun-Hui",,,Compressed Sensing with Very Sparse Gaussian Random Projections,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"We study the use of very sparse random projections [12, 11] for compressed sensing (sparse signal recovery) when the nonzero coordinates of signals can be either positive or negative. In our setting, the entries of a Gaussian design matrix are randomly sparsified so that only a very small fraction of entries are nonzero. Our proposed decoding algorithm is simple and efficient in that the major cost is one linear scan of the coordinates. Using our proposed tie estimator, we are able to recover a K-sparse signal of length N using 1.551eK log K/delta measurements (where delta <= 0.05 is the confidence) in one scan. The practical performance of our method, however, can be substantially better than this bound. The Gaussian design assumption is not essential although it simplifies the analysis. Prior studies have shown that existing one-scan (or roughly one-scan) recovery algorithms using sparse matrices would require substantially (e.g., one order of magnitude) more measurements than L1 decoding by linear programming, when the nonzero coordinates of signals can be either negative or positive. In this paper, following a well-known experimental setup [1], we show that, at the same number of measurements, the recovery accuracies of our proposed method are similar to the standard L1 decoding.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,617,625,,,,,,,,,,,,,,,,WOS:000508399700068,0
C,"van Hoof, H; Peters, J; Neumann, G",,"Lebanon, G; Vishwanathan, SVN",,"van Hoof, Herke; Peters, Jan; Neumann, Gerhard",,,Learning of Non-Parametric Control Policies with High-Dimensional State Features,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"Learning complex control policies from high-dimensional sensory input is a challenge for reinforcement learning algorithms. Kernel methods that approximate values functions or transition models can address this problem. Yet, many current approaches rely on instable greedy maximization. In this paper, we develop a policy search algorithm that integrates robust policy updates and kernel embeddings. Our method can learn non-parametric control policies for infinite horizon continuous MDPs with high-dimensional sensory representations. We show that our method outperforms related approaches, and that our algorithm can learn an underpowered swing-up task task directly from high-dimensional image data.",,,,,"Peters, Jan/P-6027-2019; Peters, Jan R/D-5068-2009; van Hoof, Herke/N-7775-2017","Peters, Jan/0000-0002-5266-8091; Peters, Jan R/0000-0002-5266-8091; van Hoof, Herke/0000-0002-1583-3692; Neumann, Gerhard/0000-0002-5483-4225",,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,995,1003,,,,,,,,,,,,,,,,WOS:000508399700109,0
C,"Yang, ZR; Peltonen, J; Kaski, S",,"Lebanon, G; Vishwanathan, SVN",,"Yang, Zhirong; Peltonen, Jaakko; Kaski, Samuel",,,Majorization-Minimization for Manifold Embedding,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"Nonlinear dimensionality reduction by manifold embedding has become a popular and powerful approach both for visualization and as preprocessing for predictive tasks, but more efficient optimization algorithms are still crucially needed. Majorization-Minimization (MM) is a promising approach that monotonically decreases the cost function, but it remains unknown how to tightly majorize the manifold embedding objective functions such that the resulting MM algorithms are efficient and robust. We propose a new MM procedure that yields fast MM algorithms for a wide variety of manifold embedding problems. In our majorization step, two parts of the cost function are respectively upper bounded by quadratic and Lipschitz surrogates, and the resulting upper bound can be minimized in closed form. For cost functions amenable to such QL-majorization, the MM yields monotonic improvement and is efficient: In experiments, the newly developed MM algorithms outperformed five state-of-the-art optimization approaches in manifold embedding tasks.",,,,,"Yang, Zhirong/E-8312-2012; Kaski, Samuel/B-6684-2008; Peltonen, Jaakko/ABD-1698-2020; Peltonen, Jaakko T/O-5172-2016","Yang, Zhirong/0000-0001-8412-5684; Kaski, Samuel/0000-0003-1925-9154; Peltonen, Jaakko/0000-0003-3485-8585; Peltonen, Jaakko T/0000-0003-3485-8585",,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,1088,1097,,,,,,,,,,,,,,,,WOS:000508399700119,0
C,"Dai, B; He, N; Pan, YP; Boots, B; Song, L",,"Singh, A; Zhu, J",,"Dai, Bo; He, Niao; Pan, Yunpeng; Boots, Byron; Song, Le",,,Learning from Conditional Distributions via Dual Embeddings,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Many machine learning tasks, such as learning with invariance and policy evaluation in reinforcement learning, can be characterized as problems of learning from conditional distributions. In such problems, each sample x itself is associated with a conditional distribution p(z vertical bar x) represented by samples {z(i)}(i=1)(M), and the goal is to learn a function f that links these conditional distributions to target values y. These problems become very challenging when we only have limited samples or in the extreme case only one sample from each conditional distribution. Commonly used approaches either assume that z is independent of x, or require an overwhelmingly large set of samples from each conditional distribution. To address these challenges, we propose a novel approach which employs a new min-max reformulation of the learning from conditional distribution problem. With such new reformulation, we only need to deal with the joint distribution p(z, x). We also design an efficient learning algorithm, Embedding-SGD, and establish theoretical sample complexity for such problems. Finally, our numerical experiments, on both synthetic and real-world datasets, show that the proposed approach can significantly improve over existing algorithms.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,1458,1467,,,,,,,,,,,,,,,,WOS:000509368500156,0
C,"Ghoshal, A; Honorio, J",,"Singh, A; Zhu, J",,"Ghoshal, Asish; Honorio, Jean",,,Information-theoretic limits of Bayesian network structure learning,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"In this paper, we study the information-theoretic limits of learning the structure of Bayesian networks (BNs), on discrete as well as continuous random variables, from a finite number of samples. We show that the minimum number of samples required by any procedure to recover the correct structure grows as Omega(m) and Omega(k log m + k(2)/m) for non-sparse and sparse BNs respectively, where m is the number of variables and k is the maximum number of parents per node. We provide a simple recipe, based on an extension of the Fano's inequality, to obtain information-theoretic limits of structure recovery for any exponential family BN. We instantiate our result for specific conditional distributions in the exponential family to characterize the fundamental limits of learning various commonly used BNs, such as conditional probability table based networks, Gaussian BNs, noisy-OR networks, and logistic regression networks. En route to obtaining our main results, we obtain tight bounds on the number of sparse and non-sparse essential-DAGs. Finally, as a byproduct, we recover the information-theoretic limits of sparse variable selection for logistic regression.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,767,775,,,,,,,,,,,,,,,,WOS:000509368500082,0
C,"Moore, DA; Russell, SJ",,"Singh, A; Zhu, J",,"Moore, David A.; Russell, Stuart J.",,,Signal-based Bayesian Seismic Monitoring,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Detecting weak seismic events from noisy sensors is a difficult perceptual task. We formulate this task as Bayesian inference and propose a generative model of seismic events and signals across a network of spatially distributed stations. Our system, SIGVISA, is the first to directly model seismic waveforms, allowing it to incorporate a rich representation of the physics underlying the signal generation process. We use Gaussian processes over wavelet parameters to predict detailed waveform fluctuations based on historical events, while degrading smoothly to simple parametric envelopes in regions with no historical seismicity. Evaluating on data from the western US, we recover three times as many events as previous work, and reduce mean location errors by a factor of four while greatly increasing sensitivity to low-magnitude events.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,1293,1301,,,,,,,,,,,,,,,,WOS:000509368500138,0
C,"Quattoni, A; Carreras, X; Galle, M",,"Singh, A; Zhu, J",,"Quattoni, Ariadna; Carreras, Xavier; Galle, Matthias",,,A Maximum Matching Algorithm for Basis Selection in Spectral Learning,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"We present a solution to scale spectral algorithms for learning sequence functions. We are interested in the case where these functions are sparse (that is, for most sequences they return 0). Spectral algorithms reduce the learning problem to the task of computing an SVD decomposition over a special type of matrix called the Hankel matrix. This matrix is designed to capture the relevant statistics of the training sequences. What is crucial is that to capture long range dependencies we must consider very large Hankel matrices. Thus the computation of the SVD becomes a critical bottleneck. Our solution finds a subset of rows and columns of the Hankel that realizes a compact and informative Hankel submatrix. The novelty lies in the way that this subset is selected: we exploit a maximal bipartite matching combinatorial algorithm to look for a sub-block with full structural rank, and show how computation of this sub-block can be further improved by exploiting the specific structure of Hankel matrices.",,,,,,"Carreras, Xavier/0000-0001-7432-4540; Galle, Matthias/0000-0001-5677-5911",,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,1477,1485,,,,,,,,,,,,,,,,WOS:000509368500158,0
C,"Yurtsever, A; Udell, M; Tropp, JA; Cevher, V",,"Singh, A; Zhu, J",,"Yurtsever, Alp; Udell, Madeleine; Tropp, Joel A.; Cevher, Volkan",,,Sketchy Decisions: Convex Low-Rank Matrix Optimization with Optimal Storage,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"This paper concerns a fundamental class of convex matrix optimization problems. It presents the first algorithm that uses optimal storage and provably computes a low-rank approximation of a solution. In particular, when all solutions have low rank, the algorithm converges to a solution. This algorithm, SketchyCGM, modifies a standard convex optimization scheme, the conditional gradient method, to store only a small randomized sketch of the matrix variable. After the optimization terminates, the algorithm extracts a low-rank approximation of the solution from the sketch. In contrast to non-convex heuristics, the guarantees for SketchyCGM do not rely on statistical models for the problem data. Numerical work demonstrates the benefits of SketchyCGM over heuristics.",,,,,,"Udell, Madeleine/0000-0002-3985-915X",,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,1188,1196,,,,,,,,,,,,,,,,WOS:000509368500127,0
C,"Chen, TQ; Singh, SME; Taskar, B; Guestrin, CRL",,"Lebanon, G; Vishwanathan, SVN",,"Chen, Tianqi; Singh, Sameer; Taskar, Ben; Guestrin, Carlos",,,Efficient Second-Order Gradient Boosting for Conditional Random Fields,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"Conditional random fields (CRFs) are an important class of models for accurate structured prediction, but effective design of the feature functions is a major challenge when applying CRF models to real world data. Gradient boosting, which is used to automatically induce and select feature functions, is a natural candidate solution to the problem. However, it is non-trivial to derive gradient boosting algorithms for CRFs due to the dense Hessian matrices introduced by variable dependencies. Existing approaches thus use only first-order information when optimizing likelihood, and hence face convergence issues. We incorporate second-order information by deriving a Markov Chain mixing rate bound to quantify the dependencies, and introduce a gradient boosting algorithm that iteratively optimizes an adaptive upper bound of the objective function. The resulting algorithm induces and selects features for CRFs via functional space optimization, with provable convergence guarantees. Experimental results on three real world datasets demonstrate that the mixing rate based upper bound is effective for learning CRFs with non-linear potentials.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,147,155,,,,,,,,,,,,,,,,WOS:000508399700017,0
C,"Gonzalez-Brenes, JP",,"Lebanon, G; Vishwanathan, SVN",,"Gonzalez-Brenes, Jose P.",,,Modeling Skill Acquisition Over Time with Sequence and Topic Modeling,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"Online education provides data from students solving problems at different levels of proficiency over time. Unfortunately, methods that use these data for inferring student knowledge rely on costly domain expertise. We propose three novel data-driven methods that bridge sequence modeling with topic models to infer students' time varying knowledge. These methods differ in complexity, interpretability, accuracy and human supervision. For example, our most interpretable method has similar classification accuracy to the models created by domain experts, but requires much less effort. On the other hand, the most accurate method is completely data-driven and improves predictions by up to 15% in AUC, an evaluation metric for classifiers.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,296,305,,,,,,,,,,,,,,,,WOS:000508399700033,0
C,"Gunasekar, S; Yamada, M; Yin, DW; Chang, Y",,"Lebanon, G; Vishwanathan, SVN",,"Gunasekar, Suriya; Yamada, Makoto; Yin, Dawei; Chang, Yi",,,Consistent Collective Matrix Completion under Joint Low Rank Structure,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"We address the collective matrix completion problem of jointly recovering a collection of matrices with shared structure from partial (and potentially noisy) observations. To ensure well-posedness of the problem, we impose a joint low rank structure, wherein each component matrix is low rank and the latent space of the low rank factors corresponding to each entity is shared across the entire collection. We first develop a rigorous algebra for representing and manipulating collective-matrix structure, and identify sufficient conditions for consistent estimation of collective matrices. We then propose a tractable convex estimator for solving the collective matrix completion problem, and provide the first non-trivial theoretical guarantees for consistency of collective matrix completion. We show that under reasonable assumptions stated in Sec. 3.1, with high probability, the proposed estimator exactly recovers the true matrices whenever sample complexity requirements dictated by Theorem 1 are met. The sample complexity requirement derived in the paper are optimum up to logarithmic factors, and significantly improve upon the requirements obtained by trivial extensions of standard matrix completion. Finally, we propose a scalable approximate algorithm to solve the proposed convex program, and corroborate our results through simulated and real life experiments.",,,,,,"Chang, Yi/0000-0003-2697-8093",,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,306,314,,,,,,,,,,,,,,,,WOS:000508399700034,0
C,"Shi, TL; Steinhardt, J; Liang, P",,"Lebanon, G; Vishwanathan, SVN",,"Shi, Tianlin; Steinhardt, Jacob; Liang, Percy",,,Learning Where to Sample in Structured Prediction,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"In structured prediction, most inference algorithms allocate a homogeneous amount of computation to all parts of the output, which can be wasteful when different parts vary widely in terms of difficulty. In this paper, we propose a heterogeneous approach that dynamically allocates computation to the different parts. Given a pre-trained model, we tune its inference algorithm (a sampler) to increase test-time throughput. The inference algorithm is parametrized by a meta-model and trained via reinforcement learning, where actions correspond to sampling candidate parts of the output, and rewards are log-likelihood improvements. The meta-model is based on a set of domain-general meta-features capturing the progress of the sampler. We test our approach on five datasets and show that it attains the same accuracy as Gibbs sampling but is 2 to 5 times faster.",,,,,,"Steinhardt, Jacob Noah/0000-0002-0257-3860",,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,875,884,,,,,,,,,,,,,,,,WOS:000508399700096,0
C,"Tank, A; Foti, NJ; Fox, EB",,"Lebanon, G; Vishwanathan, SVN",,"Tank, Alex; Foti, Nicholas J.; Fox, Emily B.",,,Streaming Variational Inference for Bayesian Nonparametric Mixture Models,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"In theory, Bayesian nonparametric (BNP) models are well suited to streaming data scenarios due to their ability to adapt model complexity with the observed data. Unfortunately, such benefits have not been fully realized in practice; existing inference algorithms are either not applicable to streaming applications or not extensible to BNP models. For the special case of Dirichlet processes, streaming inference has been considered. However, there is growing interest in more flexible BNP models building on the class of normalized random measures (NRMs). We work within this general framework and present a streaming variational inference algorithm for NRM mixture models. Our algorithm is based on assumed density filtering (ADF), leading straightforwardly to expectation propagation (EP) for large-scale batch inference as well. We demonstrate the efficacy of the algorithm on clustering documents in large, streaming text corpora.",,,,,,"Fox, Emily/0000-0003-3188-9685",,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,968,976,,,,,,,,,,,,,,,,WOS:000508399700106,0
C,"Wang, YN; Singh, A",,"Lebanon, G; Vishwanathan, SVN",,"Wang, Yining; Singh, Aarti",,,Column Subset Selection with Missing Data via Active Sampling,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"Column subset selection of massive data matrices has found numerous applications in real-world data systems. In this paper, we propose and analyze two sampling based algorithms for column subset selection without access to the complete input matrix. To our knowledge, these are the first algorithms for column subset selection with missing data that are provably correct. The proposed methods work for row/column coherent matrices by employing the idea of adaptive sampling. Furthermore, when the input matrix has a noisy low-rank structure, one algorithm enjoys a relative error bound.",,,,,,"Wang, Yining/0000-0001-9410-0392",,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,1033,1041,,,,,,,,,,,,,,,,WOS:000508399700113,0
C,"Zhou, MY",,"Lebanon, G; Vishwanathan, SVN",,"Zhou, Mingyuan",,,Infinite Edge Partition Models for Overlapping Community Detection and Link Prediction,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"A hierarchical gamma process infinite edge partition model is proposed to factorize the binary adjacency matrix of an unweighted undirected relational network under a Bernoulli-Poisson link. The model describes both homophily and stochastic equivalence, and is scalable to big sparse networks by focusing its computation on pairs of linked nodes. It can not only discover overlapping communities and inter-community interactions, but also predict missing edges. A simplified version omitting inter-community interactions is also provided and we reveal its interesting connections to existing models. The number of communities is automatically inferred in a nonparametric Bayesian manner, and efficient inference via Gibbs sampling is derived using novel data augmentation techniques. Experimental results on four real networks demonstrate the models' scalability and state-of-the-art performance.",,,,,"Zhou, Mingyuan/AAE-8717-2021",,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,1135,1143,,,,,,,,,,,,,,,,WOS:000508399700124,0
C,"Botev, A; Zheng, BW; Barber, D",,"Singh, A; Zhu, J",,"Botev, Aleksandar; Zheng, Bowen; Barber, David",,,Complementary Sum Sampling for Likelihood Approximation in Large Scale Classification,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,We consider training probabilistic classifiers in the case that the number of classes is too large to perform exact normalisation over all classes. We show that the source of high variance in standard sampling approximations is due to simply not including the correct class of the datapoint into the approximation. To account for this we explicitly sum over a subset of classes and sample the remaining. We show that this simple approach is competitive with recently introduced non likelihood-based approximations.,,,,,,"Botev, Aleksandar/0000-0001-9021-1124",,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,1030,1038,,,,,,,,,,,,,,,,WOS:000509368500110,0
C,"Chiang, KY; Hsieh, CJ; Dhillon, IS",,"Singh, A; Zhu, J",,"Chiang, Kai-Yang; Hsieh, Cho-Jui; Dhillon, Inderjit S.",,,Rank Aggregation and Prediction with Item Features,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"We study the problem of rank aggregation with features, where both pairwise comparisons and item features are available to help the rank aggregation task. Observing that traditional rank aggregation methods disregard features, while models adapted from learning-to-rank task are sensitive to feature noise, we propose a general model to learn a total ranking by balancing between comparisons and feature information jointly. As a result, our proposed model takes advantage of item features and is also robust to noise. More importantly, we study the effectiveness of item features in our model and show that given sufficiently informative features, the sample complexity of our model can be asymptotically lower than models based only on comparisons for deriving an accurate ranking. The results theoretically justify that our model can achieve efficient learning by leveraging item feature information. In addition, we show that the proposed model can also be extended to two other related problems-online rank aggregation and rank prediction of new items. Finally, experiments show that our model is more effective and robust compared to existing methods on both synthetic and real datasets.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,748,756,,,,,,,,,,,,,,,,WOS:000509368500080,0
C,"Goldfarb, D; Iyengar, G; Zhou, CX",,"Singh, A; Zhu, J",,"Goldfarb, Donald; Iyengar, Garud; Zhou, Chaoxu",,,Linear Convergence of Stochastic FrankWolfe Variants,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"In this paper, we show that the Away-step Stochastic Frank-Wolfe (ASFW) and Pairwise Stochastic Frank-Wolfe (PSFW) algorithms converge linearly in expectation. We also show that if an algorithm convergences linearly in expectation then it converges linearly almost surely. In order to prove these results, we develop a novel proof technique based on concepts of empirical processes and concentration inequalities. As far as we know, this technique has not been used previously to derive the convergence rates of stochastic optimization algorithms. In large-scale numerical experiments, ASFW and PSFW perform as well as or better than their stochastic competitors in actual CPU time.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,1066,1074,,,,,,,,,,,,,,,,WOS:000509368500114,0
C,"Kull, M; Silva, TDE; Flach, P",,"Singh, A; Zhu, J",,"Kull, Meelis; de Menezes e Silva Filho, Telmo; Flach, Peter",,,Beta calibration: a well-founded and easily implemented improvement on logistic calibration for binary classifiers,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"For optimal decision making under variable class distributions and misclassification costs a classifier needs to produce well-calibrated estimates of the posterior probability. Isotonic calibration is a powerful non-parametric method that is however prone to overfitting on smaller datasets; hence a parametric method based on the logistic curve is commonly used. While logistic calibration is designed for normally distributed per-class scores, we demonstrate experimentally that many classifiers including Naive Bayes and Adaboost suffer from a particular distortion where these score distributions are heavily skewed. In such cases logistic calibration can easily yield probability estimates that are worse than the original scores. Moreover, the logistic curve family does not include the identity function, and hence logistic calibration can easily uncalibrate a perfectly calibrated classifier. In this paper we solve all these problems with a richer class of calibration maps based on the beta distribution. We derive the method from first principles and show that fitting it is as easy as fitting a logistic curve. Extensive experiments show that beta calibration is superior to logistic calibration for Naive Bayes and Adaboost.",,,,,,"Silva Filho, Telmo/0000-0003-0826-6885",,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,623,631,,,,,,,,,,,,,,,,WOS:000509368500067,0
C,"Li, P; Mazumdar, A; Milenkovic, O",,"Singh, A; Zhu, J",,"Li, Pan; Mazumdar, Arya; Milenkovic, Olgica",,,Efficient Rank Aggregation via Lehmer Codes,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"We propose a novel rank aggregation method based on converting permutations into their corresponding Lehmer codes or other subdiagonal images. Lehmer codes, also known as inversion vectors, are vector representations of permutations in which each coordinate can take values not restricted by the values of other coordinates. This transformation allows for decoupling of the coordinates and for performing aggregation via simple scalar median or mode computations. We present simulation results illustrating the performance of this completely parallelizable approach and analytically prove that both the mode and median aggregation procedure recover the correct centroid aggregate with small sample complexity when the permutations are drawn according to the well-known Mallows models. The proposed Lehmer code approach may also be used on partial rankings, with similar performance guarantees.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,450,459,,,,,,,,,,,,,,,,WOS:000509368500049,0
C,"Poulis, S; Dasgupta, S",,"Singh, A; Zhu, J",,"Poulis, Stefanos; Dasgupta, Sanjoy",,,Learning with feature feedback: from theory to practice,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"In supervised learning, a human annotator only needs to assign each data point (document, image, etc.) its correct label. But in many situations, the human can also provide richer feedback at essentially no extra cost. In this paper, we examine a particular type of feature feedback that has been used, with some success, in information retrieval and in computer vision. We formalize two models of feature feedback, give learning algorithms for them, and quantify their usefulness in the learning process. Our experiments also show the efficacy of these methods.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,1104,1113,,,,,,,,,,,,,,,,WOS:000509368500118,0
C,"Bach, SH; Huang, B; Getoor, L",,"Lebanon, G; Vishwanathan, SVN",,"Bach, Stephen H.; Huang, Bert; Getoor, Lise",,,Unifying Local Consistency and MAX SAT Relaxations for Scalable Inference with Rounding Guarantees,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"We prove the equivalence of first-order local consistency relaxations and the MAX SAT relaxation of Goemans and Williamson (1994) for a class of MRFs we refer to as logical MRFs. This allows us to combine the advantages of each into a single MAP inference technique: solving the local consistency relaxation with any of a number of highly scalable message-passing algorithms, and then obtaining a high-quality discrete solution via a guaranteed rounding procedure when the relaxation is not tight. Logical MRFs are a general class of models that can incorporate many common dependencies, such as logical implications and mixtures of supermodular and submodular potentials. They can be used for many structured prediction tasks, including natural language processing, computer vision, and computational social science. We show that our new inference technique can improve solution quality by as much as 20% without sacrificing speed on problems with over one million dependencies.",,,,,"Huang, Bert/E-2576-2016","Huang, Bert/0000-0002-8548-7246",,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,46,55,,,,,,,,,,,,,,,,WOS:000508399700006,0
C,"Barzilai, A; Crammer, K",,"Lebanon, G; Vishwanathan, SVN",,"Barzilai, Aviad; Crammer, Koby",,,Convex Multi-Task Learning by Clustering,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,We consider the problem of multi-task learning in which tasks belong to hidden clusters. We formulate the learning problem as a novel convex optimization problem in which linear classifiers are combinations of (a small number of) some basis. Our formulation jointly learns both the basis and the linear combination. We propose a scalable optimization algorithm for finding the optimal solution. Our new methods outperform existing state-of-the-art methods on multi-task sentiment classification tasks.,,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,65,73,,,,,,,,,,,,,,,,WOS:000508399700008,0
C,"Jampani, V; Eslami, SMA; Tarlow, D; Kohli, P; Winn, J",,"Lebanon, G; Vishwanathan, SVN",,"Jampani, Varun; Eslami, S. M. Ali; Tarlow, Daniel; Kohli, Pushmeet; Winn, John",,,Consensus Message Passing for Layered Graphical Models,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"Generative models provide a powerful framework for probabilistic reasoning. However, in many domains their use has been hampered by the practical difficulties of inference. This is particularly the case in computer vision, where models of the imaging process tend to be large, loopy and layered. For this reason bottom-up conditional models have traditionally dominated in such domains. We find that widely-used, general-purpose message passing inference algorithms such as Expectation Propagation (EP) and Variational Message Passing (VMP) fail on the simplest of vision models. With these models in mind, we introduce a modification to message passing that learns to exploit their layered structure by passing consensus messages that guide inference towards good solutions. Experiments on a variety of problems show that the proposed technique leads to significantly more accurate inference results, not only when compared to standard EP and VMP, but also when compared to competitive bottom-up conditional models.",,,,,"Jain, Varun/HHN-1250-2022",,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,425,433,,,,,,,,,,,,,,,,WOS:000508399700047,0
C,"Li, X; Guo, YH",,"Lebanon, G; Vishwanathan, SVN",,"Li, Xin; Guo, Yuhong",,,Max-Margin Zero-Shot Learning for Multi-class Classification,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"Due to the dramatic expanse of data categories and the lack of labeled instances, zero-shot learning, which transfers knowledge from observed classes to recognize unseen classes, has started drawing a lot of attention from the research community. In this paper, we propose a semi-supervised max-margin learning framework that integrates the semi-supervised classification problem over observed classes and the unsupervised clustering problem over unseen classes together to tackle zero-shot multi-class classification. By further integrating label embedding into this framework, we produce a dual formulation that permits convenient incorporation of auxiliary label semantic knowledge to improve zero-shot learning. We conduct extensive experiments on three standard image data sets to evaluate the proposed approach by comparing to two state-of-the-art methods. Our results demonstrate the efficacy of the proposed framework.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,626,634,,,,,,,,,,,,,,,,WOS:000508399700069,0
C,"Melnyk, I; Banerjee, A",,"Lebanon, G; Vishwanathan, SVN",,"Melnyk, Igor; Banerjee, Arindam",,,A Spectral Algorithm for Inference in Hidden semi-Markov Models,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"Hidden semi-Markov models (HSMMs) are latent variable models which allow latent state persistence and can be viewed as a generalization of the popular hidden Markov models (HMMs). In this paper, we introduce a novel spectral algorithm to perform inference in HSMMs. Our approach is based on estimating certain sample moments, whose order depends only logarithmically on the maximum length of the hidden state persistence. Moreover, the algorithm requires only a few spectral decompositions and is therefore computationally efficient. Empirical evaluations on synthetic and real data demonstrate the promise of the algorithm.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,690,698,,,,,,,,,,,,,,,,WOS:000508399700076,0
C,"Song, S; Chaudhuri, K; Sarwate, AD",,"Lebanon, G; Vishwanathan, SVN",,"Song, Shuang; Chaudhuri, Kamalika; Sarwate, Anand D.",,,Learning from Data with Heterogeneous Noise using SGD,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"We consider learning from data of variable quality that may be obtained from different heterogeneous sources. Addressing learning from heterogeneous data in its full generality is a challenging problem. In this paper, we adopt instead a model in which data is observed through heterogeneous noise, where the noise level reflects the quality of the data source. We study how to use stochastic gradient algorithms to learn in this model. Our study is motivated by two concrete examples where this problem arises naturally: learning with local differential privacy based on data from multiple sources with different privacy requirements, and learning from data with labels of variable quality. The main contribution of this paper is to identify how heterogeneous noise impacts performance. We show that given two datasets with heterogeneous noise, the order in which to use them in standard SGD depends on the learning rate. We propose a method for changing the learning rate as a function of the heterogeneity, and prove new regret bounds for our method in two cases of interest. Finally, we evaluate the performance of our algorithm on real data.",,,,,"song, song/GWN-2626-2022","Sarwate, Anand/0000-0001-6123-5282",,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,894,902,,,,,,,,,,,26705435,,,,,WOS:000508399700098,0
C,"Bhargava, A; Ganti, R; Nowak, R",,"Singh, A; Zhu, J",,"Bhargava, Aniruddha; Ganti, Ravi; Nowak, Robert",,,"Active Positive Semidefinite Matrix Completion: Algorithms, Theory and Applications","ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"In this paper we provide simple, computationally efficient, active algorithms for completion of symmetric positive semidefinite matrices. Our proposed algorithms are based on adaptive Nystrom sampling, and are allowed to actively query any element in the matrix, and obtain a possibly noisy estimate of the queried element. We establish sample complexity guarantees on the recovery of the matrix in the max-norm and in the process establish new theoretical results, potentially of independent interest, on adaptive Nystrom sampling. We demonstrate the efficacy of our algorithms on problems in multi-armed bandits and kernel dimensionality reduction.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,1349,1357,,,,,,,,,,,,,,,,WOS:000509368500144,0
C,"Bhowmik, A; Ghosh, J; Koyejo, O",,"Singh, A; Zhu, J",,"Bhowmik, Avradeep; Ghosh, Joydeep; Koyejo, Oluwasanmi",,,Frequency Domain Predictive Modelling with Aggregated Data,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Existing work in spatio-temporal data analysis invariably assumes data available as individual measurements with localised estimates. However, for many applications like econometrics, financial forecasting and climate science, data is often obtained as aggregates. Data aggregation presents severe mathematical challenges to learning and inference, and application of standard techniques is susceptible to ecological fallacy. In this manuscript we investigate the problem of predictive linear modelling in the scenario where data is aggregated in a non-uniform manner across targets and features. We introduce a novel formulation of the problem in the frequency domain, and develop algorithmic techniques that exploit the duality properties of Fourier analysis to bypass the inherent structural challenges of this setting. We provide theoretical guarantees for generalisation error for our estimation procedure and extend our analysis to capture approximation effects arising from aliasing. Finally, we perform empirical evaluation to demonstrate the efficacy of our algorithmic aproach in predictive modelling on synthetic data, and on three real datasets from agricultural studies, ecological surveys and climate science.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,971,980,,,,,,,,,,,,,,,,WOS:000509368500104,0
C,"Fellows, IE; Handcock, MS",,"Singh, A; Zhu, J",,"Fellows, Ian E.; Handcock, Mark S.",,,Removing Phase Transitions from Gibbs Measures,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Gibbs measures are a fundamental class of distributions for the analysis of high dimensional data. Phase transitions, which are also known as degeneracy in the network science literature, are an emergent property of these models that well describe many physical systems. However, the reach of the Gibbs measure is now far outside the realm of physical systems, and in many of these domains multiphase behavior is a nuisance. This nuisance often makes distribution fitting impossible due to failure of the MCMC sampler, and even when an MLE fit is possible, if the solution is near a phase transition point, the plausibility of the fit can be highly questionable. We introduce a modification to the Gibbs distribution that reduces the effects of phase transitions, and with properly chosen hyper-parameters, provably removes all multiphase behavior. We show that this new distribution is just as easy to fit via MCM-CMLE as the Gibbs measure, and provide examples in the Ising model from statistical physics and ERGMs from network science.",,,,,"Handcock, Mark S./AAY-3318-2021","Handcock, Mark S./0000-0002-9985-2785",,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,289,297,,,,,,,,,,,,,,,,WOS:000509368500032,0
C,"Ranganath, R; Tang, LP; Charlin, L; Blei, DM",,"Lebanon, G; Vishwanathan, SVN",,"Ranganath, Rajesh; Tang, Linpeng; Charlin, Laurent; Blei, David M.",,,Deep Exponential Families,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"We describe deep exponential families (DEFs), a class of latent variable models that are inspired by the hidden structures used in deep neural networks. DEFs capture a hierarchy of dependencies between latent variables, and are easily generalized to many settings through exponential families. We perform inference using recent black box variational inference techniques. We then evaluate various DEFs on text and combine multiple DEFs into a model for pairwise recommendation data. In an extensive study, we show going beyond one layer improves predictions for DEFs. We demonstrate that DEFs find interesting exploratory structure in large data sets, and give better predictive performance than state-of-the-art models.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,762,771,,,,,,,,,,,,,,,,WOS:000508399700084,0
C,"Wauthier, FL; Donnelly, P",,"Lebanon, G; Vishwanathan, SVN",,"Wauthier, Fabian L.; Donnelly, Peter",,,A Greedy Homotopy Method for Regression with Nonconvex Constraints,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"The goal of this paper is to estimate sparse linear regression models, where for a given partition G of input variables, the selected variables are chosen from a diverse set of groups in G. We consider a novel class of nonconvex constraint functions, and develop RepLasso, a greedy homotopy method that exploits geometrical properties of the constraint functions to build a sequence of suitably adapted convex surrogate problems. We prove that in some situations RepLasso recovers the global minima path of the nonconvex problem. Moreover, even if it does not recover the global minima, we prove that it will often do no worse than the Lasso in terms of (signed) support recovery, while in practice outperforming it. We show empirically that the strategy can also be used to improve over various other Lasso-style algorithms. Finally, a GWAS of ankylosing spondylitis highlights our method's practical utility.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,1051,1060,,,,,,,,,,,,,,,,WOS:000508399700115,0
C,"Haghiri, S; Ghoshdastidar, D; von Luxburg, U",,"Singh, A; Zhu, J",,"Haghiri, Siavash; Ghoshdastidar, Debarghya; von Luxburg, Ulrike",,,Comparison-Based Nearest Neighbor Search,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"We consider machine learning in a comparison-based setting where we are given a set of points in a metric space, but we have no access to the actual distances between the points. Instead, we can only ask an oracle whether the distance between two points i and j is smaller than the distance between the points i and k. We are concerned with data structures and algorithms to find nearest neighbors based on such comparisons. We focus on a simple yet effective algorithm that recursively splits the space by first selecting two random pivot points and then assigning all other points to the closer of the two (comparison tree). We prove that if the metric space satisfies certain expansion conditions, then with high probability the height of the comparison tree is logarithmic in the number of points, leading to efficient search performance. We also provide an upper bound for the failure probability to return the true nearest neighbor. Experiments show that the comparison tree is competitive with algorithms that have access to the actual distance values, and needs less triplet comparisons than other competitors.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,851,859,,,,,,,,,,,,,,,,WOS:000509368500091,0
C,"Lu, ST; Hong, MY; Wang, ZD",,"Singh, A; Zhu, J",,"Lu, Songtao; Hong, Mingyi; Wang, Zhengdao",,,A Stochastic Nonconvex Splitting Method for Symmetric Nonnegative Matrix Factorization,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Symmetric nonnegative matrix factorization (SymNMF) plays an important role in applications of many data analytics problems such as community detection, document clustering and image segmentation. In this paper, we consider a stochastic SymNMF problem in which the observation matrix is generated in a random and sequential manner. We propose a stochastic nonconvex splitting method, which not only guarantees convergence to the set of stationary points of the problem (in the mean-square sense), but further achieves a sublinear convergence rate. Numerical results show that for clustering problems over both synthetic and real world datasets, the proposed algorithm converges quickly to the set of stationary points.",,,,,"Hong, Mingyi/H-6274-2013",,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,812,821,,,,,,,,,,,,,,,,WOS:000509368500087,0
C,"Mueller, J; Reshef, DN; Du, G; Jaakkola, T",,"Singh, A; Zhu, J",,"Mueller, Jonas; Reshef, David N.; Du, George; Jaakkola, Tommi",,,Learning Optimal Interventions,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Our goal is to identify beneficial interventions from observational data. We consider interventions that are narrowly focused (impacting few covariates) and may be tailored to each individual or globally enacted over a population. For applications where harmful intervention is drastically worse than proposing no change, we propose a conservative definition of the optimal intervention. Assuming the underlying relationship remains invariant under intervention, we develop efficient algorithms to identify the optimal intervention policy from limited data and provide theoretical guarantees for our approach in a Gaussian Process setting. Although our methods assume covariates can be precisely adjusted, they remain capable of improving outcomes in misspecified settings where interventions incur unintentional downstream effects. Empirically, our approach identifies good interventions in two practical applications: gene perturbation and writing improvement.",,,,,"Mueller, Jonas/AAY-6891-2020",,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,1039,1047,,,,,,,,,,,,,,,,WOS:000509368500111,0
C,"Naesseth, CA; Ruiz, FJR; Linderman, SW; Blei, DM",,"Singh, A; Zhu, J",,"Naesseth, Christian A.; Ruiz, Francisco J. R.; Linderman, Scott W.; Blei, David M.",,,Reparameterization Gradients through Acceptance-Rejection Sampling Algorithms,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Variational inference using the reparameterization trick has enabled large-scale approximate Bayesian inference in complex probabilistic models, leveraging stochastic optimization to sidestep intractable expectations. The reparameterization trick is applicable when we can simulate a random variable by applying a differentiable deterministic function on an auxiliary random variable whose distribution is fixed. For many distributions of interest (such as the gamma or Dirichlet), simulation of random variables relies on acceptance-rejection sampling. The discontinuity introduced by the accept{reject step means that standard reparameterization tricks are not applicable. We propose a new method that lets us leverage reparameterization gradients even when variables are outputs of a acceptance-rejection sampling algorithm. Our approach enables reparameterization on a larger class of variational distributions. In several studies of real and synthetic data, we show that the variance of the estimator of the gradient is significantly lower than other state-of-the-art methods. This leads to faster convergence of stochastic gradient variational inference.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,489,498,,,,,,,,,,,,,,,,WOS:000509368500053,0
C,"Niculescu-Mizil, A; Abbasnejad, E",,"Singh, A; Zhu, J",,"Niculescu-Mizil, Alexandru; Abbasnejad, Ehsan",,,Label Filters for Large Scale Multilabel Classification,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"When assigning labels to a test instance, most multilabel and multiclass classifiers systematically evaluate every single label to decide whether it is relevant or not. This linear scan over labels becomes prohibitive when the number of labels is very large. To alleviate this problem we propose a two step approach where computationally efficient label filters pre-select a small set of candidate labels before the base multiclass or multilabel classifier is applied. The label filters select candidate labels by projecting a test instance on a filtering line, and retaining only the labels that have training instances in the vicinity of this projection. The filter parameters are learned directly from data by solving a constraint optimization problem, and are independent of the base multilabel classifier. The proposed label filters can be used in conjunction with any multiclass or multilabel classifier that requires a linear scan over the labels, and speed up prediction by orders of magnitude without significant impact on performance.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,1448,1457,,,,,,,,,,,,,,,,WOS:000509368500155,0
C,"Perekrestenko, D; Cevher, V; Jaggi, M",,"Singh, A; Zhu, J",,"Perekrestenko, Dmytro; Cevher, Volkan; Jaggi, Martin",,,Faster Coordinate Descent via Adaptive Importance Sampling,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Coordinate descent methods employ random partial updates of decision variables in order to solve huge-scale convex optimization problems. In this work, we introduce new adaptive rules for the random selection of their updates. By adaptive, we mean that our selection rules are based on the dual residual or the primal-dual gap estimates and can change at each iteration. We theoretically characterize the performance of our selection rules and demonstrate improvements over the state-of-the-art, and extend our theory and algorithms to general convex objectives. Numerical evidence with hinge-loss support vector machines and Lasso confirm that the practice follows the theory.",,,,,,"Jaggi, Martin/0000-0003-1579-5558",,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,869,877,,,,,,,,,,,,,,,,WOS:000509368500093,0
C,"Ruozzi, N",,"Singh, A; Zhu, J",,"Ruozzi, Nicholas",,,A Lower Bound on the Partition Function of Attractive Graphical Models in the Continuous Case,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Computing the partition function of an arbitrary graphical model is generally intractable. As a result, approximate inference techniques such as loopy belief propagation and expectation propagation are used to compute an approximation to the true partition function. However, due to general issues of intractability in the continuous case, our understanding of these approximations is relatively limited. In particular, a number of theoretical results known for these approximations in the discrete case are missing in the continuous case. In this work, we use graph covers to extend several such results from the discrete case to the continuous case. Specifically, we provide a graph cover based upper bound for continuous graphical models, and we use this characterization (along with a continuous analog of a discrete correlation-type inequality) to show that the Bethe partition function also provides a lower bound on the true partition function of attractive graphical models in the continuous case.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,1048,1056,,,,,,,,,,,,,,,,WOS:000509368500112,0
C,"Wang, SF; Wolf, S; Fowlkes, CC; Yarkony, J",,"Singh, A; Zhu, J",,"Wang, Shaofei; Wolf, Steffen; Fowlkes, Charless C.; Yarkony, Julian",,,Tracking Objects with Higher Order Interactions via Delayed Column Generation,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,We study the problem of multi-target tracking and data association in video. We formulate this in terms of selecting a subset of high-quality tracks subject to the constraint that no pair of selected tracks is associated with a common detection (of an object). This objective is equivalent to the classic NP-hard problem of finding a maximum-weight set packing (MWSP) where tracks correspond to sets and is made further difficult since the number of candidate tracks grows exponentially in the number of detections. We present a relaxation of this combinatorial problem that uses a column generation formulation where the pricing problem is solved via dynamic programming to efficiently explore the space of tracks. We employ row generation to tighten the bound in such a way as to preserve efficient inference in the pricing problem. We show the practical utility of this algorithm for pedestrian and particle tracking.,,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,1132,1140,,,,,,,,,,,,,,,,WOS:000509368500121,0
C,"Ma, YF; Sutherland, DJ; Garnett, R; Schneider, J",,"Lebanon, G; Vishwanathan, SVN",,"Ma, Yifei; Sutherland, Dougal J.; Garnett, Roman; Schneider, Jeff",,,Active Pointillistic Pattern Search,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"We introduce the problem of active pointillistic pattern search (APPS), which seeks to discover regions of a domain exhibiting desired behavior with limited observations. Unusually, the patterns we consider are defined by large-scale properties of an underlying function that we can only observe at a limited number of points. Given a description of the desired patterns (in the form of a classifier taking functional inputs), we sequentially decide where to query function values to identify as many regions matching the pattern as possible, with high confience. For one broad class of models the expected reward of each unobserved point can be computed analytically. We demonstrate the proposed algorithm on three difficult search problems: locating polluted regions in a lake via mobile sensors, forecasting winning electoral districts with minimal polling, and identifying vortices in a fluid flow simulation.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,672,680,,,,,,,,,,,,,,,,WOS:000508399700074,0
C,"Ortega, PA; Kim, KE; Lee, DD",,"Lebanon, G; Vishwanathan, SVN",,"Ortega, Pedro A.; Kim, Kee-Eung; Lee, Daniel D.",,,Reactive bandits with attitude,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"We consider a general class of K-armed bandits that adapt to the actions of the player. A single continuous parameter characterizes the attitude of the bandit, ranging from stochastic to cooperative or to fully adversarial in nature. The player seeks to maximize the expected return from the adaptive bandit, and the associated optimization problem is related to the free energy of a statistical mechanical system under an external field. When the underlying stochastic distribution is Gaussian, we derive an analytic solution for the long run optimal player strategy for different regimes of the bandit. In the fully adversarial limit, this solution is equivalent to the Nash equilibrium of a two-player, zero-sum semi-infinite game. We show how optimal strategies can be learned from sequential draws and reward observations in these adaptive bandits using Bayesian filtering and Thompson sampling. Results show the qualitative difference in policy regret between our proposed strategy and other well-known bandit algorithms.",,,,,"Lee, Daniel D./B-5753-2013",,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,726,734,,,,,,,,,,,,,,,,WOS:000508399700080,0
C,"Yu, YL; Zheng, X; Marchetti-Bowick, M; Xing, EP",,"Lebanon, G; Vishwanathan, SVN",,"Yu, Yaoliang; Zheng, Xun; Marchetti-Bowick, Micol; Xing, Eric P.",,,Minimizing Nonconvex Non-Separable Functions,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"Regularization has played a key role in deriving sensible estimators in high dimensional statistical inference. A substantial amount of recent works has argued for nonconvex regularizers in favor of their superior theoretical properties and excellent practical performances. In a different but analogous vein, nonconvex loss functions are promoted because of their robustness against outliers. However, these nonconvex formulations are computationally more challenging, especially in the presence of nonsmoothness and nonseparability. To address this issue, we propose a new proximal gradient meta-algorithm by rigorously extending the proximal average to the nonconvex setting. We formally prove its nice convergence properties, and illustrate its effectiveness on two applications: multi-task graph-guided fused lasso and robust support vector machines. Experiments demonstrate that our method compares favorably against other alternatives.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,1107,1115,,,,,,,,,,,,,,,,WOS:000508399700121,0
C,"Farahmand, AM; Barreto, AMS; Nikovski, DN",,"Singh, A; Zhu, J",,"Farahmand, Amir-massoud; Barreto, Andre M. S.; Nikovski, Daniel N.",,,Value-Aware Loss Function for Model-based Reinforcement Learning,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"We consider the problem of estimating the transition probability kernel to be used by a model-based reinforcement learning (RL) algorithm. We argue that estimating a generative model that minimizes a probabilistic loss, such as the log-loss, is an overkill because it does not take into account the underlying structure of decision problem and the RL algorithm that intends to solve it. We introduce a loss function that takes the structure of the value function into account. We provide a finite-sample upper bound for the loss function showing the dependence of the error on model approximation error, number of samples, and the complexity of the model space. We also empirically compare the method with the maximum likelihood estimator on a simple problem.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,1486,1494,,,,,,,,,,,,,,,,WOS:000509368500159,0
C,"Hu, HZ; Sun, W; Venkatraman, A; Hebert, M; Bagnell, JA",,"Singh, A; Zhu, J",,"Hu, Hanzhang; Sun, Wen; Venkatraman, Arun; Hebert, Martial; Bagnell, J. Andrew",,,Gradient Boosting on Stochastic Data Streams,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Boosting is a popular ensemble algorithm that generates more powerful learners by linearly combining base models from a simpler hypothesis class. In this work, we investigate the problem of adapting batch gradient boosting for minimizing convex loss functions to online setting where the loss at each iteration is i.i.d sampled from an unknown distribution. To generalize from batch to online, we first introduce the definition of online weak learning edge with which for strongly convex and smooth loss functions, we present an algorithm, Streaming Gradient Boosting (SGB) with exponential shrinkage guarantees in the number of weak learners. We further present an adaptation of SGB to optimize non-smooth loss functions, for which we derive a O(ln N/N) convergence rate. We also show that our analysis can extend to adversarial online learning setting under a stronger assumption that the online weak learning edge will hold in adversarial setting. We finally demonstrate experimental results showing that in practice our algorithms can achieve competitive results as classic gradient boosting while using less computation.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,595,603,,,,,,,,,,,,,,,,WOS:000509368500064,0
C,"Hyvarinen, A; Morioka, H",,"Singh, A; Zhu, J",,"Hyvarinen, Aapo; Morioka, Hiroshi",,,Nonlinear ICA of Temporally Dependent Stationary Sources,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"We develop a nonlinear generalization of independent component analysis (ICA) or blind source separation, based on temporal dependencies (e.g. autocorrelations). We introduce a nonlinear generative model where the independent sources are assumed to be temporally dependent, non-Gaussian, and stationary, and we observe arbitrarily nonlinear mixtures of them. We develop a method for estimating the model (i.e. separating the sources) based on logistic regression in a neural network which learns to discriminate between a short temporal window of the data vs. a temporal window of temporally permuted data. We prove that the method estimates the sources for general smooth mixing nonlinearities, assuming the sources have sufficiently strong temporal dependencies, and these dependencies are in a certain way different from dependencies found in Gaussian processes. For Gaussian (and similar) sources, the method estimates the nonlinear part of the mixing. We thus provide the first rigorous and general proof of identifiability of nonlinear ICA for temporally dependent stationary sources, together with a practical method for its estimation.",,,,,,"Hyvarinen, Aapo/0000-0002-5806-4432",,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,460,469,,,,,,,,,,,,,,,,WOS:000509368500050,0
C,"Mehta, NA",,"Singh, A; Zhu, J",,"Mehta, Nishant A.",,,Fast rates with high probability in exp-concave statistical learning,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"We present an algorithm for the statistical learning setting with a bounded expconcave loss in d dimensions that obtains excess risk O(d log(1/delta)/n) with probability 1-delta. The core technique is to boost the confidence of recent in-expectation O(d/n) excess risk bounds for empirical risk minimization (ERM), without sacrificing the rate, by leveraging a Bernstein condition which holds due to exp-concavity. We also show that a regret bound for any online learner in this setting translates to a high probability excess risk bound for the corresponding online-to-batch conversion of the online learner. Lastly, we present high probability bounds for the expconcave model selection aggregation problem that are quantile-adaptive in a certain sense. One bound obtains a nearly optimal rate without requiring the loss to be Lipschitz continuous, and another requires Lipschitz continuity but obtains the optimal rate.",,,,,,"Mehta, Nishant/0000-0002-9639-0124",,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,1085,1093,,,,,,,,,,,,,,,,WOS:000509368500116,0
C,"Park, Y; Hallac, D; Boyd, S; Leskovec, J",,"Singh, A; Zhu, J",,"Park, Youngsuk; Hallac, David; Boyd, Stephen; Leskovec, Jure",,,Learning the Network Structure of Heterogeneous Data via Pairwise Exponential Markov Random Fields,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Markov random fields (MRFs) are a useful tool for modeling relationships present in large and high-dimensional data. Often, this data comes from various sources and can have diverse distributions, for example a combination of numerical, binary, and categorical variables. Here, we define the pairwise exponential Markov random field (PE-MRF), an approach capable of modeling exponential family distributions in heterogeneous domains. We develop a scalable method of learning the graphical structure across the variables by solving a regularized approximated maximum likelihood problem. Specifically, we first derive a tractable upper bound on the log-partition function. We then use this upper bound to derive the group graphical lasso, a generalization of the classic graphical lasso problem to heterogeneous domains. To solve this problem, we develop a fast algorithm based on the alternating direction method of multipliers (ADMM). We also prove that our estimator is sparsistent, with guaranteed recovery of the true underlying graphical structure, and that it has a polynomially faster runtime than the current state-of-the-art method for learning such distributions. Experiments on synthetic and realworld examples demonstrate that our approach is both efficient and accurate at uncovering the structure of heterogeneous data.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,1302,1310,,,,,,,,,,,30931433,,,,,WOS:000509368500139,0
C,"Stevens, A; Pu, YC; Sun, YN; Spell, G; Carin, L",,"Singh, A; Zhu, J",,"Stevens, Andrew; Pu, Yunchen; Sun, Yannan; Spell, Gregory; Carin, Lawrence",,,Tensor-Dictionary Learning with Deep Kruskal-Factor Analysis,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"A multi-way factor analysis model is introduced for tensor-variate data of any order. Each data item is represented as a (sparse) sum of Kruskal decompositions, a Kruskal-factor analysis (KFA). KFA is nonparametric and can infer both the tensor-rank of each dictionary atom and the number of dictionary atoms. The model is adapted for online learning, which allows dictionary learning on large data sets. After KFA is introduced, the model is extended to a deep convolutional tensor-factor analysis, supervised by a Bayesian SVM. The experiments section demonstrates the improvement of KFA over vectorized approaches (e.g., BPFA), tensor decompositions, and convolutional neural networks (CNN) in multi-way denoising, blind inpainting, and image classification. The improvement in PSNR for the inpainting results over other methods exceeds 1dB in several cases and we achieve state of the art results on Caltech101 image classification.",,,,,,"Carin, Lawrence/0000-0001-6277-7948",,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,121,129,,,,,,,,,,,,,,,,WOS:000509368500014,0
C,"Cheng, DH; He, XR; Liu, Y",,"Lebanon, G; Vishwanathan, SVN",,"Cheng, Dehua; He, Xinran; Liu, Yan",,,Model Selection for Topic Models via Spectral Decomposition,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"Topic models have achieved significant successes in analyzing large-scale text corpus. In practical applications, we are always confronted with the challenge of model selection, i.e., how to appropriately set the number of topics. Following the recent advances in topic models via tensor decomposition, we make a first attempt to provide theoretical analysis on model selection in latent Dirichlet allocation. With mild conditions, we derive the upper bound and lower bound on the number of topics given a text collection of finite size. Experimental results demonstrate that our bounds are correct and tight. Furthermore, using Gaussian mixture model as an example, we show that our methodology can be easily generalized to model selection analysis in other latent models.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,183,191,,,,,,,,,,,,,,,,WOS:000508399700021,0
C,"El Halabi, M; Cevher, V",,"Lebanon, G; Vishwanathan, SVN",,"El Halabi, Marwa; Cevher, Volkan",,,A totally unimodular view of structured sparsity,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"This paper describes a simple framework for structured sparse recovery based on convex optimization. We show that many structured sparsity models can be naturally represented by linear matrix inequalities on the support of the unknown parameters, where the constraint matrix has a totally unimodular (TU) structure. For such structured models, tight convex relaxations can be obtained in polynomial time via linear programming. Our modeling framework unifies the prevalent structured sparsity norms in the literature, introduces new interesting ones, and renders their tightness and tractability arguments transparent.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,223,231,,,,,,,,,,,,,,,,WOS:000508399700025,0
C,"Hara, S; Morimura, T; Takahashi, T; Yanagisawa, H; Suzuki, T",,"Lebanon, G; Vishwanathan, SVN",,"Hara, Satoshi; Morimura, Tetsuro; Takahashi, Toshihiro; Yanagisawa, Hiroki; Suzuki, Taiji",,,A Consistent Method for Graph Based Anomaly Localization,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"The anomaly localization task aims at detecting faulty sensors automatically by monitoring the sensor values. In this paper, we propose an anomaly localization algorithm with a consistency guarantee on its results. Although several algorithms were proposed in the last decade, the consistency of the localization results was not discussed in the literature. To the best of our knowledge, this is the first study that provides theoretical guarantees for the localization results. Our new approach is to formulate the task as solving the sparsest subgraph problem on a difference graph. Since this problem is NP-hard, we then use a convex quadratic programming approximation algorithm, which is guaranteed to be consistent under suitable conditions. Across the simulations on both synthetic and real world datasets, we verify that the proposed method achieves higher anomaly localization performance compared to existing methods.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,333,341,,,,,,,,,,,,,,,,WOS:000508399700037,0
C,"Paul, S; Magdon-Ismail, M; Drineas, P",,"Lebanon, G; Vishwanathan, SVN",,"Paul, Saurabh; Magdon-Ismail, Malik; Drineas, Petros",,,Feature Selection for Linear SVM with Provable Guarantees,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"We give two provably accurate feature-selection techniques for the linear SVM. The algorithms run in deterministic and randomized time respectively. Our algorithms can be used in an unsupervised or supervised setting. The supervised approach is based on sampling features from support vectors. We prove that the margin in the feature space is preserved to within epsilon-relative error of the margin in the full feature space in the worst-case. In the unsupervised setting, we also provide worst-case guarantees of the radius of the minimum enclosing ball, thereby ensuring comparable generalization as in the full feature space and resolving an open problem posed in Dasgupta et al. (2007). We present extensive experiments on real-world datasets to support our theory and to demonstrate that our methods are competitive and often better than prior state-of-the-art, for which there are no known provable guarantees.",,,,,,"Drineas, Petros/0000-0003-1994-8670",,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,735,743,,,,,,,,,,,,,,,,WOS:000508399700081,0
C,"Ren, SG; Huang, S; Onofrey, J; Papademetris, X; Qian, XN",,"Lebanon, G; Vishwanathan, SVN",,"Ren, Shaogang; Huang, Shuai; Onofrey, John; Papademetris, Xenophon; Qian, Xiaoning",,,A Scalable Algorithm for Structured Kernel Feature Selection,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"Kernel methods are powerful tools for nonlinear feature representation. Incorporated with structured LASSO, the kernelized structured LASSO is an effective feature selection approach that can preserve the nonlinear input-output relationships as well as the structured sparseness. But as the data dimension increases, the method can quickly become computationally prohibitive. In this paper we propose a stochastic optimization algorithm that can efficiently address this computational problem on account of the redundant kernel representations of the given data. Experiments on simulation data and PET 3D brain image data show that our method can achieve superior accuracy with less computational cost than existing methods.",,,,,,"Onofrey, John/0000-0002-9432-0448",,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,781,789,,,,,,,,,,,,,,,,WOS:000508399700086,0
C,"Zhang, LJ; Yang, TB; Jin, R; Zhou, ZH",,"Lebanon, G; Vishwanathan, SVN",,"Zhang, Lijun; Yang, Tianbao; Jin, Rong; Zhou, Zhi-Hua",,,A Simple Homotopy Algorithm for Compressive Sensing,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"In this paper, we consider the problem of recovering the s largest elements of an arbitrary vector from noisy measurements. Inspired by previous work, we develop an homotopy algorithm which solves the l(1)-regularized least square problem for a sequence of decreasing values of the regularization parameter. Compared to the previous method, our algorithm is more efficient in the sense it only updates the solution once for each intermediate problem, and more practical in the sense it has a simple stopping criterion by checking the sparsity of the intermediate solution. Theoretical analysis reveals that our method enjoys a linear convergence rate in reducing the recovery error. Furthermore, our guarantee for recovering the top s elements of the target vector is tighter than previous results, and that for recovering the target vector itself matches the state of the art in compressive sensing.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,1116,1124,,,,,,,,,,,,,,,,WOS:000508399700122,0
C,"Fruit, R; Lazaric, A",,"Singh, A; Zhu, J",,"Fruit, Ronan; Lazaric, Alessandro",,,Exploration-Exploitation in MDPs with Options,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"While a large body of empirical results show that temporally-extended actions and options may significantly affect the learning performance of an agent, the theoretical understanding of how and when options can be beneficial in online reinforcement learning is relatively limited. In this paper, we derive an upper and lower bound on the regret of a variant of UCRL using options. While we first analyze the algorithm in the general case of semi-Markov decision processes (SMDPs), we show how these results can be translated to the specific case of MDPs with options and we illustrate simple scenarios in which the regret of learning with options can be provably much smaller than the regret suffered when learning with primitive actions.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,576,584,,,,,,,,,,,,,,,,WOS:000509368500062,0
C,"Hou, L; Samaras, D; Kurc, TM; Gao, Y; Saltz, JH",,"Singh, A; Zhu, J",,"Hou, Le; Samaras, Dimitris; Kurc, Tahsin M.; Gao, Yi; Saltz, Joel H.",,,ConvNets with Smooth Adaptive Activation Functions for Regression,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Within Neural Networks (NN), the parameters of Adaptive Activation Functions (AAF) control the shapes of activation functions. These parameters are trained along with other parameters in the NN. AAFs have improved performance of Convolutional Neural Networks (CNN) in multiple classification tasks. In this paper, we propose and apply AAFs on CNNs for regression tasks. We argue that applying AAFs in the regression (second-to-last) layer of a NN can significantly decrease the bias of the regression NN. However, using existing AAFs may lead to overfitting. To address this problem, we propose a Smooth Adaptive Activation Function (SAAF) with a piecewise polynomial form which can approximate any continuous function to arbitrary degree of error, while having a bounded Lipschitz constant for given bounded model parameters. As a result, NNs with SAAF can avoid overfitting by simply regularizing model parameters. We empirically evaluated CNNs with SAAFs and achieved state-of-the-art results on age and pose estimation datasets.",,,,,"gao, yi/HCI-8298-2022",,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,430,439,,,,,,,,,,,31106299,,,,,WOS:000509368500047,0
C,"Lei, LH; Jordan, MI",,"Singh, A; Zhu, J",,"Lei, Lihua; Jordan, Michael I.",,,Less than a Single Pass: Stochastically Controlled Stochastic Gradient,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"We develop and analyze a procedure for gradient-based optimization that we refer to as stochastically controlled stochastic gradient (SCSG). As a member of the SVRG family of algorithms, SCSG makes use of gradient estimates at two scales. Unlike most existing algorithms in this family, both the computation cost and the communication cost of SCSG do not necessarily scale linearly with the sample size n; indeed, these costs are independent of n when the target accuracy is small. An experimental evaluation of SCSG on the MNIST dataset shows that it can yield accurate results on this dataset on a single commodity machine with a memory footprint of only 2.6MB and only eight disk accesses.",,,,,,"Jordan, Michael/0000-0001-8935-817X",,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,148,156,,,,,,,,,,,,,,,,WOS:000509368500017,0
C,"Linderman, SW; Johnson, MJ; Miller, AC; Adams, RP; Blei, DM; Paninski, L",,"Singh, A; Zhu, J",,"Linderman, Scott W.; Johnson, Matthew J.; Miller, Andrew C.; Adams, Ryan P.; Blei, David M.; Paninski, Liam",,,Bayesian Learning and Inference in Recurrent Switching Linear Dynamical Systems,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Many natural systems, such as neurons firing in the brain or basketball teams traversing a court, give rise to time series data with complex, nonlinear dynamics. We can gain insight into these systems by decomposing the data into segments that are each explained by simpler dynamic units. Building on switching linear dynamical systems (SLDS), we develop a model class and Bayesian inference algorithms that not only discover these dynamical units but also, by learning how transition probabilities depend on observations or continuous latent states, explain their switching behavior. Our key innovation is to design these recurrent SLDS models to enable recent Polya-gamma auxiliary variable techniques and thus make approximate Bayesian learning and inference in these models easy, fast, and scalable.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,914,922,,,,,,,,,,,,,,,,WOS:000509368500098,0
C,"Park, D; Kyrillidis, A; Caramanis, C; Sanghavi, S",,"Singh, A; Zhu, J",,"Park, Dohuyng; Kyrillidis, Anastasios; Caramanis, Constantine; Sanghavi, Sujay",,,Non-square matrix sensing without spurious local minima via the Burer-Monteiro approach,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"We consider the non-square matrix sensing problem, under restricted isometry property (RIP) assumptions. We focus on the non-convex formulation, where any rank-r matrix X is an element of R-m (x) (n) is represented as UV inverted perpendicular, where U is an element of R-m (x) (r) and V is an element of R-n (x) (r). In this paper, we complement recent findings on the non-convex geometry of the analogous PSD setting [5], and show that matrix factorization does not introduce any spurious local minima, under RIP.",,,,,,"Caramanis, Constantine/0000-0001-9939-8378",,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,65,74,,,,,,,,,,,,,,,,WOS:000509368500008,0
C,"Pike-Burke, C; Grunewalder, S",,"Singh, A; Zhu, J",,"Pike-Burke, Ciara; Grunewalder, Steffen",,,Optimistic Planning for the Stochastic Knapsack Problem,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"The stochastic knapsack problem is a stochastic resource allocation problem that arises frequently and yet is exceptionally hard to solve. We derive and study an optimistic planning algorithm specifically designed for the stochastic knapsack problem. Unlike other optimistic planning algorithms for MDPs, our algorithm, OpStoK, avoids the use of discounting and is adaptive to the amount of resources available. We achieve this behavior by means of a concentration inequality that simultaneously applies to capacity and reward estimates. Crucially, we are able to guarantee that the aforementioned confidence regions hold collectively over all time steps by an application of Doob's inequality. We demonstrate that the method returns an epsilon-optimal solution to the stochastic knapsack problem with high probability. To the best of our knowledge, our algorithm is the first which provides such guarantees for the stochastic knapsack problem. Furthermore, our algorithm is an anytime algorithm and will return a good solution even if stopped prematurely. This is particularly important given the difficulty of the problem. We also provide theoretical conditions to guarantee OpStoK does not expand all policies and demonstrate favorable performance in a simple experimental setting.",,,,,,"Pike-Burke, Ciara/0000-0002-5847-1193; Grunewalder, Steffen/0000-0002-4017-2048",,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,1114,1122,,,,,,,,,,,,,,,,WOS:000509368500119,0
C,"Rakhlin, A; Sridharan, K",,"Singh, A; Zhu, J",,"Rakhlin, Alexander; Sridharan, Karthik",,,Efficient Online Multiclass Prediction on Graphs via Surrogate Losses,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"We develop computationally efficient algorithms for online multi-class prediction. Our construction is based on carefully-chosen data-dependent surrogate loss functions, and the new methods enjoy strong mistake bound guarantees. To illustrate the technique, we study the combinatorial problem of node classification and develop a prediction strategy that is linear-time per round. In contrast, the offline benchmark is NP-hard to compute in general. We demonstrate the empirical performance of the method on several datasets.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,1403,1411,,,,,,,,,,,,,,,,WOS:000509368500150,0
C,"Vaswani, S; Schmidt, M; Lakshmanan, LVS",,"Singh, A; Zhu, J",,"Vaswani, Sharan; Schmidt, Mark; Lakshmanan, Laks V. S.",,,Horde of Bandits using Gaussian Markov Random Fields,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"The gang of bandits (GOB) model [7] is a recent contextual bandits framework that shares information between a set of bandit problems, related by a known (possibly noisy) graph. This model is useful in problems like recommender systems where the large number of users makes it vital to transfer information between users. Despite its effectiveness, the existing GOB model can only be applied to small problems due to its quadratic time-dependence on the number of nodes. Existing solutions to combat the scalability issue require an often-unrealistic clustering assumption. By exploiting a connection to Gaussian Markov random fields (CMRFs), we show that the GOB model can be made to scale to much larger graphs without additional assumptions. In addition, we propose a Thompson sampling algorithm which uses the recent CMRF sampling-by-perturbation technique, allowing it to scale to even larger problems (leading to a horde of bandits). We give regret bounds and experimental results for GOB with Thompson sampling and epoch-greedy algorithms, indicating that these methods are as good as or significantly better than ignoring the graph or adopting a clustering-based approach. Finally, when an existing graph is not available, we propose a heuristic for learning it on the fly and show promising results.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,690,699,,,,,,,,,,,,,,,,WOS:000509368500074,0
C,"Wang, BL; Gao, J; Qi, YJ",,"Singh, A; Zhu, J",,"Wang, Beilun; Gao, Ji; Qi, Yanjun",,,A Fast and Scalable Joint Estimator for Learning Multiple Related Sparse Gaussian Graphical Models,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Estimating multiple sparse Gaussian Graphical Models (sGGMs) jointly for many related tasks (large K) under a high-dimensional (large p) situation is an important task. Most previous studies for the joint estimation of multiple sGGMs rely on penalized log-likelihood estimators that involve expensive and difficult non-smooth optimizations. We propose a novel approach, FASJEM for fast and scalable joint structure-estimation of multiple sGGMs at a large scale. As the first study of joint sGGM using the M-estimator framework, our work has three major contributions: (1) We solve FASJEM through an entry-wise manner which is parallelizable. (2) We choose a proximal algorithm to optimize FASJEM. This improves the computational efficiency from O(Kp(3)) to O(Kp(2)) and reduces the memory requirement from O(Kp(2)) to O(K). (3) We theoretically prove that FASJEM achieves a consistent estimation with a convergence rate of O(log(Kp)/n(tot)). On several synthetic and four real-world datasets, FASJEM shows significant improvements over baselines on accuracy, computational complexity and memory costs.",,,,,,"Gao, Ji/0000-0002-4026-8138",,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,1168,1177,,,,,,,,,,,,,,,,WOS:000509368500125,0
C,"Hamm, J",,"Lebanon, G; Vishwanathan, SVN",,"Hamm, Jihun",,,Preserving Privacy of Continuous High-dimensional Data with Minimax Filters,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"Preserving privacy of high-dimensional and continuous data such as images or biometric data is a challenging problem. This paper formulates this problem as a learning game between three parties: 1) data contributors using a filter to sanitize data samples, 2) a cooperative data aggregator learning a target task using the filtered samples, and 3) an adversary learning to identify contributors using the same filtered samples. Minimax filters that achieve the optimal privacy-utility trade-off from broad families of filters and loss/classifiers are defined, and algorithms for learning the filers in batch or distributed settings are presented. Experiments with several real-world tasks including facial expression recognition, speech emotion recognition, and activity recognition from motion, show that the minimax filter can simultaneously achieve similar or better target task accuracy and lower privacy risk, often significantly lower than previous methods.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,324,332,,,,,,,,,,,,,,,,WOS:000508399700036,0
C,"Jamieson, K; Katariya, S; Deshpande, A; Nowak, R",,"Lebanon, G; Vishwanathan, SVN",,"Jamieson, Kevin; Katariya, Sumeet; Deshpande, Atul; Nowak, Robert",,,Sparse Dueling Bandits,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"The dueling bandit problem is a variation of the classical multi-armed bandit in which the allowable actions are noisy comparisons between pairs of arms. This paper focuses on a new approach for finding the best arm according to the Borda criterion using noisy comparisons. We prove that in the absence of structural assumptions, the sample complexity of this problem is proportional to the sum of the inverse squared gaps between the Borda scores of each suboptimal arm and the best arm. We explore this dependence further and consider structural constraints on the pairwise comparison matrix (a particular form of sparsity natural to this problem) that can significantly reduce the sample complexity. This motivates a new algorithm called Successive Elimination with Comparison Sparsity (SECS) that exploits sparsity to find the Borda winner using fewer samples than standard algorithms. We also evaluate the new algorithm experimentally with synthetic and real data. The results show that the sparsity model and the new algorithm can provide significant improvements over standard approaches.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,416,424,,,,,,,,,,,,,,,,WOS:000508399700046,0
C,"Takahashi, R; Morimura, T",,"Lebanon, G; Vishwanathan, SVN",,"Takahashi, Rikiya; Morimura, Tetsuro",,,Predicting Preference Reversals via Gaussian Process Uncertainty Aversion,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"Modeling of a product or service's attractiveness as a function of its own attributes (e.g., price and quality) is one of the foundations in econometric forecasts, which have been provided with an assumption that each human rationally has a consistent preference order among his choice decisions. Yet the preference orders by real humans become irrationally reversed, when the choice set of available options is manipulated. In order to accurately predict choice decisions involving preference reversals, which existing econometric methods have failed to incorporate, the authors introduce a new cognitive choice model whose parameters are efficiently fitted with a global convex optimization algorithm. The proposed model captures each human as a Bayesian decision maker facing a mental conflict between objective evaluation samples and a subjective prior, where the underlying objective evaluation function is rationally independent from contexts while the subjective prior is irrationally determined by each choice set. As the key idea to analytically handle the irrationality and to yield the convex optimization, the Bayesian decision mechanism is implemented as a closed-form Gaussian process regression using similarities among the available options in each context. By explaining the irrational decisions as a consequence of averting uncertainty, the proposed model outperformed the existing econometric models in predicting the irrational choice decisions recorded in real-world datasets.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,958,967,,,,,,,,,,,,,,,,WOS:000508399700105,0
C,"Calandriello, D; Lazaric, A; Valko, M",,"Singh, A; Zhu, J",,"Calandriello, Daniele; Lazaric, Alessandro; Valko, Michal",,,Distributed Adaptive Sampling for Kernel Matrix Approximation,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Most kernel-based methods, such as kernel regression, kernel PCA, ICA, or k-means clustering, do not scale to large datasets, because constructing and storing the kernel matrix K-n requires at least O(n(2)) time and space for n samples. Recent works [1, 9] show that sampling points with replacement according to their ridge leverage scores (RLS) generates small dictionaries of relevant points with strong spectral approximation guarantees for K-n. The drawback of RLS-based methods is that computing exact RLS requires constructing and storing the whole kernel matrix. In this paper, we introduce SQUEAK, a new algorithm for kernel approximation based on RLS sampling that sequentially processes the dataset, storing a dictionary which creates accurate kernel matrix approximations with a number of points that only depends on the effective dimension d(eff) (gamma) of the dataset. Moreover since all the RLS estimations are efficiently performed using only the small dictionary, SQUEAK never constructs the whole matrix Kn, runs in linear time (O) over tilde (nd(eff) (gamma)(3)) w.r.t. n, and requires only a single pass over the dataset. We also propose a parallel and distributed version of SQUEAK achieving similar accuracy in as little as (O) over tilde (log(n)d(eff)(gamma)(3)) time.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,1421,1429,,,,,,,,,,,,,,,,WOS:000509368500152,0
C,"Gardner, JR; Guo, C; Weinberger, KQ; Garnett, R; Grosse, R",,"Singh, A; Zhu, J",,"Gardner, Jacob R.; Guo, Chuan; Weinberger, Kilian Q.; Garnett, Roman; Grosse, Roger",,,Discovering and Exploiting Additive Structure for Bayesian Optimization,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Bayesian optimization has proven invaluable for black-box optimization of expensive functions. Its main limitation is its exponential complexity with respect to the dimensionality of the search space using typical kernels. Luckily, many objective functions can be decomposed into additive sub-problems, which can be optimized independently. We investigate how to automatically discover such (typically unknown) additive structure while simultaneously exploiting it through Bayesian optimization. We propose an efficient algorithm based on Metropolis-Hastings sampling and demonstrate its efficacy empirically on synthetic and real-world data sets. Throughout all our experiments we reliably discover hidden additive structure whenever it exists and exploit it to yield significantly faster convergence.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,1311,1319,,,,,,,,,,,,,,,,WOS:000509368500140,0
C,"McDonald, DJ",,"Singh, A; Zhu, J",,"McDonald, Daniel J.",,,Minimax density estimation for growing dimension,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"This paper presents minimax rates for density estimation when the data dimension d is allowed to grow with the number of observations n rather than remaining fixed as in previous analyses. We prove a non-asymptotic lower bound which gives the worst-case rate over standard classes of smooth densities, and we show that kernel density estimators achieve this rate. We also give oracle choices for the bandwidth and derive the fastest rate d can grow with n to maintain estimation consistency.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,194,203,,,,,,,,,,,,,,,,WOS:000509368500022,0
C,"Nitanda, A; Suzuki, T",,"Singh, A; Zhu, J",,"Nitanda, Atsushi; Suzuki, Taiji",,,Stochastic Difference of Convex Algorithm and its Application to Training Deep Boltzmann Machines,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Difference of convex functions (DC) programming is an important approach to nonconvex optimization problems because these structures can be encountered in several fields. Effective optimization methods, called DC algorithms, have been developed in deterministic optimization literature. In machine learning, a lot of important learning problems such as the Boltzmann machines (BMs) can be formulated as DC programming. However, there is no DC-like algorithm guaranteed by convergence rate analysis for stochastic problems that are more suitable settings for machine learning tasks. In this paper, we propose a stochastic variant of DC algorithm and give computational complexities to converge to a stationary point under several situations. Moreover, we show our method includes expectation-maximization (EM) and Monte Carlo EM (MCEM) algorithm as special cases on training BMs. In other words, we extend EM/MCEM algorithm to more effective methods from DC viewpoint with theoretical convergence guarantees. Experimental results indicate that our method performs well for training binary restricted Boltzmann machines and deep Boltzmann machines without pre-training.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,470,478,,,,,,,,,,,,,,,,WOS:000509368500051,0
C,"Rowland, M; Pacchiano, A; Weller, A",,"Singh, A; Zhu, J",,"Rowland, Mark; Pacchiano, Aldo; Weller, Adrian",,,Conditions Beyond Treewidth for Tightness of Higher-order LP Relaxations,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Linear programming (LP) relaxations are a popular method to attempt to find a most likely configuration of a discrete graphical model. If a solution to the relaxed problem is obtained at an integral vertex then the solution is guaranteed to be exact and we say that the relaxation is tight. We consider binary pairwise models and introduce new methods which allow us to demonstrate refined conditions for tightness of LP relaxations in the Sherali-Adams hierarchy. Our results include showing that for higher order LP relaxations, treewidth is not precisely the right way to characterize tightness. This work is primarily theoretical, with insights that can improve efficiency in practice.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,10,18,,,,,,,,,,,,,,,,WOS:000509368500002,0
C,"Sinha, K; Keivani, O",,"Singh, A; Zhu, J",,"Sinha, Kaushik; Keivani, Omid",,,Sparse Randomized Partition Trees for Nearest Neighbor Search,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Randomized partition trees have recently been shown to be very effective in solving nearest neighbor search problem. In spite of enjoying strong theoretical guarantee, it suffers from high space complexity, since each internal node of the tree needs to store a d dimensional projection direction leading to a O(nd) space complexity for a dataset of size n. Inspired by the fast Johnson-Lindenstrauss transform, in this paper, we propose a sparse version of randomized partition tree where each internal node needs to store only a few non-zero entries, as opposed to all d entries, leading to significant space savings without sacrificing much in terms of nearest neighbor search accuracy. As a by product of this, query time of our proposed method is slightly better than that of its non-sparse counterpart for large dataset size. Our theoretical results indicate that our proposed method enjoys the same theoretical guarantee as that of the original non-sparse RP-tree. Experimental evaluations on four real world dataset strongly suggest that nearest neighbor search performance of our proposed sparse RP-tree is very similar to that of its non-sparse counterpart in terms of accuracy and number of retrieved points.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,681,689,,,,,,,,,,,,,,,,WOS:000509368500073,0
C,"Zhang, YC; Lee, JD; Wainwright, MJ; Jordan, MI",,"Singh, A; Zhu, J",,"Zhang, Yuchen; Lee, Jason D.; Wainwright, Martin J.; Jordan, Michael I.",,,On the Learnability of Fully-connected Neural Networks,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Despite the empirical success of deep neural networks, there is limited theoretical understanding of the learnability of these models with respect to polynomial-time algorithms. In this paper, we characterize the learnability of fully-connected neural networks via both positive and negative results. We focus on l(1)-regularized networks, where the l(1)-norm of the incoming weights of every neuron is assumed to be bounded by a constant B > 0. Our first result shows that such networks are properly learnable in poly(n, d, exp(1/epsilon(2))) time, where n and d are the sample size and the input dimension, and epsilon > 0 is the gap to optimality. The bound is achieved by repeatedly sampling over a low-dimensional manifold so as to ensure approximate optimality, but avoids the exp(d) cost of exhaustively searching over the parameter space. We also establish a hardness result showing that the exponential dependence on 1/epsilon is unavoidable unless RP = NP. Our second result shows that the exponential dependence on 1/epsilon can be avoided by exploiting the underlying structure of the data distribution. In particular, if the positive and negative examples can be separated with margin gamma > 0 by an unknown neural network, then the network can be learned in poly(n, d, 1/epsilon) time. The bound is achieved by an ensemble method which uses the first algorithm as a weak learner. We further show that the separability assumption can be weakened to tolerate noisy labels. Finally, we show that the exponential dependence on 1/gamma is unimprovable under a certain cryptographic assumption.",,,,,"Zhang, Yuchen/GYI-8858-2022","Jordan, Michael/0000-0001-8935-817X",,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,83,91,,,,,,,,,,,,,,,,WOS:000509368500010,0
C,"Bhaskara, A; Suresh, AT; Zadimoghaddam, M",,"Lebanon, G; Vishwanathan, SVN",,"Bhaskara, Aditya; Suresh, Ananda Theertha; Zadimoghaddam, Morteza",,,Sparse Solutions to Nonnegative Linear Systems and Applications,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"We give an efficient algorithm for finding sparse approximate solutions to linear systems of equations with nonnegative coefficients. Unlike most known results for sparse recovery, we do not require any assumption on the matrix other than non-negativity. Our algorithm is combinatorial in nature, inspired by techniques for the set cover problem, as well as the multiplicative weight update method. We then present a natural application to learning mixture models in the PAC framework. For learning a mixture of k axisaligned Gaussians in d dimensions, we give an algorithm that outputs a mixture of O(k/epsilon(3)) Gaussians that is epsilon-close in statistical distance to the true distribution, without any separation assumptions. The time and sample complexity is roughly O(kd/epsilon(3))(d). This is polynomial when d is constant precisely the regime in which known methods fail to identify the components efficiently. Given that non-negativity is a natural assumption, we believe that our result may find use in other settings in which we wish to approximately explain data using a small number of a (large) candidate set of components.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,83,92,,,,,,,,,,,,,,,,WOS:000508399700010,0
C,"Guo, FJ; Blundell, C; Wallach, H; Heller, K",,"Lebanon, G; Vishwanathan, SVN",,"Guo, Fangjian; Blundell, Charles; Wallach, Hanna; Heller, Katherine",,,The Bayesian Echo Chamber: Modeling Social Influence via Linguistic Accommodation,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"We present the Bayesian Echo Chamber, a new Bayesian generative model for social interaction data. By modeling the evolution of people's language usage over time, this model discovers latent influence relationships between them. Unlike previous work on inferring influence, which has primarily focused on simple temporal dynamics evidenced via turn-taking behavior, our model captures more nuanced influence relationships, evidenced via linguistic accommodation patterns in interaction content. The model, which is based on a discrete analog of the multivariate Hawkes process, permits a fully Bayesian inference algorithm. We validate our model's ability to discover latent influence patterns using transcripts of arguments heard by the US Supreme Court and the movie 12 Angry Men. We showcase our model's capabilities by using it to infer latent influence patterns from Federal Open Market Committee meeting transcripts, demonstrating state-of-the-art performance at uncovering social dynamics in group discussions.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,315,323,,,,,,,,,,,,,,,,WOS:000508399700035,0
C,"Hensman, J; Matthews, AGD; Ghahramani, Z",,"Lebanon, G; Vishwanathan, SVN",,"Hensman, James; Matthews, Alex G. de G.; Ghahramani, Zoubin",,,Scalable Variational Gaussian Process Classification,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"Gaussian process classification is a popular method with a number of appealing properties. We show how to scale the model within a variational inducing point framework, outperforming the state of the art on benchmark datasets. Importantly, the variational formulation can be exploited to allow classification in problems with millions of data points, as we demonstrate in experiments.",,,,,,"Hensman, James/0000-0002-4989-3589",,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,351,360,,,,,,,,,,,,,,,,WOS:000508399700039,0
C,"Li, B; Vorobeychik, Y",,"Lebanon, G; Vishwanathan, SVN",,"Li, Bo; Vorobeychik, Yevgeniy",,,Scalable Optimization of Randomized Operational Decisions in Adversarial Classification Settings,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"When learning, such as classification, is used in adversarial settings, such as intrusion detection, intelligent adversaries will attempt to evade the resulting policies. The literature on adversarial machine learning aims to develop learning algorithms which are robust to such adversarial evasion, but exhibits two significant limitations: a) failure to account for operational constraints and b) a restriction that decisions are deterministic. To overcome these limitations, we introduce a conceptual separation between learning, used to infer attacker preferences, and operational decisions, which account for adversarial evasion, enforce operational constraints, and naturally admit randomization. Our approach gives rise to an intractably large linear program. To overcome scalability limitations, we introduce a novel method for estimating a compact parity basis representation for the operational decision function. Additionally, we develop an iterative constraint generation approach which embeds adversary's best response calculation, to arrive at a scalable algorithm for computing near-optimal randomized operational decisions. Extensive experiments demonstrate the efficacy of our approach.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,599,607,,,,,,,,,,,,,,,,WOS:000508399700066,0
C,"Alabdulmohsin, I",,"Singh, A; Zhu, J",,"Alabdulmohsin, Ibrahim",,,An information-theoretic route from generalization in expectation to generalization in probability,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"One fundamental goal in any learning algorithm is to mitigate its risk for overfitting. Mathematically, this requires that the learning algorithm enjoys a small generalization risk, which is defined either in expectation or in probability. Both types of generalization are commonly used in the literature. For instance, generalization in expectation has been used to analyze algorithms, such as ridge regression and SGD, whereas generalization in probability is used in the VC theory, among others. Recently, a third notion of generalization has been studied, called uniform generalization, which requires that the generalization risk vanishes uniformly in expectation across all bounded parametric losses. It has been shown that uniform generalization is, in fact, equivalent to an information-theoretic stability constraint, and that it recovers classical results in learning theory. It is achievable under various settings, such as sample compression schemes, finite hypothesis spaces, finite domains, and differential privacy. However, the relationship between uniform generalization and concentration remained unknown. In this paper, we answer this question by proving that, while a generalization in expectation does not imply a generalization in probability, a uniform generalization in expectation does imply concentration. We establish a chain rule for the uniform generalization risk of the composition of hypotheses and use it to derive a large deviation bound. Finally, we prove that the bound is tight.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,92,100,,,,,,,,,,,,,,,,WOS:000509368500011,0
C,"Bojarski, M; Choromanska, A; Choromanski, K; Fagan, F; Gouy-Pailler, C; Morvan, A; Sakr, N; Sarlos, T; Atif, J",,"Singh, A; Zhu, J",,"Bojarski, Mariusz; Choromanska, Anna; Choromanski, Krzysztof; Fagan, Francois; Gouy-Pailler, Cedric; Morvan, Anne; Sakr, Nourhan; Sarlos, Tamas; Atif, Jamal",,,Structured adaptive and random spinners for fast machine learning computations,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"We consider an efficient computational framework for speeding up several machine learning algorithms with almost no loss of accuracy. The proposed framework relies on projections via structured matrices that we call Structured Spinners, which are formed as products of three structured matrix-blocks that incorporate rotations. The approach is highly generic, i.e. i) structured matrices under consideration can either be fully-randomized or learned, ii) our structured family contains as special cases all previously considered structured schemes, iii) the setting extends to the non-linear case where the projections are followed by non-linear functions, and iv) the method finds numerous applications including kernel approximations via random feature maps, dimensionality reduction algorithms, new fast cross-polytope LSH techniques, deep learning, convex optimization algorithms via Newton sketches, quantization with random projection trees, and more. The proposed framework comes with theoretical guarantees characterizing the capacity of the structured model in reference to its unstructured counterpart and is based on a general theoretical principle that we describe in the paper. As a consequence of our theoretical analysis, we provide the first theoretical guarantees for one of the most efficient existing LSH algorithms based on the HD3HD2HD1 structured matrix [Andoni et al., 2015]. The exhaustive experimental evaluation confirms the accuracy and efficiency of structured spinners for a variety of different applications.",,,,,"Gouy-Pailler, Cdric/ABF-8297-2021","Gouy-Pailler, Cdric/0000-0003-1298-7845",,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,1020,1029,,,,,,,,,,,,,,,,WOS:000509368500109,0
C,"Bucur, IG; Claassen, T; Heskes, T",,"Singh, A; Zhu, J",,"Bucur, Ioan Gabriel; Claassen, Tom; Heskes, Tom",,,Robust Causal Estimation in the Large-Sample Limit without Strict Faithfulness,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Causal effect estimation from observational data is an important and much studied research topic. The instrumental variable (IV) and local causal discovery (LCD) patterns are canonical examples of settings where a closed-form expression exists for the causal effect of one variable on another, given the presence of a third variable. Both rely on faithfulness to infer that the latter only influences the target effect via the cause variable. In reality, it is likely that this assumption only holds approximately and that there will be at least some form of weak interaction. This brings about the paradoxical situation that, in the large-sample limit, no predictions are made, as detecting the weak edge invalidates the setting. We introduce an alternative approach by replacing strict faithfulness with a prior that reflects the existence of many 'weak' (irrelevant) and 'strong' interactions. We obtain a posterior distribution over the target causal effect estimator which shows that, in many cases, we can still make good estimates. We demonstrate the approach in an application on a simple linearGaussian setting, using the MultiNest sampling algorithm, and compare it with established techniques to show our method is robust even when strict faithfulness is violated.",,,,,"Heskes, Tom/A-1443-2010","Heskes, Tom/0000-0002-3398-5235",,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,1523,1531,,,,,,,,,,,,,,,,WOS:000509368500163,0
C,"Graham, MM; Storkey, AJ",,"Singh, A; Zhu, J",,"Graham, Matthew M.; Storkey, Amos J.",,,Asymptotically exact inference in differentiable generative models,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,Many generative models can be expressed as a differentiable function of random inputs drawn from some simple probability density. This framework includes both deep generative architectures such as Variational Autoencoders and a large class of procedurally defined simulator models. We present a method for performing efficient MCMC inference in such models when conditioning on observations of the model output. For some models this offers an asymptotically exact inference method where Approximate Bayesian Computation might otherwise be employed. We use the intuition that inference corresponds to integrating a density across the manifold corresponding to the set of inputs consistent with the observed outputs. This motivates the use of a constrained variant of Hamiltonian Monte Carlo which leverages the smooth geometry of the manifold to coherently move between inputs exactly consistent with observations. We validate the method by performing inference tasks in a diverse set of models.,,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,499,508,,,,,,,,,,,,,,,,WOS:000509368500054,0
C,"Hong, DZ; Gu, QQ; Whitehouse, K",,"Singh, A; Zhu, J",,"Hong, Dezhi; Gu, Quanquan; Whitehouse, Kamin",,,High-dimensional Time Series Clustering via Cross-Predictability,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"The key to time series clustering is how to characterize the similarity between any two time series. In this paper, we explore a new similarity metric called cross-predictability: the degree to which a future value in each time series is predicted by past values of the others. However, it is challenging to estimate such cross-predictability among time series in the high-dimensional regime, where the number of time series is much larger than the length of each time series. We address this challenge with a sparsity assumption: only time series in the same cluster have significant cross-predictability with each other. We demonstrate that this approach is computationally attractive, and provide a theoretical proof that the proposed algorithm will identify the correct clustering structure with high probability under certain conditions. To the best of our knowledge, this is the first practical high-dimensional time series clustering algorithm with a provable guarantee. We evaluate with experiments on both synthetic data and real-world data, and results indicate that our method can achieve more than 80% clustering accuracy on real-world data, which is 20% higher than the state-of-art baselines.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,642,651,,,,,,,,,,,,,,,,WOS:000509368500069,0
C,"Chaudhuri, S; Tewari, A",,"Lebanon, G; Vishwanathan, SVN",,"Chaudhuri, Sougata; Tewari, Ambuj",,,Online Ranking with Top-1 Feedback,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"We consider a setting where a system learns to rank a fixed set of m items. The goal is produce good item rankings for users with diverse interests who interact online with the system for T rounds. We consider a novel top-1 feedback model: at the end of each round, the relevance score for only the top ranked object is revealed. However, the performance of the system is judged on the entire ranked list. We provide a comprehensive set of results regarding learnability under this challenging setting. For PairwiseLoss and DCG, two popular ranking measures, we prove that the minimax regret is Theta(T-2/3). Moreover, the minimax regret is achievable using an efficient strategy that only spends O(mlogm) time per round. The same efficient strategy achieves Theta(T-2/3) regret for Precision@k. Surprisingly, we show that for normalized versions of these ranking measures, i.e., AUC, NDCG & MAP, no online ranking algorithm can have sublinear regret.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,129,137,,,,,,,,,,,,,,,,WOS:000508399700015,0
C,"Chen, WL; Chen, YX; Weinberger, KQ",,"Lebanon, G; Vishwanathan, SVN",,"Chen, Wenlin; Chen, Yixin; Weinberger, Kilian Q.",,,Filtered Search for Submodular Maximization with Controllable Approximation Bounds,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"Most existing submodular maximization algorithms provide theoretical guarantees with approximation bounds. However, in many cases, users may be interested in an anytime algorithm that can offer a flexible tradeoff between computation time and optimality guarantees. In this paper, we propose a filtered search (FS) framework that allows the user to set an arbitrary approximation bound guarantee with a tunable knob, from 0 (arbitrarily bad) to 1 (globally optimal). FS naturally handles monotone and non-monotone functions as well as unconstrained problems and problems with cardinality, matroid, and knapsack constraints. Further, it can also be applied to (non-negative) non-submodular functions and still gives controllable approximation bounds based on their submodularity ratio. Finally, FS encompasses the greedy algorithm as a special case. Our framework is based on theory in A* search, but is substantially more efficient because it only requires heuristics that are critically admissible (CA) rather than admissible-a condition that gives more effective pruning and is substantially easier to implement.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,156,164,,,,,,,,,,,,,,,,WOS:000508399700018,0
C,"Katsumata, S; Takeda, A",,"Lebanon, G; Vishwanathan, SVN",,"Katsumata, Shuichi; Takeda, Akiko",,,Robust Cost Sensitive Support Vector Machine,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"In this paper we consider robust classifications and show equivalence between the regularized classifications. In general, robust classifications are used to create a classifier robust to data by taking into account the uncertainty of the data. Our result shows that regularized classifications inherit robustness and provide reason on why some regularized classifications tend to be robust against data. Although most robust classification problems assume that every uncertain data lie within an identical bounded set, this paper considers a generalized model where the sizes of the bounded sets are different for each data. These models can be transformed into regularized classification models where the penalties for each data are assigned according to their losses. We see that considering such models opens up for new applications. For an example, we show that this robust classification technique can be used for Imbalanced Data Learning. We conducted experimentation with actual data and compared it with other IDL algorithms such as Cost Sensitive SVMs. This is a novel usage for the robust classification scheme and encourages it to be a suitable candidate for imbalanced data learning.",,,,,"Katsumata, Shuichi/M-7656-2016","Katsumata, Shuichi/0000-0002-8496-0476",,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,434,443,,,,,,,,,,,,,,,,WOS:000508399700048,0
C,"Liu, K; Bellet, A; Sha, F",,"Lebanon, G; Vishwanathan, SVN",,"Liu, Kuan; Bellet, Aurelien; Sha, Fei",,,Similarity Learning for High-Dimensional Sparse Data,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"A good measure of similarity between data points is crucial to many tasks in machine learning. Similarity and metric learning methods learn such measures automatically from data, but they do not scale well respect to the dimensionality of the data. In this paper, we propose a method that can learn efficiently similarity measure from high-dimensional sparse data. The core idea is to parameterize the similarity measure as a convex combination of rank-one matrices with specific sparsity structures. The parameters are then optimized with an approximate Frank-Wolfe procedure to maximally satisfy relative similarity constraints on the training data. Our algorithm greedily incorporates one pair of features at a time into the similarity measure, providing an efficient way to control the number of active features and thus reduce overfitting. It enjoys very appealing convergence guarantees and its time and memory complexity depends on the sparsity of the data instead of the dimension of the feature space. Our experiments on real-world high-dimensional datasets demonstrate its potential for classification, dimensionality reduction and data exploration.",,,,,,"Bellet, Aurelien/0000-0003-3440-1251",,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,653,662,,,,,,,,,,,,,,,,WOS:000508399700072,0
C,"Sasaki, H; Noh, YK; Sugiyama, M",,"Lebanon, G; Vishwanathan, SVN",,"Sasaki, Hiroaki; Noh, Yung-Kyun; Sugiyama, Masashi",,,Direct Density-Derivative Estimation and Its Application in KL-Divergence Approximation,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"Estimation of density derivatives is a versatile tool in statistical data analysis. A naive approach is to first estimate the density and then compute its derivative. However, such a two-step approach does not work well because a good density estimator does not necessarily mean a good density-derivative estimator. In this paper, we give a direct method to approximate the density derivative without estimating the density itself. Our proposed estimator allows analytic and computationally efficient approximation of multi-dimensional high-order density derivatives, with the ability that all hyper-parameters can be chosen objectively by cross-validation. We further show that the proposed density-derivative estimator is useful in improving the accuracy of non-parametric KL-divergence estimation via metric learning. The practical superiority of the proposed method is experimentally demonstrated in change detection and feature selection.",,,,,"Sugiyama, Masashi/AEO-1176-2022; Sasaki, Hiroaki/GXG-5024-2022","Sugiyama, Masashi/0000-0001-6658-6743; ",,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,809,818,,,,,,,,,,,,,,,,WOS:000508399700089,0
C,"Abeille, M; Lazaric, A",,"Singh, A; Zhu, J",,"Abeille, Marc; Lazaric, Alessandro",,,Thompson Sampling for Linear-Quadratic Control Problems,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"We consider the exploration-exploitation tradeoff in linear quadratic (LQ) control problems, where the state dynamics is linear and the cost function is quadratic in states and controls. We analyze the regret of Thompson sampling (TS) (a.k.a. posterior-sampling for reinforcement learning) in the frequentist setting, i.e., when the parameters characterizing the LQ dynamics are fixed. Despite the empirical and theoretical success in a wide range of problems from multi-armed bandit to linear bandit, we show that when studying the frequentist regret TS in control problems, we need to trade-off the frequency of sampling optimistic parameters and the frequency of switches in the control policy. This results in an overall regret of O(T-2/3), which is significantly worse than the regret O(root T) achieved by the optimism-in-face-of-uncertainty algorithm in LQ control problems.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,1246,1254,,,,,,,,,,,,,,,,WOS:000509368500133,0
C,"Saad, F; Mansinghka, V",,"Singh, A; Zhu, J",,"Saad, Feras; Mansinghka, Vikash",,,"Detecting Dependencies in Sparse, Multivariate Databases Using Probabilistic Programming and Non-parametric Bayes","ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Datasets with hundreds of variables and many missing values are commonplace. In this setting, it is both statistically and computationally challenging to detect true predictive relationships between variables and also to suppress false positives. This paper proposes an approach that combines probabilistic programming, information theory, and non-parametric Bayes. It shows how to use Bayesian non-parametric modeling to (i) build an ensemble of joint probability models for all the variables; (ii) efficiently detect marginal independencies; and (iii) estimate the conditional mutual information between arbitrary subsets of variables, subject to a broad class of constraints. Users can access these capabilities using BayesDB, a probabilistic programming platform for probabilistic data analysis, by writing queries in a simple, SQL-like language. This paper demonstrates empirically that the method can (i) detect context-specific (in)dependencies on challenging synthetic problems and (ii) yield improved sensitivity and specificity over baselines from statistics and machine learning, on a real-world database of over 300 sparsely observed indicators of macroeconomic development and public health.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,632,641,,,,,,,,,,,,,,,,WOS:000509368500068,0
C,"Shilton, A; Gupta, S; Rana, S; Venkatesh, S",,"Singh, A; Zhu, J",,"Shilton, Alistair; Gupta, Sunil; Rana, Santu; Venkatesh, Svetha",,,Regret Bounds for Transfer Learning in Bayesian Optimisation,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,This paper studies the regret bound of two transfer learning algorithms in Bayesian optimisation. The first algorithm models any difference between the source and target functions as a noise process. The second algorithm proposes a new way to model the difference between the source and target as a Gaussian process which is then used to adapt the source data. We show that in both cases the regret bounds are tighter than in the no transfer case. We also experimentally compare the performance of these algorithms relative to no transfer learning and demonstrate benefits of transfer learning.,,,,,,"Shilton, Alistair/0000-0002-0849-3271; gupta, sunil/0000-0002-4669-9940; venkatesh, svetha/0000-0001-8675-6631; Rana, Santu/0000-0003-2247-850X",,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,307,315,,,,,,,,,,,,,,,,WOS:000509368500034,0
C,"Vinyes, M; Obozinski, G",,"Singh, A; Zhu, J",,"Vinyes, Marina; Obozinski, Guillaume",,,Fast column generation for atomic norm regularization,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"We consider optimization problems that consist in minimizing a quadratic function under an atomic norm(1) regularization or constraint. In the line of work on conditional gradient algorithms, we show that the fully corrective Frank-Wolfe (FCFW) algorithm - which is most naturally reformulated as a column generation algorithm in the regularized case can be made particularly efficient for difficult problems in this family by solving the simplicial or conical subproblems produced by FCFW using a special instance of a classical active set algorithm for quadratic programming (Nocedal and Wright, 2006) that generalizes the min-norm point algorithm (Wolfe, 1976). Our experiments show that the algorithm takes advantages of warm-starts and of the sparsity induced by the norm, displays fast linear convergence, and clearly outperforms the state-of-the-art, for both complex and classical norms, including the standard group Lasso.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,547,556,,,,,,,,,,,,,,,,WOS:000509368500059,0
C,"Gan, Z; Henao, R; Carlson, D; Carin, L",,"Lebanon, G; Vishwanathan, SVN",,"Gan, Zhe; Henao, Ricardo; Carlson, David; Carin, Lawrence",,,Learning Deep Sigmoid Belief Networks with Data Augmentation,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"Deep directed generative models are developed. The multi-layered model is designed by stacking sigmoid belief networks, with sparsity-encouraging priors placed on the model parameters. Learning and inference of layer-wise model parameters are implemented in a Bayesian setting. By exploring the idea of data augmentation and introducing auxiliary Polya-Gamma variables, simple and efficient Gibbs sampling and mean-field variational Bayes (VB) inference are implemented. To address large-scale datasets, an online version of VB is also developed. Experimental results are presented for three publicly available datasets: MNIST, Caltech 101 Silhouettes and OCR letters.",,,,,,"Henao, Ricardo/0000-0003-4980-845X; Carlson, David/0000-0003-1005-6385",,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,268,276,,,,,,,,,,,,,,,,WOS:000508399700030,0
C,"Cowley, BR; Semedo, JD; Zandvakili, A; Smith, MA; Kohn, A; Yu, BM",,"Singh, A; Zhu, J",,"Cowley, Benjamin R.; Semedo, Joao D.; Zandvakili, Amin; Smith, Matthew A.; Kohn, Adam; Yu, Byron M.",,,Distance Covariance Analysis,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"We propose a dimensionality reduction method to identify linear projections that capture interactions between two or more sets of variables. The method, distance covariance analysis (DCA), can detect both linear and nonlinear relationships, and can take dependent variables into account. On previous testbeds and a new testbed that systematically assesses the ability to detect both linear and nonlinear interactions, DCA performs better than or comparable to existing methods, while being one of the fastest methods. To showcase the versatility of DCA, we also applied it to three different neurophysiological datasets.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,242,251,,,,,,,,,,,,,,,,WOS:000509368500027,0
C,"Korba, A; Clemencon, S; Sibony, E",,"Singh, A; Zhu, J",,"Korba, Anna; Clemencon, Stephan; Sibony, Eric",,,A Learning Theory of Ranking Aggregation,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Originally formulated in Social Choice theory, Ranking Aggregation, also referred to as Consensus Ranking, has motivated the development of numerous statistical models since the middle of the 20th century. Recently, the analysis of ranking/preference data has been the subject of a renewed interest in machine-learning, boosted by modern applications such as meta-search engines, giving rise to the design of various scalable algorithmic approaches for approximately computing ranking medians, viewed as solutions of a discrete (generally NP-hard) minimization problem. This paper develops a statistical learning theory for ranking aggregation in a general probabilistic setting (avoiding any rigid ranking model assumptions), assessing the generalization ability of empirical ranking medians. Universal rate bounds are established and the situations where convergence occurs at an exponential rate are fully characterized. Minimax lower bounds are also proved, showing that the rate bounds we obtain are optimal.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,1001,1010,,,,,,,,,,,,,,,,WOS:000509368500107,0
C,"Liu, ZQ; Smola, AJ; Soska, K; Wang, YX; Zheng, QH",,"Singh, A; Zhu, J",,"Liu, Ziqi; Smola, Alexander J.; Soska, Kyle; Wang, Yu-Xiang; Zheng, Qinghua",,,Attributing Hacks,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"In this paper, we describe an algorithm for estimating the provenance of hacks on websites. That is, given properties of sites and the temporal occurrence of attacks, we are able to attribute individual attacks to joint causes and vulnerabilities, as well as estimating the evolution of these vulnerabilities over time. Specifically, we use hazard regression with a time-varying additive hazard function parameterized in a generalized linear form. The activation coefficients on each feature are continuous-time functions over time. We formulate the problem of learning these functions as a constrained variational maximum likelihood estimation problem with total variation penalty and show that the optimal solution is a 0th order spline (a piecewise constant function) with a finite number of adaptively chosen knots. This allows the inference problem to be solved efficiently and at scale by solving a finite dimensional optimization problem. Extensive experiments on real data sets show that our method significantly outperforms Cox's proportional hazard model. We also conduct case studies and verify that the fitted functions are indeed recovering vulnerable features.",,,,,,"Wang, Yu-Xiang/0000-0002-6403-212X",,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,794,802,,,,,,,,,,,,,,,,WOS:000509368500085,0
C,"Wang, MY; Song, YS",,"Singh, A; Zhu, J",,"Wang, Miaoyan; Song, Yun S.",,,Tensor Decompositions via Two-Mode Higher-Order SVD (HOSVD),"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Tensor decompositions have rich applications in statistics and machine learning, and developing efficient, accurate algorithms for the problem has received much attention recently. Here, we present a new method built on Kruskal's uniqueness theorem to decompose symmetric, nearly orthogonally decomposable tensors. Unlike the classical higher-order singular value decomposition which unfolds a tensor along a single mode, we consider unfoldings along two modes and use rank-1 constraints to characterize the underlying components. This tensor decomposition method provably handles a greater level of noise compared to previous methods and achieves a high estimation accuracy. Numerical results demonstrate that our algorithm is robust to various noise distributions and that it performs especially favorably as the order increases.",,,,,,"Wang, Miaoyan/0000-0002-1287-3787",,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,614,622,,,,,,,,,,,,,,,,WOS:000509368500066,0
C,"Yamada, M; Takeuchi, K; Iwata, T; Shawe-Taylor, J; Kaski, S",,"Singh, A; Zhu, J",,"Yamada, Makoto; Takeuchi, Koh; Iwata, Tomoharu; Shawe-Taylor, John; Kaski, Samuel",,,Localized Lasso for High-Dimensional Regression,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"We introduce the localized Lasso, which learns models that both are interpretable and have a high predictive power in problems with high dimensionality d and small sample size n. More specifically, we consider a function defined by local sparse models, one at each data point. We introduce sample-wise network regularization to borrow strength across the models, and sample-wise exclusive group sparsity (a.k.a., l(1,2) norm) to introduce diversity into the choice of feature sets in the local models. The local models are interpretable in terms of similarity of their sparsity patterns. The cost function is convex, and thus has a globally optimal solution. Moreover, we propose a simple yet efficient iterative least-squares based optimization procedure for the localized Lasso, which does not need a tuning parameter, and is guaranteed to converge to a globally optimal solution. The solution is empirically shown to outperform alternatives for both simulated and genomic personalized/precision medicine data.",,,,,"Kaski, Samuel/B-6684-2008","Kaski, Samuel/0000-0003-1925-9154; Klein, Nigel/0000-0003-3925-9258; Shawe-Taylor, John/0000-0002-2030-0073",,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,325,333,,,,,,,,,,,,,,,,WOS:000509368500036,0
C,"Gao, SY; Ver Steeg, G; Galstyan, A",,"Lebanon, G; Vishwanathan, SVN",,"Gao, Shuyang; Ver Steeg, Greg; Galstyan, Aram",,,Efficient Estimation of Mutual Information for Strongly Dependent Variables,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"We demonstrate that a popular class of non-parametric mutual information (MI) estimators based on k-nearest-neighbor graphs requires number of samples that scales exponentially with the true MI. Consequently, accurate estimation of MI between two strongly dependent variables is possible only for prohibitively large sample size. This important yet overlooked shortcoming of the existing estimators is due to their implicit reliance on local uniformity of the underlying joint distribution. We introduce a new estimator that is robust to local non-uniformity, works well with limited data, and is able to capture relationship strengths over many orders of magnitude. We demonstrate the superior performance of the proposed estimator on both synthetic and real-world data.",,,,,,"Ver Steeg, Greg/0000-0002-0793-141X; Galstyan, Aram/0000-0003-4215-0886",,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,277,286,,,,,,,,,,,,,,,,WOS:000508399700031,0
C,"Krishnamurthy, A; Kandasamy, K; Poczos, B; Wasserman, L",,"Lebanon, G; Vishwanathan, SVN",,"Krishnamurthy, Akshay; Kandasamy, Kirthevasan; Poczos, Barnabas; Wasserman, Larry",,,On Estimating L-2(2) Divergence,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"We give a comprehensive theoretical characterization of a nonparametric estimator for the L-2(2) divergence between two continuous distributions. We first bound the rate of convergence of our estimator, showing that it is root n-consistent provided the densities are sufficiently smooth. In this smooth regime, we then show that our estimator is asymptotically normal, construct asymptotic confidence intervals, and establish a Berry-Esseen style inequality characterizing the rate of convergence to normality. We also show that this estimator is minimax optimal.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,498,506,,,,,,,,,,,,,,,,WOS:000508399700055,0
C,"Lever, G; Stafford, R",,"Lebanon, G; Vishwanathan, SVN",,"Lever, Guy; Stafford, Ronnie",,,Modelling Policies in MDPs in Reproducing Kernel Hilbert Space,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"We consider modelling policies for MDPs in (vector-valued) reproducing kernel Hilbert function spaces (RKHS). This enables us to work non-parametrically in a rich function class, and provides the ability to learn complex policies. We present a framework for performing gradient-based policy optimization in the RKHS, deriving the functional gradient of the return for our policy, which has a simple form and can be estimated efficiently. The policy representation naturally focuses on the relevant region of state space defined by the policy trajectories, and does not rely on a-priori defined basis points; this can be an advantage in high dimensions where suitable basis points may be difficult to define a-priori. The method is adaptive in the sense that the policy representation will naturally adapt to the complexity of the policy being modelled, which is achieved with standard efficient sparsification tools in an RKHS. We argue that finding a good kernel on states can be easier then remetrizing a high dimensional feature space. We demonstrate the approach on benchmark domains and a simulated quadrocopter navigation task.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,590,598,,,,,,,,,,,,,,,,WOS:000508399700065,0
C,"Lucic, M; Ohannessian, MI; Karbasi, A; Krause, A",,"Lebanon, G; Vishwanathan, SVN",,"Lucic, Mario; Ohannessian, Mesrob, I; Karbasi, Amin; Krause, Andreas",,,"Tradeoffs for Space, Time, Data and Risk in Unsupervised Learning","ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"Faced with massive data, is it possible to trade off (statistical) risk, and (computational) space and time? This challenge lies at the heart of large-scale machine learning. Using k-means clustering as a prototypical unsupervised learning problem, we show how we can strategically summarize the data (control space) in order to trade off risk and time when data is generated by a probabilistic model. Our summarization is based on coreset constructions from computational geometry. We also develop an algorithm, TRAM, to navigate the space/time/data/risk tradeoff in practice. In particular, we show that for a fixed risk (or data size), as the data size increases (resp. risk increases) the running time of TRAM decreases. Our extensive experiments on real data sets demonstrate the existence and practical utility of such tradeoffs, not only for k-means but also for Gaussian Mixture Models.",,,,,,"Krause, Andreas/0000-0001-7260-9673",,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,663,671,,,,,,,,,,,,,,,,WOS:000508399700073,0
C,"Reddi, SJ; Ramdas, A; Poczos, B; Singh, A; Wasserman, L",,"Lebanon, G; Vishwanathan, SVN",,"Reddi, Sashank J.; Ramdas, Aaditya; Poczos, Barnabas; Singh, Aarti; Wasserman, Larry",,,On the High Dimensional Power of a Linear-Time Two Sample Test under Mean-shift Alternatives,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"Nonparametric two sample testing deals with the question of consistently deciding if two distributions are different, given samples from both, without making any parametric assumptions about the form of the distributions. The current literature is split into two kinds of tests - those which are consistent without any assumptions about how the distributions may differ (general alternatives), and those which are designed to specifically test easier alternatives, like a difference in means (mean-shift alternatives). The main contribution of this paper is to explicitly characterize the power of a popular non-parametric two sample test, designed for general alternatives, under a mean-shift alternative in the high-dimensional setting. Specifically, we explicitly derive the power of the linear-time Maximum Mean Discrepancy statistic using the Gaussian kernel, where the dimension and sample size can both tend to infinity at any rate, and the two distributions differ in their means. As a corollary, we find that if the signal-to-noise ratio is held constant, then the test's power goes to one if the number of samples increases faster than the dimension increases. This is the first explicit power derivation for a general nonparametric test in the high-dimensional setting, and the first analysis of how tests designed for general alternatives perform against easier ones.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,772,780,,,,,,,,,,,,,,,,WOS:000508399700085,0
C,"Schnitzler, F; Yu, JY; Mannor, S",,"Lebanon, G; Vishwanathan, SVN",,"Schnitzler, Francois; Yu, Jia Yuan; Mannor, Shie",,,Sensor Selection for Crowdsensing Dynamical Systems,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"We model crowdsensing as the selection of sensors with unknown variance to monitor a large linear dynamical system. To achieve low estimation error, we propose a Thompson sampling approach combining submodular optimization and a scalable online variational inference algorithm to maintain the posterior distribution over the variance. We also consider three alternative parameter estimation algorithms. We illustrate the behavior of our sensor selection algorithms on real traffic data from the city of Dublin. Our online algorithm achieves significantly lower estimation error than sensor selection using a fixed variance value for all sensors.",,,,,,"Schnitzler, Francois/0000-0003-1304-2157; Mannor, Shie/0000-0003-4439-7647",,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,829,837,,,,,,,,,,,,,,,,WOS:000508399700091,0
C,"Sun, SQ; Wang, H; Xu, JB",,"Lebanon, G; Vishwanathan, SVN",,"Sun, Siqi; Wang, Hai; Xu, Jinbo",,,Inferring Block Structure of Graphical Models in Exponential Families,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"Learning the structure of a graphical model is a fundamental problem and it is used extensively to infer the relationship between random variables. In many real world applications, we usually have some prior knowledge about the underlying graph structure, such as degree distribution and block structure. In this paper, we propose a novel generative model for describing the block structure in general exponential families, and optimize it by an Expectation-Maximization(EM) algorithm with variational Bayes. Experimental results show that our method performs well on both synthetic and real data. Furthermore, our method can predict overlapping block structure of a graphical model in general exponential families.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,939,947,,,,,,,,,,,,,,,,WOS:000508399700103,0
C,"Zhe, SD; Xu, ZL; Chu, XQ; Qi, Y; Park, Y",,"Lebanon, G; Vishwanathan, SVN",,"Zhe, Shandian; Xu, Zenglin; Chu, Xinqi; Qi, Yuan; Park, Youngja",,,Scalable Nonparametric Multiway Data Analysis,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"Multiway data analysis deals with multiway arrays, i.e., tensors, and the goal is twofold: predicting missing entries by modeling the interactions between array elements and discovering hidden patterns, such as clusters or communities in each mode. Despite the success of existing tensor factorization approaches, they are either unable to capture nonlinear interactions, or computationally expensive to handle massive data. In addition, most of the existing methods lack a principled way to discover latent clusters, which is important for better understanding of the data. To address these issues, we propose a scalable nonparametric tensor decomposition model. It employs Dirichlet process mixture (DPM) prior to model the latent clusters; it uses local Gaussian processes (GPs) to capture nonlinear relationships and to improve scalability. An efficient online variational Bayes Expectation-Maximization algorithm is proposed to learn the model. Experiments on both synthetic and real-world data show that the proposed model is able to discover latent clusters with higher prediction accuracy than competitive methods. Furthermore, the proposed model obtains significantly better predictive performance than the state-of-the-art large scale tensor decomposition algorithm, GigaTensor, on two large datasets with billions of entries.",,,,,"Xu, Zenglin/HHZ-8366-2022","Xu, Zenglin/0000-0001-5550-6461",,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,1125,1134,,,,,,,,,,,,,,,,WOS:000508399700123,0
C,"Erraqabi, A; Lazaric, A; Valko, M; Brunskill, E; Liu, YE",,"Singh, A; Zhu, J",,"Erraqabi, Akram; Lazaric, Alessandro; Valko, Michal; Brunskill, Emma; Liu, Yun-En",,,Trading off Rewards and Errors in Multi-Armed Bandits,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"In multi-armed bandits, the most common objective is the maximization of the cumulative reward. Alternative settings include active exploration, where a learner tries to gain accurate estimates of the rewards of all arms. While these objectives are contrasting, in many scenarios it is desirable to trade off rewards and errors. For instance, in educational games the designer wants to gather generalizable knowledge about the behavior of the students and teaching strategies (small estimation errors) but, at the same time, the system needs to avoid giving a bad experience to the players, who may leave the system permanently (large reward). In this paper, we formalize this tradeoff and introduce the FORCINGBALANCE algorithm whose performance is provably close to the best possible tradeoff strategy. Finally, we demonstrate on realworld educational data that ForcingBalance returns useful information about the arms without compromising the overall reward.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,709,717,,,,,,,,,,,,,,,,WOS:000509368500076,0
C,"Jiang, K; Sra, S; Kulis, B",,"Singh, A; Zhu, J",,"Jiang, Ke; Sra, Suvrit; Kulis, Brian",,,Combinatorial Topic Models using Small-Variance Asymptotics,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Modern topic models typically have a probabilistic formulation, and derive their inference algorithms based on Latent Dirichlet Allocation (LDA) and its variants. In contrast, we approach topic modeling via combinatorial optimization, and take a small-variance limit of LDA to derive a new objective function. We minimize this objective by using ideas from combinatorial optimization, obtaining a new, fast, and high-quality topic modeling algorithm. In particular, we show that our results are not only significantly better than traditional SVA algorithms, but also truly competitive with popular LDA-based approaches; we also discuss the (dis)similarities between our approach and its probabilistic counterparts.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,421,429,,,,,,,,,,,,,,,,WOS:000509368500046,0
C,"Kallus, N",,"Singh, A; Zhu, J",,"Kallus, Nathan",,,A Framework for Optimal Matching for Causal Inference,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"We propose a novel framework for matching estimators for causal effect from observational data that is based on minimizing the dual norm of estimation error when expressed as an operator. We show that many popular matching estimators can be expressed as optimal in this framework, including nearest-neighbor matching, coarsened exact matching, and mean-matched sampling. This reveals their motivation and aptness as structural priors formulated by embedding the effect in a particular functional space. This also gives rise to a range of new, kernel-based matching estimators that arise when one embeds the effect in a reproducing kernel Hilbert space. Depending on the case, these estimators can be found using either quadratic optimization or integer optimization. We show that estimators based on universal kernels are universally consistent without model specification. In empirical results using both synthetic and real data, the new, kernel-based estimators outperform all standard causal estimators in estimation error.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,372,381,,,,,,,,,,,,,,,,WOS:000509368500041,0
C,"Kwon, J; Perchet, V",,"Singh, A; Zhu, J",,"Kwon, Joon; Perchet, Vianney",,,Online Learning and Blackwell Approachability with Partial Monitoring: Optimal Convergence Rates,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Blackwell approachability is an online learning setup generalizing the classical problem of regret minimization by allowing for instance multi-criteria optimization, global (online) optimization of a convex loss, or online linear optimization under some cumulative constraint. We consider partial monitoring where the decision maker does not necessarily observe the outcomes of his decision (unlike the traditional regret/bandit literature). Instead, he receives a random signal correlated to the decision-outcome pair, or only to the outcome. We construct, for the first time, approachability algorithms with convergence rate of order O(T-1/2) when the signal is independent of the decision and of order O(T-1/3) in the case of general signals. Those rates are optimal in the sense that they cannot be improved without further assumption on the structure of the objectives and/or the signals.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,604,613,,,,,,,,,,,,,,,,WOS:000509368500065,0
C,"Zaytsev, A; Burnaev, E",,"Singh, A; Zhu, J",,"Zaytsev, A.; Burnaev, E.",,,Minimax Approach to Variable Fidelity Data Interpolation,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Engineering problems often involve data sources of variable fidelity with different costs of obtaining an observation. In particular, one can use both a cheap low fidelity function (e.g. a computational experiment with a CFD code) and an expensive high fidelity function (e.g. a wind tunnel experiment) to generate a data sample in order to construct a regression model of a high fidelity function. The key question in this setting is how the sizes of the high and low fidelity data samples should be selected in order to stay within a given computational budget and maximize accuracy of the regression model prior to committing resources on data acquisition. In this paper we obtain minimax interpolation errors for single and variable fidelity scenarios for a multivariate Gaussian process regression. Evaluation of the minimax errors allows us to identify cases when the variable fidelity data provides better interpolation accuracy than the exclusively high fidelity data for the same computational budget. These results allow us to calculate the optimal shares of variable fidelity data samples under the given computational budget constraint. Real and synthetic data experiments suggest that using the obtained optimal shares often outperforms natural heuristics in terms of the regression accuracy.",,,,,"Zaytsev, Alexey A./A-9656-2016","Zaytsev, Alexey A./0000-0002-1653-0204",,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,652,661,,,,,,,,,,,,,,,,WOS:000509368500070,0
C,"Benavoli, A; Mangili, F",,"Lebanon, G; Vishwanathan, SVN",,"Benavoli, Alessio; Mangili, Francesca",,,Gaussian Processes for Bayesian hypothesis tests on regression functions,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"Gaussian processes have been used in different application domains such as classification, regression etc. In this paper we show that they can also be employed as a universal tool for developing a large variety of Bayesian statistical hypothesis tests for regression functions. In particular, we will use GPs for testing whether (i) two functions are equal; (ii) a function is monotone (even accounting for seasonality effects); (iii) a function is periodic; (iv) two functions are proportional. By simulation studies, we will show that, beside being more flexible, GP tests are also competitive in terms of performance with state-of-art algorithms.",,,,,,"benavoli, alessio/0000-0002-2522-7178",,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,74,82,,,,,,,,,,,,,,,,WOS:000508399700009,0
C,"Choromanska, A; Henaff, M; Mathieu, M; Ben Arous, G; LeCun, Y",,"Lebanon, G; Vishwanathan, SVN",,"Choromanska, Anna; Henaff, Mikael; Mathieu, Michael; Ben Arous, Gerard; LeCun, Yann",,,The Loss Surfaces of Multilayer Networks,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"We study the connection between the highly non-convex loss function of a simple model of the fully-connected feed-forward neural network and the Hamiltonian of the spherical spin-glass model under the assumptions of: i) variable independence, ii) redundancy in network parametrization, and iii) uniformity. These assumptions enable us to explain the complexity of the fully decoupled neural network through the prism of the results from random matrix theory. We show that for large-size decoupled networks the lowest critical values of the random loss function form a layered structure and they are located in a well-defined band lower-bounded by the global minimum. The number of local minima outside that band diminishes exponentially with the size of the network. We empirically verify that the mathematical model exhibits similar behavior as the computer simulations, despite the presence of high dependencies in real networks. We conjecture that both simulated annealing and SGD converge to the band of low critical points, and that all critical points found there are local minima of high quality measured by the test error. This emphasizes a major difference between large-and small-size networks where for the latter poor quality local minima have nonzero probability of being recovered. Finally, we prove that recovering the global minimum becomes harder as the network size increases and that it is in practice irrelevant as global minimum often leads to overfitting.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,192,204,,,,,,,,,,,,,,,,WOS:000508399700022,0
C,"Lee, J; Choi, S",,"Lebanon, G; Vishwanathan, SVN",,"Lee, Juho; Choi, Seungjin",,,Bayesian Hierarchical Clustering with Exponential Family: Small-Variance Asymptotics and Reducibility,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"Bayesian hierarchical clustering (BHC) is an agglomerative clustering method, where a probabilistic model is defined and its marginal likelihoods are evaluated to decide which clusters to merge. While BHC provides a few advantages over traditional distance-based agglomerative clustering algorithms, successive evaluation of marginal likelihoods and careful hyperparameter tuning are cumbersome and limit the scalability. In this paper we relax BHC into a non-probabilistic formulation, exploring small-variance asymptotics in conjugate-exponential models. We develop a novel clustering algorithm, referred to as relaxed BHC (RBHC), from the asymptotic limit of the BHC model that exhibits the scalability of distance-based agglomerative clustering algorithms as well as the flexibility of Bayesian nonparametric models. We also investigate the reducibility of the dissimilarity measure emerged from the asymptotic limit of the BHC model, allowing us to use scalable algorithms such as the nearest neighbor chain algorithm. Numerical experiments on both synthetic and real-world datasets demonstrate the validity and high performance of our method.",,,,,"Lee, Juho/AAA-2901-2022",,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,581,589,,,,,,,,,,,,,,,,WOS:000508399700064,0
C,"Peharz, R; Tschiatschek, S; Pernkopf, F; Domingos, P",,"Lebanon, G; Vishwanathan, SVN",,"Peharz, Robert; Tschiatschek, Sebastian; Pernkopf, Franz; Domingos, Pedro",,,On Theoretical Properties of Sum-Product Networks,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"Sum-product networks (SPNs) are a promising avenue for probabilistic modeling and have been successfully applied to various tasks. However, some theoretic properties about SPNs are not yet well understood. In this paper we fill some gaps in the theoretic foundation of SPNs. First, we show that the weights of any complete and consistent SPN can be transformed into locally normalized weights without changing the SPN distribution. Second, we show that consistent SPNs cannot model distributions significantly (exponentially) more compactly than decomposable SPNs. As a third contribution, we extend the inference mechanisms known for SPNs with finite states to generalized SPNs with arbitrary input distributions.",,,,,,"Tschiatschek, Sebastian/0000-0002-2592-0108; Peharz, Robert/0000-0002-8644-9655",,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,744,752,,,,,,,,,,,,,,,,WOS:000508399700082,0
C,"Solin, A; Sarkka, S",,"Lebanon, G; Vishwanathan, SVN",,"Solin, Arno; Sarkka, Sirno",,,State Space Methods for Efficient Inference in Student-t Process Regression,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"The added flexibility of Student-t processes (TPs) over Gaussian processes (GPs) robustifies inference in outlier-contaminated noisy data. The uncertainties are better accounted for than in GP regression, because the predictive covariances explicitly depend on the training observations. For an entangled noise model, the canonical-form TP regression problem can be solved analytically, but the naive TP and GP solutions share the same cubic computational cost in the number of training observations. We show how a large class of temporal TP regression models can be reformulated as state space models, and how a forward filtering and backward smoothing recursion can be derived for solving the inference analytically in linear time complexity. This is a novel finding that generalizes the previously known connection between Gaussian process regression and Kalman filtering to more general elliptical processes and non-Gaussian Bayesian filtering. We derive this connection, demonstrate the benefits of the approach with examples, and finally apply the method to empirical data.",,,,,"Solin, Arno/G-6859-2012","Solin, Arno/0000-0002-0958-7886",,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,885,893,,,,,,,,,,,,,,,,WOS:000508399700097,0
C,"Weller, A",,"Lebanon, G; Vishwanathan, SVN",,"Weller, Adrian",,,Revisiting the Limits of MAP Inference by MWSS on Perfect Graphs,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"A recent, promising approach to identifying a configuration of a discrete graphical model with highest probability (termed MAP inference) is to reduce the problem to finding a maximum weight stable set (MWSS) in a derived weighted graph, which, if perfect, allows a solution to be found in polynomial time. Weller and Jebara (2013) investigated the class of binary pairwise models where this method may be applied. However, their analysis made a seemingly innocuous assumption which simplifies analysis but led to only a subset of possible reparameterizations being considered. Here we introduce novel techniques and consider all cases, demonstrating that this greatly expands the set of tractable models. We provide a simple, exact characterization of the new, enlarged set and show how such models may be efficiently identified, thus settling the power of the approach on this class.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,1061,1069,,,,,,,,,,,,,,,,WOS:000508399700116,0
C,"Adams, RJ; Marlin, BM",,"Singh, A; Zhu, J",,"Adams, Roy J.; Marlin, Benjamin M.",,,Learning Time Series Detection Models from Temporally Imprecise Labels,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"In this paper, we consider the problem of learning time series detection models from temporally imprecise labels. In this problem, the data consist of a set of input time series, and supervision is provided by a sequence of noisy time stamps corresponding to the occurrence of positive class events. Such temporally imprecise labels occur in areas like mobile health research when human annotators are tasked with labeling the occurrence of very short duration events. We propose a general learning framework for this problem that can accommodate different base classifiers and noise models. We present results on real mobile health data showing that the proposed framework significantly outperforms a number of alternatives including assuming that the label time stamps are noise-free, transforming the problem into the multiple instance learning framework, and learning on labels that were manually aligned.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,157,165,,,,,,,,,,,30465043,,,,,WOS:000509368500018,0
C,"Esperanca, PM; Aslett, LJM; Holmes, CC",,"Singh, A; Zhu, J",,"Esperanca, Pedro M.; Aslett, Louis J. M.; Holmes, Chris C.",,,Encrypted accelerated least squares regression,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Information that is stored in an encrypted format is, by definition, usually not amenable to statistical analysis or machine learning methods. In this paper we present detailed analysis of coordinate and accelerated gradient descent algorithms which are capable of fitting least squares and penalised ridge regression models, using data encrypted under a fully homomorphic encryption scheme. Gradient descent is shown to dominate in terms of encrypted computational speed, and theoretical results are proven to give parameter bounds which ensure correctness of decryption. The characteristics of encrypted computation are empirically shown to favour a non-standard acceleration technique. This demonstrates the possibility of approximating conventional statistical regression methods using encrypted data without compromising privacy.",,,,,,"Aslett, Louis/0000-0003-2211-233X",,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,334,343,,,,,,,,,,,,,,,,WOS:000509368500037,0
C,"Jain, P; Jin, C; Kakade, SM; Netrapalli, P",,"Singh, A; Zhu, J",,"Jain, Prateek; Jin, Chi; Kakade, Sham M.; Netrapalli, Praneeth",,,Global Convergence of Non-Convex Gradient Descent for Computing Matrix Squareroot,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"While there has been a significant amount of work studying gradient descent techniques for non-convex optimization problems over the last few years, all existing results establish either local convergence with good rates or global convergence with highly suboptimal rates, for many problems of interest. In this paper, we take the first step in getting the best of both worlds establishing global convergence and obtaining a good rate of convergence for the problem of computing squareroot of a positive definite (PD) matrix, which is a widely studied problem in numerical linear algebra with applications in machine learning and statistics among others. Given a PD matrix M and a PD starting point U-0, we show that gradient descent with appropriately chosen step-size finds an epsilon-accurate squareroot of M in O (alpha log(parallel to M - U-0(2)parallel to(F)/epsilon) iterations, where alpha =(Delta) (max{U-0 parallel to(2)(2), parallel to M parallel to(2)} / min{sigma(2)(min)(U-0), sigma(min) (M)})(3/2). Our result is the first to establish global convergence for this problem and that it is robust to errors in each iteration.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,479,488,,,,,,,,,,,,,,,,WOS:000509368500052,0
C,"Zafar, MB; Valera, I; Rodriguez, MG; Gummadi, KP",,"Singh, A; Zhu, J",,"Zafar, Muhammad Bilal; Valera, Isabel; Rodriguez, Manuel Gomez; Gummadi, Krishna P.",,,Fairness Constraints: Mechanisms for Fair Classification,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Algorithmic decision making systems are ubiquitous across a wide variety of online as well as offline services. These systems rely on complex learning methods and vast amounts of data to optimize the service functionality, satisfaction of the end user and profitability. However, there is a growing concern that these automated decisions can lead, even in the absence of intent, to a lack of fairness, i.e., their outcomes can disproportionately hurt (or, benefit) particular groups of people sharing one or more sensitive attributes (e.g., race, sex). In this paper, we introduce a flexible mechanism to design fair classifiers by leveraging a novel intuitive measure of decision boundary (un)fairness. We instantiate this mechanism with two well-known classifiers, logistic regression and support vector machines, and show on real-world data that our mechanism allows for a fine-grained control on the degree of fairness, often at a small cost in terms of accuracy. A Python implementation of our mechanism is available at fate-computing.mpi-sws.org",,,,,"Rodriguez, Manuel Gomez/AAB-5005-2021",,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,962,970,,,,,,,,,,,,,,,,WOS:000509368500103,0
C,"Zhao, RB; Tan, VYF; Xu, H",,"Singh, A; Zhu, J",,"Zhao, Renbo; Tan, Vincent Y. F.; Xu, Huan",,,Online Nonnegative Matrix Factorization with General Divergences,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"We develop a unified and systematic framework for performing online nonnegative matrix factorization under a wide variety of important divergences. The online nature of our algorithms makes them particularly amenable to large-scale data. We prove that the sequence of learned dictionaries converges almost surely to the set of critical points of the expected loss function. Experimental results demonstrate the computational efficiency and outstanding performances of our algorithms on several real-life applications, including topic modeling, document clustering and foreground-background separation.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,37,45,,,,,,,,,,,,,,,,WOS:000509368500005,0
C,"Li, YH; Scarlett, J; Ravikumar, P; Cevher, V",,"Lebanon, G; Vishwanathan, SVN",,"Li, Yen-Huan; Scarlett, Jonathan; Ravikumar, Pradeep; Cevher, Volkan",,,Sparsistency of l(1)-Regularized M-Estimators,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"We consider the model selection consistency or sparsistency of a broad set of l(1)-regularized M-estimators for linear and nonlinear statistical models in a unified fashion. For this purpose, we propose the local structured smoothness condition (LSSC) on the loss function. We provide a general result giving deterministic sufficient conditions for sparsistency in terms of the regularization parameter, ambient dimension, sparsity level, and number of measurements. We show that several important statistical models have M-estimators that indeed satisfy the LSSC, and as a result, the sparsistency guarantees for the corresponding l(1)-regularized M-estimators can be derived as simple applications of our main theorem.",,,,,"Scarlett, Jonathan/AGK-0892-2022","Li, Yen-Huan/0000-0003-2454-7249",,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,644,652,,,,,,,,,,,,,,,,WOS:000508399700071,0
C,"Ver Steeg, G; Galstyan, A",,"Lebanon, G; Vishwanathan, SVN",,"Ver Steeg, Greg; Galstyan, Aram",,,Maximally Informative Hierarchical Representations of High-Dimensional Data,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"We consider a set of probabilistic functions of some input variables as a representation of the inputs. We present bounds on how informative a representation is about input data. We extend these bounds to hierarchical representations so that we can quantify the contribution of each layer towards capturing the information in the original data. The special form of these bounds leads to a simple, bottom-up optimization procedure to construct hierarchical representations that are also maximally informative about the data. This optimization has linear computational complexity and constant sample complexity in the number of variables. These results establish a new approach to unsupervised learning of deep representations that is both principled and practical. We demonstrate the usefulness of the approach on both synthetic and real-world data.",,,,,,"Ver Steeg, Greg/0000-0002-0793-141X",,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,1004,1012,,,,,,,,,,,,,,,,WOS:000508399700110,0
C,"Wang, YX; Sharpnack, J; Smola, A; Tibshirani, RJ",,"Lebanon, G; Vishwanathan, SVN",,"Wang, Yu-Xiang; Sharpnack, James; Smola, Alex; Tibshirani, Ryan J.",,,Trend Filtering on Graphs,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"We introduce a family of adaptive estimators on graphs, based on penalizing the l(1) norm of discrete graph differences. This generalizes the idea of trend filtering [11, 26], used for univariate nonparametric regression, to graphs. Analogous to the univariate case, graph trend filtering exhibits a level of local adaptivity unmatched by the usual l(2)-based graph smoothers. It is also defined by a convex minimization problem that is readily solved (e.g., by fast ADMM or Newton algorithms). We demonstrate the merits of graph trend filtering through examples and theory.",,,,,,"Wang, Yu-Xiang/0000-0002-6403-212X",,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,1042,1050,,,,,,,,,,,,,,,,WOS:000508399700114,0
C,"Khanna, R; Elenberg, ER; Dimakis, AG; Neghaban, S; Ghosh, J",,"Singh, A; Zhu, J",,"Khanna, Rajiv; Elenberg, Ethan R.; Dimakis, Alexandros G.; Neghaban, Sahand; Ghosh, Joydeep",,,Scalable Greedy Feature Selection viaWeak Submodularity,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Greedy algorithms are widely used for problems in machine learning such as feature selection and set function optimization. Unfortunately, for large datasets, the running time of even greedy algorithms can be quite high. This is because for each greedy step we need to refit a model or calculate a function using the previously selected choices and the new candidate. Two algorithms that are faster approximations to the greedy forward selection were introduced recently [Mirzasoleiman et al., 2013, 2015]. They achieve better performance by exploiting distributed computation and stochastic evaluation respectively. Both algorithms have provable performance guarantees for submodular functions. In this paper we show that divergent from previously held opinion, submodularity is not required to obtain approximation guarantees for these two algorithms. Specifically, we show that a generalized concept of weak submodularity suffices to give multiplicative approximation guarantees. Our result extends the applicability of these algorithms to a larger class of functions. Furthermore, we show that a bounded submodularity ratio can be used to provide data dependent bounds that can sometimes be tighter also for submodular functions. We empirically validate our work by showing superior performance of fast greedy approximations versus several established baselines on artificial and real datasets.",,,,,"Khanna, Rajiv/GPK-2566-2022","Khanna, Rajiv/0000-0003-1314-3126",,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,1560,1568,,,,,,,,,,,,,,,,WOS:000509368500167,0
C,"Perolat, J; Strub, F; Piot, B; Pietquin, O",,"Singh, A; Zhu, J",,"Perolat, Julien; Strub, Florian; Piot, Bilal; Pietquin, Olivier",,,Learning Nash Equilibrium for General-Sum Markov Games from Batch Data,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"This paper addresses the problem of learning a Nash equilibrium in.-discounted multiplayer general-sum Markov Games (MGs) in a batch setting. As the number of players increases in MG, the agents may either collaborate or team apart to increase their final rewards. One solution to address this problem is to look for a Nash equilibrium. Although, several techniques were found for the subcase of two-player zero-sum MGs, those techniques fail to find a Nash equilibrium in general-sum Markov Games. In this paper, we introduce a new definition of epsilon-Nash equilibrium in MGs which grasps the strategy's quality for multiplayer games. We prove that minimizing the norm of two Bellman-like residuals implies to learn such an epsilon-Nash equilibrium. Then, we show that minimizing an empirical estimate of the L-p norm of these Bellman-like residuals allows learning for general-sum games within the batch setting. Finally, we introduce a neural network architecture that successfully learns a Nash equilibrium in generic multiplayer generalsum turn-based MGs.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,232,241,,,,,,,,,,,,,,,,WOS:000509368500026,0
C,"Slawski, M",,"Singh, A; Zhu, J",,"Slawski, Martin",,,Compressed Least Squares Regression revisited,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"We revisit compressed least squares (CLS) regression as originally analyzed in Maillard and Munos (2009) and later on in Kaban (2014) with some refinements. Given a set of high-dimensional inputs, CLS applies a random projection and then performs least squares regression based on the projected inputs of lower dimension. This approach can be beneficial with regard to both computation (yielding a smaller least squares problem) and statistical performance (reducing the estimation error). We will argue below that the outcome of previous analysis of the procedure is not meaningful in typical situations, yielding a bound on the prediction error that is inferior to ordinary least squares while requiring the dimension of the projected data to be of the same order as the original dimension. As a fix, we subsequently present a modified analysis with meaningful implications that much better reflects empirical results with simulated and real data.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,1207,1215,,,,,,,,,,,,,,,,WOS:000509368500129,0
C,"Ding, WC; Ishwar, P; Saligrama, V",,"Lebanon, G; Vishwanathan, SVN",,"Ding, Weicong; Ishwar, Prakash; Saligrama, Venkatesh",,,A Topic Modeling Approach to Ranking,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,We propose a topic modeling approach to the prediction of preferences in pairwise comparisons. We develop a new generative model for pairwise comparisons that accounts for multiple shared latent rankings that are prevalent in a population of users. This new model also captures inconsistent user behavior in a natural way. We show how the estimation of latent rankings in the new generative model can be formally reduced to the estimation of topics in a statistically equivalent topic modeling problem. We leverage recent advances in the topic modeling literature to develop an algorithm that can learn shared latent rankings with provable consistency as well as sample and computational complexity guarantees. We demonstrate that the new approach is empirically competitive with the current state-of-the-art approaches in predicting preferences on some semi-synthetic and real world datasets.,,,,,,"Saligrama, Venkatesh/0000-0002-0675-2268; Ishwar, Prakash/0000-0002-2621-1549",,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,214,222,,,,,,,,,,,,,,,,WOS:000508399700024,0
C,"Roychowdhury, A; Kulis, B",,"Lebanon, G; Vishwanathan, SVN",,"Roychowdhury, Anirban; Kulis, Brian",,,"Gamma Processes, Stick-Breaking, and Variational Inference","ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"While most Bayesian nonparametric models in machine learning have focused on the Dirichlet process, the beta process, or their variants, the gamma process has recently emerged as a useful nonparametric prior in its own right. Current inference schemes for models involving the gamma process are restricted to MCMC-based methods, which limits their scalability. In this paper, we present a variational inference framework for models involving gamma process priors. Our approach is based on a novel stick-breaking constructive definition of the gamma process. We prove correctness of this stick-breaking process by using the characterization of the gamma process as a completely random measure (CRM), and we explicitly derive the rate measure of our construction using Poisson process machinery. We also derive error bounds on the truncation of the infinite process required for variational inference, similar to the truncation analyses for other nonparametric models based on the Dirichlet and beta processes. Our representation is then used to derive a variational inference algorithm for a particular Bayesian nonparametric latent structure formulation known as the infinite Gamma-Poisson model, where the latent variables are drawn from a gamma process prior with Poisson likelihoods. Finally, we present results for our algorithm on non-negative matrix factorization tasks on document corpora, and show that we compare favorably to both sampling-based techniques and variational approaches based on beta-Bernoulli priors, as well as a direct DP-based construction of the gamma process.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,800,808,,,,,,,,,,,,,,,,WOS:000508399700088,0
C,"Bian, AA; Mirzasoleiman, B; Buhmann, JM; Krause, A",,"Singh, A; Zhu, J",,"Bian, Andrew An; Mirzasoleiman, Baharan; Buhmann, Joachim M.; Krause, Andreas",,,Guaranteed Non-convex Optimization: Submodular Maximization over Continuous Domains,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Submodular continuous functions are a category of (generally) non-convex/non-concave functions with a wide spectrum of applications. We characterize these functions and demonstrate that they can be maximized efficiently with approximation guarantees. Specifically, i) We introduce the weak DR property that gives a unified characterization of submodularity for all set, integerlattice and continuous functions; ii) for maximizing monotone DR-submodular continuous functions under general down-closed convex constraints, we propose a FRANK-WOLFE variant with (1- 1/epsilon) approximation guarantee, and sub-linear convergence rate; iii) for maximizing general non-monotone submodular continuous functions subject to box constraints, we propose a DOUBLEGREEDY algorithm with 1/3 approximation guarantee. Submodular continuous functions naturally find applications in various real-world settings, including influence and revenue maximization with continuous assignments, sensor energy management, facility location, etc. Experimental results show that the proposed algorithms efficiently generate superior solutions compared to baseline algorithms.",,,,,"Buhmann, Joachim/AAU-4760-2020","Bian, Yatao/0000-0002-2368-4084",,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,111,120,,,,,,,,,,,,,,,,WOS:000509368500013,0
C,"Brouwer, T; Lio, P",,"Singh, A; Zhu, J",,"Brouwer, Thomas; Lio, Pietro",,,Bayesian Hybrid Matrix Factorisation for Data Integration,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"We introduce a novel Bayesian hybrid matrix factorisation model (HMF) for data integration, based on combining multiple matrix factorisation methods, that can be used for in- and out-of-matrix prediction of missing values. The model is very general and can be used to integrate many datasets across different entity types, including repeated experiments, similarity matrices, and very sparse datasets. We apply our method on two biological applications, and extensively compare it to state-of-the-art machine learning and matrix factorisation models. For in-matrix predictions on drug sensitivity datasets we obtain consistently better performances than existing methods. This is especially the case when we increase the sparsity of the datasets. Furthermore, we perform out-of-matrix predictions on methylation and gene expression datasets, and obtain the best results on two of the three datasets, especially when the predictivity of datasets is high.",,,,,"P. Li, Pietro Lio/AAV-3358-2021","P. Li, Pietro Lio/0000-0002-0540-5053",,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,557,566,,,,,,,,,,,,,,,,WOS:000509368500060,0
C,"Lian, XR; Wang, MD; Liu, J",,"Singh, A; Zhu, J",,"Lian, Xiangru; Wang, Mengdi; Liu, Ji",,,Finite-sum Composition Optimization via Variance Reduced Gradient Descent,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"The stochastic composition optimization proposed recently by Wang et al. [2014] minimizes the objective with the compositional expectation form: minx (EiFi. E(j)G(j))(x): It summarizes many important applications in machine learning, statistics, and finance. In this paper, we consider the finite-sum scenario for composition optimization: min(x) f(x) := 1/n Sigma(n)(i=1) F-i (1/m Sigma(m)(j=1) G(j)(x)). We propose two algorithms to solve this problem by combining the stochastic compositional gradient descent (SCGD) and the stochastic variance reduced gradient (SVRG) technique. A constant linear convergence rate is proved for strongly convex optimization, which substantially improves the sublinear rate O(K-0.8) of the best known algorithm.",,,,,,"Wang, Mengdi/0000-0002-2101-9507",,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,1159,1167,,,,,,,,,,,,,,,,WOS:000509368500124,0
C,"Ping, W; Ihler, A",,"Singh, A; Zhu, J",,"Ping, Wei; Ihler, Alexander",,,Belief Propagation in Conditional RBMs for Structured Prediction,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Restricted Boltzmann machines (RBMs) and conditional RBMs (CRBMs) are popular models for a wide range of applications. In previous work, learning on such models has been dominated by contrastive divergence (CD) and its variants. Belief propagation (BP) algorithms are believed to be slow for structured prediction on conditional RBMs (e.g., Mnih et al. [2011]), and not as good as CD when applied in learning (e.g., Larochelle et al. [2012]). In this work, we present a matrix-based implementation of belief propagation algorithms on CRBMs, which is easily scalable to tens of thousands of visible and hidden units. We demonstrate that, in both maximum likelihood and max-margin learning, training conditional RBMs with BP as the inference routine can provide significantly better results than current state-of-the-art CD methods on structured prediction problems. We also include practical guidelines on training CRBMs with BP, and some insights on the interaction of learning and inference algorithms for CRBMs.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,1141,1149,,,,,,,,,,,,,,,,WOS:000509368500122,0
C,"Raj, A; Kumar, A; Mroueh, Y; Fletcher, PT; Scholkopf, B",,"Singh, A; Zhu, J",,"Raj, Anant; Kumar, Abhishek; Mroueh, Youssef; Fletcher, P. Thomas; Scholkopf, Bernhard",,,Local Group Invariant Representations via Orbit Embeddings,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Invariance to nuisance transformations is one of the desirable properties of effective representations. We consider transformations that form a group and propose an approach based on kernel methods to derive local group invariant representations. Locality is achieved by defining a suitable probability distribution over the group which in turn induces distributions in the input feature space. We learn a decision function over these distributions by appealing to the powerful framework of kernel methods and generate local invariant random feature maps via kernel approximations. We show uniform convergence bounds for kernel approximation and provide generalization bounds for learning with these features. We evaluate our method on three real datasets, including Rotated MNIST and CIFAR-10, and observe that it outperforms competing kernel based approaches. The proposed method also outperforms deep CNN on Rotated-MNIST and performs comparably to the recently proposed group-equivariant CNN.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,1225,1235,,,,,,,,,,,,,,,,WOS:000509368500131,0
C,"Wang, YC; Ye, XJ; Zhou, HM; Zha, HY; Song, L",,"Singh, A; Zhu, J",,"Wang, Yichen; Ye, Xiaojing; Zhou, Haomin; Zha, Hongyuan; Song, Le",,,Linking Micro Event History to Macro Prediction in Point Process Models,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"User behaviors in social networks are microscopic with fine grained temporal information. Predicting a macroscopic quantity based on users' collective behaviors is an important problem. However, existing works are mainly problem-specific models for the microscopic behaviors and typically design approximations or heuristic algorithms to compute the macroscopic quantity. In this paper, we propose a unifying framework with a jump stochastic differential equation model that systematically links the microscopic event data and macroscopic inference, and the theory to approximate its probability distribution. We showed that our method can better predict the user behaviors in real-world applications.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,1375,1384,,,,,,,,,,,,,,,,WOS:000509368500147,0
C,"Atan, O; Tekin, C; van der Schaar, M",,"Lebanon, G; Vishwanathan, SVN",,"Atan, Onur; Tekin, Cem; van der Schaar, Mihaela",,,Global Multi-armed Bandits with Holder Continuity,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"Standard Multi-Armed Bandit (MAB) problems assume that the arms are independent. However, in many application scenarios, the information obtained by playing an arm provides information about the remainder of the arms. Hence, in such applications, this informativeness can and should be exploited to enable faster convergence to the optimal solution. In this paper, formalize a new class of multi-armed bandit methods, Global Multi-armed Bandit (GMAB), in which arms are globally informative through a global parameter, i.e., choosing an arm reveals information about all the arms. We propose a greedy policy for the GMAB which always selects the arm with the highest estimated expected reward, and prove that it achieves bounded parameter-dependent regret. Hence, this policy selects suboptimal arms only finitely many times, and after a finite number of initial time steps, the optimal arm is selected in all of the remaining time steps with probability one. In addition, we also study how the informativeness of the arms about each other's rewards affects the speed of learning. Specifically, we prove that the parameter-free (worst-case) regret is sublinear in time, and decreases with the informativeness of the arms. We also prove a sublinear in time Bayesian risk bound for the GMAB which reduces to the well-known Bayesian risk bound for linearly parameterized bandits when the arms are fully informative. GMABs have applications ranging from drug dosage control to dynamic pricing.",,,,,,"van der schaar, Mihaela/0000-0003-3933-6049",,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,28,36,,,,,,,,,,,,,,,,WOS:000508399700004,0
C,"Iyer, R; Bilmes, J",,"Lebanon, G; Vishwanathan, SVN",,"Iyer, Rishabh; Bilmes, Jeff",,,Submodular Point Processes with Applications to Machine Learning,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"We introduce a class of discrete point processes that we call the Submodular Point Processes (SPPs). These processes are characterized via a submodular (or supermodular) function, and naturally model notions of information, coverage and diversity, as well as cooperation. Unlike Log-submodular and Log-supermodular distributions (Log-SPPs) such as determinantal point processes (DPPs), SPPs are themselves submodular (or supermodular). In this paper, we analyze the computational complexity of probabilistic inference in SPPs. We show that computing the partition function for SPPs (and Log-SPPs), requires exponential complexity in the worst case, and also provide algorithms which approximate SPPs up to polynomial factors. Moreover, for several subclasses of interesting submodular functions that occur in applications, we show how we can provide efficient closed form expressions for the partition functions, and thereby marginals and conditional distributions. We also show how SPPs are closed under mixtures, thus enabling maximum likelihood based strategies for learning mixtures of submodular functions. Finally, we argue how SPPs complement existing Log-SPP distributions, and are a natural model for several applications.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,388,397,,,,,,,,,,,,,,,,WOS:000508399700043,0
C,"Kveton, B; Wen, Z; Ashkan, A; Szepesvari, C",,"Lebanon, G; Vishwanathan, SVN",,"Kveton, Branislav; Wen, Zheng; Ashkan, Azin; Szepesvari, Csaba",,,Tight Regret Bounds for Stochastic Combinatorial Semi-Bandits,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"A stochastic combinatorial semi-bandit is an online learning problem where at each step a learning agent chooses a subset of ground items subject to constraints, and then observes stochastic weights of these items and receives their sum as a payoff. In this paper, we close the problem of computationally and sample efficient learning in stochastic combinatorial semi-bandits. In particular, we analyze a UCB-like algorithm for solving the problem, which is known to be computationally efficient; and prove O(KL(1/Delta) log n) and O(root KLnlog n) upper bounds on its n-step regret, where L is the number of ground items, K is the maximum number of chosen items, and Delta is the gap between the expected returns of the optimal and best suboptimal solutions. The gap-dependent bound is tight up to a constant factor and the gap-free bound is tight up to a polylogarithmic factor.",,,,,"wen, zheng/HII-3705-2022",,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,535,543,,,,,,,,,,,,,,,,WOS:000508399700059,0
C,"Gao, YH; Parameswaran, A; Peng, J",,"Singh, A; Zhu, J",,"Gao, Yihan; Parameswaran, Aditya; Peng, Jian",,,On the Interpretability of Conditional Probability Estimates in the Agnostic Setting,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"We study the interpretability of conditional probability estimates for binary classification under the agnostic setting or scenario. Under the agnostic setting, conditional probability estimates do not necessarily reflect the true conditional probabilities. Instead, they have a certain calibration property: among all data points that the classifier has predicted P(Y = 1 vertical bar X) = p, p portion of them actually have label Y = 1. For cost-sensitive decision problems, this calibration property provides adequate support for us to use Bayes Decision Theory. In this paper, we define a novel measure for the calibration property together with its empirical counterpart, and prove an uniform convergence result between them. This new measure enables us to formally justify the calibration property of conditional probability estimations, and provides new insights on the problem of estimating and calibrating conditional probabilities.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,1367,1374,,,,,,,,,,,,,,,,WOS:000509368500146,0
C,"Hong, Y; Yang, X; Kwitt, R; Styner, M; Niethammer, M",,"Singh, A; Zhu, J",,"Hong, Yi; Yang, Xiao; Kwitt, Roland; Styner, Martin; Niethammer, Marc",,,Regression Uncertainty on the Grassmannian,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"Trends in longitudinal or cross-sectional studies over time are often captured through regression models. In their simplest manifestation, these regression models are formulated in R-n. However, in the context of imaging studies, the objects of interest which are to be regressed are frequently best modeled as elements of a Riemannian manifold. Regression on such spaces can be accomplished through geodesic regression. This paper develops an approach to compute confidence intervals for geodesic regression models. The approach is general, but illustrated and specifically developed for the Grassmann manifold, which allows us, e.g., to regress shapes or linear dynamical systems. Extensions to other manifolds can be obtained in a similar manner. We demonstrate our approach for regression with 2D/3D shapes using synthetic and real data.",,,,,"Kwitt, Roland/HII-6060-2022; Styner, Martin/AAS-9949-2020; Kwitt, Roland/AFS-8639-2022","Styner, Martin/0000-0002-8747-5118; ",,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,785,793,,,,,,,,,,,,,,,,WOS:000509368500084,0
C,"Kifer, D; Rogers, R",,"Singh, A; Zhu, J",,"Kifer, Daniel; Rogers, Ryan",,,A New Class of Private Chi-Square Tests,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54",Proceedings of Machine Learning Research,,,,20th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 20-22, 2017","Fort Lauderdale, FL",,,,,"In this paper, we develop new test statistics for private hypothesis testing. These statistics are designed specifically so that their asymptotic distributions, after accounting for noise added for privacy concerns, match the asymptotics of the classical (non-private) chi-square tests for testing if the multinomial data parameters lie in lower dimensional manifolds (examples include goodness of fit and independence testing). Empirically, these new test statistics outperform prior work, which focused on noisy versions of existing statistics.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2017,54,,,,,,991,1000,,,,,,,,,,,,,,,,WOS:000509368500106,0
C,"Balazs, G; Gyorgy, A; Szepesvari, C",,"Lebanon, G; Vishwanathan, SVN",,"Balazs, Gabor; Gyorgy, Andras; Szepesvari, Csaba",,,Near-optimal max-affine estimators for convex regression,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"This paper considers least squares estimators for regression problems over convex, uniformly bounded, uniformly Lipschitz function classes minimizing the empirical risk over max-affine functions (the maximum of finitely many affine functions). Based on new results on nonlinear nonparametric regression and on the approximation accuracy of max-affine functions, these estimators are proved to achieve the optimal rate of convergence up to logarithmic factors. Preliminary experiments indicate that a simple randomized approximation to the optimal estimator is competitive with state-of-the-art alternatives.",,,,,"Balazs, Gabor/AAN-9631-2020","Gyorgy, Andras/0000-0003-0586-4337",,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,56,64,,,,,,,,,,,,,,,,WOS:000508399700007,0
C,"Defossez, A; Bach, F",,"Lebanon, G; Vishwanathan, SVN",,"Defossez, Alexandre; Bach, Francis",,,Averaged Least-Mean-Squares: Bias-Variance Trade-offs and Optimal Sampling Distributions,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"We consider the least-squares regression problem and provide a detailed asymptotic analysis of the performance of averaged constant-step-size stochastic gradient descent. In the strongly-convex case, we provide an asymptotic expansion up to explicit exponentially decaying terms. Our analysis leads to new insights into stochastic approximation algorithms: (a) it gives a tighter bound on the allowed step-size; (b) the generalization error may be divided into a variance term which is decaying as O (1/n), independently of the step-size gamma, and a bias term that decays as O(1/gamma(2)n(2)); (c) when allowing non-uniform sampling of examples over a dataset, the choice of a good sampling density depends on the trade-off between bias and variance: when the variance term dominates, optimal sampling densities do not lead to much gain, while when the bias term dominates, we can choose larger step-sizes that lead to significant improvements.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,205,213,,,,,,,,,,,,,,,,WOS:000508399700023,0
C,"Kulesza, A; Jiang, N; Singh, S",,"Lebanon, G; Vishwanathan, SVN",,"Kulesza, Alex; Jiang, Nan; Singh, Satinder",,,Low-Rank Spectral Learning with Weighted Loss Functions,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"Kulesza et al. [2014] recently observed that low-rank spectral learning algorithms, which discard the smallest singular values of a moment matrix during training, can behave in unexpected ways, producing large errors even when the discarded singular values are arbitrarily small. In this paper we prove that when learning predictive state representations those problematic cases disappear if we introduce a particular weighted loss function and learn using sufficiently large sets of statistics; our main result is a bound on the loss of the learned low-rank model in terms of the singular values that are discarded. Practically speaking, this suggests that regardless of the model rank we should use the largest possible sets of statistics, and we show empirically that this is true on both synthetic and real-world domains.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,517,525,,,,,,,,,,,,,,,,WOS:000508399700057,0
C,"Lee, JY; Zaheer, M; Gunnemann, S; Smola, AJ",,"Lebanon, G; Vishwanathan, SVN",,"Lee, Jay-Yoon; Zaheer, Manzi; Gunnemann, Stephan; Smola, Alexander J.",,,Preferential Attachment in Graphs with Affinities,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 38",JMLR Workshop and Conference Proceedings,,,,18th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-12, 2015","San Diego, CA",,,,,"Preferential attachment models for random graphs are successful in capturing many characteristics of real networks such as power law behavior. At the same time they lack flexibility to take vertex to vertex affinities into account, a feature that is commonly used in many link recommendation algorithms. We propose a random graph model based on both node attributes and preferential attachment. This approach overcomes the limitation of existing models on expressing vertex affinity and on reflecting properties of different subgraphs. We analytically prove that our model preserves the power law behavior in the degree distribution as expressed by natural graphs and we show that it satisfies the small world property. Experiments show that our model provides an excellent fit of many natural graph statistics and we provide an algorithm to infer the associated affinity function efficiently.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2015,38,,,,,,571,580,,,,,,,,,,,,,,,,WOS:000508399700063,0
C,"Chierichetti, F; Panconesi, A; Re, G; Trevisan, L",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Chierichetti, Flavio; Panconesi, Alessandro; Re, Giuseppe; Trevisan, Luca",,,Spectral Robustness for Correlation Clustering Reconstruction in Semi-Adversarial Models,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Correlation Clustering is an important clustering problem with many applications. We study the reconstruction version of this problem, in which one seeks to reconstruct a latent clustering that has been corrupted by random noise and adversarial modifications. Concerning the latter, there is a standard post-adversarial model in the literature, in which adversarial modifications come after the noise. Here, we introduce and analyse a pre-adversarial model, in which adversarial modifications come before the noise. Given an input coming from such a semi-adversarial generative model, the goal is to approximately reconstruct with high probability the latent clustering. We focus on the case where the hidden clusters have nearly equal size and show the following. In the pre-adversarial setting, spectral algorithms are optimal, in the sense that they reconstruct all the way to the information-theoretic threshold beyond which no reconstruction is possible. This is in contrast to the post-adversarial setting, in which their ability to restore the hidden clusters stops before the threshold, but the gap is optimally filled by SDP-based algorithms. These results highlight a heretofore unknown robustness of spectral algorithms, showing them less brittle than previously thought.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852305016,0
C,"Fromme, L; Bogojeska, J; Kuhn, J",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Fromme, Lukas; Bogojeska, Jasmina; Kuhn, Jonas",,,ContextGen: Targeted Data Generation for Low Resource Domain Specific Text Classification,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"To address the challenging low-resource non-topical text classification problems in domain specific settings we introduce ContextGen a novel approach that uses targeted text generation with no fine tuning to augment the available small annotated dataset. It first adapts the powerful GPT-2 text generation model to generate samples relevant for the domain by using properly designed context text as input for generation. Then it assigns class labels to the newly generated samples after which they are added to the initial training set. We demonstrate the superior performance of a state-of-the-art text classifier trained with the augmented labelled dataset for four different non-topical tasks in the low resource setting, three of which are from specialized domains.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072703005,0
C,"Futami, F; Iwata, T; Ueda, N; Sato, I; Sugiyama, M",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Futami, Futoshi; Iwata, Tomoharu; Ueda, Naonori; Sato, Issei; Sugiyama, Masashi",,,Predictive variational Bayesian inference as risk-seeking optimization,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Since the Bayesian inference works poorly under model misspecification, various solutions have been explored to counteract the shortcomings. Recently proposed predictive Bayes (PB) that directly optimizes the Kullback Leibler divergence between the empirical distribution and the approximate predictive distribution shows excellent performances not only under model misspecification but also for over-parametrized models. However, its behavior and superiority are still unclear, which limits the applications of PB. Specifically, the superiority of PB has been shown only in terms of the predictive test log-likelihood and the performance in the sense of parameter estimation has not been investigated yet. Also, it is not clear why PB is superior with misspecified and over-parameterized models. In this paper, we clarify these ambiguities by studying PB in the framework of risk-seeking optimization. To achieve this, first, we provide a consistency theory for PB and then present intuition of robustness of PB to model mis-specification using a response function theory. Thereafter, we theoretically and numerically show that PB has an implicit regularization effect that leads to flat local minima in over-parametrized models.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072705008,0
C,"Ghalamkari, K; Sugiyama, M",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Ghalamkari, Kazu; Sugiyama, Mahito",,,Fast Rank-1 NMF for Missing Data with KL Divergence,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We propose a fast non-gradient-based method of rank-1 non-negative matrix factorization (NMF) for missing data, called A1GM, that minimizes the KL divergence from an input matrix to the reconstructed rank-1 matrix. Our method is based on our new finding of an analytical closed-formula of the best rank-1 non-negative multiple matrix factorization (NMMF), a variety of NMF. NMMF is known to exactly solve NMF for missing data if positions of missing values satisfy a certain condition, and A1GM transforms a given matrix so that the analytical solution to NMMF can be applied. We empirically show that A1GM is more efficient than a gradient method with competitive reconstruction errors.",,,,,,"Ghalamkari, Kazu/0000-0002-4779-2856",,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072703001,0
C,"Herbinger, J; Bischl, B; Casalicchio, G",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Herbinger, Julia; Bischl, Bernd; Casalicchio, Giuseppe",,,REPID: Regional Effect Plots with implicit Interaction Detection,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Machine learning models can automatically learn complex relationships, such as nonlinear and interaction effects. Interpretable machine learning methods such as partial dependence plots visualize marginal feature effects but may lead to misleading interpretations when feature interactions are present. Hence, employing additional methods that can detect and measure the strength of interactions is paramount to better understand the inner workings of machine learning models. We demonstrate several drawbacks of existing global interaction detection approaches, characterize them theoretically, and evaluate them empirically. Furthermore, we introduce regional effect plots with implicit interaction detection, a novel framework to detect interactions between a feature of interest and other features. The framework also quantifies the strength of interactions and provides interpretable and distinct regions in which feature effects can be interpreted more reliably, as they are less confounded by interactions. We prove the theoretical eligibility of our method and show its applicability on various simulation and real-world examples.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852304033,0
C,"Kviman, O; Melin, H; Koptage, H; Elvira, V; Lagergren, J",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Kviman, Oskar; Melin, Harald; Koptage, Hazal; Elvira, Victor; Lagergren, Jens",,,Multiple Importance Sampling ELBO and Deep Ensembles of Variational Approximations,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"In variational inference (VI), the marginal log-likelihood is estimated using the standard evidence lower bound (ELBO), or improved versions as the importance weighted ELBO (IWELBO). We propose the multiple importance sampling ELBO (MISELBO), a versatile yet simple framework. MISELBO is applicable in both amortized and classical VI, and it uses ensembles, e.g., deep ensembles, of independently inferred variational approximations. As far as we are aware, the concept of deep ensembles in amortized VI has not previously been established. We prove that MISELBO provides a tighter bound than the average of standard ELBOs, and demonstrate empirically that it gives tighter bounds than the average of IWELBOs. MISELBO is evaluated in density-estimation experiments that include MNIST and several real-data phylogenetic tree inference problems. First, on the MNIST dataset, MISELBO boosts the density-estimation performances of a state-of-the-art model, nouveau VAE. Second, in the phylogenetic tree inference setting, our framework enhances a state-of-the-art VI algorithm that uses normalizing flows. On top of the technical benefits of MISELBO, it allows to unveil connections between VI and recent advances in the importance sampling literature, paving the way for further methodological advances.",,,,,,"Koptagel, Hazal/0000-0002-6234-4656",,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852305010,0
C,"Lheritier, A; Bondoux, N",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Lheritier, Alix; Bondoux, Nicolas",,,A Cramer Distance perspective on Quantile Regression based Distributional Reinforcement Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Distributional reinforcement learning (DRL) extends the value-based approach by approximating the full distribution over future returns instead of the mean only, providing a richer signal that leads to improved performances. Quantile Regression (QR) based methods like QR-DQN project arbitrary distributions into a parametric subset of staircase distributions by minimizing the 1-Wasserstein distance. However, due to biases in the gradients, the quantile regression loss is used instead for training, guaranteeing the same minimizer and enjoying unbiased gradients. Non-crossing constraints on the quantiles have been shown to improve the performance of QR-DQN for uncertainty-based exploration strategies. The contribution of this work is in the setting of fixed quantile levels and is twofold. First, we prove that the Cramer distance yields a projection that coincides with the 1-Wasserstein one and that, under noncrossing constraints, the squared Cramer and the quantile regression losses yield collinear gradients, shedding light on the connection between these important elements of DRL. Second, we propose a low complexity algorithm to compute the Cramer distance.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,5774,5789,,,,,,,,,,,,,,,,WOS:000841852300008,0
C,"Liu, SC; Qu, M; Zhang, ZB; Cai, HY; Tang, J",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Liu, Shengchao; Qu, Meng; Zhang, Zuobai; Cai, Huiyu; Tang, Jian",,,Structured Multi-task Learning for Molecular Property Prediction,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Multi-task learning for molecular property prediction is becoming increasingly important in drug discovery. However, in contrast to other domains, the performance of multitask learning in drug discovery is still not satisfying as the number of labeled data for each task is too limited, which calls for additional data to complement the data scarcity. In this paper, we study multi-task learning for molecular property prediction in a novel setting, where a relation graph between tasks is available. We first construct a dataset including around 400 tasks as well as a task relation graph. Then to better utilize such relation graph, we propose a method called SGNN-EBM to systematically investigate the structured task modeling from two perspectives. (1) In the latent space, we model the task representations by applying a state graph neural network (SGNN) on the relation graph. (2) In the output space, we employ structured prediction with the energybased model (EBM), which can be efficiently trained through noise-contrastive estimation (NCE) approach. Empirical results justify the effectiveness of SGNN-EBM. Code is available on this GitHub repository.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852303015,0
C,"Marteau-Ferey, U; Bach, F; Rudi, A",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Marteau-Ferey, Ulysse; Bach, Francis; Rudi, Alessandro",,,Sampling from Arbitrary Functions via PSD Models,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"In many areas of applied statistics and machine learning, generating an arbitrary number of independent and identically distributed (i.i.d.) samples from a given distribution is a key task. When the distribution is known only through evaluations of the density, current methods either scale badly with the dimension or require involved implementations. Instead, we take a two-step approach by first modeling the probability distribution and then sampling from that model. We use the recently introduced class of positive semi-definite (PSD) models, which have been shown to be efficient for approximating probability densities. We show that these models can approximate a large class of densities concisely using few evaluations, and present a simple algorithm to effectively sample from these models. We also present preliminary empirical results to illustrate our assertions.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072702038,0
C,"Sheng, S; Vasan, GCK; Choi, CP; Sharpnack, J; Jones, T",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Sheng, Stephen; Vasan, Keerthi G. C.; Choi, Chi Po; Sharpnack, James; Jones, Tucker",,,An Unsupervised Hunt for Gravitational Lenses,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Strong gravitational lenses allow us to peer into the farthest reaches of space by bending the light from a background object around a massive object in the foreground. Unfortunately, these lenses are extremely rare, and manually finding them in astronomy surveys is difficult and time-consuming. We are thus tasked with finding them in an automated fashion with few if any, known lenses to form positive samples. To assist us with training, we can simulate realistic lenses within our survey images to form positive samples. Naively training a ResNet model with these simulated lenses results in a poor precision for the desired high recall, because the simulations contain artifacts that are learned by the model. In this work, we develop a lens detection method that combines simulation, data augmentation, semi-supervised learning, and GANs to improve this performance by an order of magnitude. We perform ablation studies and examine how performance scales with the number of non-lenses and simulated lenses. These findings allow researchers to go into a survey mostly blind and still classify strong gravitational lenses with high precision and recall.",,,,,,"G. C., Keerthi Vasan/0000-0002-2645-679X",,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852304016,0
C,"Tomaszewska, P; Zychowski, A; Mandziuk, J",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Tomaszewska, Paulina; Zychowski, Adam; Mandziuk, Jacek",,,Duel-based Deep Learning system for solving IQ tests,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"One of the relevant aspects of Artificial General Intelligence is the ability of machines to demonstrate abstract reasoning skills, for instance, through solving (human) IQ tests. This work presents a new approach to machine IQ tests solving formulated as Raven's Progressive Matrices (RPMs), called Duel-IQ. The proposed solution incorporates the concept of a tournament in which the best answer is chosen based on a set of duels between candidate RPM answers. The three relevant aspects are: (1) low computational and design complexity, (2) proposition of two schemes of pairing up candidate answers for the duels and (3) evaluation of the system on a dataset of shapes other than those used for training. Depending on a particular variant, the system reaches up to 82.8% accuracy on average in RPM tasks with 5 candidate answers and is on par with human performance and superior to other literature approaches of comparable complexity when training and test sets are from the same distribution.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852304042,0
C,"Triess, LT; Buhler, A; Peter, D; Flohr, FB; Zollner, JM",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Triess, Larissa T.; Buehler, Andre; Peter, David; Flohr, Fabian B.; Zoellner, J. Marius",,,Point Cloud Generation with Continuous Conditioning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Generative models can be used to synthesize 3D objects of high quality and diversity. However, there is typically no control over the properties of the generated object. This paper proposes a novel generative adversarial network (GAN) setup that generates 3D point cloud shapes conditioned on a continuous parameter. In an exemplary application, we use this to guide the generative process to create a 3D object with a custom-fit shape. We formulate this generation process in a multi-task setting by using the concept of auxiliary classifier GANs. Further, we propose to sample the generator label input for training from a kernel density estimation (KDE) of the dataset. Our ablations show that this leads to significant performance increase in regions with few samples. Extensive quantitative and qualitative experiments show that we gain explicit control over the object dimensions while maintaining good generation quality and diversity.",,,,,"Peter, David/A-2669-2013","Peter, David/0000-0001-7950-9915",,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072704025,0
C,"Wu, Y; Jin, T; Lou, H; Xu, P; Farnoud, F; Gu, QQ",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Wu, Yue; Jin, Tao; Lou, Hao; Xu, Pan; Farnoud, Farzad; Gu, Quanquan",,,Adaptive Sampling for Heterogeneous Rank Aggregation from Noisy Pairwise Comparisons,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"In heterogeneous rank aggregation problems, users often exhibit various accuracy levels when comparing pairs of items. Thus, a uniform querying strategy over users may not be optimal. To address this issue, we propose an elimination-based active sampling strategy, which estimates the ranking of items via noisy pairwise comparisons from multiple users and improves the users' average accuracy by maintaining an active set of users. We prove that our algorithm can return the true ranking of items with high probability. We also provide a sample complexity bound for the proposed algorithm, which outperforms the non-active strategies in the literature and close to oracle under mild conditions. Experiments are provided to show the empirical advantage of the proposed methods over the state-of-the-art baselines.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852305023,0
C,"Zhao, P; Wang, YX; Zhou, ZH",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Zhao, Peng; Wang, Yu-Xiang; Zhou, Zhi-Hua",,,Non-stationary Online Learning with Memory and Non-stochastic Control,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We study the problem of Online Convex Optimization (OCO) with memory, which allows loss functions to depend on past decisions and thus captures temporal effects of learning problems. In this paper, we introduce dynamic policy regret as the performance measure to design algorithms robust to non-stationary environments, which competes algorithms' decisions with a sequence of changing comparators. We propose a novel algorithm for OCO with memory that provably enjoys an optimal dynamic policy regret. The key technical challenge is how to control the switching cost, the cumulative movements of player's decisions, which is neatly addressed by a novel decomposition of dynamic policy regret and a careful design of meta-learner and base-learner that explicitly regularizes the switching cost. The results are further applied to tackle non-stationarity in online non-stochastic control [Agarwal et al., 2019], i.e., controlling a linear dynamical system with adversarial disturbance and convex cost functions. We derive a novel gradient-based controller with dynamic policy regret guarantees, which is the first controller provably competitive to a sequence of changing policies for online non-stochastic control.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072702007,0
C,"Zhu, Y; Katz-Samuels, J; Nowak, R",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Zhu, Yinglun; Katz-Samuels, Julian; Nowak, Robert",,,Near Instance Optimal Model Selection for Pure Exploration Linear Bandits,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"The model selection problem in the pure exploration linear bandit setting is introduced and studied in both the fixed confidence and fixed budget settings. The model selection problem considers a nested sequence of hypothesis classes of increasing complexities. Our goal is to automatically adapt to the instance-dependent complexity measure of the smallest hypothesis class containing the true model, rather than suffering from the complexity measure related to the largest hypothesis class. We provide evidence showing that a standard doubling trick over dimension fails to achieve the optimal instance-dependent sample complexity. Our algorithms define a new optimization problem based on experimental design that leverages the geometry of the action set to efficiently identify a near-optimal hypothesis class. Our fixed budget algorithm uses a novel application of a selection-validation trick in bandits. This provides a new method for the understudied fixed budget setting in linear bandits (even without the added challenge of model selection). We further generalize the model selection problem to the misspecified regime, adapting our algorithms in both fixed confidence and fixed budget settings.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852301008,0
C,"Amid, E; Anil, R; Warmuth, MK",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Amid, Ehsan; Anil, Rohan; Warmuth, Manfred K.",,,LocoProp: Enhancing BackProp via Local Loss Optimization,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Second-order methods have shown state-of-the-art performance for optimizing deep neural networks. Nonetheless, their large memory requirement and high computational complexity, compared to first-order methods, hinder their versatility in a typical low-budget setup. This paper introduces a general framework of layerwise loss construction for multilayer neural networks that achieves a performance closer to second-order methods while utilizing first-order optimizers only. Our methodology lies upon a three-component loss, target, and regularizer combination, for which altering each component results in a new update rule. We provide examples using squared loss and layerwise Bregman divergences induced by the convex integral functions of various transfer functions. Our experiments on benchmark models and datasets validate the efficacy of our new approach, reducing the gap between first-order and second-order optimizers.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852304006,0
C,"Baak, M; Brugman, S; D'almeida, L; Rojas, IF; Oger, JB; Urlus, R",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Baak, Max; Brugman, Simon; D'almeida, Lorraine; Rojas, Ilan Fridman; Oger, Jean-Baptiste; Urlus, Ralph",,,"Synthsonic: Fast, Probabilistic modeling and Synthesis of Tabular Data","INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"The creation of realistic, synthetic datasets has several purposes with growing demand in recent times, e.g. privacy protection and other cases where real data cannot be easily shared. A multitude of primarily neural networks (NNs), e.g. Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), or Bayesian Network (BN) approaches have been created to tackle this problem, however these require extensive compute resources, lack interpretability, and in some instances lack replication fidelity as well. We propose a hybrid, probabilistic approach for synthesizing pairwise independent tabular data, called Synthsonic. A sequence of well-understood, invertible statistical transformations removes first-order correlations, then a Bayesian Network jointly models continuous and categorical variables, and a calibrated discriminative learner captures the remaining dependencies. Replication studies on MIT's SDGym benchmark show marginally or significantly better performance than all prior BN-based approaches, while being competitive with NN-based approaches (first place in 10 out of 13 benchmark datasets). The computational time required to learn the data distribution is at least one order of magnitude lower than the NN methods. Furthermore, inspecting intermediate results during the synthetic data generation allows easy diagnostics and tailored corrections. We believe the combination of out-of-the-box performance, speed and interpretability make this method a significant addition to the synthetic data generation toolbox.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072704038,0
C,"Barakat, A; Bianchi, P; Lehmann, J",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Barakat, Anas; Bianchi, Pascal; Lehmann, Julien",,,Analysis of a Target-Based Actor-Critic Algorithm with Linear Function Approximation,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Actor-critic methods integrating target networks have exhibited a stupendous empirical success in deep reinforcement learning. However, a theoretical understanding of the use of target networks in actor-critic methods is largely missing in the literature. In this paper, we reduce this gap between theory and practice by proposing the first theoretical analysis of an online target-based actor-critic algorithm with linear function approximation in the discounted reward setting. Our algorithm uses three different timescales: one for the actor and two for the critic. Instead of using the standard single timescale temporal difference (TD) learning algorithm as a critic, we use a two timescales target-based version of TD learning closely inspired from practical actor-critic algorithms implementing target networks. First, we establish asymptotic convergence results for both the critic and the actor under Markovian sampling. Then, we provide a finite-time analysis showing the impact of incorporating a target network into actor-critic methods.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072701003,0
C,"Chen, JB; Yuan, R; Garrigos, G; Gower, RM",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Chen, Jiabin; Yuan, Rui; Garrigos, Guillaume; Gower, Robert M.",,,SAN: Stochastic Average Newton Algorithm for Minimizing Finite Sums,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We present a principled approach for designing stochastic Newton methods for solving finite sum optimization problems. Our approach has two steps. First, we re-write the stationarity conditions as a system of nonlinear equations that associates each data point to a new row. Second, we apply a Sub-sampled Newton Raphson method to solve this system of nonlinear equations. Using our approach, we develop a new Stochastic Average Newton (SAN) method, which is incremental by design, in that it requires only a single data point per iteration. It is also cheap to implement when solving regularized generalized linear models, with a cost per iteration of the order of the number of the parameters. We show through numerical experiments that SAN requires no knowledge about the problem, neither parameter tuning, while remaining competitive as compared to classical variance reduced gradient methods (e.g. SAG and SVRG), incremental Newton and quasi-Newton methods (e.g. SNM, IQN).",,,,,,"YUAN, Rui/0000-0002-1768-9639",,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,279,318,,,,,,,,,,,,,,,,WOS:000828072700013,0
C,"Cohen-Karlik, E; Ben David, A; Cohen, N; Globerson, A",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Cohen-Karlik, Edo; Ben David, Avichai; Cohen, Nadav; Globerson, Amir",,,On the Implicit Bias of Gradient Descent for Temporal Extrapolation,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"When using recurrent neural networks (RNNs) it is common practice to apply trained models to sequences longer than those seen in training. This \extrapolating usage deviates from the traditional statistical learning setup where guarantees are provided under the assumption that train and test distributions are identical. Here we set out to understand when RNNs can extrapolate, focusing on a simple case where the data generating distribution is memoryless. We first show that even with infinite training data, there exist RNN models that interpolate perfectly (i.e., they fit the training data) yet extrapolate poorly to longer sequences. We then show that if gradient descent is used for training, learning will converge to perfect extrapolation under certain assumptions on initialization. Our results complement recent studies on the implicit bias of gradient descent, showing that it plays a key role in extrapolation when learning temporal prediction models.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852305020,0
C,"Deng, S; Guo, YL; Hsu, D; Mandal, D",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Deng, Samuel; Guo, Yilin; Hsu, Daniel; Mandal, Debmalya",,,Learning Tensor Representations for Meta-Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We introduce a tensor-based model of shared representation for meta-learning from a diverse set of tasks. Prior works on learning linear representations for meta-learning assume that there is a common shared representation across different tasks, and do not consider the additional task-specific observable side information. In this work, we model the meta-parameter through an order-3 tensor, which can adapt to the observed task features of the task. We propose two methods to estimate the underlying tensor. The first method solves a tensor regression problem and works under natural assumptions on the data generating process. The second method uses the method of moments under additional distributional assumptions and has an improved sample complexity in terms of the number of tasks. We also focus on the meta-test phase, and consider estimating task-specific parameters on a new task. Substituting the estimated tensor from the first step allows us estimating the task-specific parameters with very few samples of the new task, thereby showing the benefits of learning tensor representations for meta-learning. Finally, through simulation and several real-world datasets, we evaluate our methods and show that it improves over previous linear models of shared representations for meta-learning.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852306004,0
C,"Diakonikolas, I; Kane, D; Manurangsi, P; Ren, LS",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Diakonikolas, Ilias; Kane, Daniel; Manurangsi, Pasin; Ren, Lisheng",,,Hardness of Learning a Single Neuron with Adversarial Label Noise,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We study the problem of distribution-free PAC learning a single neuron under adversarial label noise with respect to the squared loss. For a range of activation functions, including ReLUs and sigmoids, we prove strong computational hardness of learning results in the Statistical Query model and under a well-studied assumption on the complexity of refuting XOR formulas. Specifically, we establish that no polynomial-time learning algorithm, even improper, can approximate the optimal loss value within any constant factor.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852302027,0
C,"Fox, R; McAleer, S; Overman, W; Panageas, I",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Fox, Roy; McAleer, Stephen; Overman, William; Panageas, Ioannis",,,Independent Natural Policy Gradient Always Converges in Markov Potential Games,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Natural policy gradient has emerged as one of the most successful algorithms for computing optimal policies in challenging Reinforcement Learning (RL) tasks, but very little was known about its convergence properties until recently. The picture becomes more blurry when it comes to multi-agent RL (MARL), where only few works have theoretical guarantees for convergence to Nash policies. In this paper, we focus on a particular class of multi-agent stochastic games called Markov Potential Games and prove that Independent Natural Policy Gradient always converges using constant learning rates. The proof deviates from existing approaches and overcomes the challenge that Markov potential Games do not have unique optimal values (as single-agent settings exhibit), leading different initializations to different limit point values. We complement our theoretical results with experiments that indicate that Natural Policy Gradient outperforms Policy Gradient in multi-state congestion games.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072704022,0
C,"Gamba, M; Chmielewski-Anders, A; Sullivan, J; Azizpour, H; Bjorkman, M",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Gamba, Matteo; Chmielewski-Anders, Adrian; Sullivan, Josephine; Azizpour, Hossein; Bjorkman, Marten",,,Are All Linear Regions Created Equal?,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"The number of linear regions has been studied as a proxy of complexity for ReLU networks. However, the empirical success of network compression techniques like pruning and knowledge distillation, suggest that in the overparameterized setting, linear regions density might fail to capture the effective nonlinearity. In this work, we propose an efficient algorithm for discovering linear regions and use it to investigate the effectiveness of density in capturing the nonlinearity of trained VGGs and ResNets on CIFAR-10 and CIFAR-100. We contrast the results with a more principled nonlinearity measure based on function variation, highlighting the shortcomings of linear regions density. Furthermore, interestingly, our measure of nonlinearity clearly correlates with model-wise deep double descent, connecting reduced test error with reduced nonlinearity, and increased local similarity of linear regions.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852301002,0
C,"Gao, M; Tai, WM; Aragam, B",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Gao, Ming; Tai, Wai Ming; Aragam, Bryon",,,Optimal Estimation of Gaussian DAG Models,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We study the optimal sample complexity of learning a Gaussian directed acyclic graph (DAG) from observational data. Our main results establish the minimax optimal sample complexity for learning the structure of a linear Gaussian DAG model in two settings of interest: 1) Under equal variances without knowledge of the true ordering, and 2) For general linear models given knowledge of the ordering. In both cases the sample complexity is n asymptotic to q log(d/q), where q is the maximum number of parents and d is the number of nodes. We further make comparisons with the classical problem of learning (undirected) Gaussian graphical models, showing that under the equal variance assumption, these two problems share the same optimal sample complexity. In other words, at least for Gaussian models with equal error variances, learning a directed graphical model is statistically no more difficult than learning an undirected graphical model. Our results also extend to more general identification assumptions as well as subgaussian errors.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852303009,0
C,"Gasanov, E; Khaled, A; Horvath, S; Richtarik, P",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Gasanov, Elnur; Khaled, Ahmed; Horvath, Samuel; Richtarik, Peter",,,FLIX: A Simple and Communication-Efficient Alternative to Local Methods in Federated Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Federated Learning (FL) is an increasingly popular machine learning paradigm in which multiple nodes try to collaboratively learn under privacy, communication and multiple heterogeneity constraints. A persistent problem in federated learning is that it is not clear what the optimization objective should be: the standard average risk minimization of supervised learning is inadequate in handling several major constraints specific to federated learning, such as communication adaptivity and personalization control. We identify several key desiderata in frameworks for federated learning and introduce a new framework, FLIX, that takes into account the unique challenges brought by federated learning. FLIX has a standard finite-sum form, which enables practitioners to tap into the immense wealth of existing (potentially non-local) methods for distributed optimization. Through a smart initialization that does not require any communication, FLIX does not require the use of local steps but is still provably capable of performing dissimilarity regularization on par with local methods. We give several algorithms for solving the FLIX formulation efficiently under communication constraints. Finally, we corroborate our theoretical results with extensive experimentation.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852305041,0
C,"Jedra, Y; Proutiere, A",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Jedra, Yassir; Proutiere, Alexandre",,,Minimal Expected Regret in Linear Quadratic Control,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We consider the problem of online learning in Linear Quadratic Control systems whose state transition and state-action transition matrices A and B may be initially unknown. We devise an online learning algorithm and provide guarantees on its expected regret. This regret at time T is upper bounded (i) by (O) over tilde((d(u) + d(x))root d(x)T) when A and B are unknown, (ii) by (O) over tilde (d(x)(2) log(T)) if only A is unknown, and (iii) by (O) over tilde (d(x)(d(u) + d(x))log(T)) if only B is unknown and under some mild non-degeneracy condition (d(x) and d(u) denote the dimensions of the state and of the control input, respectively). These regret scalings are minimal in T, d(x) and d(u) as they match existing lower bounds in scenario (i) when d(x) <= d(u) (Simchowitz and Foster, 2020), and in scenario (ii) (Lai, 1986). We conjecture that our upper bounds are also optimal in scenario (iii) (there is no known lower bound in this setting). Existing online algorithms proceed in epochs of (typically exponentially) growing durations. The control policy is fixed within each epoch, which considerably simplifies the analysis of the estimation error on A and B and hence of the regret. Our algorithm departs from this design choice: it is a simple variant of certainty-equivalence regulators, where the estimates of A and B and the resulting control policy can be updated as frequently as we wish, possibly at every step. Quantifying the impact of such a constantly-varying control policy on the performance of these estimates and on the regret constitutes one of the technical challenges tackled in this paper.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852304034,0
C,"Katiyar, A; Basu, S; Shah, V; Caramanis, C",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Katiyar, Ashish; Basu, Soumya; Shah, Vatsal; Caramanis, Constantine",,,Recoverability Landscape of Tree Structured Markov Random Fields under Symmetric Noise,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We study the problem of learning tree-structured Markov random fields (MRF) on discrete random variables with common support when the observations are corrupted by a k-ary symmetric noise channel with unknown probability of error. For Ising models (support size = 2), past work has shown that graph structure can only be recovered up to the leaf clusters (a leaf node, its parent, and its siblings form a leaf cluster) and exact recovery is impossible. No prior work has addressed the setting of support size of 3 or more, and indeed this setting is far richer. As we show, when the support size is 3 or more, the structure of the leaf clusters may be partially or fully identifiable. We provide a precise characterization of this phenomenon and show that the extent of recoverability is dictated by the joint PMF of the random variables. In particular, we provide necessary and sufficient conditions for exact recoverability. Furthermore, we present a polynomial time, sample efficient algorithm that recovers the exact tree when this is possible, or up to the unidentifiability as promised by our characterization, when full recoverability is impossible. Finally, we demonstrate the efficacy of our algorithm experimentally.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852303032,0
C,"Koga, T; Meehan, C; Chaudhuri, K",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Koga, Tatsuki; Meehan, Casey; Chaudhuri, Kamalika",,,Privacy Amplification by Subsampling in Time Domain,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Aggregate time-series data like traffic flow and site occupancy repeatedly sample statistics from a population across time. Such data can be profoundly useful for understanding trends within a given population, but also pose a significant privacy risk, potentially revealing e.g., who spends time where. Producing a private version of a time-series satisfying the standard definition of Differential Privacy (DP) is challenging due to the large influence a single participant can have on the sequence: if an individual can contribute to each time step, the amount of additive noise needed to satisfy privacy increases linearly with the number of time steps sampled. As such, if a signal spans a long duration or is oversampled, an excessive amount of noise must be added, drowning out underlying trends. However, in many applications an individual realistically cannot participate at every time step. When this is the case, we observe that the influence of a single participant (sensitivity) can be reduced by subsampling and/or filtering in time, while still meeting privacy requirements. Using a novel analysis, we show this significant reduction in sensitivity and propose a corresponding class of privacy mechanisms. We demonstrate the utility benefits of these techniques empirically with real-world and synthetic time-series data.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072704006,0
C,"Liu, L; Pal, S; Harchaoui, Z",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Liu, Lang; Pal, Soumik; Harchaoui, Zaid",,,Entropy Regularized Optimal Transport Independence Criterion,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We introduce an independence criterion based on entropy regularized optimal transport. Our criterion can be used to test for independence between two samples. We establish non-asymptotic bounds for our test statistic and study its statistical behavior under both the null hypothesis and the alternative hypothesis. The theoretical results involve tools from U-process theory and optimal transport theory. We also offer a random feature type approximation for large-scale problems, as well as a differentiable program implementation for deep learning applications. We present experimental results on existing benchmarks for independence testing, illustrating the interest of the proposed criterion to capture both linear and nonlinear dependencies in synthetic data and real data.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852305035,0
C,"Lucic, A; ter Hoeve, M; Tolomei, G; de Rijke, M; Silvestri, F",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Lucic, Ana; ter Hoeve, Maartje; Tolomei, Gabriele; de Rijke, Maarten; Silvestri, Fabrizio",,,CF-GNNExplainer: Counterfactual Explanations for Graph Neural Networks,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Given the increasing promise of graph neural networks (GNNs) in real-world applications, several methods have been developed for explaining their predictions. Existing methods for interpreting predictions from GNNs have primarily focused on generating subgraphs that are especially relevant for a particular prediction. However, such methods are not counterfactual (CF) in nature: given a prediction, we want to understand how the prediction can be changed in order to achieve an alternative outcome. In this work, we propose a method for generating CF explanations for GNNs: the minimal perturbation to the input (graph) data such that the prediction changes. Using only edge deletions, we find that our method, CF-GNNExplainer, can generate CF explanations for the majority of instances across three widely used datasets for GNN explanations, while removing less than 3 edges on average, with at least 94% accuracy. This indicates that CF-GNNExplainer primarily removes edges that are crucial for the original predictions, resulting in minimal CF explanations.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072704027,0
C,"Martinet, G; Strzalkowski, A; Engelhardt, BE",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Martinet, Guillaume; Strzalkowski, Alexander; Engelhardt, Barbara E.",,,Variance Minimization in the Wasserstein Space for Invariant Causal Prediction,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Selecting powerful predictors for an outcome is a cornerstone task for machine learning. However, some types of questions can only be answered by identifying the predictors that causally affect the outcome. A recent approach to this causal inference problem leverages the invariance property of a causal mechanism across differing experimental environments (Peters et al., 2016; Heinze-Deml et al., 2018). This method, invariant causal prediction (ICP), has a substantial computational defect - the runtime scales exponentially with the number of possible causal variables. In this work, we show that the approach taken in ICP may be reformulated as a series of nonparametric tests that scales linearly in the number of predictors. Each of these tests relies on the minimization of a novel loss function - the Wasserstein variance - that is derived from tools in optimal transport theory and is used to quantify distributional variability across environments. We prove under mild assumptions that our method is able to recover the set of identifiable direct causes, and we demonstrate in our experiments that it is competitive with other benchmark causal discovery algorithms.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852303012,0
C,"Mukherjee, S; Tripathy, A; Nowak, R",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Mukherjee, Subhojyoti; Tripathy, Ardhendu; Nowak, Robert",,,Chernoff Sampling for Active Testing and Extension to Active Regression,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Active learning can reduce the number of samples needed to perform a hypothesis test and to estimate the parameters of a model. In this paper, we revisit the work of Chernoff that described an asymptotically optimal algorithm for performing a hypothesis test. We obtain a novel sample complexity bound for Chernoff's algorithm, with a non-asymptotic term that characterizes its performance at a fixed confidence level. We also develop an extension of Chernoff sampling that can be used to estimate the parameters of a wide variety of models and we obtain a non-asymptotic bound on the estimation error. We apply our extension of Chernoff sampling to actively learn neural network models and to estimate parameters in real-data linear and non-linear regression problems, where our approach performs favorably to state-of-the-art methods.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852301037,0
C,"Orvieto, A; Kohler, J; Pavllo, D; Hofmann, T; Lucchi, A",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Orvieto, Antonio; Kohler, Jonas; Pavllo, Dario; Hofmann, Thomas; Lucchi, Aurelien",,,Vanishing Curvature in Randomly Initialized Deep ReLU Networks,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Deep ReLU networks are at the basis of many modern neural architectures. Yet, the loss landscape of such networks and its interaction with state-of-the-art optimizers is not fully understood. One of the most crucial aspects is the landscape at random initialization, which often influences convergence speed dramatically. In their seminal works, Xavier & Bengio, 2010 and He et al., 2015 propose an initialization strategy that is supposed to prevent gradients from vanishing. Yet, we identify shortcomings of their expectation analysis as network depth increases, and show that the proposed initialization can actually fail to deliver stable gradient norms. More precisely, by leveraging an in-depth analysis of the median of the forward pass, we first show that, with high probability, vanishing gradients cannot be circumvented when the network width scales with less than Omega(depth). Second, we extend this analysis to second-order derivatives and show that random i.i.d. initialization also gives rise to Hessian matrices with eigenspectra that vanish in depth. Whenever this happens, optimizers are initialized in a very flat, saddle pointlike plateau, which is particularly hard to escape with stochastic gradient descent (SGD) as its escaping time is inversely related to curvature magnitudes. We believe that this observation is crucial for fully understanding (a) the historical difficulties of training deep nets with vanilla SGD and (b) the success of adaptive gradient methods, which naturally adapt to curvature and thus quickly escape flat plateaus.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852302016,0
C,"Zhang, ZY; Cutkosky, A; Paschalidis, IC",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Zhang, Zhiyu; Cutkosky, Ashok; Paschalidis, Ioannis Ch",,,Adversarial Tracking Control via Strongly Adaptive Online Learning with Memory,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We consider the problem of tracking an adversarial state sequence in a linear dynamical system subject to adversarial disturbances and loss functions, generalizing earlier settings in the literature. To this end, we develop three techniques, each of independent interest. First, we propose a comparator-adaptive algorithm for online linear optimization with movement cost. Without tuning, it nearly matches the performance of the optimally tuned gradient descent in hindsight. Next, considering a related problem called online learning with memory, we construct a novel strongly adaptive algorithm that uses our first contribution as a building block. Finally, we present the first reduction from adversarial tracking control to strongly adaptive online learning with memory. Summarizing these individual techniques, we obtain an adversarial tracking controller with a strong performance guarantee even when the reference trajectory has a large range of movement.(1)",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852302038,0
C,"Azari, B; Erdogmus, D",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Azari, Bahar; Erdogmus, Deniz",,,Equivariant Deep Dynamical Model for Motion Prediction,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Learning representations through deep generative modeling is a powerful approach for dynamical modeling to discover the most simplified and compressed underlying description of the data, to then use it for other tasks such as prediction. Most learning tasks have intrinsic symmetries, i.e., the input transformations leave the output unchanged, or the output undergoes a similar transformation. The learning process is, however, usually uninformed of these symmetries. Therefore, the learned representations for individually transformed inputs may not be meaningfully related. In this paper, we propose an SO(3) equivariant deep dynamical model (EqDDM) for motion prediction that learns a structured representation of the input space in the sense that the embedding varies with symmetry transformations. EqDDM is equipped with equivariant networks to parameterize the state-space emission and transition models. We demonstrate the superior predictive performance of the proposed model on various motion data.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852306009,0
C,"Choi, Y; Friedman, T; Van den Broeck, G",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Choi, YooJung; Friedman, Tal; Van den Broeck, Guy",,,Solving Marginal MAP Exactly by Probabilistic Circuit Transformations,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Probabilistic circuits (PCs) are a class of tractable probabilistic models that allow efficient, often linear-time, inference of queries such as marginals and most probable explanations (MPE). However, marginal MAP, which is central to many decision-making problems, remains a hard query for PCs unless they satisfy highly restrictive structural constraints. In this paper, we develop a pruning algorithm that removes parts of the PC that are irrelevant to a marginal MAP query, shrinking the PC while maintaining the correct solution. This pruning technique is so effective that we are able to build a marginal MAP solver based solely on iteratively trans- forming the circuit no search is required. We empirically demonstrate the efficacy of our approach on real-world datasets.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852304032,0
C,"Garg, S; Vempala, SS",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Garg, Shivam; Vempala, Santosh S.",,,How and When Random Feedback Works: A Case Study of Low-Rank Matrix Factorization,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"The success of gradient descent in ML and especially for learning neural networks is remarkable and robust. In the context of how the brain learns, one aspect of gradient descent that appears biologically difficult to realize (if not implausible) is that its updates rely on feedback from later layers to earlier layers through the same connections. Such bidirected links are relatively few in brain networks, and even when reciprocal connections exist, they may not be equi-weighted. Random Feedback Alignment (Lillicrap et al., 2016), where the backward weights are random and fixed, has been proposed as a bio-plausible alternative and found to be effective empirically. We investigate how and when feedback alignment (FA) works, focusing on one of the most basic problems with layered structure - low-rank matrix factorization. In this problem, given a matrix Y-nxm, the goal is to find a low rank factorization Z(nxr)W(rxm) that minimizes the error parallel to ZW - Y parallel to(F). Gradient descent solves this problem optimally. We show that FA finds the optimal solution when r >= rank(Y). We also shed light on how FA works. It is observed empirically that the forward weight matrices and (random) feedback matrices come closer during FA updates. Our analysis rigorously derives this phenomenon and shows how it facilitates convergence of FA*, a closely related variant of FA. We also show that FA can be far from optimal when r < rank(Y). This is the first provable separation result between gradient descent and FA. Moreover, the representations found by gradient descent and FA can be almost orthogonal even when their error parallel to ZW - Y parallel to(F) is approximately equal. As a corollary, these results also hold for training two-layer linear neural networks when the training input is isotropic, and the output is a linear function of the input.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072704007,0
C,"Hong, CK; Shelton, CR",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Hong, Chengkuan; Shelton, Christian R.",,,Deep Neyman-Scott Processes,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"A Neyman-Scott process is a special case of a Cox process. The latent and observable stochastic processes are both Poisson processes. We consider a deep Neyman-Scott process in this paper, for which the building components of a network are all Poisson processes. We develop an efficient posterior sampling via Markov chain Monte Carlo and use it for likelihood-based inference. Our method opens up room for the inference in sophisticated hierarchical point processes. We show in the experiments that more hidden Poisson processes brings better performance for likelihood fitting and events types prediction. We also compare our method with state-of-the-art models for temporal real-world datasets and demonstrate competitive abilities for both data fitting and prediction, using far fewer parameters.",,,,,"Shelton, Christian/GQJ-1146-2022","Shelton, Christian/0000-0001-6698-7838",,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072703031,0
C,"Jafarnia-Jahromi, M; Jain, R; Nayyar, A",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Jafarnia-Jahromi, Mehdi; Jain, Rahul; Nayyar, Ashutosh",,,Online Learning for Unknown Partially Observable MDPs,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Solving Partially Observable Markov Decision Processes (POMDPs) is hard. Learning optimal controllers for POMDPs when the model is unknown is harder. Online learning of optimal controllers for unknown POMDPs, which requires efficient learning using regret-minimizing algorithms that effectively tradeoff exploration and exploitation, is even harder, and no solution exists currently. In this paper, we consider infinitehorizon average-cost POMDPs with unknown transition model, though a known observation model. We propose a natural posterior sampling-based reinforcement learning algorithm (PSRL-POMDP) and show that it achieves a regret bound of O(log T), where T is the time horizon, when the parameter set is finite. In the general case (continuous parameter set), we show that the algorithm achieves O (T-2/3) regret under two technical assumptions. To the best of our knowledge, this is the first online RL algorithm for POMDPs and has sub-linear regret.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072701036,0
C,"Mokhtarian, E; Jamshidi, F; Etesami, J; Kiyavash, N",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Mokhtarian, Ehsan; Jamshidi, Fateme; Etesami, Jalal; Kiyavash, Negar",,,Causal Effect Identification with Context-specific Independence Relations of Control Variables,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We study the problem of causal effect identification from observational distribution given the causal graph and some context-specific independence (CSI) relations. It was recently shown that this problem is NP-hard, and while a sound algorithm to learn the causal effects is proposed in Tikka et al. (2019), no provably complete algorithm for the task exists. In this work, we propose a sound and complete algorithm for the setting when the CSI relations are limited to observed nodes with no parents in the causal graph. One limitation of the state of the art in terms of its applicability is that the CSI relations among all variables, even unobserved ones, must be given (as opposed to learned). Instead, We introduce a set of graphical constraints under which the CSI relations can be learned from mere observational distribution. This expands the set of identifiable causal effects beyond the state of the art.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852305034,0
C,"Richardson, TS; Liu, Y; McQueen, J; Hains, D",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Richardson, Thomas S.; Liu, Yu; McQueen, James; Hains, Doug",,,A Bayesian Model for Online Activity Sample Sizes,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"In many contexts it is useful to predict the number of individuals in some population who will initiate a particular activity during a given period. For example, the number of users who will install a software update, the number of customers who will use a new feature on a website or who will participate in an A/B test. In practical settings, there is heterogeneity amongst individuals with regard to the distribution of time until they will initiate. For these reasons it is inappropriate to assume that the number of new individuals observed on successive days will be identically distributed. Given observations on the number of unique users participating in an initial period, we present a simple but novel Bayesian method for predicting the number of additional individuals who will participate during a subsequent period. We illustrate the performance of the method in predicting sample size in online experimentation.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072701038,0
C,"Shah, A; Chen, WN; Balle, J; Kairouz, P; Theis, L",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Shah, Abhin; Chen, Wei-Ning; Balle, Johannes; Kairouz, Peter; Theis, Lucas",,,Optimal Compression of Locally Differentially Private Mechanisms,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Compressing the output of epsilon-locally differentially private (LDP) randomizers naively leads to suboptimal utility. In this work, we demonstrate the benefits of using schemes that jointly compress and privatize the data using shared randomness. In particular, we investigate a family of schemes based on Minimal Random Coding (Havasi et al., 2019) and prove that they offer optimal privacy-accuracy-communication tradeoffs. Our theoretical and empirical findings show that our approach can compress PrivUnit(2) (Bhowmick et al., 2018) and Subset Selection (Ye and Barg, 2018), the best known LDP algorithms for mean and frequency estimation, to the order of e bits of communication while preserving their privacy and accuracy guarantees.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852302006,0
C,"Shah, A; Shanmugam, K; Ahuja, K",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Shah, Abhin; Shanmugam, Karthikeyan; Ahuja, Kartik",,,Finding Valid Adjustments under Non-ignorability with Minimal DAG Knowledge,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Treatment effect estimation from observational data is a fundamental problem in causal inference. There are two very different schools of thought that have tackled this problem. On the one hand, the Pearlian framework commonly assumes structural knowledge (provided by an expert) in the form of directed acyclic graphs and provides graphical criteria such as the back-door criterion to identify the valid adjustment sets. On the other hand, the potential outcomes (PO) framework commonly assumes that all the observed features satisfy ignorability (i.e., no hidden confounding), which in general is untestable. In prior works that attempted to bridge these frameworks, there is an observational criteria to identify an anchor variable and if a subset of covariates (not involving the anchor variable) passes a suitable conditional independence criteria, then that subset is a valid back-door. Our main result strengthens these prior results by showing that under a different expert-driven structural knowledge - that one variable is a direct causal parent of the treatment variable - remarkably, testing for subsets (not involving the known parent variable) that are valid back-doors is equivalent to an invariance test. Importantly, we also cover the non-trivial case where the entire set of observed features is not ignorable (generalizing the PO framework) without requiring the knowledge of all the parents of the treatment variable. Our key technical idea involves generation of a synthetic sub-sampling (or environment) variable that is a function of the known parent variable. In addition to designing an invariance test, this sub-sampling variable allows us to leverage Invariant Risk Minimization, and thus, connects finding valid adjustments (in non-ignorable observational settings) to representation learning. We demonstrate the effectiveness and tradeoffs of these approaches on a variety of synthetic datasets as well as real causal effect estimation benchmarks.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072705025,0
C,"Terjek, D; Gonzalez-Sanchez, D",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Terjek, David; Gonzalez-Sanchez, Diego",,,Optimal transport with f-divergence regularization and generalized Sinkhorn algorithm,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Entropic regularization provides a generalization of the original optimal transport problem. It introduces a penalty term defined by the Kullback-Leibler divergence, making the problem more tractable via the celebrated Sinkhorn algorithm. Replacing the Kullback-Leibler divergence with a general f-divergence leads to a natural generalization. The case of divergences defined by superlinear functions was recently studied by Di Marino and Gerolin. Using convex analysis, we extend the theory developed so far to include all f-divergences defined by functions of Legendre type, and prove that under some mild conditions, strong duality holds, optimums in both the primal and dual problems are attained, the generalization of the c-transform is well-defined, and we give sufficient conditions for the generalized Sinkhorn algorithm to converge to an optimal solution. We propose a practical algorithm for computing an approximate solution of the optimal transport problem with f-divergence regularization via the generalized Sinkhorn algorithm. Finally, we present experimental results on synthetic 2-dimensional data, demonstrating the effects of using different f-divergences for regularization, which influences convergence speed, numerical stability and sparsity of the optimal coupling.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072705011,0
C,"Wang, HG; Yang, Y; Pati, D; Bhattacharya, A",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Wang, Honggang; Yang, Yun; Pati, Debdeep; Bhattacharya, Anirban",,,Structured Variational Inference in Bayesian State-Space Models,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Variational inference is routinely deployed in Bayesian state-space models as an efficient computational technique. Motivated by the inconsistency issue observed by Wang and Titterington (Wang & Titterington, 2004) for the mean-field approximation in linear statespace models, we consider a more expressive variational family for approximating the joint posterior of the latent variables to retain their dependence, while maintaining the mean-field (i.e. independence) structure between latent variables and parameters. In state-space models, such a latent structure adapted mean-field approximation can be efficiently computed using the belief propagation algorithm. Theoretically, we show that this adapted mean-field approximation leads to consistent variational estimates. Furthermore, we derive a non-asymptotic risk bound for an averaged a-divergence from the true data generating model, suggesting that the posterior mean of the best variational approximation for the static parameters shows optimal concentration. From a broader perspective, we add to the growing literature on statistical accuracy of variational approximations by allowing dependence between the latent variables, and the techniques developed here should be useful in related contexts.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852303014,0
C,"Ferrando, C; Wang, SF; Sheldon, D",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Ferrando, Cecilia; Wang, Shufan; Sheldon, Daniel",,,Parametric Bootstrap for Differentially Private Confidence Intervals,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"The goal of this paper is to develop a practical and general-purpose approach to construct confidence intervals for differentially private parametric estimation. We find that the parametric bootstrap is a simple and effective solution. It cleanly reasons about variability of both the data sample and the randomized privacy mechanism and applies out of the box to a wide class of private estimation routines. It can also help correct bias caused by clipping data to limit sensitivity. We prove that the parametric bootstrap gives consistent confidence intervals in two broadly relevant settings, including a novel adaptation to linear regression that avoids accessing the covariate data multiple times. We demonstrate its effectiveness for a variety of estimators, and find empirically that it provides confidence intervals with good coverage even at modest sample sizes and performs better than alternative approaches.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072701030,0
C,"Garg, S; Tosatto, S; Pan, YC; White, M; Mahmood, AR",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Garg, Shivam; Tosatto, Samuele; Pan, Yangchen; White, Martha; Mahmood, A. Rupam",,,An Alternate Policy Gradient Estimator for Softmax Policies,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Policy gradient (PG) estimators are ineffective in dealing with softmax policies that are sub-optimally saturated, which refers to the situation when the policy concentrates its probability mass on sub-optimal actions. Sub-optimal policy saturation may arise from bad policy initialization or sudden changes in the environment that occur after the policy has already converged. Current softmax PG estimators require a large number of updates to overcome policy saturation, which causes low sample efficiency and poor adaptability to new situations. To mitigate this problem, we propose a novel PG estimator for softmax policies that utilizes the bias in the critic estimate and the noise present in the reward signal to escape the saturated regions of the policy parameter space. Our theoretical analysis and experiments, conducted on bandits and various reinforcement learning environments, show that this new estimator is significantly more robust to policy saturation.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852301005,0
C,"He, JF; Zhou, DR; Gu, QQ",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"He, Jiafan; Zhou, Dongruo; Gu, Quanquan",,,Near-optimal Policy Optimization Algorithms for Learning Adversarial Linear Mixture MDPs,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Learning Markov decision processes (MDPs) in the presence of the adversary is a challenging problem in reinforcement learning (RL). In this paper, we study RL in episodic MDPs with adversarial reward and full information feedback, where the unknown transition probability function is a linear function of a given feature mapping, and the reward function can change arbitrarily episode by episode. We propose an optimistic policy optimization algorithm POWERS and show that it can achieve (O) over tilde (dH root T) regret, where H is the length of the episode, T is the number of interaction with the MDP, and d is the dimension of the feature mapping. Furthermore, we also prove a matching lower bound of (Omega) over tilde (dH root T) up to logarithmic factors. Our key technical contributions are two-fold: (1) a new value function estimator based on importance weighting; and (2) a tighter confidence set for the transition kernel. They together lead to the nearly minimax optimal regret.",,,,,"Zhou, Dongruo/GYJ-3503-2022",,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072704015,0
C,"Ho, N; Lin, TY; Jordan, MI",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Ho, Nhat; Lin, Tianyi; Jordan, Michael, I",,,On Structured Filtering-Clustering: Global Error Bound and Optimal First-Order Algorithms,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"The filtering-clustering models, including trend filtering and convex clustering, have become an important source of ideas and modeling tools in machine learning and related fields. The statistical guarantee of optimal solutions in these models has been extensively studied yet the investigations on the computational aspect have remained limited. In particular, practitioners often employ the first-order algorithms in real-world applications and are impressed by their superior performance regardless of ill-conditioned structures of difference operator matrices, thus leaving open the problem of understanding the convergence property of first-order algorithms. This paper settles this open problem and contributes to the broad interplay between statistics and optimization by identifying a global error bound condition, which is satisfied by a large class of dual filtering-clustering problems, and designing a class of generalized dual gradient ascent algorithm, which is optimal first-order algorithms in deterministic, finite-sum and online settings. Our results are new and help explain why the filtering-clustering models can be efficiently solved by first-order algorithms. We also provide the detailed convergence rate analysis for the proposed algorithms in different settings, shedding light on their potential to solve the filtering-clustering models efficiently. We also conduct experiments on real datasets and the numerical results demonstrate the effectiveness of our algorithms.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,896,921,,,,,,,,,,,,,,,,WOS:000828072700038,0
C,"Irons, N; Scetbon, M; Pal, S; Harchaoui, Z",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Irons, Nicholas; Scetbon, Meyer; Pal, Soumik; Harchaoui, Zaid",,,"Triangular Flows for Generative Modeling: Statistical Consistency, Smoothness Classes, and Fast Rates","INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Triangular flows, also known as Knothe-Rosenblatt measure couplings, comprise an important building block of normalizing flow models for generative modeling and density estimation, including popular autoregressive flows such as real-valued non-volume preserving transformation models (Real NVP). We present statistical guarantees and sample complexity bounds for triangular flow statistical models. In particular, we establish the statistical consistency and the finite sample convergence rates of the minimum Kullback-Leibler divergence statistical estimator of the Knothe-Rosenblatt measure coupling using tools from empirical process theory. Our results highlight the anisotropic geometry of function classes at play in triangular flows, shed light on optimal coordinate ordering, and lead to statistical guarantees for Jacobian flows. We conduct numerical experiments to illustrate the practical implications of our theoretical findings.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852304031,0
C,"Lorraine, J; Acuna, D; Vicol, P; Duvenaud, D",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Lorraine, Jonathan; Acuna, David; Vicol, Paul; Duvenaud, David",,,Complex Momentum for Optimization in Games,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We generalize gradient descent with momentum for optimization in differentiable games to have complex-valued momentum. We give theoretical motivation for our method by proving convergence on bilinear zero-sum games for simultaneous and alternating updates. Our method gives real-valued parameter updates, making it a drop-in replacement for standard optimizers. We empirically demonstrate that complex-valued momentum can improve convergence in realistic adversarial games like generative adversarial networks - by showing we find better solutions with an almost identical computational cost. We also show a practical complex-valued Adam variant, which we use to train BigGAN to improve inception scores on CIFAR-10.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852302008,0
C,"Minorics, L; Turkmen, C; Kernert, D; Bloebaum, P; Callot, L; Janzing, D",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Minorics, Lenon; Turkmen, Caner; Kernert, David; Bloebaum, Patrick; Callot, Laurent; Janzing, Dominik",,,Testing Granger Non-Causality in Panels with Cross-Sectional Dependencies,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"This paper proposes a new approach for testing Granger non-causality on panel data. Instead of aggregating panel member statistics, we aggregate their corresponding p-values and show that the resulting p-value approximately bounds the type I error by the chosen significance level even if the panel members are dependent. We compare our approach against the most widely used Granger causality algorithm on panel data and show that our approach yields lower FDR at the same power for large sample sizes and panels with cross-sectional dependencies. Finally, we examine COVID-19 data about confirmed cases and deaths measured in countries/regions worldwide and show that our approach is able to discover the true causal relation between confirmed cases and deaths while state-of-the-art approaches fail.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852305003,0
C,"Montasser, O; Hanneke, S; Srebro, N",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Montasser, Omar; Hanneke, Steve; Srebro, Nathan",,,Transductive Robust Learning Guarantees,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We study the problem of adversarially robust learning in the transductive setting. For classes H of bounded VC dimension, we propose a simple transductive learner that when presented with a set of labeled training examples and a set of unlabeled test examples (both sets possibly adversarially perturbed), it correctly labels the test examples with a robust error rate that is linear in the VC dimension and is adaptive to the complexity of the perturbation set. This result provides an exponential improvement in dependence on VC dimension over the best known upper bound on the robust error in the inductive setting, at the expense of competing with a more restrictive notion of optimal robust error.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852305044,0
C,"Pavutnitskiy, F; Klochkov, A; Ivanov, SO; Vialov, V; Abramov, E; Zaikovskii, A; Borovitskiy, V; Petiushko, A",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Pavutnitskiy, Fedor; Klochkov, Artem; Ivanov, Sergei O.; Vialov, Viktor; Abramov, Evgeny; Zaikovskii, Anatolii; Borovitskiy, Viacheslav; Petiushko, Aleksandr",,,Quadric Hypersurface Intersection for Manifold Learning in Feature Space,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"The knowledge that data lies close to a particular submanifold of the ambient Euclidean space may be useful in a number of ways. For instance, one may want to automatically mark any point far away from the submanifold as an outlier or to use the geometry to come up with a better distance metric. Manifold learning problems are often posed in a very high dimension, e.g. for spaces of images or spaces of words. Today, with deep representation learning on the rise in areas such as computer vision and natural language processing, many problems of this kind may be transformed into problems of moderately high dimension, typically of the order of hundreds. Motivated by this, we propose a manifold learning technique suitable for moderately high dimension and large datasets. The manifold is learned from the training data in the form of an intersection of quadric hypersurfaces-simple but expressive objects. At test time, this manifold can be used to introduce a computationally efficient outlier score for arbitrary new data points and to improve a given similarity metric by incorporating the learned geometric structure into it.",,,,,"Borovitskiy, Viacheslav/R-7216-2017","Borovitskiy, Viacheslav/0000-0002-3539-333X",,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852305022,0
C,"Shi, XP; Zheng, PF; Ding, AA; Gao, Y; Zhang, WZ",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Shi, Xupeng; Zheng, Pengfei; Ding, A. Adam; Gao, Yuan; Zhang, Weizhong",,,Finding Dynamics Preserving Adversarial Winning Tickets,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Modern deep neural networks (DNNs) are vulnerable to adversarial attacks and adversarial training has been shown to be a promising method for improving the adversarial robustness of DNNs. Pruning methods have been considered in adversarial context to reduce model capacity and improve adversarial robustness simultaneously in training. Existing adversarial pruning methods generally mimic the classical pruning methods for natural training, which follow the three-stage 'training-pruning-fine-tuning' pipelines. We observe that such pruning methods do not necessarily preserve the dynamics of dense networks, making it potentially hard to be fine-tuned to compensate the accuracy degradation in pruning. Based on recent works of Neural Tangent Kernel (NTK), we systematically study the dynamics of adversarial training and prove the existence of trainable sparse sub-network at initialization which can be trained to be adversarial robust from scratch. This theoretically verifies the lottery ticket hypothesis in adversarial context and we refer such sub-network structure as Adversarial Winning Ticket (AWT). We also show empirical evidences that AWT preserves the dynamics of adversarial training and achieve equal performance as dense adversarial training.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,510,528,,,,,,,,,,,,,,,,WOS:000828072700021,0
C,"Sontakke, SA; Iota, S; Hu, ZZ; Mehrjou, A; Itti, L; Scholkopf, B",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Sontakke, Sumedh A.; Iota, Stephen; Hu, Zizhao; Mehrjou, Arash; Itti, Laurent; Schoelkopf, Bernhard",,,GalilAI: Out-of-Task Distribution Detection using Causal Active Experimentation for Safe Transfer RL,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Out-of-distribution (OOD) detection is a well-studied topic in supervised learning. Extending the successes in supervised learning methods to the reinforcement learning (RL) setting, however, is difficult due to the data generating process RL agents actively query their environment for data, and the data are a function of the policy followed by the agent. An agent could thus neglect a shift in the environment if its policy did not lead it to explore the aspect of the environment that shifted. Therefore, to achieve safe and robust generalization in RL, there exists an unmet need for OOD detection through active experimentation. Here, we attempt to bridge this lacuna by first defining a causal framework for OOD scenarios or environments encountered by RL agents in the wild. Then, we propose a novel task: that of Out-of-Task Distribution (OOTD) detection. We introduce an RL agent that actively experiments in a test environment and subsequently concludes whether it is OOTD or not. We name our method GalilAI, in honor of Galileo Galilei, as it discovers, among other causal processes, that gravitational acceleration is independent of the mass of a body. Finally, we propose a simple probabilistic neural network baseline for comparison, which extends extant Model-Based RL. We find that GalilAI outperforms the baseline significantly. See visualizations of our method here.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852301042,0
C,"Le, T; Nguyen, T; Phung, D; Nguyen, VA",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,Tam Le; Truyen Nguyen; Dinh Phung; Viet Anh Nguyen,,,Sobolev Transport: A Scalable Metric for Probability Measures with Graph Metrics,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Optimal transport (OT) is a popular measure to compare probability distributions. However, OT suffers a few drawbacks such as (i) a high complexity for computation, (ii) indefiniteness which limits its applicability to kernel machines. In this work, we consider probability measures supported on a graph metric space and propose a novel Sobolev transport metric. We show that the Sobolev transport metric yields a closed-form formula for fast computation and it is negative definite. We show that the space of probability measures endowed with this transport distance is isometric to a bounded convex set in a Euclidean space with a weighted l(p) distance. We further exploit the negative definiteness of the Sobolev transport to design positive-definite kernels, and evaluate their performances against other baselines in document classification with word embeddings and in topological data analysis.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852304017,0
C,"Tang, YH; Rowland, M; Munos, R; Valko, M",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Tang, Yunhao; Rowland, Mark; Munos, Remi; Valko, Michal",,,Marginalized Operators for Off-policy Reinforcement Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"In this work, we propose marginalized operators, a new class of off-policy evaluation operators for reinforcement learning. Marginalized operators strictly generalize generic multi-step operators, such as Retrace, as special cases. Marginalized operators also suggest a form of sample-based estimates with potential variance reduction, compared to sample-based estimates of the original multi-step operators. We show that the estimates for marginalized operators can be computed in a scalable way, which also generalizes prior results on marginalized importance sampling as special cases. Finally, we empirically demonstrate that marginalized operators provide performance gains to off-policy evaluation and downstream policy optimization algorithms.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,655,679,,,,,,,,,,,,,,,,WOS:000828072700028,0
C,"Tiapkin, D; Gasnikov, A",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Tiapkin, Daniil; Gasnikov, Alexander",,,Primal-Dual Stochastic Mirror Descent for MDPs,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We consider the problem of learning the optimal policy for infinite-horizon Markov decision processes (MDPs). For this purpose, some variant of Stochastic Mirror Descent is proposed for convex programming problems with Lipschitz-continuous functionals. An important detail is the ability to use inexact values of functional constraints and compute the value of dual variables. We analyze this algorithm in a general case and obtain an estimate of the convergence rate that does not accumulate errors during the operation of the method. Using this algorithm, we get the first parallel algorithm for mixing average-reward MDPs with a generative model without reduction to discounted MDP. One of the main features of the presented method is low communication costs in a distributed centralized setting, even with very large networks.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852304011,0
C,"Yang, JC; Orvieto, A; Lucchi, A; He, NA",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Yang, Junchi; Orvieto, Antonio; Lucchi, Aurelien; He, Niao",,,Faster Single-loop Algorithms for Minimax Optimization without Strong Concavity,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Gradient descent ascent (GDA), the simplest single-loop algorithm for nonconvex minimax optimization, is widely used in practical applications such as generative adversarial networks (GANs) and adversarial training. Albeit its desirable simplicity, recent work shows inferior convergence rates of GDA in theory, even when assuming strong concavity of the objective in terms of one variable. This paper establishes new convergence results for two alternative single-loop algorithms - alternating GDA and smoothed GDA - under the mild assumption that the objective satisfies the Polyak-Lojasiewicz (PL) condition about one variable. We prove that, to find an epsilon-stationary point, (i) alternating GDA and its stochastic variant (without mini batch) respectively require O(kappa(2)epsilon(-2)) and O(kappa(4)epsilon(-4)) iterations, while (ii) smoothed GDA and its stochastic variant (without mini batch) respectively require O(kappa epsilon(-2)) and O (kappa(2)epsilon(-4)) iterations. The latter greatly improves over the vanilla GDA and gives the hitherto best known complexity results among single-loop algorithms under similar settings. We further showcase the empirical efficiency of these algorithms in training GANs and robust nonlinear regression.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072705023,0
C,"Zhang, XZ; Chen, YD; Zhu, J; Sun, W",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Zhang, Xuezhou; Chen, Yiding; Zhu, Jerry; Sun, Wen",,,Corruption-Robust Offline Reinforcement Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We study the adversarial robustness in offline reinforcement learning. Given a batch dataset consisting of tuples (s, a, r, s'), an adversary is allowed to arbitrarily modify epsilon fraction of the tuples. From the corrupted dataset the learner aims to robustly identify a near-optimal policy. We first show that a worst-case Omega(Hd epsilon) optimality gap is unavoidable in linear MDP of dimension d, even if the adversary only corrupts the reward element in a tuple. This contrasts with dimension-free results in robust supervised learning and best-known lower-bound in the online RL setting with corruption. Next, we propose robust variants of the Least-Square Value Iteration (LSVI) algorithm utilizing robust supervised learning oracles, which achieve near-matching performances in cases both with and without global data coverage. The algorithm requires the knowledge of epsilon to design the pessimism bonus in the no-coverage case. Surprisingly, the knowledge of epsilon is necessary, as we show that being adaptive to unknown epsilon. This again contrasts with recent results on corruption-robust online RL and implies that corruption-robust offline RL is a strictly harder problem.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,5757,5773,,,,,,,,,,,,,,,,WOS:000841852300007,0
C,"Zhang, Y; Berrevoets, J; van der Schaar, M",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Zhang, Yao; Berrevoets, Jeroen; van der Schaar, Mihaela",,,Identifiable Energy-based Representations: An Application to Estimating Heterogeneous Causal Effects,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Conditional average treatment effects (CATEs) allow us to understand the effect heterogeneity across a large population of individuals. However, typical CATE learners assume all confounding variables are measured in order for the CATE to be identifiable. This requirement can be satisfied by collecting many variables, at the expense of increased sample complexity for estimating CATEs. To combat this, we propose an energy-based model (EBM) that learns a low-dimensional representation of the variables by employing a noise contrastive loss function. With our EBM we introduce a preprocessing step that alleviates the dimensionality curse for any existing learner developed for estimating CATEs. We prove that our EBM keeps the representations partially identifiable up to some universal constant, as well as having universal approximation capability. These properties enable the representations to converge and keep the CATE estimates consistent. Experiments demonstrate the convergence of the representations, as well as show that estimating CATEs on our representations performs better than on the variables or the representations obtained through other dimensionality reduction methods.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072704010,0
C,"Zhao, YL; Tian, YD; Lee, JD; Du, SS",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Zhao, Yulai; Tian, Yuandong; Lee, Jason D.; Du, Simon S.",,,Provably Efficient Policy Optimization for Two-Player Zero-Sum Markov Games,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Policy-based methods with function approximation are widely used for solving two-player zero-sum games with large state and/or action spaces. However, it remains elusive how to obtain optimization and statistical guarantees for such algorithms. We present a new policy optimization algorithm with function approximation and prove that under standard regularity conditions on the Markov game and the function approximation class, our algorithm finds a near-optimal policy within a polynomial number of samples and iterations. To our knowledge, this is the first provably efficient policy optimization algorithm with function approximation that solves two-player zero-sum Markov games.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072702034,0
C,"Bisla, D; Wang, J; Choromanska, A",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Bisla, Devansh; Wang, Jing; Choromanska, Anna",,,Low-Pass Filtering SGD for Recovering Flat Optima in the Deep Learning Optimization Landscape,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"In this paper, we study the sharpness of a deep learning (DL) loss landscape around local minima in order to reveal systematic mechanisms underlying the generalization abilities of DL models. Our analysis is performed across varying network and optimizer hyper-parameters, and involves a rich family of different sharpness measures. We compare these measures and show that the low-pass filter based measure exhibits the highest correlation with the generalization abilities of DL models, has high robustness to both data and label noise, and furthermore can track the double descent behavior for neural networks. We next derive the optimization algorithm, relying on the lowpass filter (LPF), that actively searches the flat regions in the DL optimization landscape using SGD-like procedure. The update of the proposed algorithm, that we call LPF-SGD, is determined by the gradient of the convolution of the filter kernel with the loss function and can be efficiently computed using MC sampling. We empirically show that our algorithm achieves superior generalization performance compared to the common DL training strategies. On the theoretical front we prove that LPF-SGD converges to a better optimal point with smaller generalization error than SGD.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852302031,0
C,"Brando, A; Gimeno, J; Rodriguez-Serrano, JA; Vitria, J",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Brando, Axel; Gimeno, Joan; Rodriguez-Serrano, Jose A.; Vitria, Jordi",,,Deep Non-Crossing Quantiles through the Partial Derivative,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Quantile Regression (QR) provides a way to approximate a single conditional quantile. To have a more informative description of the conditional distribution, QR can be merged with deep learning techniques to simultaneously estimate multiple quantiles. However, the minimisation of the QR-loss function does not guarantee non-crossing quantiles, which affects the validity of such predictions and introduces a critical issue in certain scenarios. In this article, we propose a generic deep learning algorithm for predicting an arbitrary number of quantiles that ensures the quantile monotonicity constraint up to the machine precision and maintains its modelling performance with respect to alternative models. The presented method is evaluated over several real-world datasets obtaining state-of-the-art results as well as showing that it scales to large-size data sets.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852302014,0
C,"Brown, G; Hod, S; Kalemaj, I",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Brown, Gavin; Hod, Shlomi; Kalemaj, Iden",,,Performative Prediction in a Stateful World,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Deployed supervised machine learning models make predictions that interact with and influence the world. This phenomenon is called performative prediction by Perdomo et al. (ICML 2020). It is an ongoing challenge to understand the influence of such predictions as well as design tools so as to control that influence. We propose a theoretical framework where the response of a target population to the deployed classifier is modeled as a function of the classifier and the current state (distribution) of the population. We show necessary and sufficient conditions for convergence to an equilibrium of two retraining algorithms, repeated risk minimization and a lazier variant. Furthermore, convergence is near an optimal classifier. We thus generalize results of Perdomo et al., whose performativity framework does not assume any dependence on the state of the target population. A particular phenomenon captured by our model is that of distinct groups that acquire information and resources at different rates to be able to respond to the latest deployed classifier. We study this phenomenon theoretically and empirically.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,6045,6061,,,,,,,,,,,,,,,,WOS:000841852300020,0
C,"Cobb, O; Van Looveren, A; Klaise, J",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Cobb, Oliver; Van Looveren, Arnaud; Klaise, Janis",,,Sequential Multivariate Change Detection with Calibrated and Memoryless False Detection Rates,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Responding appropriately to the detections of a sequential change detector requires knowledge of the rate at which false positives occur in the absence of change. Setting detection thresholds to achieve a desired false positive rate is challenging. Existing works resort to setting time-invariant thresholds that focus on the expected runtime of the detector in the absence of change, either bounding it loosely from below or targeting it directly but with asymptotic arguments that we show cause significant miscalibration in practice. We present a simulation-based approach to setting time-varying thresholds that allows a desired expected runtime to be accurately targeted whilst additionally keeping the false positive rate constant across time steps. Whilst the approach to threshold setting is metric agnostic, we show how the cost of using the popular quadratic time MMD estimator can be reduced from O((NB)-B-2) to O(N-2 + NB) during configuration and from O(N-2) to O (N) during operation, where N and B are the numbers of reference and bootstrap samples respectively.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,226,239,,,,,,,,,,,,,,,,WOS:000828072700011,0
C,"Csillag, A; Piazza, C; Rarnos, T; Romano, JV; Oliveira, R; Orenstein, P",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Csillag, Aniel; Piazza, Carolina; Rarnos, Thiago; Romano, Joao Vitor; Oliveira, Roberto; Orenstein, Paulo",,,ExactBoost: Directly Boosting the Margin in Combinatorial and Non-decomposable Metrics,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Many classification algorithms require the use of surrogate losses when the intended loss function is combinatorial or non-decomposable. This paper introduces a fast and exact stagewise optimization algorithm, dubbed ExactBoost, that boosts instead to the actual loss function. By developing a novel extension of margin theory to the non-decomposable setting, it is possible to provably bound the generalization error of ExactBoost for many important metrics with different levels of non-decomposability. Through extensive examples, it is shown that such theoretical guarantees translate to competitive empirical performance. In particular, when used as an ensembler, ExactBoost is able to significantly outperform other surrogate-based and exact algorithms available.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852303020,0
C,"Guo, WS; Kandasamy, K; Gonzalez, JE; Jordan, MI; Stoica, I",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Guo, Wenshuo; Kandasamy, Kirthevasan; Gonzalez, Joseph E.; Jordan, Michael, I; Stoica, Ion",,,Learning Competitive Equilibria in Exchange Economies with Bandit Feedback,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"The sharing of scarce resources among multiple rational agents is one of the classical problems in economics. In exchange economies, which are used to model such situations, agents begin with an initial endowment of resources and exchange them in a way that is mutually beneficial until they reach a competitive equilibrium (CE). The allocations at a CE are Pareto efficient and fair. Consequently, they are used widely in designing mechanisms for fair division. However, computing CEs requires the knowledge of agent preferences which are unknown in several applications of interest. In this work, we explore a new online learning mechanism, which, on each round, allocates resources to the agents and collects stochastic feedback on their experience in using that allocation. Its goal is to learn the agent utilities via this feedback and imitate the allocations at a CE in the long run. We quantify CE behavior via two losses and propose a randomized algorithm which achieves sublinear loss under a parametric class of utilities. Empirically, we demonstrate the effectiveness of this mechanism through numerical simulations.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,6200,6224,,,,,,,,,,,,,,,,WOS:000841852300028,0
C,"Kuang, ZB; Arachie, C; Liang, BY; Narayana, P; DeSalvo, G; Quinn, M; Huang, B; Downs, G; Yang, Y",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Kuang, Zhaobin; Arachie, Chidubem; Liang, Bangyong; Narayana, Pradyumna; DeSalvo, Giulia; Quinn, Michael; Huang, Bert; Downs, Geoffrey; Yang, Yang",,,Firebolt: Weak Supervision Under Weaker Assumptions,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Modern machine learning demands a large amount of training data. Weak supervision is a promising approach to meet this demand. It aggregates multiple labeling functions (LFs) noisy, user-provided labeling heuristics to rapidly and cheaply curate probabilistic labels for large-scale unlabeled data. However, standard assumptions in weak supervision such as user-specified class bal- ance, similar accuracy of an LF in classifying different classes, and full knowledge of LF de- pendency at inference time might be undesir- able in practice. In response, we present Firebolt, a new weak supervision framework that seeks to operate under weaker assumptions. In particular, Firebolt learns the class balance and class-specific accuracy of LFs jointly from unlabeled data. It carries out inference in an efficient and interpretable manner. We analyze the parameter estimation error of Firebolt and characterize its impact on downstream model performance. Furthermore, we show that on five publicly available datasets, Firebolt outperforms a state-of-the-art weak supervision method by up to 5.8 points in AUC. We also provide a case study in the production setting of a tech company, where a Firebolt-supervised model outperforms the existing weakly-supervised production model by 1.3 points in AUC and speeds up label model training and inference from one hour to three minutes.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852302028,0
C,"Kwon, Y; Zou, J",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Kwon, Yongchan; Zou, James",,,Beta Shapley: a Unified and Noise-reduced Data Valuation Framework for Machine Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Data Shapley has recently been proposed as a principled framework to quantify the contribution of individual datum in machine learning. It can effectively identify helpful or harmful data points for a learning algorithm. In this paper, we propose Beta Shapley, which is a substantial generalization of Data Shapley. Beta Shapley arises naturally by relaxing the efficiency axiom of the Shapley value, which is not critical for machine learning settings. Beta Shapley unifies several popular data valuation methods and includes data Shapley as a special case. Moreover, we prove that Beta Shapley has several desirable statistical properties and propose efficient algorithms to estimate it. We demonstrate that Beta Shapley outperforms state-of-the-art data valuation methods on several downstream ML tasks such as: 1) detecting mislabeled training data; 2) learning with subsamples; and 3) identifying points whose addition or removal have the largest positive or negative impact on the model.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852303011,0
C,"Liu, X; Zhu, H; Ton, JF; Wynne, G; Duncan, A",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Liu, Xing; Zhu, Harrison; Ton, Jean-Francois; Wynne, George; Duncan, Andrew",,,Grassmann Stein Variational Gradient Descent,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Stein variational gradient descent (SVGD) is a deterministic particle inference algorithm that provides an efficient alternative to Markov chain Monte Carlo. However, SVGD has been found to suffer from variance underestimation when the dimensionality of the target distribution is high. Recent developments have advocated projecting both the score function and the data onto real lines to sidestep this issue, although this can severely overestimate the epistemic (model) uncertainty. In this work, we propose Grassmann Stein variational gradient descent (GSVGD) as an alternative approach, which permits projections onto arbitrary dimensional subspaces. Compared with other variants of SVGD that rely on dimensionality reduction, GSVGD updates the projectors simultaneously for the score function and the data, and the optimal projectors are determined through a coupled Grassmann-valued diffusion process which explores favourable subspaces. Both our theoretical and experimental results suggest that GSVGD enjoys efficient state-space exploration in high-dimensional problems that have an intrinsic low-dimensional structure.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072702003,0
C,"Mucke, N; Reiss, E; Rungenhagen, J; Klein, M",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Muecke, Nicole; Reiss, Enrico; Rungenhagen, Jonas; Klein, Markus",,,Data splitting improves statistical performance in overparameterized regimes,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"While large training datasets generally offer improvement in model performance, the training process becomes computationally expensive and time consuming. Distributed learning is a common strategy to reduce the overall training time by exploiting multiple computing devices. Recently, it has been observed in the single machine setting that overparameterization is essential for benign overfitting in ridgeless regression in Hilbert spaces. We show that in this regime, data splitting has a regularizing effect, hence improving statistical performance and computational complexity at the same time. We further provide a unified framework that allows to analyze both the finite and infinite dimensional setting. We numerically demonstrate the effect of different model parameters.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852304035,0
C,"Njike, BN; Siebert, X",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Njike, Boris Ndjia; Siebert, Xavier",,,Multi-class classification in nonparametric active learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Several works have recently focused on nonparametric active learning, especially in the binary classification setting under Holder smoothness assumptions on the regression function. These works have highlighted the benefit of active learning by providing better rates of convergence compared to the passive counterpart. In this paper, we extend these results to multiclass classification under a more general smoothness assumption, which takes into account a broader class of underlying distributions. We present a new algorithm called MKAL for multiclass k-nearest neighbors active learning, and prove its theoretical benefits. Additionally, we empirically study MKAL on several datasets and discuss its merits and potential improvements.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852301026,0
C,"Porwal, V; Srivastava, P; Sinha, G",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Porwal, Vibhor; Srivastava, Piyush; Sinha, Gaurav",,,Almost Optimal Universal Lower Bound for Learning Causal DAGs with Atomic Interventions,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"A well-studied challenge that arises in the structure learning problem of causal directed acyclic graphs (DAG) is that using observational data, one can only learn the graph up to a Markov equivalence class (MEC). The remaining undirected edges have to be oriented using interventions, which can be very expensive to perform in applications. Thus, the problem of minimizing the number of interventions needed to fully orient the MEC has received a lot of recent attention, and is also the focus of this work. We prove two main results. The first is a new universal lower bound on the number of atomic interventions that any algorithm (whether active or passive) would need to perform in order to orient a given MEC. Our second result shows that this bound is, in fact, within a factor of two of the size of the smallest set of atomic interventions that can orient the MEC. Our lower bound is provably better than previously known lower bounds. The proof of our lower bound is based on the new notion of clique-block shared-parents (CBSP) orderings, which are topological orderings of DAGs without v-structures and satisfy certain special properties. Further, using simulations on synthetic graphs and by giving examples of special graph families, we show that our bound is often significantly better.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072705027,0
C,"Sebbouh, O; Cuturi, M; Peyre, G",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Sebbouh, Othmane; Cuturi, Marco; Peyre, Gabriel",,,Randomized Stochastic Gradient Descent Ascent,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"An increasing number of machine learning problems, such as robust or adversarial variants of existing algorithms, require minimizing a loss function that is itself defined as a maximum. Carrying a loop of stochastic gradient ascent (SGA) steps on the (inner) maximization problem, followed by an SGD step on the (outer) minimization, is known as Epoch Stochastic Gradient Descent Ascent (ESGDA). While successful in practice, the theoretical analysis of ESGDA remains challenging, with no clear guidance on choices for the inner loop size nor on the interplay between inner/outer step sizes. We propose RSGDA (Randomized SGDA), a variant of ESGDA with stochastic loop size with a simpler theoretical analysis. RSGDA comes with the first (among SGDA algorithms) almost sure convergence rates when used on nonconvex min/strongly-concave max settings. RSGDA can be parameterized using optimal loop sizes that guarantee the best convergence rates known to hold for SGDA. We test RSGDA on toy and larger scale problems, using distributionally robust optimization and single-cell data matching using optimal transport as a testbed.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072703002,0
C,"Wirth, E; Pokutta, S",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Wirth, Elias; Pokutta, Sebastian",,,Conditional Gradients for the Approximately Vanishing Ideal,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"The vanishing ideal of a set of points X subset of R-n is the set of polynomials that evaluate to 0 over all points x is an element of X and admits an efficient representation by a finite set of polynomials called generators. To accommodate the noise in the data set, we introduce the Conditional Gradients Approximately Vanishing Ideal algorithm (CGAVI) for the construction of the set of generators of the approximately vanishing ideal. The constructed set of generators captures polynomial structures in data and gives rise to a feature map that can, for example, be used in combination with a linear classifier for supervised learning. In CGAVI, we construct the set of generators by solving specific instances of (constrained) convex optimization problems with the Pairwise Frank-Wolfe algorithm (PFW). Among other things, the constructed generators inherit the LASSO generalization bound and not only vanish on the training but also on out-sample data. Moreover, CGAVI admits a compact representation of the approximately vanishing ideal by constructing few generators with sparse coefficient vectors.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072702011,0
C,"Wu, JF; Braverman, V; Yang, LF",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Wu, Jingfeng; Braverman, Vladimir; Yang, Lin F.",,,Gap-Dependent Unsupervised Exploration for Reinforcement Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"For the problem of task-agnostic reinforcement learning (RL), an agent first collects samples from an unknown environment without the supervision of reward signals, then is revealed with a reward and is asked to compute a corresponding near-optimal policy. Existing approaches mainly concern the worst-case scenarios, in which no structural information of the reward/transitiondynamics is utilized. Therefore the best sample upper bound is proportional to (O) over tilde (1/epsilon(2)), where epsilon > 0 is the target accuracy of the obtained policy, and can be overly pessimistic. To tackle this issue, we provide an efficient algorithm that utilizes a gap parameter, rho > 0, to reduce the amount of exploration. In particular, for an unknown finite-horizon Markov decision process, the algorithm takes only (O) over tilde (1/epsilon . (H(3)SA/rho + H(4)S(2)A)) episodes of exploration, and is able to obtain an epsilon-optimal policy for a post-revealed reward with sub-optimality gap at least rho, where S is the number of states, A is the number of actions, and H is the length of the horizon, obtaining a nearly quadratic saving in terms of epsilon. We show that, information-theoretically, this bound is nearly tight for rho < Theta(1/(HS)) and H > 1. We further show that proportional to (O) over tilde (1) sample bound is possible for H = 1 (i.e., multi-armed bandit) or with a sampling simulator, establishing a stark separation between those settings and the RL setting.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072704008,0
C,"Xing, Y; Song, QF; Cheng, G",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Xing, Yue; Song, Qifan; Cheng, Guang",,,Unlabeled Data Help: Minimax Analysis and Adversarial Robustness,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"The recent proposed self-supervised learning (SSL) approaches successfully demonstrate the great potential of supplementing learning algorithms with additional unlabeled data. However, it is still unclear whether the existing SSL algorithms can fully utilize the information of both labelled and unlabeled data. This paper gives an affirmative answer for the reconstruction-based SSL algorithm (Lee et al., 2020) under several statistical models. While existing literature only focuses on establishing the upper bound of the convergence rate, we provide a rigorous minimax analysis, and successfully justify the rate-optimality of the reconstruction-based SSL algorithm under different data generation models. Furthermore, we incorporate the reconstruction-based SSL into the existing adversarial training algorithms and show that learning from unlabeled data helps improve the robustness.",,,,,,"Xing, Yue/0000-0001-7723-0048",,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,136,168,,,,,,,,,,,,,,,,WOS:000828072700008,0
C,"Yuan, R; Gower, RM; Lazaric, A",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Yuan, Rui; Gower, Robert M.; Lazaric, Alessandro",,,A general sample complexity analysis of vanilla policy gradient,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We adapt recent tools developed for the analysis of Stochastic Gradient Descent (SGD) in non-convex optimization to obtain convergence and sample complexity guarantees for the vanilla policy gradient (PG). Our only assumptions are that the expected return is smooth w.r.t. the policy parameters, that its H-step truncated gradient is close to the exact gradient, and a certain ABC assumption. This assumption requires the second moment of the estimated gradient to be bounded by A >= 0 times the suboptimality gap, B >= 0 times the norm of the full batch gradient and an additive constant C >= 0, or any combination of aforementioned. We show that the ABC assumption is more general than the commonly used assumptions on the policy space to prove convergence to a stationary point. We provide a single convergence theorem that recovers the (O) over tilde(epsilon(-4)) sample complexity of PG. Our results also affords greater flexibility in the choice of hyper parameters such as the step size and places no restriction on the batch size m, including the single trajectory case (i.e., m = 1). We then instantiate our theorem in different settings, where we both recover existing results and obtained improved sample complexity, e.g., for convergence to the global optimum for Fisher-non-degenerated parameterized policies.",,,,,,"YUAN, Rui/0000-0002-1768-9639",,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072703019,0
C,"Amini, AA; Aragam, B; Zhou, Q",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Amini, Arash A.; Aragam, Bryon; Zhou, Qing",,,On perfectness in Gaussian graphical models,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Knowing when a graphical model perfectly encodes the conditional independence structure of a distribution is essential in applications, and this is particularly important when performing inference from data. When the model is perfect, there is a one-to-one correspondence between conditional independence statements in the distribution and separation statements in the graph. Previous work has shown that almost all models based on linear directed acyclic graphs as well as Gaussian chain graphs are perfect, the latter of which subsumes Gaussian graphical models (i.e., the undirected Gaussian models) as a special case. In this paper, we directly approach the problem of perfectness for the Gaussian graphical models, and provide a new proof, via a more transparent parametrization, that almost all such models are perfect. Our approach is based on, and substantially extends, a construction of Lnenicka and Matus showing the existence of a perfect Gaussian distribution for any graph. The analysis involves constructing a probability measure on the set of normalized covariance matrices Markov with respect to a graph that may be of independent interest.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852301041,0
C,"Bar, O; Drory, A; Giryes, R",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Bar, Oshrat; Drory, Amnon; Giryes, Raja",,,A Spectral Perspective of DNN Robustness to Label Noise,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Deep networks usually require a massive amount of labeled data for their training. Yet, such data may include some mistakes in the labels. Interestingly, networks have been shown to be robust to such errors. This work uses spectral analysis of their learned mapping to provide an explanation for their robustness. In particular, we relate the smoothness regularization that usually exists in conventional training to the attenuation of high frequencies, which mainly characterize noise. By using a connection between the smoothness and the spectral norm of the network weights, we suggest that one may further improve robustness via spectral normalization. Empirical experiments validate our claims and show the advantage of this normalization for classification with label noise.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072703036,0
C,"Bergamin, F; Mattei, PA; Havtorn, JD; Senetaire, H; Schmutz, H; Maaloe, L; Hauberg, S; Frellsen, J",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Bergamin, Federico; Mattei, Pierre-Alexandre; Havtorn, Jakob D.; Senetaire, Hugo; Schmutz, Hugo; Maaloe, Lars; Hauberg, Soren; Frellsen, Jes",,,Model-agnostic out-of-distribution detection using combined statistical tests,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We present simple methods for out-of-distribution detection using a trained generative model. These techniques, based on classical statistical tests, are model-agnostic in the sense that they can be applied to any differentiable generative model. The idea is to combine a classical parametric test (Rao's score test) with the recently introduced typicality test. These two test statistics are both theoretically well-founded and exploit different sources of information based on the likelihood for the typicality test and its gradient for the score test. We show that combining them using Fisher's method overall leads to a more accurate out-of-distribution test. We also discuss the benefits of casting out-of-distribution detection as a statistical testing problem, noting in particular that false positive rate control can be valuable for practical out-of-distribution detection. Despite their simplicity and generality, these methods can be competitive with model-specific out-of-distribution detection algorithms without any assumptions on the out-distribution.",,,,,"Hauberg, Soren/L-2104-2016","Hauberg, Soren/0000-0001-7223-877X",,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852305013,0
C,"Chen, HC; Sun, Q",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Chen, Hengchao; Sun, Qiang",,,Distributed Sparse Multicategory Discriminant Analysis,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"This paper proposes a convex formulation for sparse multicategory linear discriminant analysis and then extend it to the distributed setting when data are stored across multiple sites. The key observation is that for the purpose of classification it suffices to recover the discriminant subspace which is invariant to orthogonal transformations. Theoretically, we establish statistical properties ensuring that the distributed sparse multicategory linear discriminant analysis performs as good as the centralized version after a few rounds of communications. Numerical studies lend strong support to our methodology and theory.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,604,624,,,,,,,,,,,,,,,,WOS:000828072700025,0
C,"Clivio, O; Falck, F; Lehmann, B; Deligiannidis, G; Holmes, C",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Clivio, Oscar; Falck, Fabian; Lehmann, Brieuc; Deligiannidis, George; Holmes, Chris",,,Neural Score Matching for High-Dimensional Causal Inference,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Traditional methods for matching in causal inference are impractical for high-dimensional datasets. They suffer from the curse of dimensionality: exact matching and coarsened exact matching find exponentially fewer matches as the input dimension grows, and propensity score matching may match highly unrelated units together. To overcome this problem, we develop theoretical results which motivate the use of neural networks to obtain non-trivial, multivariate balancing scores of a chosen level of coarseness, in contrast to the classical, scalar propensity score. We leverage these balancing scores to perform matching for high-dimensional causal inference and call this procedure neural score matching. We show that our method is competitive against other matching approaches on semi-synthetic high-dimensional datasets, both in terms of treatment effect estimation and reducing imbalance.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852301024,0
C,"Dasgupta, S; Mahajan, G; So, G",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Dasgupta, Sanjoy; Mahajan, Gaurav; So, Geelon",,,Convergence of online k-means,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We prove asymptotic convergence for a general class of k-means algorithms performed over streaming data from a distribution-the centers asymptotically converge to the set of stationary points of the k-means objective function. To do so, we show that online k-means over a distribution can be interpreted as stochastic gradient descent with a stochastic learning rate schedule. Then, we prove convergence by extending techniques used in optimization literature to handle settings where center-specific learning rates may depend on the past trajectory of the centers.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852303001,0
C,"Dellaporta, C; Knoblauch, J; Damoulas, T; Briol, FX",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Dellaporta, Charita; Knoblauch, Jeremias; Damoulas, Theodoros; Briol, Francois-Xavier",,,Robust Bayesian Inference for Simulator-based Models via the MMD Posterior Bootstrap,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Simulator-based models are models for which the likelihood is intractable but simulation of synthetic data is possible. They are often used to describe complex real-world phenomena, and as such can often be misspecified in practice. Unfortunately, existing Bayesian approaches for simulators are known to perform poorly in those cases. In this paper, we propose a novel algorithm based on the posterior bootstrap and maximum mean discrepancy estimators. This leads to a highly-parallelisable Bayesian inference algorithm with strong robustness properties. This is demonstrated through an in-depth theoretical study which includes generalisation bounds and proofs of frequentist consistency and robustness of our posterior. The approach is then assessed on a range of examples including a g-and-k distribution and a toggle-switch model.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072701001,0
C,"Dresdner, G; Vladarean, ML; Ratsch, G; Locatello, F; Cevher, V; Yurtsever, A",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Dresdner, Gideon; Vladarean, Maria-Luiza; Raetsch, Gunnar; Locatello, Francesco; Cevher, Volkan; Yurtsever, Alp",,,Faster One-Sample Stochastic Conditional Gradient Method for Composite Convex Minimization,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We propose a stochastic conditional gradient method (CGM) for minimizing convex finite-sum objectives formed as a sum of smooth and non-smooth terms. Existing CGM variants for this template either suffer from slow convergence rates, or require carefully increasing the batch size over the course of the algorithm's execution, which leads to computing full gradients. In contrast, the proposed method, equipped with a stochastic average gradient (SAG) estimator, requires only one sample per iteration. Nevertheless, it guarantees fast convergence rates on par with more sophisticated variance reduction techniques. In applications we put special emphasis on problems with a large number of separable constraints. Such problems are prevalent among semidefinite programming (SDP) formulations arising in machine learning and theoretical computer science. We provide numerical experiments on matrix completion, unsupervised clustering, and sparsest-cut SDPs.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852302037,0
C,"Dyro, R; Schmerling, E; Arechiga, N; Pavone, M",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Dyro, Robert; Schmerling, Edward; Arechiga, Nikos; Pavone, Marco",,,Second-Order Sensitivity Analysis for Bilevel Optimization,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"In this work we derive a second-order approach to bilevel optimization, a type of mathematical programming in which the solution to a parameterized optimization problem (the lower problem) is itself to be optimized (in the upper problem) as a function of the parameters. Many existing approaches to bilevel optimization employ first-order sensitivity analysis, based on the implicit function theorem (IFT), for the lower problem to derive a gradient of the lower problem solution with respect to its parameters; this IFT gradient is then used in a first-order optimization method for the upper problem. This paper extends this sensitivity analysis to provide second-order derivative information of the lower problem (which we call the IFT Hessian), enabling the usage of faster-converging second-order optimization methods at the upper level. Our analysis shows that (i) much of the computation already used to produce the IFT gradient can be reused for the IFT Hessian, (ii) errors bounds derived for the IFT gradient readily apply to the IFT Hessian, (iii) computing IFT Hessians can significantly reduce overall computation by extracting more information from each lower level solve. We corroborate our findings and demonstrate the broad range of applications of our method by applying it to problem instances of least squares hyperparameter auto-tuning, multi-class SVM autotuning, and inverse optimal control.",,,,,,"Pavone, Marco/0000-0002-0206-4337",,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852303025,0
C,"Farias, VF; Li, AA; Peng, TY",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Farias, Vivek F.; Li, Andrew A.; Peng, Tianyi",,,Uncertainty Quantification For Low-Rank Matrix Completion With Heterogeneous and Sub-Exponential Noise,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"The problem of low-rank matrix completion with heterogeneous and sub-exponential (as opposed to homogeneous and Gaussian) noise is particularly relevant to a number of applications in modern commerce. Examples include panel sales data and data collected from web-commerce systems such as recommendation engines. An important unresolved question for this problem is characterizing the distribution of estimated matrix entries under common low-rank estimators. Such a characterization is essential to any application that requires quantification of uncertainty in these estimates and has heretofore only been available under the assumption of homogenous Gaussian noise. Here we characterize the distribution of estimated matrix entries when the observation noise is heterogeneous sub-exponential and provide, as an application, explicit formulas for this distribution when observed entries are Poisson or Binary distributed.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072701010,0
C,"Fisher, A",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Fisher, Aaron",,,Online Control of the False Discovery Rate under Decision Deadlines,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Online testing procedures aim to control the extent of false discoveries over a sequence of hypothesis tests, allowing for the possibility that early-stage test results influence the choice of hypotheses to be tested in later stages. Typically, online methods assume that a permanent decision regarding the current test (reject or not reject) must be made before advancing to the next test. We instead assume that each hypothesis requires an immediate preliminary decision, but also allows us to update that decision until a preset deadline. Roughly speaking, this lets us apply a Benjamini-Hochberg-type procedure over a moving window of hypotheses, where the threshold parameters for upcoming tests can be determined based on preliminary results. We show that our approach can control the false discovery rate (FDR) at every stage of testing, even under arbitrary p-value dependencies. That said, our approach offers much greater flexibility if the p-values exhibit a known independence structure. For example, if the p-value sequence is finite and all p-values are independent, then we can additionally control FDR at adaptively chosen stopping times.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852302032,0
C,"Hao, BT; Lazic, N; Yin, D; Abbasi-Yadkori, Y; Szepesvari, C",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Hao, Botao; Lazic, Nevena; Yin, Dong; Abbasi-Yadkori, Yasin; Szepesvari, Csaba",,,Confident Least Square Value Iteration with Local Access to a Simulator,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Learning with simulators is ubiquitous in modern reinforcement learning (RL). The simulator can either correspond to a simplified version of the real environment (such as a physics simulation of a robot arm) or to the environment itself (such as in games like Atari and Go). Among algorithms that are provably sample-efficient in this setting, most make the unrealistic assumption that all possible environment states are known before learning begins, or perform global optimistic planning which is computationally inefficient. In this work, we focus on simulation-based RL under a more realistic local access protocol, where the state space is unknown and the simulator can only be queried at states that have previously been observed (initial states and those returned by previous queries). We propose an algorithm named CONFIDENT-LSVI based on the template of least-square value iteration. CONFIDENT-LSVI incrementally builds a coreset of important states and uses the simulator to revisit them. Assuming that the linear function class has low approximation error under the Bellman optimality operator (a.k.a. low inherent Bellman error), we bound the algorithm performance in terms of this error, and show that it is query- and computationally-efficient.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072702021,0
C,"Hung, TT; Gupta, S; Rana, S; Venkatesh, S",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Hung Tran-The; Gupta, Sunil; Rana, Santu; Venkatesh, Svetha",,,Regret Bounds for Expected Improvement Algorithms in Gaussian Process Bandit Optimization,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"The expected improvement (EI) algorithm is one of the most popular strategies for optimization under uncertainty due to its simplicity and efficiency. Despite its popularity, the theoretical aspects of this algorithm have not been properly analyzed. In particular, whether in the noisy setting, the EI strategy with a standard incumbent converges is still an open question of the Gaussian process bandit optimization problem. We aim to answer this question by proposing a variant of EI with a standard incumbent defined via the GP predictive mean. We prove that our algorithm converges, and achieves a cumulative regret bound of O((gamma T)root T) , where gamma T is the maximum information gain between T observations and the Gaussian process model. Based on this variant of EI, we further propose an algorithm called Improved GP-EI that converges faster than previous counterparts. In particular, our proposed variants of EI do not require the knowledge of the RKHS norm and the noise's sub-Gaussianity parameter as in previous works. Empirical validation in our paper demonstrates the effectiveness of our algorithms compared to several baselines.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852303008,0
C,"Kanamori, K; Takagi, T; Kobayashi, K; Ike, Y",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Kanamori, Kentaro; Takagi, Takuya; Kobayashi, Ken; Ike, Yuichi",,,Counterfactual Explanation Trees: Transparent and Consistent Actionable Recourse with Decision Trees,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Counterfactual Explanation (CE) is a post-hoc explanation method that provides a perturbation for altering the prediction result of a classifier. An individual can interpret the perturbation as an action to obtain the desired decision results. Existing CE methods focus on providing an action, which is optimized for a given single instance. However, these CE methods do not address the case where we have to assign actions to multiple instances simultaneously. In such a case, we need a framework of CE that assigns actions to multiple instances in a transparent and consistent way. In this study, we propose Counterfactual Explanation Tree (CET) that assigns effective actions with decision trees. Due to the properties of decision trees, our CET has two advantages: (1) Transparency: the reasons for assigning actions are summarized in an interpretable structure, and (2) Consistency: these reasons do not conflict with each other. We learn a CET in two steps: (i) compute one effective action for multiple instances and (ii) partition the instances to balance the effectiveness and interpretability. Numerical experiments and user studies demonstrated the efficacy of our CET in comparison with existing methods.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072701041,0
C,"Li, ZH; Scarlett, J",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Li, Zihan; Scarlett, Jonathan",,,Gaussian Process Bandit Optimization with Few Batches,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"In this paper, we consider the problem of black-box optimization using Gaussian Process (GP) bandit optimization with a small number of batches. Assuming the unknown function has a low norm in the Reproducing Kernel Hilbert Space (RKHS), we introduce a batch algorithm inspired by batched finitearm bandit algorithms, and show that it achieves the cumulative regret upper bound O*(root T gamma(T)) using O(log log T) batches within time horizon T, where the O*(.) notation hides dimension-independent logarithmic factors and gamma(T) is the maximum information gain associated with the kernel. This bound is near-optimal for several kernels of interest and improves on the typical O*(root T gamma(T)) bound, and our approach is arguably the simplest among algorithms attaining this improvement. In addition, in the case of a constant number of batches (not depending on T), we propose a modified version of our algorithm, and characterize how the regret is impacted by the number of batches, focusing on the squared exponential and Matern kernels. The algorithmic upper bounds are shown to be nearly minimax optimal via analogous algorithm-independent lower bounds.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,92,107,,,,,,,,,,,,,,,,WOS:000828072700006,0
C,"Maalouf, A; Tukan, M; Price, E; Kane, D; Feldman, D",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Maalouf, Alaa; Tukan, Murad; Price, Eric; Kane, Daniel; Feldman, Dan",,,Coresets for Data Discretization and Sine Wave Fitting,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"In the monitoring problem, the input is an unbounded stream P = p(1), p(2) center dot center dot center dot of integers in [N] := {1, center dot center dot center dot, N}, that are obtained from a sensor (such as GPS or heart beats of a human). The goal (e.g., for anomaly detection) is to approximate the n points received so far in P by a single frequency sin, e.g. min(c is an element of C) cost(P, c) + lambda(c), where cost(P, c) = Sigma(n)(i=1) sin(2)(2 pi/N p(i)c), C subset of [N] is a feasible set of solutions, and lambda is a given regularization function. For any approximation error epsilon > 0, we prove that every set P of n integers has a weighted subset S subset of P (sometimes called core-set) of cardinality vertical bar S vertical bar is an element of O(log(N)(O(1))) that approximates cost(P, c) (for every c is an element of[N]) up to a multiplicative factor of 1 +/- epsilon. Using known coreset techniques, this implies streaming algorithms using only O((log(N) log(n))(O(1))) memory. Our results hold for a large family of functions. Experimental results and open source code are provided.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852305007,0
C,"Manupriya, P; Menta, TR; Nath, JS; Balasubramanian, VN",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Manupriya, Piyushi; Menta, Tarun Ram; Nath, J. Saketha; Balasubramanian, Vineeth N.",,,Improving Attribution Methods by Learning Submodular Functions,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"This work explores the novel idea of learning a submodular scoring function to improve the specificity/selectivity of existing feature attribution methods. Submodular scores are natural for attribution as they are known to accurately model the principle of diminishing returns. A new formulation for learning a deep submodular set function that is consistent with the real-valued attribution maps obtained by existing attribution methods is proposed. The final attribution value of a feature is then defined as the marginal gain in the induced submodular score of the feature in the context of other highly attributed features, thus decreasing the attribution of redundant yet discriminatory features. Experiments on multiple datasets illustrate that the proposed attribution method achieves higher specificity along with good discriminative power.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072702010,0
C,"Memmel, M; Liu, PZ; Tateo, D; Peters, J",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Memmel, Marius; Liu, Puze; Tateo, Davide; Peters, Jan",,,Dimensionality Reduction and Prioritized Exploration for Policy Search,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Black-box policy optimization is a class of reinforcement learning algorithms that explores and updates the policies at the parameter level. This class of algorithms is widely applied in robotics with movement primitives or non-differentiable policies. Furthermore, these approaches are particularly relevant where exploration at the action level could cause actuator damage or other safety issues. However, Black-box optimization does not scale well with the increasing dimensionality of the policy, leading to high demand for samples, which are expensive to obtain in real-world systems. In many practical applications, policy parameters do not contribute equally to the return. Identifying the most relevant parameters allows to narrow down the exploration and speed up the learning. Furthermore, updating only the effective parameters requires fewer samples, improving the scalability of the method. We present a novel method to prioritize the exploration of effective parameters and cope with full covariance matrix updates. Our algorithm learns faster than recent approaches and requires fewer samples to achieve state-of-the-art results. To select the effective parameters, we consider both the Pearson correlation coefficient and the Mutual Information. We showcase the capabilities of our approach on the Relative Entropy Policy Search algorithm in several simulated environments, including robotics simulations.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072702008,0
C,"Tuo, R; Wang, WJ",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Tuo, Rui; Wang, Wenjia",,,Uncertainty Quantification for Bayesian Optimization,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Bayesian optimization is a class of global optimization techniques. In Bayesian optimization, the underlying objective function is modeled as a realization of a Gaussian process. Although the Gaussian process assumption implies a random distribution of the Bayesian optimization outputs, quantification of this uncertainty is rarely studied in the literature. In this work, we propose a novel approach to assess the output uncertainty of Bayesian optimization algorithms, which proceeds by constructing confidence regions of the maximum point (or value) of the objective function. These regions can be computed efficiently, and their confidence levels are guaranteed by the uniform error bounds for sequential Gaussian process regression newly developed in the present work. Our theory provides a unified uncertainty quantification framework for all existing sequential sampling policies and stopping criteria.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072702039,0
C,"Vinterbo, SA",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Vinterbo, Staal A.",,,Differential privacy for symmetric log-concave mechanisms,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Adding random noise to database query results is an important tool for achieving privacy. A challenge is to minimize this noise while still meeting privacy requirements. Recently, a sufficient and necessary condition for (epsilon, delta)-differential privacy for Gaussian noise was published. This condition allows the computation of the minimum privacypreserving scale for this distribution. We extend this work and provide a sufficient and necessary condition for (epsilon, delta)-differential privacy for all symmetric and log-concave noise densities. Our results allow fine-grained tailoring of the noise distribution to the dimensionality of the query result. We demonstrate that this can yield significantly lower mean squared errors than those incurred by the currently used Laplace and Gaussian mechanisms for the same epsilon and delta.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,6270,6291,,,,,,,,,,,,,,,,WOS:000841852300031,0
C,"Zhu, YQ; Wang, YX",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Zhu, Yuqing; Wang, Yu-Xiang",,,Adaptive Private-K-Selection with Adaptive K and Application to Multi-label PATE,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We provide an end-to-end Renyi DP based-framework for differentially private top-k selection. Unlike previous approaches, which require a data-independent choice on k, we propose to privately release a data-dependent choice of k such that the gap between k-th and the (k + 1)st quality is large. This is achieved by a novel application of the Report-Noisy-Max. Not only does this eliminate one hyperparameter, the adaptive choice of k also certifies the stability of the top-k indices in the unordered set so we can release them using a variant of propose-test-release (PTR) without adding noise. We show that our construction improves the privacy-utility tradeoffs compared to the previous top-k selection algorithms theoretically and empirically. Additionally, we apply our algorithm to Private Aggregation of Teacher Ensembles (PATE) in multi-label classification tasks with a large number of labels and show that it leads to significant performance gains.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,5622,5635,,,,,,,,,,,,,,,,WOS:000841852300002,0
C,"Arnold, SMR; L'Ecuyer, P; Chen, LY; Chen, YF; Sha, F",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Arnold, Sebastien M. R.; L'Ecuyer, Pierre; Chen, Liyu; Chen, Yi-fan; Sha, Fei",,,Policy Learning and Evaluation with Randomized Quasi-Monte Carlo,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Hard integrals arise frequently in reinforcement learning, for example when computing expectations in policy evaluation and policy iteration. They are often analytically intractable and typically estimated with Monte Carlo methods, whose sampling contributes to high variance in policy values and gradients. In this work, we propose to replace Monte Carlo samples with low-discrepancy point sets. We combine policy gradient methods with Randomized Quasi-Monte Carlo, yielding variance-reduced formulations of policy gradient and actor-critic algorithms. These formulations are effective for policy evaluation and policy improvement, as they outperform state-of-the-art algorithms on standardized continuous control benchmarks. Our empirical analyses validate the intuition that replacing Monte Carlo with Quasi-Monte Carlo yields significantly more accurate gradient estimates.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072701004,0
C,"Doddi, H; Deka, D; Talukdar, S; Salapaka, M",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Doddi, Harish; Deka, Deepjyoti; Talukdar, Saurav; Salapaka, Murti",,,Efficient and passive learning of networked dynamical systems driven by non-white exogenous inputs,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We consider a networked linear dynamical system with p agents/nodes. We study the problem of learning the underlying graph of interactions/dependencies from observations of the nodal trajectories over a time-interval T. We present a regularized non-casual consistent estimator for this problem and analyze its sample complexity over two regimes: (a) where the interval T consists of n i.i.d. observation windows of length T/n (restart and record), and (b) where T is one continuous observation window (consecutive). Using the theory of M-estimators, we show that the estimator recovers the underlying interactions, in either regime, in a time-interval that is logarithmic in the system size p. To the best of our knowledge, this is the first work to analyze the sample complexity of learning linear dynamical systems driven by unobserved not-white wide-sense stationary (WSS) inputs.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852304023,0
C,"Goibert, M; Clemencon, S; Irurozki, E; Mozharovskyi, P",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Goibert, Morgane; Clemencon, Stephan; Irurozki, Ekhine; Mozharovskyi, Pavlo",,,"Statistical Depth Functions for Ranking Distributions: Definitions, Statistical Learning and Applications","INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"The concept of median/consensus has been widely investigated in order to provide a statistical summary of ranking data, i.e. realizations of a random permutation Sigma of a finite set,{1, ..., n} with n >= 1 say. As it sheds light onto only one aspect of Sigma's dis- tribution P, it may neglect other informative features. It is the purpose of this paper to define analogues of quantiles, ranks and statistical procedures based on such quantities for the analysis of ranking data by means of a metric-based notion of depth function on the symmetric group. Overcoming the absence of vector space structure on S-n, the latter defines a center-outward ordering of the permutations in the support of P and extends the classic metric-based formulation of consensus ranking (medians corresponding then to the deepest permutations). The axiomatic properties that ranking depths should ideally possess are listed, while computational and generalization issues are studied at length. Beyond the theoretical analysis carried out, the relevance of the novel concepts and methods introduced for a wide variety of statistical tasks are also supported by numerous numerical experiments.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852304037,0
C,"Huang, JW; Jiang, N",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Huang, Jiawei; Jiang, Nan",,,On the Convergence Rate of Off-Policy Policy Optimization Methods with Density-Ratio Correction,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"In this paper, we study the convergence properties of off-policy policy optimization algorithms with state-action density ratio correction under function approximation setting, where the objective function is formulated as a max-max-min problem. We first clearly characterize the bias of the learning objective, and then present two strategies with finite-time convergence guarantees. In our first strategy, we propose an algorithm called P-SREDA with convergence rate O(epsilon(-3)), whose dependency on epsilon is optimal. Besides, in our second strategy, we design a new off-policy actor-critic style algorithm named O-SPIM. We prove that O-SPIM converges to a stationary point with total complexity O(epsilon(-4)), which matches the convergence rate of some recent actor-critic algorithms under on-policy setting.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072702032,0
C,"Kao, H; Subramanian, V",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Kao, Hsu; Subramanian, Vijay",,,Common Information based Approximate State Representations in Multi-Agent Reinforcement Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Due to information asymmetry, finding optimal policies for Decentralized Partially Observable Markov Decision Processes (Dec-POMDPs) is hard with the complexity growing doubly exponentially in the horizon length. The challenge increases greatly in the multi-agent reinforcement learning (MARL) setting where the transition probabilities, observation kernel, and reward function are unknown. Here, we develop a general compression framework with approximate common and private state representations, based on which decentralized policies can be constructed. We derive the optimality gap of executing dynamic programming (DP) with the approximate states in terms of the approximation error parameters and the remaining time steps. When the compression is exact (no error), the resulting DP is equivalent to the one in existing work. Our general framework generalizes a number of methods proposed in the literature. The results shed light on designing practically useful deep-MARL network structures under the centralized learning distributed execution scheme.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852301018,0
C,"Kubler, JM; Jitkrittum, W; Scholkopf, B; Muandet, K",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Kuebler, Jonas M.; Jitkrittum, Wittawat; Schoelkopf, Bernhard; Muandet, Krikamol",,,A Witness Two-Sample Test,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"The Maximum Mean Discrepancy (MMD) has been the state-of-the-art nonparametric test for tackling the two-sample problem. Its statistic is given by the difference in expectations of the witness function, a real-valued function defined as the mean of kernel evaluations on a set of basis points. Typically the kernel is optimized on a training set, and hypothesis testing is performed on a separate test set to avoid overfitting (i.e., control type-I error). That is, the test set is used to simultaneously estimate the expectations and define the basis points, while the training set only serves to select the kernel and is discarded. In this work, we propose to use the training set to also define the weights and the basis points for better data efficiency. We show that 1) the new test is consistent and has a well-controlled type-I error; 2) the optimal witness function is given by a precision-weighted mean in the reproducing kernel Hilbert space associated with the kernel; and 3) the test power of the proposed test is comparable or exceeds that of the MMD and other modern tests, as verified empirically on challenging synthetic and real problems (e.g., Higgs data).",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072701021,0
C,"Nowak-Vila, A; Rudi, A; Bach, F",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Nowak-Vila, Alex; Rudi, Alessandro; Bach, Francis",,,On the Consistency of Max-Margin Losses,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"The foundational concept of Max-Margin in machine learning is ill-posed for output spaces with more than two labels such as in structured prediction. In this paper, we show that the Max-Margin loss can only be consistent to the classification task under highly restrictive assumptions on the discrete loss measuring the error between outputs. These conditions are satisfied by distances defined in tree graphs, for which we prove consistency, thus being the first losses shown to be consistent for Max-Margin beyond the binary setting. We finally address these limitations by correcting the concept of Max-Margin and introducing the Restricted-Max-Margin, where the maximization of the loss-augmented scores is maintained, but performed over a subset of the original domain. The resulting loss is also a generalization of the binary support vector machine and it is consistent under milder conditions on the discrete loss.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072704032,0
C,"Roelofs, R; Cain, N; Shlens, J; Mozer, MC",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Roelofs, Rebecca; Cain, Nicholas; Shlens, Jonathon; Mozer, Michael C.",,,Mitigating Bias in Calibration Error Estimation,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"For an AI system to be reliable, the confidence it expresses in its decisions must match its accuracy. To assess the degree of match, examples are typically binned by confidence and the per-bin mean confidence and accuracy are compared. Most research in calibration focuses on techniques to reduce this empirical measure of calibration error, ECEBIN. We instead focus on assessing statistical bias in this empirical measure, and we identify better estimators. We propose a framework through which we can compute the bias of a particular estimator for an evaluation data set of a given size. The framework involves synthesizing model outputs that have the same statistics as common neural architectures on popular data sets. We find that binning-based estimators with bins of equal mass (number of instances) have lower bias than estimators with bins of equal width. Our results indicate two reliable calibration-error estimators: the debiased estimator (Brocker, 2012; Ferro and Fricker, 2012) and a method we propose, ECESWEEP, which uses equal-mass bins and chooses the number of bins to be as large as possible while preserving monotonicity in the calibration function. With these estimators, we observe improvements in the effectiveness of recalibration methods and in the detection of model miscalibration.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072704005,0
C,"Santara, A; Aggarwal, G; Li, S; Gentile, C",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Santara, Anirban; Aggarwal, Gaurav; Li, Shuai; Gentile, Claudio",,,Learning to Plan Variable Length Sequences of Actions with a Cascading Bandit Click Model of User Feedback,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Motivated by problems of ranking with partial information, we introduce a variant of the cascading bandit model that considers flexible length sequences with varying rewards and losses. We formulate two generative models for this problem within the generalized linear setting, and design and analyze upper confidence algorithms for it. Our analysis delivers tight regret bounds which, when specialized to standard cascading bandits, results in sharper guarantees than previously available in the literature. We evaluate our algorithms against a representative sample of cascading bandit baselines on a number of real-world datasets and show significantly improved empirical performance.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,767,797,,,,,,,,,,,,,,,,WOS:000828072700032,0
C,"Talwai, P; Shameli, A; Simchi-Levi, D",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Talwai, Prem; Shameli, Ali; Simchi-Levi, David",,,Sobolev Norm Learning Rates for Conditional Mean Embeddings,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We develop novel learning rates for conditional mean embeddings by applying the theory of interpolation for reproducing kernel Hilbert spaces (RKHS). We derive explicit, adaptive convergence rates for the sample estimator under the misspecifed setting, where the target operator is not Hilbert-Schmidt or bounded with respect to the input/output RKHSs. We demonstrate that in certain parameter regimes, we can achieve uniform convergence rates in the output RKHS. We hope our analyses will allow the much broader application of conditional mean embeddings to more complex ML/RL settings involving infinite dimensional RKHSs and continuous state spaces.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852304039,0
C,"Velikanov, M; Kail, R; Anokhin, I; Vashurin, R; Panov, M; Zaytsev, A; Yarotsky, D",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Velikanov, Maksim; Kail, Roman; Anokhin, Ivan; Vashurin, Roman; Panov, Maxim; Zaytsev, Alexey; Yarotsky, Dmitry",,,Embedded Ensembles: Infinite Width Limit and Operating Regimes,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"A memory efficient approach to ensembling neural networks is to share most weights among the ensembled models by means of a single reference network. We refer to this strategy as Embedded Ensembling (EE); its particular examples are BatchEnsembles and Monte-Carlo dropout ensembles. In this paper we perform a systematic theoretical and empirical analysis of embedded ensembles with different number of models. Theoretically, we use a Neural-Tangent-Kernel-based approach to derive the wide network limit of the gradient descent dynamics. In this limit, we identify two ensemble regimes {independent and collective {depending on the architecture and initialization strategy of ensemble models. We prove that in the independent regime the embedded ensemble behaves as an ensemble of independent models. We confirm our theoretical prediction with a wide range of experiments with finite networks, and further study empirically various effects such as transition between the two regimes, scaling of ensemble performance with the network width and number of models, and dependence of performance on a number of architecture and hyperparameter choices.",,,,,"Yarotsky, Dmitry/A-8811-2016","Yarotsky, Dmitry/0000-0002-5432-7143",,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072703011,0
C,"Zenati, H; Bietti, A; Diemert, E; Mairal, J; Martin, M; Gaillard, P",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Zenati, Houssam; Bietti, Alberto; Diemert, Eustache; Mairal, Julien; Martin, Matthieu; Gaillard, Pierre",,,Efficient Kernel UCB for Contextual Bandits,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"In this paper, we tackle the computational efficiency of kernelized UCB algorithms in contextual bandits. While standard methods require a O(CT3) complexity where T is the horizon and the constant C is related to optimizing the UCB rule, we propose an efficient contextual algorithm for large-scale problems. Specifically, our method relies on incremental Nystrom approximations of the joint kernel embedding of contexts and actions. This allows us to achieve a complexity of O(CTm2) where m is the number of Nystrom points. To recover the same regret as the standard kernelized UCB algorithm, m needs to be of order of the effective dimension of the problem, which is at most O(root T) and nearly constant in some cases.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,5689,5720,,,,,,,,,,,,,,,,WOS:000841852300005,0
C,"Zharmagambetov, A; Carreira-Perpinan, MA",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Zharmagambetov, Arman; Carreira-Perpinan, Miguel A.",,,"Learning Interpretable, Tree-Based Projection Mappings for Nonlinear Embeddings","INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Model interpretability is a topic of renewed interest given today's widespread practical use of machine learning, and the need to trust or understand automated predictions. We consider the problem of optimally learning interpretable out-of-sample mappings for nonlinear embedding methods such as t-SNE. We argue for the use of sparse oblique decision trees because they strike a good tradeoff between accuracy and interpretability which can be controlled via a hyperparameter, thus allowing one to achieve a model with a desired explanatory complexity. The resulting optimization problem is difficult because decision trees are not differentiable. By using an equivalent formulation of the problem, we give an algorithm that can learn such a tree for any given nonlinear embedding objective. We illustrate experimentally how the resulting trees provide insights into the data beyond what a simple 2D visualization of the embedding does.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852304002,0
J,"Atan, O; Tekin, C; van der Schaar, M",,,,"Atan, Onur; Tekin, Cem; van der Schaar, Mihaela",,,Global Bandits,IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS,,,,,International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY, 2015","San Diego, CA",,,,,"Multiarmed bandits (MABs) model sequential decision-making problems, in which a learner sequentially chooses arms with unknown reward distributions in order to maximize its cumulative reward. Most of the prior works on MAB assume that the reward distributions of each arm are independent. But in a wide variety of decision problems-from drug dosage to dynamic pricing-the expected rewards of different arms are correlated, so that selecting one arm provides information about the expected rewards of other arms as well. We propose and analyze a class of models of such decision problems, which we call global bandits (GB). In the case in which rewards of all arms are deterministic functions of a single unknown parameter, we construct a greedy policy that achieves bounded regret, with a bound that depends on the single true parameter of the problem. Hence, this policy selects suboptimal arms only finitely many times with probability one. For this case, we also obtain a bound on regret that is independent of the true parameter; this bound is sublinear, with an exponent that depends on the informativeness of the arms. We also propose a variant of the greedy policy that achieves O (root T) worst case and O(1) parameter-dependent regret. Finally, we perform experiments on dynamic pricing and show that the proposed algorithms achieve significant gains with respect to the well-known benchmarks.",,,,,,"van der schaar, Mihaela/0000-0003-3933-6049",,,,,,,,,,,,,2162-237X,2162-2388,,,,DEC,2018,29,12,,,,,5798,5811,,10.1109/TNNLS.2018.2818742,0,,,,,,,,29993936,,,,,WOS:000451230100001,0
C,"Bernard, C; Biau, G; Da Veiga, S; Scornet, E",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Bernard, Clement; Biau, Gerard; Da Veiga, Sebastien; Scornet, Erwan",,,SHAFF: Fast and consistent SHApley eFfect estimates via random Forests,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Interpretability of learning algorithms is crucial for applications involving critical decisions, and variable importance is one of the main interpretation tools. Shapley effects are now widely used to interpret both tree ensembles and neural networks, as they can efficiently handle dependence and interactions in the data, as opposed to most other variable importance measures. However, estimating Shapley effects is a challenging task, because of the computational complexity and the conditional expectation estimates. Accordingly, existing Shapley algorithms have flaws: a costly running time, or a bias when input variables are dependent. Therefore, we introduce SHAFF, SHApley eFfects via random Forests, a fast and accurate Shapley effect estimate, even when input variables are dependent. We show SHAFF efficiency through both a theoretical analysis of its consistency, and the practical performance improvements over competitors with extensive experiments. An implementation of SHAFF in C++ and R is available online.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072705026,0
C,"Bordt, S; von Luxburg, U",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Bordt, Sebastian; von Luxburg, Ulrike",,,A Bandit Model for Human-Machine Decision Making with Private Information and Opacity,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Applications of machine learning inform human decision makers in a broad range of tasks. The resulting problem is usually formulated in terms of a single decision maker. We argue that it should rather be described as a two-player learning problem where one player is the machine and the other the human. While both players try to optimize the final decision, the setup is often characterized by (1) the presence of private information and (2) opacity, that is imperfect understanding between the decision makers. We prove that both properties can complicate decision making considerably. A lower bound quantifies the worst-case hardness of optimally advising a decision maker who is opaque or has access to private information. An upper bound shows that a simple coordination strategy is nearly minimax optimal. More efficient learning is possible under certain assumptions on the problem, for example that both players learn to take actions independently. Such assumptions are implicit in existing literature, for example in medical applications of machine learning, but have not been described or justified theoretically.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852301034,0
C,"Fan, JJ; Haasler, I; Karlsson, J; Chen, YX",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Fan, Jiaojiao; Haasler, Isabel; Karlsson, Johan; Chen, Yongxin",,,On the complexity of the optimal transport problem with graph-structured cost,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Multi-marginal optimal transport (MOT) is a generalization of optimal transport to multiple marginals. Optimal transport has evolved into an important tool in many machine learning applications, and its multi-marginal extension opens up for addressing new challenges in the field of machine learning. However, the usage of MOT has been largely impeded by its computational complexity which scales exponentially in the number of marginals. Fortunately, in many applications, such as barycenter or interpolation problems, the cost function adheres to structures, which has recently been exploited for developing efficient computational methods. In this work we derive computational bounds for these methods. In particular, with m marginal distributions supported on n points, we provide a (O) over tilde (d(T)mn(w(G))+(1) epsilon(-2)) bound for a 6-accuracy when the problem is associated with a graph that can be factored as a junction tree with diameter d(T) and tree-width w(G). For the special case of the Wasserstein barycenter problem, which corresponds to a star-shaped tree, our bound is in alignment with the existing complexity bound for it.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852303024,0
C,"Ghassami, A; Ying, A; Shpitser, I; Tchetgen, ET",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Ghassami, AmirEmad; Ying, Andrew; Shpitser, Ilya; Tchetgen, Eric Tchetgen",,,Minimax Kernel Machine Learning for a Class of Doubly Robust Functionals with Application to Proximal Causal Inference,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Robins et al. (2008) introduced a class of influence functions (IFs) which could be used to obtain doubly robust moment functions for the corresponding parameters. However, that class does not include the IF of parameters for which the nuisance functions are solutions to integral equations. Such parameters are particularly important in the field of causal inference, specifically in the recently proposed proximal causal inference framework of Tchetgen Tchetgen et al. (2020), which allows for estimating the causal effect in the presence of latent confounders. In this paper, we first extend the class of Robins et al. to include doubly robust IFs in which the nuisance functions are solutions to integral equations. Then we demonstrate that the double robustness property of these IFs can be leveraged to construct estimating equations for the nuisance functions, which enables us to solve the integral equations without resorting to parametric models. We frame the estimation of the nuisance functions as a minimax optimization problem. We provide convergence rates for the nuisance functions and conditions required for asymptotic linearity of the estimator of the parameter of interest. The experiment results demonstrate that our proposed methodology leads to robust and high-performance estimators for average causal effect in the proximal causal inference framework.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852301030,0
C,"Glasgow, M; Yuan, HL; Ma, TY",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Glasgow, Margalit; Yuan, Honglin; Ma, Tengyu",,,Sharp Bounds for Federated Averaging (Local SGD) and Continuous Perspective,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Federated Averaging (FEDAvG), also known as Local SGD, is one of the most popular algorithms in Federated Learning (FL). Despite its simplicity and popularity, the convergence rate of FEDAvG has thus far been undetermined. Even under the simplest assumptions (convex, smooth, homogeneous, and bounded covariance), the best known upper and lower bounds do not match, and it is not clear whether the existing analysis captures the capacity of the algorithm. In this work, we first resolve this question by providing a lower bound for FEDAvG that matches the existing upper bound, which shows the existing FEDAVG upper bound analysis is not improvable. Additionally, we establish a lower bound in a heterogeneous setting that nearly matches the existing upper bound. While our lower bounds show the limitations of FEDAvG, under an additional assumption of third-order smoothness, we prove more optimistic stateof-the-art convergence results in both convex and non-convex settings. Our analysis stems from a notion we call iterate bias, which is defined by the deviation of the expectation of the SGD trajectory from the noiseless gradient descent trajectory with the same initialization. We prove novel sharp bounds on this quantity, and show intuitively how to analyze this quantity from a Stochastic Differential Equation (SDE) perspective(1).",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852303021,0
C,"Nakano, M; Kimura, A; Nishikimi, R; Yamada, T; Fujiwara, Y; Ueda, N",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Nakano, Masahiro; Kimura, Akisato; Nishikimi, Ryo; Yamada, Takeshi; Fujiwara, Yasuhiro; Ueda, Naonori",,,Nonparametric Relational Models with Superrectangulation,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"This paper addresses the question, What is the smallest object that contains all rectangular partitions with n or fewer blocks? and shows its application to relational data analysis using a new strategy we call SUPER BAYES as an alternative to Bayesian nonparametric (BNP) methods. Conventionally, standard BNP methods have combined the Aldous-Hoover-Kallenberg representation with parsimonious stochastic processes on rectangular partitioning to construct BNP relational models. As a result, conventional methods face the great difficulty of searching for a parsimonious random rectangular partition that fits the observed data well in Bayesian inference. As a way to essentially avoid such a problem, we propose a strategy to combine an extremely redundant rectangular partition as a deterministic (non-probabilistic) object. Specifically, we introduce a special kind of rectangular partitioning, which we call super-rectangulation, that contains all possible rectangular partitions. Delightfully, this strategy completely eliminates the difficult task of searching around for random rectangular partitions, since the superrectangulation is deterministically fixed in inference. Experiments on predictive performance in relational data analysis show that the super Bayesian model provides a more stable analysis than the existing BNP models, which are less likely to be trapped in bad local optima.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852303016,0
C,"Ryu, JJ; Bhatt, A; Kim, YH",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Ryu, J. Jon; Bhatt, Alankrita; Kim, Young-Han",,,Parameter-Free Online Linear Optimization with Side Information via Universal Coin Betting,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"A class of parameter-free online linear optimization algorithms is proposed that harnesses the structure of an adversarial sequence by adapting to some side information. These algorithms combine the reduction technique of Orabona and Pal (2016) for adapting coin betting algorithms for online linear optimization with universal compression techniques in information theory for incorporating sequential side information to coin betting. Concrete examples are studied in which the side information has a tree structure and consists of quantized values of the previous symbols of the adversarial sequence, including fixed-order and variable-order Markov cases. By modifying the context-tree weighting technique of Willems, Shtarkov, and Tjalkens (1995), the proposed algorithm is further refined to achieve the best performance over all adaptive algorithms with tree-structured side information of a given maximum order in a computationally efficient manner.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,6022,6044,,,,,,,,,,,,,,,,WOS:000841852300019,0
C,"Tighineanu, P; Skubch, K; Baireuther, P; Reiss, A; Berkenkamp, F; Vinogradska, J",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Tighineanu, Petru; Skubch, Kathrin; Baireuther, Paul; Reiss, Attila; Berkenkamp, Felix; Vinogradska, Julia",,,Transfer Learning with Gaussian Processes for Bayesian Optimization,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Bayesian optimization is a powerful paradigm to optimize black-box functions based on scarce and noisy data. Its data efficiency can be further improved by transfer learning from related tasks. While recent transfer models meta-learn a prior based on large amount of data, in the low-data regime methods that exploit the closed-form posterior of Gaussian processes (GPs) have an advantage. In this setting, several analytically tractable transfer-model posteriors have been proposed, but the relative advantages of these methods are not well understood. In this paper, we provide a unified view on hierarchical GP models for transfer learning, which allows us to analyze the relationship between methods. As part of the analysis, we develop a novel closed-form boosted GP transfer model that fits between existing approaches in terms of complexity. We evaluate the performance of the different approaches in large-scale experiments and highlight strengths and weaknesses of the different transfer-learning methods.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,6152,6181,,,,,,,,,,,,,,,,WOS:000841852300026,0
C,"Wu, H; Wirth, A",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Wu, Hao; Wirth, Anthony",,,Asymptotically Optimal Locally Private Heavy Hitters via Parameterized Sketches,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We study the frequency estimation problem under the local differential privacy model. Frequency estimation is a fundamental computational question, and differential privacy has become the de-facto standard, with the local version (LDP) affording even greater protection. On large input domains, sketching methods and hierarchical search methods are commonly and successfully, in practice, applied for reducing the size of the domain, and for identifying frequent elements. It is therefore of interest whether the current theoretical analysis of such algorithms is tight, or whether we can obtain algorithms in a similar vein that achieve optimal error guarantee. We introduce two algorithms for LDP frequency estimation. One solves the fundamental frequency oracle problem; the other solves the well-known heavy hitters identification problem. As a function of failure probability, beta, the former achieves optimal worst-case estimation error for every beta, the latter is optimal when beta is at least inverse polynomial in n, the number of users. In each algorithm, server running time and memory usage are (O) over tilde (n) and (O) over tilde(root n), respectively, while user running time and memory usage are both (O) over tilde (1). Our frequency-oracle algorithm achieves lower estimation error than Bassily et al. (NeurIPS 2017). On the other hand, our heavy hitters identification method improves the worstcase error of TreeHist (ibid) by a factor of Omega(root log n); it avoids invoking error-correcting codes, known to be theoretically powerful, but yet to be implemented efficiently.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852302009,0
C,"Yalcin, B; Zhang, HX; Lavaei, J; Sojoudi, S",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Yalcin, Baturalp; Zhang, Haixiang; Lavaei, Javad; Sojoudi, Somayeh",,,Factorization Approach for Low-complexity Matrix Completion Problems: Exponential Number of Spurious Solutions and Failure of Gradient Methods,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Burer-Monteiro (B-M) factorization approach can efficiently solve low-rank matrix optimization problems under the Restricted Isometry Property (RIP) condition. It is natural to ask whether BM factorization-based methods can succeed on any low-rank matrix optimization problems with a low information-theoretic complexity, i.e., polynomial-time solvable problems that have a unique solution. We provide negative answer to this question. We investigate the landscape of B-M factorized polynomial-time solvable matrix completion (MC) problems, which are the most popular subclass of low-rank matrix optimization problems without the RIP condition. We construct an instance of polynomial-time solvable MC problems with exponentially many spurious local minima, which leads to the failure of most gradient-based methods. We define a new complexity metric that measures the solvability of low-rank matrix optimization problems based on B-M factorization approach. In addition, we show that more measurements can deteriorate the landscape, which further reveals the unfavorable behavior of B-M factorization.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,319,341,,,,,,,,,,,,,,,,WOS:000828072700014,0
C,"Yan, T; Zhang, CC",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Yan, Tom; Zhang, Chicheng",,,Margin-distancing for safe model explanation,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"The growing use of machine learning models in consequential settings has highlighted an important and seemingly irreconcilable tension between transparency and vulnerability to gaming. While this has sparked sizable debate in legal literature, there has been comparatively less technical study of this contention. In this work, we propose a clean-cut formulation of this tension and a way to make the tradeoff between transparency and gaming. We identify the source of gaming as being points close to the decision boundary of the model. And we initiate an investigation on how to provide example-based explanations that are expansive and yet consistent with a version space that is sufficiently uncertain with respect to the boundary points' labels. Finally, we furnish our theoretical results with empirical investigations of this tradeoff on real-world datasets.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072705010,0
C,"Yoon, T; Park, Y; Ryu, EK; Wang, YY",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Yoon, TaeHo; Park, Youngsuk; Ryu, Ernest K.; Wang, Yuyang",,,Robust Probabilistic Time Series Forecasting,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Probabilistic time series forecasting has played critical role in decision-making processes due to its capability to quantify uncertainties. Deep forecasting models, however, could be prone to input perturbations, and the notion of such perturbations, together with that of robustness, has not even been completely established in the regime of probabilistic forecasting. In this work, we propose a framework for robust probabilistic time series forecasting. First, we generalize the concept of adversarial input perturbations, based on which we formulate the concept of robustness in terms of bounded Wasserstein deviation. Then we extend the randomized smoothing technique to attain robust probabilistic forecasters with theoretical robustness certificates against certain classes of adversarial perturbations. Lastly, extensive experiments demonstrate that our methods are empirically effective in enhancing the forecast quality under additive adversarial attacks and forecast consistency under supplement of noisy observations. The code for our experiments is available at https://github.com/tetrzim/robust-probabilistic-forecasting.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072701018,0
C,"Ash, JT; Goel, S; Krishnamurthy, A; Misra, D",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Ash, Jordan T.; Goel, Surbhi; Krishnamurthy, Akshay; Misra, Dipendra",,,Investigating the Role of Negatives in Contrastive Representation Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Noise contrastive learning is a popular technique for unsupervised representation learning. In this approach, a representation is obtained via reduction to supervised learning, where given a notion of semantic similarity, the learner tries to distinguish a similar (positive) example from a collection of random (negative) examples. The success of modern contrastive learning pipelines relies on many design decisions, such as the choice of data augmentation, the number of negative examples, and the batch size; however, there is limited understanding as to how these parameters interact and affect downstream performance. We focus on disambiguating the role of one of these parameters: the number of negative examples. Theoretically, we show the existence of a collision-coverage trade-off suggesting that the optimal number of negative examples should scale with the number of underlying concepts in the data. Empirically, we scrutinize the role of the number of negatives in both NLP and vision tasks.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852301029,0
C,"Chen, NY; Gao, XF; Xiong, Y",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Chen, Ningyuan; Gao, Xuefeng; Xiong, Yi",,,Debiasing Samples from Online Learning Using Bootstrap,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"It has been recently shown in the literature (Nie et al., 2018; Shin et al., 2019a,b) that the sample averages from online learning experiments arc biased when used to estimate the mean reward. To correct the bias, off-policy evaluation methods, including importance sampling and doubly robust estimators, typically calculate the conditional propensity score, which is ill-defined for non-randomized policies such as UCB. This paper provides a procedure to debias the samples using bootstrap, which doesn't require the knowledge of the reward distribution and can be applied to any adaptive policies. Numerical experiments demonstrate the effective bias reduction for samples generated by popular multiarmed bandit algorithms such as ExploreThen-Commit (ETC), UCB, Thompson sampling (TS) and epsilon-greedy (EG). We analyze and provide theoretical justifications for the procedure under the ETC algorithm, including the asymptotic convergence of the bias decay rate in the real and bootstrap worlds.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852302040,0
C,"Chi, JF; Shen, J; Dai, XY; Zhang, WN; Tian, Y; Zhao, H",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Chi, Jianfeng; Shen, Jian; Dai, Xinyi; Zhang, Weinan; Tian, Yuan; Zhao, Han",,,Towards Return Parity in Markov Decision Processes,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Algorithmic decisions made by machine learning models in high-stakes domains may have lasting impacts over time. However, naive applications of standard fairness criterion in static settings over temporal domains may lead to delayed and adverse effects. To understand the dynamics of performance disparity, we study a fairness problem in Markov decision processes (MDPs). Specifically, we propose return parity, a fairness notion that requires MDPs from different demographic groups that share the same state and action spaces to achieve approximately the same expected time-discounted rewards. We first provide a decomposition theorem for return disparity, which decomposes the return disparity of any two MDPs sharing the same state and action spaces into the distance between group-wise reward functions, the discrepancy of group policies, and the discrepancy between state visitation distributions induced by the group policies. Motivated by our decomposition theorem, we propose algorithms to mitigate return disparity via learning a shared group policy with state visitation distributional alignment using integral probability metrics. We conduct experiments to corroborate our results, showing that the proposed algorithm can successfully close the disparity gap while maintaining the performance of policies on two real-world recommender system benchmark datasets.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072701009,0
C,"Dinitz, M; Srinivasan, A; Tsepenekas, L; Vullikanti, A",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Dinitz, Michael; Srinivasan, Aravind; Tsepenekas, Leonidas; Vullikanti, Anil",,,Fair Disaster Containment via Graph-Cut Problems,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Graph cut problems are fundamental in combinatorial Optimization, and are a central object of study in both theory and practice. Further, the study of fairness in Algorithmic Design and Machine Learning has recently received significant attention, with many different notions proposed and analyzed for a variety of contexts. In this paper we initiate the study of fairness for graph cut problems by giving the first fair definitions for them, and subsequently we demonstrate appropriate algorithmic techniques that yield a rigorous theoretical analysis. Specifically, we incorporate two different notions of fairness, namely demographic and probabilistic individual fairness, in a particular cut problem that models disaster containment scenarios. Our results include a variety of approximation algorithms with provable theoretical guarantees.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,6321,6333,,,,,,,,,,,,,,,,WOS:000841852300033,0
C,"Gorbunov, E; Loizou, N; Gidel, G",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Gorbunov, Eduard; Loizou, Nicolas; Gidel, Gauthier",,,Extragradient Method: O(1/K) Last-Iterate Convergence for Monotone Variational Inequalities and Connections With Cocoercivity,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Extragradient method (EG) (Korpelevich, 1976) is one of the most popular methods for solving saddle point and variational inequalities problems (VIP). Despite its long history and significant attention in the optimization community, there remain important open questions about convergence of EG. In this paper, we resolve one of such questions and derive the first last-iterate O(1/K) convergence rate for EG for monotone and Lipschitz VIP without any additional assumptions on the operator unlike the only known result of this type (Golowich et al., 2020b) that relies on the Lipschitzness of the Jacobian of the operator. The rate is given in terms of reducing the squared norm of the operator. Moreover, we establish several results on the (non-)cocoercivity of the update operators of EG, Optimistic Gradient Method, and Hamiltonian Gradient Method, when the original operator is monotone and Lipschitz.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,366,402,,,,,,,,,,,,,,,,WOS:000828072700016,0
C,"Kim, M; Guerrero, R; Pham, HX; Pavlovic, V",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Kim, Minyoung; Guerrero, Ricardo; Pham, Hai X.; Pavlovic, Vladimir",,,Variational Continual Proxy-Anchor for Deep Metric Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"The recent proxy-anchor method achieved outstanding performance in deep metric learning, which can be acknowledged to its data efficient loss based on hard example mining, as well as far lower sampling complexity than pair-based approaches. In this paper we extend the proxy-anchor method by posing it within the continual learning framework, motivated from its batch-expected loss form (instead of instance-expected, typical in deep learning), which can potentially incur the catastrophic forgetting of historic batches. By regarding each batch as a task in continual learning, we adopt the Bayesian variational continual learning approach to derive a novel loss function. Interestingly the resulting loss has two key modifications to the original proxy-anchor loss: i) we inject noise to the proxies when optimizing the proxy-anchor loss, and ii) we encourage momentum update to avoid abrupt model changes. As a result, the learned model achieves higher test accuracy than proxy-anchor due to the robustness to noise in data (through model perturbation during training), and the reduced batch forgetting effect. We demonstrate the improved results on several benchmark datasets.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072704029,0
C,"Kristiadi, A; Hein, M; Hennig, P",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Kristiadi, Agustinus; Hein, Matthias; Hennig, Philipp",,,Being a Bit Frequentist Improves Bayesian Neural Networks,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Despite their compelling theoretical properties, Bayesian neural networks (BNNs) tend to perform worse than frequentist methods in classification-based uncertainty quantification (UQ) tasks such as out-of-distribution (OOD) detection. In this paper, based on empirical findings in prior works, we hypothesize that this issue is because even recent Bayesian methods have never considered OOD data in their training processes, even though this OOD training technique is an integral part of state-of-the-art frequentist UQ methods. To validate this, we treat OOD data as a first-class citizen in BNN training by exploring four different ways of incorporating OOD data into Bayesian inference. We show in extensive experiments that OOD-trained BNNs are competitive to recent frequentist baselines. This work thus provides strong baselines for future work in Bayesian UQ.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,529,545,,,,,,,,,,,,,,,,WOS:000828072700022,0
C,"Li, YS; Shi, Z; Zhang, XH; Ziebart, BD",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Li, Yeshu; Shi, Zhan; Zhang, Xinhua; Ziebart, Brian D.",,,Distributionally Robust Structure Learning for Discrete Pairwise Markov Networks,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We consider the problem of learning the underlying structure of a general discrete pairwise Markov network. Existing approaches that rely on empirical risk minimization may perform poorly in settings with noisy or scarce data. To overcome these limitations, we propose a computationally efficient and robust learning method for this problem with near-optimal sample complexities. Our approach builds upon distributionally robust optimization (DRO) and maximum conditional log-likelihood. The proposed DRO estimator minimizes the worst-case risk over an ambiguity set of adversarial distributions within bounded transport cost or f-divergence of the empirical data distribution. We show that the primal minimax learning problem can be efficiently solved by leveraging sufficient statistics and greedy maximization in the ostensibly intractable dual formulation. Based on DRO's approximation to Lipschitz and variance regularization, we derive near-optimal sample complexities matching existing results. Extensive empirical evidence with different corruption models corroborates the effectiveness of the proposed methods.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852303019,0
C,"Ma, YY; Zhang, XH",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Ma, Yingyi; Zhang, Xinhua",,,Warping Layer: Representation Learning for Label Structures in Weakly Supervised Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Many learning tasks only receive weak supervision, such as semi-supervised learning and few-shot learning. With limited labeled data, prior structures become especially important, and prominent examples include hierarchies and mutual exclusions in the class space. However, most existing approaches only learn the representations separately in the feature space and the label space, and do not explicitly enforce the logical relationships. In this paper, we propose a novel warping layer that jointly learns representations in both spaces, and thanks to the modularity and differentiability, it can be directly embedded into generative models to leverage the prior hierarchical structure and unlabeled data. The effectiveness of the warping layer is demonstrated on both few-shot and semi-supervised learning, outperforming the state of the art in practice.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852301033,0
C,"Mikheeva, O; Kazlauskaite, I; Hartshorne, A; Kjellstrom, H; Ek, CH; Campbell, NDF",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Mikheeva, Olga; Kazlauskaite, Ieva; Hartshorne, Adam; Kjellstrom, Hedvig; Ek, Carl Henrik; Campbell, Neill D. F.",,,Aligned Multi-Task Gaussian Process,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Multi-task learning requires accurate identification of the correlations between tasks. In real-world time-series, tasks are rarely perfectly temporally aligned; traditional multi-task models do not account for this and subsequent errors in correlation estimation will result in poor predictive performance and uncertainty quantification. We introduce a method that automatically accounts for temporal misalignment in a unified generative model that improves predictive performance. Our method uses Gaussian processes (GPs) to model the correlations both within and between the tasks. Building on the previous work by Kazlauskaite et al. (2019), we include a separate monotonic warp of the input data to model temporal misalignment. In contrast to previous work, we formulate a lower bound that accounts for uncertainty in both the estimates of the warping process and the underlying functions. Also, our new take on a monotonic stochastic process, with efficient path-wise sampling for the warp functions, allows us to perform full Bayesian inference in the model rather than MAP estimates. Missing data experiments, on synthetic and real time-series, demonstrate the advantages of accounting for misalignments (vs standard unaligned method) as well as modelling the uncertainty in the warping process (vs baseline MAP alignment approach).",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072703003,0
C,"Mukhopadhyay, S; Sahoo, S; Sinha, A",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Mukhopadhyay, Samrat; Sahoo, Sourav; Sinha, Abhishek",,,k-experts - Online Policies and Fundamental Limits,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We introduce the k-experts problem - a generalization of the classic Prediction with Expert's Advice framework. Unlike the classic version, where the learner selects exactly one expert from a pool of N experts at each round, in this problem, the learner can select a subset of k experts at each round (1 <= k <= N). The reward obtained by the learner at each round is assumed to be a function of the k selected experts. The primary objective is to design an online learning policy with a small regret. In this pursuit, we propose SAGE (Sampled Hedge) - a framework for designing efficient online learning policies by leveraging statistical sampling techniques. For a wide class of reward functions, we show that SAGE either achieves the first sublinear regret guarantee or improves upon the existing ones. Furthermore, going beyond the notion of regret, we fully characterize the mistake bounds achievable by online learning policies for stable loss functions. We conclude the paper by establishing a tight regret lower bound for a variant of the k-experts problem and carrying out experiments with standard datasets.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,342,365,,,,,,,,,,,,,,,,WOS:000828072700015,0
C,"Nalenz, M; Augustin, T",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Nalenz, Make; Augustin, Thomas",,,Compressed Rule Ensemble Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Ensembles of decision rules extracted from tree ensembles, like RuleFit, promise a good trade-off between predictive performance and model simplicity. However, they are affected by competing interests: While a sufficiently large number of binary, non-smooth rules is necessary to fit smooth, well generalizing decision boundaries, a too high number of rules in the ensemble severely jeopardizes interpretability. As a way out of this dilemma, we propose to take an extra step in the rule extraction step and compress clusters of similar rules into ensemble rules. The outputs of the individual rules in each cluster are pooled to produce a single soft output, reflecting the original ensemble's marginal smoothing behaviour. The final model, that we call Compressed Rule Ensemble (CRE), fits a linear combination of ensemble rules. We empirically show that CRE is both sparse and accurate on various datasets, carrying over the ensemble behaviour while remaining interpretable. Predictions can be explained by looking at the active ensemble rules, allowing external validation. We showcase that ensemble rules are also useful for a wider range of models that utilize decision rules extracted from tree ensembles.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852304024,0
C,"Ho, N; Feller, A; Greif, E; Miratrix, LW; Pillai, NS",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Nhat Ho; Feller, Avi; Greif, Evan; Miratrix, Luke W.; Pillai, Natesh S.",,,Weak Separation in Mixture Models and Implications for Principal Stratification,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Principal stratification is a popular framework for addressing post-randomization complications, often in conjunction with finite mixture models for estimating the causal effects of interest. Unfortunately, standard estimators of mixture parameters, like the MLE, are known to exhibit pathological behavior. We study this behavior in a simple but fundamental example, a two-component Gaussian mixture model in which only the component means and variances are unknown, and focus on the setting in which the components are weakly separated. In this case, we show that the asymptotic convergence rate of the MLE is quite poor, such as O(n(-1/6)) or even O(n(-1/8)). We then demonstrate via both theoretical arguments and extensive simulations that the MLE behaves like a threshold estimator in finite samples, in the sense that the MLE can give strong evidence that the means are equal when the truth is otherwise. We also explore the behavior of the MLE when the MLE is non-zero, showing that it is difficult to estimate both the sign and magnitude of the means in this case. We provide diagnostics for all of these pathologies and apply these ideas to re-analyzing two randomized evaluations of job training programs, JOBS II and Job Corps. Our results suggest that the corresponding maximum likelihood estimates should be interpreted with caution in these cases.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072705021,0
C,"Pan, H; Kondor, R",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Pan, Horace; Kondor, Risi",,,Permutation Equivariant Layers for Higher Order Interactions,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,Recent work on permutation equivariant neural networks has mostly focused on the first order case (sets) and second order case (graphs). We describe the machinery for generalizing permutation equivariance to arbitrary k-ary interactions between entities for any value of k. We demonstrate the effectiveness of higher order permutation equivariant models on several real world applications and find that our results compare favorably to existing permutation invariant/equivariant baselines.,,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,5987,6001,,,,,,,,,,,,,,,,WOS:000841852300017,0
C,"Sekhon, A; Wang, Z; Qi, YJ",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Sekhon, Arshdeep; Wang, Zhe; Qi, Yanjun",,,Beyond Data Samples: Aligning Differential Networks Estimation with Scientific Knowledge,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Learning the differential statistical dependency network between two contexts is essential for many real-life applications, mostly in the high dimensional low sample regime. In this paper, we propose a novel differential network estimator that allows integrating various sources of knowledge beyond data samples. The proposed estimator is scalable to a large number of variables and achieves a sharp asymptotic convergence rate. Empirical experiments on extensive simulated data and four real-world applications (one on neuroimaging and three from functional genomics) show that our approach achieves improved differential network estimation and provides better supports to downstream tasks like classification. Our results highlight significant benefits of integrating group, spatial and anatomic knowledge during differential genetic network identification and brain connectome change discovery.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852305017,0
C,"Zhou, SJ; Zhao, H; Zhang, SH; Wang, LZ; Chang, H; Wang, Z; Zhu, WW",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Zhou, Shiji; Zhao, Han; Zhang, Shanghang; Wang, Lianzhe; Chang, Heng; Wang, Zhi; Zhu, Wenwu",,,Online Continual Adaptation with Active Self-Training,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Models trained with offline data often suffer from continual distribution shifts and expensive labeling in changing environments. This calls for a new online learning paradigm where the learner can continually adapt to changing environments with limited labels. In this paper, we propose a new online setting - Online Active Continual Adaptation, where the learner aims to continually adapt to changing distributions using both unlabeled samples and active queries of limited labels. To this end, we propose Online Self-Adaptive Mirror Descent (OSAMD), which adopts an online teacher-student structure to enable online self-training from unlabeled data, and a margin-based criterion that decides whether to query the labels to track changing distributions. Theoretically, we show that, in the separable case, OSAMD has an O(T-2/3) dynamic regret bound under mild assumptions, which is aligned with the Omega(T-2/3) lower bound of online learning algorithms with full labels. In the general case, we show a regret bound of O(T-2/3 + alpha*T), where alpha* denotes the separability of domains and is usually small. Our theoretical results show that OSAMD can fast adapt to changing environments with active queries. Empirically, we demonstrate that OSAMD achieves favorable regrets under changing environments with limited labels on both simulated and real-world data, which corroborates our theoretical findings.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852303013,0
C,"Ariafar, S; Gilmer, J; Nado, Z; Snoek, J; Jenatton, R; Dahl, GE",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Ariafar, Setareh; Gilmer, Justin; Nado, Zachary; Snoek, Jasper; Jenatton, Rodolphe; Dahl, George E.",,,"Predicting the utility of search spaces for black-box optimization: a simple, budget-aware approach","INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Black box optimization requires specifying a search space to explore for solutions, e.g. a d-dimensional compact space, and this choice is critical for getting the best results at a reasonable budget. Unfortunately, determining a high-quality search space can be challenging in many applications. For example, when tuning hyperparameters for machine learning pipelines on a new problem given a limited budget, one must strike a balance between excluding potentially promising regions and keeping the search space small enough to be tractable. The goal of this work is to motivate-through example applications in tuning deep neural networks-the problem of predicting the quality of search spaces conditioned on budgets, as well as to provide a simple scoring method based on a utility function applied to a probabilistic response surface model, similar to Bayesian optimization. We show that the method we present can compute meaningful budget-conditional scores in a variety of situations. We also provide experimental evidence that accurate scores can be useful in constructing and pruning search spaces. Ultimately, we believe scoring search spaces should become standard practice in the experimental workflow for deep learning.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852305025,0
C,"Balazs, G",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Balazs, Gabor",,,Adaptively Partitioning Max-Affine Estimators for Convex Regression,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"This paper considers convex shape-restricted nonparametric regression over subgaussian domain and noise with the squared loss. It introduces a tractable convex piecewise-linear estimator which precomputes a partition of the training data by an adaptive version of farthest-point clustering, approximately fits hyperplanes over the partition cells by minimizing the regularized empirical risk, and projects the result into the max-affine class. The analysis provides an upper bound on the generalization error of this estimator matching the rate of Lipschitz nonparametric regression and proves its adaptivity to the intrinsic dimension of the data mitigating the effect of the curse of dimensionality. The experiments conclude with competitive performance, improved overfitting robustness, and significant computational savings compared to existing convex regression methods.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,860,874,,,,,,,,,,,,,,,,WOS:000828072700036,0
C,"Bian, J; Jun, KS",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Bian, Jie; Jun, Kwang-Sung",,,Maillard Sampling: Boltzmann Exploration Done Optimally,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"The PhD thesis of Maillard (2013) presents a rather obscure algorithm for the K-armed bandit problem. This less-known algorithm, which we call Maillard sampling (MS), computes the probability of choosing each arm in a closed form, which is not true for Thompson sampling, a widely-adopted bandit algorithm in the industry. This means that the bandit-logged data from running MS can be readily used for counterfactual evaluation, unlike Thompson sampling. Motivated by such merit, we revisit MS and perform an improved analysis to show that it achieves both the asymptotical optimality and root KT log T minimax regret bound where T is the time horizon, which matches the known bounds for asymptotically optimal UCB. We then propose a variant of MS called MS+ that improves its minimax bound to root KT logK. MS+ can also be tuned to be aggressive (i.e., less exploration) without losing the asymptotic optimality, a unique feature unavailable from existing bandit algorithms. Our numerical evaluation shows the effectiveness of MS+.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,54,72,,,,,,,,,,,,,,,,WOS:000828072700004,0
C,"Cao, XY; Liu, WY; Vempala, SS",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Cao, Xinyuan; Liu, Weiyang; Vempala, Santosh S.",,,Provable Lifelong Learning of Representations,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"In lifelong learning, tasks (or classes) to be learned arrive sequentially over time in arbitrary order. During training, knowledge from previous tasks can be captured and transferred to subsequent ones to improve sample efficiency. We consider the setting where all target tasks can be represented in the span of a small number of unknown linear or nonlinear features of the input data. We propose a lifelong learning algorithm that maintains and refines the internal feature representation. We prove that for any desired accuracy on all tasks, the dimension of the representation remains close to that of the underlying representation. The resulting sample complexity improves significantly on existing bounds. In the setting of linear features, our algorithm is provably efficient and the sample complexity for input dimension d, in tasks with k features up to error epsilon is (O) over tilde (dk(1.5)/epsilon + km/epsilon). We also prove a matching lower bound for any lifelong learning algorithm that uses a single task learner as a black box. We complement our analysis with an empirical study, including a heuristic lifelong learning algorithm for deep neural networks. Our method performs favorably on challenging realistic image datasets corn pared to state-of-the-art continual learning methods.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,6334,6356,,,,,,,,,,,,,,,,WOS:000841852300034,0
C,"Corinzia, L; Penna, P; Szpankowski, W; Buhmann, JM",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Corinzia, Luca; Penna, Paolo; Szpankowski, Wojciech; Buhmann, Joachim M.",,,Statistical and computational thresholds for the planted k-densest sub-hypergraph problem,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"In this work, we consider the problem of recovery a planted k-densest sub-hypergraph on a d-uniform hypergraph. This fundamental problem appears in different contexts, e.g., community detection, average-case complexity, and neuroscience applications as a structural variant of tensor PCA problem. We provide tight statistical upper and lower bounds for the exact recovery threshold by the maximum-likelihood estimator, as well as algorithmic bounds based on approximate message passing algorithms. The problem exhibits a typical statistical-computational gap observed in analogous sparse settings that widens with increasing sparsity of the problem. The bounds show that the signal structure impacts the location of the statistical and computational phase transition not captured by the known existing bounds for the tensor PCA model. This effect is due to the generic planted signal prior that this latter model addresses.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852306007,0
C,"Deng, YT; Zhou, XY; Kim, B; Tewari, A; Gupta, A; Shroff, N",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Deng, Yuntian; Zhou, Xingyu; Kim, Baekjin; Tewari, Ambuj; Gupta, Abhishek; Shroff, Ness",,,Weighted Gaussian Process Bandits for Non-stationary Environments,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"In this paper, we consider the Gaussian process (GP) bandit optimization problem in a non-stationary environment. To capture external changes, the black-box function is allowed to be time-varying within a reproducing kernel Hilbert space (RKHS). To this end, we develop WGP-UCB, a novel UCB-type algorithm based on weighted Gaussian process regression. A key challenge is how to cope with infinite-dimensional feature maps. To that end, we leverage kernel approximation techniques to prove a sublinear regret bound, which is the first (frequentist) sublinear regret guarantee on weighted time-varying bandits with general nonlinear rewards. This result generalizes both non-stationary linear bandits and standard GP-UCB algorithms. Further, a novel concentration inequality is achieved for weighted Gaussian process regression with general weights. We also provide universal upper bounds and weight-dependent upper bounds for weighted maximum information gains. These results are of independent interest for applications such as news ranking and adaptive pricing, where weights can be adopted to capture the importance or quality of data. Finally, we conduct experiments to highlight the favorable gains of the proposed algorithm in many cases when compared to existing methods.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852301016,0
C,"Furuya, T; Suetake, K; Taniguchi, K; Kusumoto, H; Saiin, R; Daimon, T",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Furuya, Takashi; Suetake, Kazuma; Taniguchi, Koichi; Kusumoto, Hiroyuki; Saiin, Ryuji; Daimon, Tomohiro",,,Spectral Pruning for Recurrent Neural Networks,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Recurrent neural networks (RNNs) are a class of neural networks used in sequential tasks. However, in general, RNNs have a large number of parameters and involve enormous computational costs by repeating the recurrent structures in many time steps. As a method to overcome this difficulty, RNN pruning has attracted increasing attention in recent years, and it brings us benefits in terms of the reduction of computational cost as the time step progresses. However, most existing methods of RNN pruning are heuristic. The purpose of this paper is to study the theoretical scheme for RNN pruning method. We propose an appropriate pruning algorithm for RNNs inspired by spectral pruning, and provide the generalization error bounds for compressed RNNs. We also provide numerical experiments to demonstrate our theoretical results and show the effectiveness of our pruning method compared with the existing methods.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072703023,0
C,"Gao, F; Zhang, Y; Zhang, J",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Gao, Fei; Zhang, Yan; Zhang, Jiang",,,Neural Enhanced Dynamic Message Passing,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Predicting stochastic spreading processes on complex networks is critical in epidemic control, opinion propagation, and viral marketing. We focus on the problem of inferring the time-dependent marginal probabilities of states for each node which collectively quantifies the spreading results. Dynamic Message Passing (DMP) has been developed as an efficient inference algorithm for several spreading models, and it is asymptotically exact on locally tree-like networks. However, DMP can struggle in diffusion networks with lots of local loops. We address this limitation by using Graph Neural Networks (GNN) to learn the dependency amongst messages implicitly. Specifically, we propose a hybrid model in which the GNN module runs jointly with DMP equations. The GNN module refines the aggregated messages in DMP iterations by learning from simulation data. We demonstrate numerically that after training, our model's inference accuracy substantially outperforms DMP in conditions of various network structure and dynamics parameters. Moreover, compared to pure data-driven models, the proposed hybrid model has a better generalization ability for out-of-training cases, profiting from the explicitly utilized dynamics priors in the hybrid model. A PyTorch implementation of our model is at https://github.com/FeiGSSS/NEDMP.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852304041,0
C,"Garcelon, E; Avadhanula, V; Lazaric, A; Pirotta, M",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Garcelon, Evrard; Avadhanula, Vashist; Lazaric, Alessandro; Pirotta, Matteo",,,Top K Ranking for Multi-Armed Bandit with Noisy Evaluations,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We consider a multi-armed bandit setting where, at the beginning of each round, the learner receives noisy independent, and possibly biased, evaluations of the true reward of each arm and it selects K arms with the objective of accumulating as much reward as possible over T rounds. Under the assumption that at each round the true reward of each arm is drawn from a fixed distribution, we derive different algorithmic approaches and theoretical guarantees depending on how the evaluations are generated. First, we show a (O) over tilde (T-2/3) regret in the general case when the observation functions are a genearalized linear function of the true rewards. On the other hand, we show that an improved (O) over tilde( root T) regret can be derived when the observation functions are noisy linear functions of the true rewards. Finally, we report an empirical validation that confirms our theoretical findings, provides a thorough comparison to alternative approaches, and further supports the interest of this setting in practice.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,6242,6269,,,,,,,,,,,,,,,,WOS:000841852300030,0
C,"Ke, CY; Honorio, J",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Ke, Chuyang; Honorio, Jean",,,Federated Myopic Community Detection with One-shot Communication,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"In this paper, we study the problem of recovering the community structure of a network under federated myopic learning. Under this paradigm, we have several clients, each of them having a myopic view, i.e., observing a small subgraph of the network. Each client sends a censored evidence graph to a central server. We provide an efficient algorithm, which computes a consensus signed weighted graph from clients evidence, and recovers the underlying network structure in the central server. We analyze the topological structure conditions of the network, as well as the signal and noise levels of the clients that allow for recovery of the network structure. Our analysis shows that exact recovery is possible and can be achieved in polynomial time. In addition, our experiments show that in an extremely sparse network with 10 000 nodes, our method can achieve exact recovery of the community structure even if every client has access to only 20 nodes. We also provide information-theoretic limits for the central server to recover the network structure from any single client evidence. Finally, as a byproduct of our analysis, we provide a novel Cheeger-type inequality for general signed weighted graphs.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072705003,0
C,"Ren, TZ; Cui, FH; Atsidakou, A; Sanghavi, S; Ho, N",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Ren, Tongzheng; Cui, Fuheng; Atsidakou, Alexia; Sanghavi, Sujay; Ho, Nhat",,,Towards Statistical and Computational Complexities of Polyak Step Size Gradient Descent,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We study the statistical and computational complexities of the Polyak step size gradient descent algorithm under generalized smoothness and Lojasiewicz conditions of the population loss function, namely, the limit of the empirical loss function when the sample size goes to infinity, and the stability between the gradients of the empirical and population loss functions, namely, the polynomial growth on the concentration bound between the gradients of sample and population loss functions. We demonstrate that the Polyak step size gradient descent iterates reach a final statistical radius of convergence around the true parameter after logarithmic number of iterations in terms of the sample size. It is computationally cheaper than the polynomial number of iterations on the sample size of the fixed-step size gradient descent algorithm to reach the same final statistical radius when the population loss function is not locally strongly convex. Finally, we illustrate our general theory under three statistical examples: generalized linear model, mixture model, and mixed linear regression model.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072704002,0
C,"Sebenius, I; Paananen, T; Vehtari, A",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Sebenius, Isaac; Paananen, Topi; Vehtari, Aki",,,Feature Collapsing for Gaussian process variable ranking,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"At present, there is no consensus on the most effective way to establish feature relevance for Gaussian process models. The most common heuristic, Automatic Relevance Determination, has several downsides; many alternate methods incur unacceptable computational costs. Existing methods based on sensitivity analysis of the posterior predictive distribution are promising, but are biased and show room for improvement. This paper proposes Feature Collapsing as a novel method for performing GP feature relevance determination in an effective, consistent, unbiased, and computationally-inexpensive manner compared to existing algorithms.",,,,,,"Vehtari, Aki/0000-0003-2164-9469",,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852305039,0
C,"Shen, ZB; Hassani, H; Kale, S; Karbasi, A",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Shen, Zebang; Hassani, Hamed; Kale, Satyen; Karbasi, Amin",,,Federated Functional Gradient Boosting,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Motivated by the tremendous success of boosting methods in the standard centralized model of learning, we initiate the theory of boosting in the Federated Learning setting. The primary challenges in the Federated Learning setting are heterogeneity in client data and the requirement that no client data can be transmitted to the server. We develop federated functional gradient boosting (FFGB) an algorithm that is designed to handle these challenges. Under appropriate assumptions on the weak learning oracle, the FFGB algorithm is proved to efficiently converge to certain neighborhoods of the global optimum. The radii of these neighborhoods depend upon the level of heterogeneity measured via the total variation distance and the much tighter Wasserstein-1 distance, and diminish to zero as the setting becomes more homogeneous. In practice, as suggested by our theoretical findings, we propose using FFGB to warm-start existing Federated Learning solvers and observe significant performance boost in highly heterogeneous settings. The code can be found here.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852302011,0
C,"Shi, C; Sridhar, D; Misra, V; Blei, DM",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Shi, Claudia; Sridhar, Dhanya; Misra, Vishal; Blei, David M.",,,On the Assumptions of Synthetic Control Methods,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Synthetic control (SC) methods have been widely applied to estimate the causal effect of large-scale interventions, e.g. the state-wide effect of a change in policy. The idea of synthetic controls is to approximate one unit's counterfactual outcomes using a weighted combination of some other units' observed outcomes. The motivating question of this paper is: how does the SC strategy lead to valid causal inferences? We address this question by re-formulating the causal inference problem targeted by SC with a more fine-grained model, where we change the unit of the analysis from large units (e.g. states) to small units (e.g. individuals in states). Under this re-formulation, we derive sufficient conditions for the non-parametric causal identification of the causal effect. We highlight two implications of the reformulation: (1) it clarifies where linearity comes from, and how it falls naturally out of the more fine-grained and flexible model, and (2) it suggests new ways of using available data with SC methods for valid causal inference, in particular, new ways of selecting observations from which to estimate the counterfactual.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852301027,0
C,"Silva, A; Chopra, R; Gombolay, M",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Silva, Andrew; Chopra, Rohit; Gombolay, Matthew",,,Cross-Loss Influence Functions to Explain Deep Network Representations,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"As machine learning is increasingly deployed in the real world, it is paramount that we develop the tools necessary to analyze the decision-making of the models we train and deploy to end-users. Recently, researchers have shown that influence functions, a statistical measure of sample impact, can approximate the effects of training samples on classification accuracy for deep neural networks. However, this prior work only applies to supervised learning, where training and testing share an objective function. No approaches currently exist for estimating the influence of unsupervised training examples for deep learning models. To bring explainability to unsupervised and semi-supervised training regimes, we derive the first theoretical and empirical demonstration that influence functions can be extended to handle mismatched training and testing (i.e., cross-loss) settings. Our formulation enables us to compute the influence in an unsupervised learning setup, explain cluster memberships, and identify and augment biases in language models. Our experiments show that our cross-loss influence estimates even exceed matched-objective influence estimation relative to ground-truth sample impact.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,1,17,,,,,,,,,,,,,,,,WOS:000828072700001,0
C,"Xu, MME; van der Maaten, L; Hannun, A",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Xu, Mimee; van der Maaten, Laurens; Hannun, Awni",,,Data Appraisal Without Data Sharing,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"One of the most effective approaches to improving the performance of a machine learning model is to procure additional training data. A model owner seeking relevant training data from a data owner needs to appraise the data before acquiring it. However, without a formal agreement, the data owner does not want to share data. The resulting Catch-22 prevents efficient data markets from forming. This paper proposes adding a data appraisal stage that requires no data sharing between data owners and model owners. Specifically, we use multi-party computation to implement an appraisal function computed on private data. The appraised value serves as a guide to facilitate data selection and transaction. We propose an efficient data appraisal method based on forward influence functions that approximates data value through its first-order loss reduction on the current model. The method requires no additional hyper-parameters or re-training. We show that in private, forward influence functions provide an appealing trade-off between high quality appraisal and required computation, in spite of label noise, class imbalance, and missing data. Our work seeks to inspire an open market that incentivizes efficient, equitable exchange of domain-specific training data.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852305042,0
C,"Zhang, ZX; Mroueh, Y; Goldfeld, Z; Sriperumbudur, BK",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Zhang, Zhengxin; Mroueh, Youssef; Goldfeld, Ziv; Sriperumbudur, Bharath K.",,,Cycle Consistent Probability Divergences Across Different Spaces,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Discrepancy measures between probability distributions are at the core of statistical inference and machine learning. In many applications, distributions of interest are supported on different spaces, and yet a meaningful correspondence between data points is desired. Motivated to explicitly encode consistent bidirectional maps into the discrepancy measure, this work proposes a novel unbalanced Monge optimal transport formulation for matching, up to isometrics, distributions on different spaces. Our formulation arises as a principled relaxation of the Gromov-Haussdroff distance between metric spaces, and employs two cycle-consistent maps that push forward each distribution onto the other. We study structural properties of the proposed discrepancy and, in particular, show that it captures the popular cycle-consistent generative adversarial network (GAN) framework as a special case, thereby providing the theory to explain it. Motivated by computational efficiency, we then kernelize the discrepancy and restrict the mappings to parametric function classes. The resulting kernelized version is coined the generalized maximum mean discrepancy (GMMD). Convergence rates for empirical estimation of GMMD are studied and experiments to support our theory are provided.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852301032,0
C,"Adibi, A; Mokhtari, A; Hassani, H",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Adibi, Arman; Mokhtari, Aryan; Hassani, Hamed",,,Minimax Optimization: The Case of Convex-Submodular,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Minimax optimization has been central in addressing various applications in machine learning, game theory, and control theory. Prior literature has thus far mainly focused on studying such problems in the continuous domain, e.g., convex-concave minimax optimization is now understood to a significant extent. Nevertheless, minimax problems extend far beyond the continuous domain to mixed continuous-discrete domains or even fully discrete domains. In this paper, we study mixed continuous-discrete minimax problems where the minimization is over a continuous variable belonging to Euclidean space and the maximization is over subsets of a given ground set. We introduce the class of convex-submodular minimax problems, where the objective is convex with respect to the continuous variable and submodular with respect to the discrete variable. Even though such problems appear frequently in machine learning applications, little is known about how to address them from algorithmic and theoretical perspectives. For such problems, we first show that obtaining saddle points are hard up to any approximation, and thus introduce new notions of (near-) optimality. We then provide several algorithmic procedures for solving convex and monotone-submodular minimax problems and characterize their convergence rates, computational complexity, and quality of the final solution according to our notions of optimally. Our proposed algorithms are iterative and combine tools from both discrete and continuous optimization. Finally, we provide numerical experiments to showcase the effectiveness of our purposed methods.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072703028,0
C,"Azam, SS; Kim, T; Hosseinalipour, S; Joe-Wong, C; Bagchi, S; Brinton, C",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Azam, Sheikh Shams; Kim, Taejin; Hosseinalipour, Seyyedali; Joe-Wong, Carlee; Bagchi, Saurabh; Brinton, Christopher",,,Can we Generalize and Distribute Private Representation Learning?,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We study the problem of learning representations that are private yet informative, i.e., provide information about intended ally targets while hiding sensitive adversary attributes. We propose Exclusion-Inclusion Generative Adversarial Network (EIGAN), a generalized private representation learning (PRL) architecture that accounts for multiple ally and adversary attributes unlike existing PRL solutions. While centrally-aggregated dataset is a prerequisite for most PRL techniques, data in real-world is often siloed across multiple distributed nodes unwilling to share the raw data because of privacy concerns. We address this practical constraint by developing D-EIGAN, the first distributed PRL method that learns representations at each node without transmitting the source data. We theoretically analyze the behavior of adversaries under the optimal EIGAN and D-EIGAN encoders and the impact of dependencies among ally and adversary tasks on the optimization objective. Our experiments on various datasets demonstrate the advantages of EIGAN in terms of performance, robustness, and scalability. In particular, EIGAN outperforms the previous state-of-the-art by a significant accuracy margin (47% improvement), and D-EIGAN's performance is consistently on par with EIGAN under different network settings.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852305038,0
C,"Baharav, TZ; Cheng, G; Pilanci, M; Tse, D",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Baharav, Tavor Z.; Cheng, Gary; Pilanci, Mert; Tse, David",,,Approximate Function Evaluation via Multi-Armed Bandits,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We study the problem of estimating the value of a known smooth function f at an unknown point mu is an element of R-n, where each component mu(i) can be sampled via a noisy oracle. Sampling more frequently components of mu corresponding to directions of the function with larger directional derivatives is more sample-efficient. However, as mu is unknown, the optimal sampling frequencies are also unknown. We design an instance-adaptive algorithm that learns to sample according to the importance of each coordinate, and with probability at least 1 - delta returns an epsilon accurate estimate of f(mu). We generalize our algorithm to adapt to heteroskedastic noise, and prove asymptotic optimality when f is linear. We corroborate our theoretical results with numerical experiments, showing the dramatic gains afforded by adaptivity.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,108,135,,,,,,,,,,,,,,,,WOS:000828072700007,0
C,"Barrier, A; Garivier, A; Kocak, T",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Barrier, Antoine; Garivier, Aurelien; Kocak, Tomas",,,A Non-asymptotic Approach to Best-Arm Identification for Gaussian Bandits,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We propose a new strategy for best-arm identification with fixed confidence of Gaussian variables with bounded means and unit variance. This strategy, called EXPLORATION-BIASED SAMPLING, is not only asymptotically optimal: it is to the best of our knowledge the first strategy with non-asymptotic bounds that asymptotically matches the sample complexity. But the main advantage over other algorithms like TRACK-AND-STOP is an improved behavior regarding exploration: EXPLORATION-BIASED SAMPLING is biased towards exploration in a subtle but natural way that makes it more stable and interpretable. These improvements are allowed by a new analysis of the sample complexity optimization problem, which yields a faster numerical resolution scheme and several quantitative regularity results that we believe of high independent interest.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852304028,0
C,"Ding, N",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Ding, Ni",,,Kantorovich Mechanism for Pufferfish Privacy,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Pufferfish privacy achieves epsilon-indistinguishability over a set of secret pairs in the disclosed data. This paper studies how to attain epsilon-pufferfish privacy by exponential mechanism, an additive noise scheme that generalizes the Laplace noise. It is shown that the disclosed data is epsilon-pufferfish private if the noise is calibrated to the sensitivity of the Kantorovich optimal transport plan. Such a plan can be obtained directly from the data statistics conditioned on the secret, the prior knowledge of the system. The sufficient condition is further relaxed to reduce the noise power. It is also proved that the Gaussian mechanism based on the Kantorovich approach attains the delta-approximation of epsilon-pufferfish privacy.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072705009,0
C,"Erdogdu, MA; Hosseinzadeh, R; Zhang, MS",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Erdogdu, Murat A.; Hosseinzadeh, Rasa; Zhang, Matthew S.",,,Convergence of Langevin Monte Carlo in Chi-Squared and Renyi Divergence,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We study sampling from a target distribution nu(*) = e(-f) using the unadjusted Langevin Monte Carlo (LMC) algorithm when the potential f satisfies a strong dissipativity condition and it is first-order smooth with a Lipschitz gradient. We prove that, initialized with a Gaussian random vector that has sufficiently small variance, iterating the LMC algorithm for (O) over tilde(lambda(2)d epsilon(-1)) steps is sufficient to reach 6-neighborhood of the target in both Chi-squared and Renyi divergence, where lambda is the logarithmic Sobolev constant of nu(*). Our results do not require warm-start to deal with the exponential dimension dependency in Chi-squared divergence at initialization. In particular, for strongly convex and first-order smooth potentials, we show that the LMC algorithm achieves the rate estimate (O) over tilde (d epsilon(-1)) which improves the previously known rates in both of these divergences, under the same assumptions. Translating this rate to other metrics, our results also recover the state-of-the-art rates in KL divergence, total variation and 2-Wasserstein distance in the same setup. Finally, as we rely on the log-Sobolev inequality, our framework covers a range of non-convex potentials that are first-order smooth and exhibit strong convexity outside of a compact region.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852302025,0
C,"Ghoshal, S; Saha, A",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Ghoshal, Suprovat; Saha, Aadirupa",,,Exploiting Correlation to Achieve Faster Learning Rates in Low-Rank Preference Bandits,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We introduce the Correlated Preference Bandits problem with random utility based choice models (RUMs), where the goal is to identify the best item from a given pool of n items through online subsetwise preference feedback. We investigate whether models with a simple correlation structure, e.g. low rank, can result in faster learning rates. While we show that the problem can be impossible to solve for the general 'low rank' choice models, faster learning rates can be attained assuming more structured item correlations. In particular, we introduce a new class of Block-Rank based RUM model, where the best item is shown to be (epsilon, delta)-PAC learnable with only O(r epsilon(-2) log(n/delta)) samples. This improves on the standard sample complexity bound of (O) over tilde (n epsilon(-2) log(1/delta)) known for the usual learning algorithms which might not exploit the item-correlations (r << n). We complement the above sample complexity with a matching lower bound (up to logarithmic factors), justifying the tightness of our analysis. Further, we extend the results to a more general 'noisy Block-Rank' model, which ensures robustness of our techniques. Overall, our results justify the advantage of playing subsetwise queries over pairwise preferences (k = 2), we show the latter provably fails to exploit correlation.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,456,482,,,,,,,,,,,,,,,,WOS:000828072700019,0
C,"Goujaud, B; Scieur, D; Dieuleveut, A; Taylor, A; Pedregosa, F",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Goujaud, Baptiste; Scieur, Damien; Dieuleveut, Aymeric; Taylor, Adrien; Pedregosa, Fabian",,,Super-Acceleration with Cyclical Step-sizes,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We develop a convergence-rate analysis of momentum with cyclical step-sizes. We show that under some assumption on the spectral gap of Hessians in machine learning, cyclical step-sizes are provably faster than constant step-sizes. More precisely, we develop a convergence rate analysis for quadratic objectives that provides optimal parameters and shows that cyclical learning rates can improve upon traditional lower complexity bounds. We further propose a systematic approach to design optimal first order methods for quadratic minimization with a given spectral structure. Finally, we provide a local convergence rate analysis beyond quadratic minimization for the proposed methods and illustrate our findings through benchmarks on least squares and logistic regression problems.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072703006,0
C,"Isik, B; Weissman, T; No, A",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Isik, Berivan; Weissman, Tsachy; No, Albert",,,An Information-Theoretic Justification for Model Pruning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We study the neural network (NN) compression problem, viewing the tension between the compression ratio and NN performance through the lens of rate-distortion theory. We choose a distortion metric that reflects the effect of NN compression on the model output and derive the tradeoff between rate (compression) and distortion. In addition to characterizing theoretical limits of NN compression, this formulation shows that pruning, implicitly or explicitly, must be a part of a good compression algorithm. This observation bridges a gap between parts of the literature pertaining to NN and data compression, respectively, providing insight into the empirical success of model pruning. Finally, we propose a novel pruning strategy derived from our information-theoretic formulation and show that it outperforms the relevant baselines on CIFAR-10 and ImageNet datasets.",,,,,,"Isik, Berivan/0000-0002-4926-5443",,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072703040,0
C,"Kramer, N; Schmidt, J; Hennig, P",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Kraemer, Nicholas; Schmidt, Jonathan; Hennig, Philipp",,,Probabilistic Numerical Method of Lines for Time-Dependent Partial Differential Equations,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"This work develops a class of probabilistic algorithms for the numerical solution of nonlinear, time-dependent partial differential equations (PDEs). Current state-of-the-art PDE solvers treat the space- and time-dimensions separately, serially, and with black-box algorithms, which obscures the interactions between spatial and temporal approximation errors and misguides the quantification of the overall error. To fix this issue, we introduce a probabilistic version of a technique called method of lines. The proposed algorithm begins with a Gaussian process interpretation of finite difference methods, which then interacts naturally with filtering-based probabilistic ordinary differential equation (ODE) solvers because they share a common language: Bayesian inference. Joint quantification of space- and time-uncertainty becomes possible without losing the performance benefits of well-tuned ODE solvers. Thereby, we extend the toolbox of probabilistic programs for differential equation simulation to PDEs.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,625,639,,,,,,,,,,,,,,,,WOS:000828072700026,0
C,"Ortega, LA; Cabanas, R; Masegosa, AR",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Ortega, Luis A.; Cabanas, Rafael; Masegosa, Andres R.",,,Diversity and Generalization in Neural Network Ensembles,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Ensembles are widely used in machine learning and, usually, provide state-of-the-art performance in many prediction tasks. From the very beginning, the diversity of an ensemble has been identified as a key factor for the superior performance of these models. But the exact role that diversity plays in ensemble models is poorly understood, specially in the context of neural networks. In this work, we combine and expand previously published results in a theoretically sound framework that describes the relationship between diversity and ensemble performance for a wide range of ensemble methods. More precisely, we provide sound answers to the following questions: how to measure diversity, how diversity relates to the generalization error of an ensemble, and how diversity is promoted by neural network ensemble algorithms. This analysis covers three widely used loss functions, namely, the squared loss, the cross-entropy loss, and the 0-1 loss; and two widely used model combination strategies, namely, model averaging and weighted majority vote. We empirically validate this theoretical analysis with neural network ensembles.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852306012,0
C,"Padakandla, A; Magner, A",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Padakandla, Arun; Magner, Abram",,,PAC Learning of Quantum Measurement Classes: Sample Complexity Bounds and Universal Consistency,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We formulate a quantum analogue of the fundamental classical PAC learning problem. As on a quantum computer, we model data to be encoded by modifying specific attributes - spin axis of an electron, plane of polarization of a photon - of sub-atomic particles. Any interaction, including reading off, extracting or learning from such data is via quantum measurements, thus leading us to a problem of PAC learning Quantum Measurement Classes. We propose and analyze the sample complexity of a new ERM algorithm that respects quantum non-commutativity. Our study entails that we define the VC dimension of Positive Operator Valued Measure(ments) (POVMs) concept classes. Our sample complexity bounds involve optimizing over partitions of jointly measurable classes. Finally, we identify universally consistent sequences of POVM classes. Technical components of this work include computations involving tensor products, trace and uniform convergence bounds.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852305037,0
C,"Pokle, A; Tian, JJ; Li, YC; Risteski, A",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Pokle, Ashwini; Tian, Jinjin; Li, Yuchen; Risteski, Andrej",,,Contrasting the landscape of contrastive and non-contrastive learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"A lot of recent advances in unsupervised feature learning are based on designing features which are invariant under semantic data augmentations. A common way to do this is contrastive learning, which uses positive and negative samples. Some recent works however have shown promising results for non-contrastive learning, which does not require negative samples. However, the noncontrastive losses have obvious collapsed minima, in which the encoders output a constant feature embedding, independent of the input. A folk conjecture is that so long as these collapsed solutions are avoided, the produced feature representations should be good. In our paper, we cast doubt on this story: we show through theoretical results and controlled experiments that even on simple data models, non-contrastive losses have a preponderance of non-collapsed bad minima. Moreover, we show that the training process does not avoid these minima. Code for this work can be found at https//github com/ashwinipokle/contrastive.landscape.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852303003,0
C,"Price, I; Rasp, S",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Price, Ilan; Rasp, Stephan",,,Increasing the accuracy and resolution of precipitation forecasts using deep generative models,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Accurately forecasting extreme rainfall is notoriously difficult, but is also ever more crucial for society as climate change increases the frequency of such extremes. Global numerical weather prediction models often fail to capture extremes, and are produced at too low a resolution to be actionable, while regional, high-resolution models are hugely expensive both in computation and labour. In this paper we explore the use of deep generative models to simultaneously correct and downscale (super-resolve) global ensemble forecasts over the Continental US. Specifically, using fine-grained radar observations as our ground truth, we train a conditional Generative Adversarial Network-coined CorrectorGAN-via a custom training procedure and augmented loss function, to produce ensembles of high-resolution, bias-corrected forecasts based on coarse, global precipitation forecasts in addition to other relevant meteorological fields. Our model outperforms an interpolation baseline, as well as super-resolution-only and CNN-based univariate methods, and approaches the performance of an operational regional high-resolution model across an array of established probabilistic metrics. Crucially, CorrectorGAN, once trained, produces predictions in seconds on a single machine. These results raise exciting questions about the necessity of regional models, and whether data-driven downscaling and correction methods can be transferred to data-poor regions that so far have had no access to high-resolution forecasts.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852305004,0
C,"Seddik, ME; Wu, CM; Lutzeyer, JF; Vazirgiannis, M",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Seddik, Mohamed El Amine; Wu, Changmin; Lutzeyer, Johannes F.; Vazirgiannis, Michalis",,,Node Feature Kernels Increase Graph Convolutional Network Robustness,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"The robustness of the much used Graph Convolutional Networks (GCNs) to perturbations of their input is becoming a topic of increasing importance. In this paper the random GCN is introduced for which a random matrix theory analysis is possible. This analysis suggests that if the graph is sufficiently perturbed, or in the extreme case random, then the GCN fails to benefit from the node features. It is furthermore observed that enhancing the message passing step in GCNs by adding the node feature kernel to the adjacency matrix of the graph structure solves this problem. An empirical study of a GCN utilised for node classification on six real datasets further confirms the theoretical findings and demonstrates that perturbations of the graph structure can result in GCNs performing significantly worse than Multi-Layer Perceptrons run on the node features alone. In practice, adding a node feature kernel to the message passing of perturbed graphs results in a significant improvement of the GCN's performance, thereby rendering it more robust to graph perturbations. Our code is publicly available at: https: //github.com/ChangminWu/RobustGCN.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,6225,6241,,,,,,,,,,,,,,,,WOS:000841852300029,0
C,"Shah, K; Deshpande, A; Goyal, N",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Shah, Kulin; Deshpande, Amit; Goyal, Navin",,,Learning and Generalization in Overparameterized Normalizing Flows,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"In supervised learning, it is known that overparameterized neural networks with one hidden layer provably and efficiently learn and generalize, when trained using stochastic gradient descent with a sufficiently small learning rate and suitable initialization. In contrast, the benefit of overparameterization in unsupervised learning is not well understood. Normalizing flows (NFs) constitute an important class of models in unsupervised learning for sampling and density estimation. In this paper, we theoretically and empirically analyze these models when the underlying neural network is a one-hidden-layer overparametrized network. Our main contributions are two-fold: (1) On the one hand, we provide theoretical and empirical evidence that for constrained NFs (this class of NFs underlies most NF constructions) with the one-hidden-layer network, overparametrization hurts training. (2) On the other hand, we prove that unconstrained NFs, a recently introduced model, can efficiently learn any reasonable data distribution under minimal assumptions when the underlying network is overparametrized and has one hidden-layer.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852303034,0
C,"Shin, S; Lee, S; Ok, J",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Shin, Suho; Lee, Seungjoon; Ok, Jungseul",,,Multi-armed Bandit Algorithm against Strategic Replication,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We consider a multi-armed bandit problem in which a set of arms is registered by each agent, and the agent receives reward when its arm is selected. An agent might strategically submit more arms with replications, which can bring more reward by abusing the bandit algorithm's exploration-exploitation balance. Our analysis reveals that a standard algorithm indeed fails at preventing replication and suffers from linear regret in time T. We aim to design a bandit algorithm which demotivates replications and also achieves a small cumulative regret. We devise Hierarchical UCB (H-UCB) of replication-proof, which has O(ln T)-regret under any equilibrium. We further propose Robust Hierarchical UCB (RH-UCB) which has a sublinear regret even in a realistic scenario with irrational agents replicating careless. We verify our theoretical findings through numerical experiments.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,403,431,,,,,,,,,,,,,,,,WOS:000828072700017,0
C,"Teng, JY; Huang, WR; He, HW",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Teng, Jiaye; Huang, Weiran; He, Haowei",,,Can Pretext-Based Self-Supervised Learning Be Boosted by Downstream Data? A Theoretical Analysis,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Pretext-based self-supervised learning learns the semantic representation via a handcrafted pretext task over unlabeled data and then uses the learned representation for downstream tasks, which effectively reduces the sample complexity of downstream tasks under Conditional Independence (CI) condition (Lee et al., 2020). However, the downstream sample complexity gets much worse if the CI condition does not hold. One interesting question is whether we can make the CI condition hold by using downstream data to refine the unlabeled data to boost self-supervised learning. At first glance, one might think that seeing downstream data in advance would always boost the downstream performance. However, we show that it is not intuitively true and point out that in some cases, it hurts the final performance instead. In particular, we prove both model-free and model-dependent lower bounds of the number of downstream samples used for data refinement. Moreover, we conduct various experiments on both synthetic and real-world datasets to verify our theoretical results.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072704012,0
C,"Yoon, JH; Jeong, DP; Kim, S",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Yoon, Jun Ho; Jeong, Daniel P.; Kim, Seyoung",,,Doubly Mixed-Effects Gaussian Process Regression,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We address the multi-task Gaussian process (GP) regression problem with the goal of decomposing input effects on outputs into components shared across or specific to tasks and samples. We propose a family of mixed-effects GPs, including doubly and translated mixed-effects GPs, that performs such a decomposition, while also modeling the complex task relationships. Instead of the tensor product widely used in multi-task GPs, we use the direct sum and Kronecker sum for Cartesian product to combine task and sample covariance functions. With this kernel, the overall input effects on outputs decompose into four components: fixed effects shared across tasks and across samples and random effects specific to each task and to each sample. We describe an efficient stochastic variational inference method for our proposed models that also significantly reduces the cost of inference for the existing mixed-effects GPs. On simulated and real-world data, we demonstrate that our approach provides higher test accuracy and interpretable decomposition.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852301015,0
C,"Amoukou, SI; Salaun, T; Brunel, NJB",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Amoukou, Salim, I; Salaun, Tangi; Brunel, Nicolas J. B.",,,Accurate Shapley Values for explaining tree-based models,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Although Shapley Values (SV) are widely used in explainable AI, they can be poorly understood and estimated, implying that their analysis may lead to spurious inferences and explanations. As a starting point, we remind an invariance principle for SV and derive the correct approach for computing the SV of categorical variables that are particularly sensitive to the encoding used. In the case of tree-based models, we introduce two estimators of Shapley Values that exploit the tree structure efficiently and are more accurate than state-of-the-art methods. Simulations and comparisons are performed with state-of-the-art algorithms and show the practical gain of our approach. Finally, we discuss the ability of SV to provide reliable local explanations. We also provide a Python package that compute our estimators at https://github.com/salimamoukou/acv00.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072702023,0
C,"Brofos, JA; Gabrie, M; Brubaker, MA; Lederman, RR",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Brofos, James A.; Gabrie, Marylou; Brubaker, Marcus A.; Lederman, Roy R.",,,Adaptation of the Independent Metropolis-Hastings Sampler with Normalizing Flow Proposals,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Markov Chain Monte Carlo (MCMC) methods are a powerful tool for computation with complex probability distributions. However the performance of such methods is critically dependent on properly tuned parameters, most of which are difficult if not impossible to know a priori for a given target distribution. Adaptive MCMC methods aim to address this by allowing the parameters to be updated during sampling based on previous samples from the chain at the expense of requiring a new theoretical analysis to ensure convergence. In this work we extend the convergence theory of adaptive MCMC methods to a new class of methods built on a powerful class of parametric density estimators known as normalizing flows. In particular, we consider an independent Metropolis-Hastings sampler where the proposal distribution is represented by a normalizing flow whose parameters are updated using stochastic gradient descent. We explore the practical performance of this procedure on both synthetic settings and in the analysis of a physical field system, and compare it against both adaptive and non-adaptive MCMC methods.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,5949,5986,,,,,,,,,,,,,,,,WOS:000841852300016,0
C,"Delalande, A",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Delalande, Alex",,,Nearly Tight Convergence Bounds for Semi-discrete Entropic Optimal Transport,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We derive nearly tight and non-asymptotic convergence bounds for solutions of entropic semi-discrete optimal transport. These bounds quantify the stability of the dual solutions of the regularized problem (sometimes called Sinkhorn potentials) w.r.t. the regularization parameter, for which we ensure a better than Lipschitz dependence. Such facts may be a first step towards a mathematical justification of epsilon-scaling heuristics for the numerical resolution of regularized semi-discrete optimal transport. Our results also entail a non-asymptotic and tight expansion of the difference between the entropic and the unregularized costs.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072701031,0
C,"Kan, K; Park, Y; Aubet, FX; Benidis, K; Gasthaus, J; Januschowski, T; Ruthotto, L",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Kan, Kelvin; Park, Youngsuk; Aubet, Francois-Xavier; Benidis, Konstantinos; Gasthaus, Jan; Januschowski, Tim; Ruthotto, Lars",,,Multivariate Quantile Function Forecaster,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We propose Multivariate Quantile Function Forecaster (MQF(2)), a global probabilistic forecasting method constructed using a multivariate quantile function and investigate its application to multi-horizon forecasting. Prior approaches are either autoregressive, implicitly capturing the dependency structure across time but exhibiting error accumulation with increasing forecast horizons, or multi-horizon sequence-to-sequence models, which do not exhibit error accumulation, but also do typically not model the dependency structure across time steps. MQF(2) combines the benefits of both approaches, by directly making predictions in the form of a multivariate quantile function, defined as the gradient of a convex function which we parametrize using input-convex neural networks. By design, the quantile function is monotone with respect to the input quantile levels and hence avoids quantile crossing. We provide two options to train MQF(2) : with energy score or with maximum likelihood. Experimental results on real-world and synthetic datasets show that our model has comparable performance with state-of-the-art methods in terms of single time step metrics while capturing the time dependency structure.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852305006,0
C,"Laforgue, P; Clerici, G; Cesa-Bianchi, N; Gilad-Bachrach, R",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Laforgue, Pierre; Clerici, Giulia; Cesa-Bianchi, Nicole; Gilad-Bachrach, Ran",,,A Last Switch Dependent Analysis of Satiation and Seasonality in Bandits,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Motivated by the fact that humans like some level of unpredictability or novelty, and might therefore get quickly bored when interacting with a stationary policy, we introduce a novel non-stationary bandit problem, where the expected reward of an arm is fully determined by the time elapsed since the arm last took part in a switch of actions. Our model generalizes previous notions of delay-dependent rewards, and also relaxes most assumptions on the reward function. This enables the modeling of phenomena such as progressive satiation and periodic behaviours. Building upon the Combinatorial Semi-Bandits (CSB) framework, we design an algorithm and prove a bound on its regret with respect to the optimal non-stationary policy (which is NP-hard to compute). Similarly to previous works, our regret analysis is based on defining and solving an appropriate trade-off between approximation and estimation. Preliminary experiments confirm the superiority of our algorithm over both the oracle greedy approach and a vanilla CSB solver.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072701002,0
C,"Lee, HS",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Lee, Hyun-Suk",,,System-Agnostic Meta-Learning for MDP-based Dynamic Scheduling via Descriptive Policy,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Dynamic scheduling is an important problem in applications from queuing to wireless networks. It addresses how to choose an item among multiple scheduling items in each timestep to achieve a long-term goal. Conventional approaches for dynamic scheduling find the optimal policy for a given specific system so that the policy from these approaches is usable only for the corresponding system characteristics. Hence, it is hard to use such approaches for a practical system in which system characteristics dynamically change. This paper proposes a novel policy structure for MDP-based dynamic scheduling, a descriptive policy, which has a system-agnostic capability to adapt to unseen system characteristics for an identical task (dynamic scheduling). To this end, the descriptive policy learns a system-agnostic scheduling principle-in a nutshell, which condition of items should have a higher priority in scheduling. The scheduling principle can be applied to any system so that the descriptive policy learned in one system can be used for another system. Experiments with simple explanatory and realistic application scenarios demonstrate that it enables system-agnostic meta-learning with very little performance degradation compared with the system-specific conventional policies.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,169,187,,,,,,,,,,,,,,,,WOS:000828072700009,0
C,"Li, CY; Rakitsch, B; Zimmer, C",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Li, Cen-You; Rakitsch, Barbara; Zimmer, Christoph",,,Safe Active Learning for Multi-Output Gaussian Processes,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Multi-output regression problems are commonly encountered in science and engineering. In particular, multi-output Gaussian processes have been emerged as a promising tool for modeling these complex systems since they can exploit the inherent correlations and provide reliable uncertainty estimates. In many applications, however, acquiring the data is expensive and safety concerns might arise (e.g. robotics, engineering). We propose a safe active learning approach for multi-output Gaussian process regression. This approach queries the most informative data or output taking the relatedness between the regressors and safety constraints into account. We prove the effectiveness of our approach by providing theoretical analysis and by demonstrating empirical results on simulated datasets and on a real-world engineering dataset. On all datasets, our approach shows improved convergence compared to its competitors.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072704028,0
C,"Richardson, OE",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Richardson, Oliver E.",,,"Loss as the Inconsistency of a Probabilistic Dependency Graph: Choose Your Model, Not Your Loss Function","INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"In a world blessed with a great diversity of loss functions, we argue that that choice between them is not a matter of taste or pragmatics, but of model. Probabilistic depencency graphs (PDGs) are probabilistic models that come equipped with a measure of inconsistency. We prove that many standard loss functions arise as the inconsistency of a natural PDG describing the appropriate scenario, and use the same approach to justify a well-known connection between regularizers and priors. We also show that the PDG inconsistency captures a large class of statistical divergences, and detail benefits of thinking of them in this way, including an intuitive visual language for deriving inequalities between them. In variational inference, we find that the ELBO, a somewhat opaque objective for latent variable models, and variants of it arise for free out of uncontroversial modeling assumptions-as do simple graphical proofs of their corresponding bounds. Finally, we observe that inconsistency becomes the log partition function (free energy) in the setting where PDGs are factor graphs.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072702033,0
C,"Rudolph, Y; Brefeld, U",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Rudolph, Yannick; Brefeld, Ulf",,,Modeling Conditional Dependencies in Multiagent Trajectories,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We study modeling joint densities over sets of random variables (next-step movements of multiple agents) which are conditioned on aligned observations (past trajectories). For this setting, we propose an autoregressive approach to model intra-timestep dependencies, where distributions over joint movements are represented by autoregressive factorizations. In our approach, factors are randomly ordered and estimated with a graph neural network to account for permutation equivariance, while a recurrent neural network encodes past trajectories. We further propose a conditional two-stream attention mechanism, to allow for efficient training of random factorizations. We experiment on trajectory data from professional soccer matches and find that we model low frequency trajectories better than variational approaches.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852305002,0
C,"Salim, A; Condat, L; Kovalev, D; Richtarik, P",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Salim, Adil; Condat, Laurent; Kovalev, Dmitry; Richtarik, Peter",,,An Optimal Algorithm for Strongly Convex Minimization under Affine Constraints,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Optimization problems under affine constraints appear in various areas of machine learning. We consider the task of minimizing a smooth strongly convex function F(x) under the affine constraint Kx = b, with an oracle providing evaluations of the gradient of F and multiplications by K and its transpose. We provide lower bounds on the number of gradient computations and matrix multiplications to achieve a given accuracy. Then we propose an accelerated primal-dual algorithm achieving these lower bounds. Our algorithm is the first optimal algorithm for this class of problems.",,,,,"Condat, Laurent/E-4978-2018","Condat, Laurent/0000-0001-7087-1002",,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072704026,0
C,"Vandaele, R; Kang, B; De Bie, T; Saeys, Y",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Vandaele, Robin; Kang, Bo; De Bie, Tijl; Saeys, Yvan",,,The Curse Revisited: When are Distances Informative for the Ground Truth in Noisy High-Dimensional Data?,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Distances between data points are widely used in machine learning applications. Yet, when corrupted by noise, these distances-and thus the models based upon them-may lose their usefulness in high dimensions. Indeed, the small marginal effects of the noise may then accumulate quickly, shifting empirical closest and furthest neighbors away from the ground truth. In this paper, we exactly characterize such effects in noisy high-dimensional data using an asymptotic probabilistic expression. Previously, it has been argued that neighborhood queries become meaningless and unstable when distance concentration occurs, which means that there is a poor relative discrimination between the furthest and closest neighbors in the data. However, we conclude that this is not necessarily the case when we decompose the data in a ground truth-which we aim to recover-and noise component. More specifically, we derive that under particular conditions, empirical neighborhood relations affected by noise are still likely to be truthful even when distance concentration occurs. We also include thorough empirical verification of our results, as well as interesting experiments in which our derived 'phase shift' where neighbors become random or not turns out to be identical to the phase shift where common dimensionality reduction methods perform poorly or well for recovering low-dimensional reconstructions of high-dimensional data with dense noise.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072702009,0
C,"Yu, YD; Lin, TY; Mazumdar, E; Jordan, MI",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Yu, Yaodong; Lin, Tianyi; Mazumdar, Eric; Jordan, Michael, I",,,Fast Distributionally Robust Learning with Variance-Reduced Min-Max Optimization,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Distributionally robust supervised learning (DRSL) is emerging as a key paradigm for building reliable machine learning systems for real-world applications-reflecting the need for classifiers and predictive models that are robust to the distribution shifts that arise from phenomena such as selection bias or non-stationarity. Existing algorithms for solving Wasserstein DRSL- one of the most popular DRSL frameworks based around robustness to perturbations in the Wasserstein distance- have serious limitations that limit their use in large-scale problems-in particular they involve solving complex subproblems and they fail to make use of stochastic gradients. We revisit Wasserstein DRSL through the lens of min-max optimization and derive scalable and efficiently implementable stochastic extragradient algorithms which provably achieve faster convergence rates than existing approaches. We demonstrate their effectiveness on synthetic and real data when compared to existing DRSL approaches. Key to our results is the use of variance reduction and random reshuffling to accelerate stochastic min-max optimization, the analysis of which may be of independent interest.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072701013,0
C,"Zhang, WR; Mei, YJ; Cummings, R",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Zhang, Wanrong; Mei, Yajun; Cummings, Rachel",,,"Private Sequential Hypothesis Testing for Statisticians: Privacy, Error Rates, and Sample Size","INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"The sequential hypothesis testing problem is a class of statistical analyses where the sample size is not fixed in advance. Instead, the decision-process takes in new observations sequentially to make real-time decisions for testing an alternative hypothesis against a null hypothesis until some stopping criterion is satisfied. In many common applications of sequential hypothesis testing, the data can be highly sensitive and may require privacy protection; for example, sequential hypothesis testing is used in clinical trials, where doctors sequentially collect data from patients and must determine when to stop recruiting patients and whether the treatment is effective. The field of differential privacy has been developed to offer data analysis tools with strong privacy guarantees, and has been commonly applied to machine learning and statistical tasks. In this work, we study the sequential hypothesis testing problem under a slight variant of differential privacy, known as Renyi differential privacy. We present a new private algorithm based on Wald's Sequential Probability Ratio Test (SPRT) that also gives strong theoretical privacy guarantees. We provide theoretical analysis on statistical performance measured by Type I and Type II error as well as the expected sample size. We also empirically validate our theoretical results on several synthetic databases, showing that our algorithms also perform well in practice. Unlike previous work in private hypothesis testing that focused only on the classical fixed sample setting, our results in the sequential setting allow a conclusion to be reached much earher, and thus saving the cost of collecting additional samples.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852305040,0
C,"Adlam, B; Levinson, J; Pennington, J",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Adlam, Ben; Levinson, Jake; Pennington, Jeffrey",,,A Random Matrix Perspective on Mixtures of Nonlinearities in High Dimensions,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"One of the distinguishing characteristics of modern deep learning systems is their use of neural network architectures with enormous numbers of parameters, often in the millions and sometimes even in the billions. While this paradigm has inspired significant research on the properties of large networks, relatively little work has been devoted to the fact that these networks are often used to model large complex datasets, which may themselves contain millions or even billions of constraints. In this work, we focus on this high-dimensional regime in which both the dataset size and the number of features tend to infinity. We analyze the performance of random feature regression with features F = f (WX + B) for a random weight matrix W and bias vector B, obtaining exact formulae for the asymptotic training and test errors for data generated by a linear teacher model. The role of the bias can be understood as parameterizing a distribution over activation functions, and our analysis directly generalizes to such distributions, even those not expressible with a traditional additive bias. Intriguingly, we find that a mixture of nonlinearities can improve both the training and test errors over the best single nonlinearity, suggesting that mixtures of nonlinearities might be useful for approximate kernel methods or neural network architecture design.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072703022,0
C,"Biggs, F; Guedj, B",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Biggs, Felix; Guedj, Benjamin",,,On Margins and Derandomisation in PAC-Bayes,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We give a general recipe for derandomising PAC-Bayesian bounds using margins, with the critical ingredient being that our randomised predictions concentrate around some value. The tools we develop straightforwardly lead to margin bounds for various classifiers, including linear prediction-a class that includes boosting and the support vector machine-single-hidden-layer neural networks with an unusual erf activation function, and deep ReLU networks. Further, we extend to partially-derandomised predictors where only some of the randomness is removed, letting us extend bounds to cases where the concentration properties of our predictors are otherwise poor.",,,,,,"Guedj, Benjamin/0000-0003-1237-7430",,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072703035,0
C,"Bruns-Smith, D; Feller, A",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Bruns-Smith, David; Feller, Avi",,,Outcome Assumptions and Duality Theory for Balancing Weights,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We study balancing weight estimators, which reweight outcomes from a source population to estimate missing outcomes in a target population. These estimators minimize the worst-case error by making an assumption about the outcome model. In this paper, we show that this outcome assumption has two immediate implications. First, we can replace the minimax optimization problem for balancing weights with a simple convex loss over the assumed outcome function class. Second, we can replace the commonly-made overlap assumption with a more appropriate quantitative measure, the minimum worst-case bias. Finally, we show conditions under which the weights remain robust when our assumptions on the outcomes are wrong.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852305024,0
C,"Bu, YH; Aminian, G; Toni, L; Wornell, GW; Rodrigues, MRD",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Bu, Yuheng; Aminian, Gholamali; Toni, Laura; Wornell, Gregory W.; Rodrigues, Miguel R. D.",,,Characterizing and Understanding the Generalization Error of Transfer Learning with Gibbs Algorithm,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We provide an information-theoretic analysis of the generalization ability of Gibbs-based transfer learning algorithms by focusing on two popular empirical risk minimization (ERM) approaches for transfer learning, alpha-weighted-ERM and two-stage-ERM. Our key result is an exact characterization of the generalization behavior using the conditional symmetrized Kullback-Leibler (KL) information between the output hypothesis and the target training samples given the source training samples. Our results can also be applied to provide novel distribution-free generalization error upper bounds on these two aforementioned Gibbs algorithms. Our approach is versatile, as it also characterizes the generalization errors and excess risks of these two Gibbs algorithms in the asymptotic regime, where they converge to the alpha-weighted-ERM and two-stage-ERM, respectively. Based on our theoretical results, we show that the benefits of transfer learning can be viewed as a bias-variance trade-off, with the bias induced by the source distribution and the variance induced by the lack of target samples. We believe this viewpoint can guide the choice of transfer learning algorithms in practice.",,,,,,"Bu, Yuheng/0000-0002-3479-4553",,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852303006,0
C,"Dance, H; Paige, B",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Dance, Hugh; Paige, Brooks",,,Fast and Scalable Spike and Slab Variable Selection in High-Dimensional Gaussian Processes,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Variable selection in Gaussian processes (GPs) is typically undertaken by thresholding the inverse lengthscales of automatic relevance determination kernels, but in high-dimensional datasets this approach can be unreliable. A more probabilistically principled alternative is to use spike and slab priors and infer a posterior probability of variable inclusion. However, existing implementations in GPs are very costly to run in both high-dimensional and large-n datasets, or are only suitable for unsupervised settings with specific kernels. As such, we develop a fast and scalable variational inference algorithm for the spike and slab GP that is tractable with arbitrary differentiable kernels. We improve our algorithm's ability to adapt to the sparsity of relevant variables by Bayesian model averaging over hyperparameters, and achieve substantial speed ups using zero temperature posterior restrictions, dropout pruning and nearest neighbour mini-batching. In experiments our method consistently outperforms vanilla and sparse variational GPs whilst retaining similar runtimes (even when n = 10(6)) and performs competitively with a spike and slab GP using MCMC but runs up to 1000 times faster.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852302017,0
C,"Ek, S; Zachariah, D; Stoica, P",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Ek, Sofia; Zachariah, Dave; Stoica, Petre",,,Learning Pareto-Efficient Decisions with Confidence,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"The paper considers the problem of multi-objective decision support when outcomes are uncertain. We extend the concept of Pareto-efficient decisions to take into account the uncertainty of decision outcomes across varying contexts. This enables quantifying trade-offs between decisions in terms of tail outcomes that are relevant in safety-critical applications. We propose a method for learning efficient decisions with statistical confidence, building on results from the conformal prediction literature. The method adapts to weak or nonexistent context covariate overlap and its statistical guarantees are evaluated using both synthetic and real data.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852304022,0
C,"Glasgow, M; Wootters, M",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Glasgow, Margalit; Wootters, Mary",,,Asynchronous Distributed Optimization with Stochastic Delays,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We study asynchronous finite sum minimization in a distributed-data setting with a central parameter server. While asynchrony is well understood in parallel settings where the data is accessible by all machines-e.g., modifications of variance-reduced gradient algorithms like SAGA work well-little is known for the distributed-data setting. We develop an algorithm ADSAGA based on SAGA for the distributed-data setting, in which the data is partitioned between many machines. We show that with m machines, under a natural stochastic delay model with an mean delay of m, ADSAGA converges in (O) over tilde ((n + root m kappa) log(1/epsilon)) iterations, where n is the number of component functions, and kappa is a condition number. This complexity sits squarely between the complexity (O) over tilde ((n + kappa) log(1/epsilon)) of SAGA without delays and the complexity (O) over tilde ((n + m kappa)log(1/epsilon)) of parallel asynchronous algorithms where the delays are arbitrary (but bounded by O(m)), and the data is accessible by all. Existing asynchronous algorithms with distributed-data setting and arbitrary delays have only been shown to converge in (O) over tilde (n(2)kappa log(1/epsilon)) iterations. We empirically compare the iteration complexity and wallclock performance of ADSAGA to existing parallel and distributed algorithms, including synchronous minibatch algorithms. Our results demonstrate the wallclock advantage of variance-reduced asynchronous approaches over SGD or synchronous approaches.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852303028,0
C,"Hanna, OA; Yang, LF; Fragouli, C",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Hanna, Osama A.; Yang, Lin F.; Fragouli, Christina",,,Solving Multi-Arm Bandit Using a Few Bits of Communication,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"The multi-armed bandit (MAB) problem is an active learning framework that aims to select the best among a set of actions by sequentially observing rewards. Recently, it has become popular for a number of applications over wireless networks, where communication constraints can form a bottleneck. Existing works usually fail to address this issue and can become infeasible in certain applications. In this paper we address the communication problem by optimizing the communication of rewards collected by distributed agents. By providing nearly matching upper and lower bounds, we tightly characterize the number of bits needed per reward for the learner to accurately learn without suffering additional regret. In particular, we establish a generic reward quantization algorithm, QuBan, that can be applied on top of any (no-regret) MAB algorithm to form a new communication-efficient counterpart, that requires only a few (as low as 3) bits to be sent per iteration while preserving the same regret bound. Our lower bound is established via constructing hard instances from a subgaussian distribution. Our theory is further corroborated by numerically experiments.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852305033,0
C,"Juba, B; Liang, LD",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Juba, Brendan; Liang, Leda",,,Conditional Linear Regression for Heterogeneous Covariances,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Often machine learning and statistical models will attempt to describe the majority of the data. However, there may be situations where only a fraction of the data can be fit well by a linear regression model. Here, we are interested in a case where such inliers can be identified by a Disjunctive Normal Form (DNF) formula. We give a polynomial time algorithm for the conditional linear regression task, which identifies a DNF condition together with the linear predictor on the corresponding portion of the data. In this work, we improve on previous algorithms by removing a requirement that the covariances of the data satisfying each of the terms of the condition have to all be very similar in spectral norm to the covariance of the overall condition.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,6182,6199,,,,,,,,,,,,,,,,WOS:000841852300027,0
C,"Rosenfeld, E; Ravikumar, P; Risteski, A",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Rosenfeld, Elan; Ravikumar, Pradeep; Risteski, Andrej",,,An Online Learning Approach to Interpolation and Extrapolation in Domain Generalization,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"A popular assumption for out-of-distribution generalization is that the training data comprises sub-datasets, each drawn from a distinct distribution; the goal is then to interpolate these distributions and extrapolate beyond them-this objective is broadly known as domain generalization. A common belief is that ERM can interpolate but not extrapolate and that the latter task is considerably more difficult, but these claims are vague and lack formal justification. In this work, we recast generalization over subgroups as an online game between a player minimizing risk and an adversary presenting new test distributions. Under an existing notion of inter- and extrapolation based on reweighting of sub-group likelihoods, we rigorously demonstrate that extrapolation is computationally much harder than interpolation, though their statistical complexity is not significantly different. Furthermore, we show that ERM-possibly with added structured noise-is provably minimax-optimal for both tasks. Our framework presents a new avenue for the formal analysis of domain generalization algorithms which may be of independent interest.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072702031,0
C,"Ryan, JP; Ament, S; Gomes, CP; Damle, A",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Ryan, John Paul; Ament, Sebastian; Gomes, Carla P.; Damle, Anil",,,The Fast Kernel Transform,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Kernel methods are a highly effective and widely used collection of modern machine learning algorithms. A fundamental limitation of virtually all such methods are computations involving the kernel matrix that naively scale quadratically (e.g., matrix-vector multiplication) or cubically (solving linear systems) with the size of the dataset N. We propose the Fast Kernel Transform (FKT), a general algorithm to compute matrix-vector multiplications (MVMs) for datasets in moderate dimensions with quasilinear complexity. Typically, analytically grounded fast multiplication methods require specialized development for specific kernels. In contrast, our scheme is based on auto-differentiation and automated symbolic computations that leverage the analytical structure of the underlying kernel. This allows the FKT to be easily applied to a broad class of kernels, including Gaussian, Matern, and Rational Quadratic covariance functions and Green's functions, including those of the Laplace and Helmholtz equations. Furthermore, the FKT maintains a high, quantifiable, and controllable level of accuracy-properties that many acceleration methods lack. We illustrate the efficacy and versatility of the FKT by providing timing and accuracy benchmarks with comparisons to adjacent methods, and by applying it to scale the stochastic neighborhood embedding (t-SNE) and Gaussian processes to large real-world datasets.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852306010,0
C,"Takezawa, Y; Sato, R; Kozareva, Z; Ravi, S; Yamada, M",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Takezawa, Yuki; Sato, Ryoma; Kozareva, Zornitsa; Ravi, Sujith; Yamada, Makoto",,,Fixed Support Tree-Sliced Wasserstein Barycenter,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"The Wasserstein barycenter has been widely studied in various fields, including natural language processing, and computer vision. However, it requires a high computational cost to solve the Wasserstein barycenter problem because the computation of the Wasserstein distance requires a quadratic time with respect to the number of supports. By contrast, the Wasserstein distance on a tree, called the tree-Wasserstein distance, can be computed in linear time and allows for the fast comparison of a large number of distributions. In this study, we propose a barycenter under the tree-Wasserstein distance, called the fixed support tree-Wasserstein barycenter (FS-TWB) and its extension, called the fixed support tree-sliced Wasserstein barycenter (FS-TSWB). More specifically, we first show that the FS-TWB and FS-TSWB problems are convex optimization problems and can be solved by using the projected subgradient descent. Moreover, we propose a more efficient algorithm to compute the subgradient and objective function value by using the properties of tree-Wasserstein barycenter problems. Through real-world experiments, we show that, by using the proposed algorithm, the FS-TWB and FS-TSWB can be solved two orders of magnitude faster than the original Wasserstein barycenter.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072701007,0
C,"Tian, Y; Scutari, G; Cao, TY; Gasnikov, A",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Tian, Ye; Scutari, Gesualdo; Cao, Tianyu; Gasnikov, Alexander",,,Acceleration in Distributed Optimization under Similarity,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We study distributed (strongly convex) optimization problems over a network of agents, with no centralized nodes. The loss functions of the agents are assumed to be similar, due to statistical data similarity or otherwise. In order to reduce the number of communications to reach a solution accuracy, we proposed a preconditioned, accelerated distributed method. An e-solution is achieved in (O) over tilde(root beta/mu/1-rho log 1/epsilon) number of communications steps, where beta/mu is the relative condition number between the global and local loss functions, and rho characterizes the connectivity of the network. This rate matches (up to poly-log factors) lower complexity communication bounds of distributed gossip-algorithms applied to the class of problems of interest. Numerical results show significant communication savings with respect to existing accelerated distributed schemes, especially when solving ill-conditioned problems.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,5721,5756,,,,,,,,,,,,,,,,WOS:000841852300006,0
C,"Trick, S; Rothkopf, CA",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Trick, Susanne; Rothkopf, Constantin A.",,,Bayesian Classifier Fusion with an Explicit Model of Correlation,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,Combining the outputs of multiple classifiers or experts into a single probabilistic classification is a fundamental task in machine learning with broad applications from classifier fusion to expert opinion pooling. Here we present a hierarchical Bayesian model of probabilistic classifier fusion based on a new correlated Dirichlet distribution. This distribution explicitly models positive correlations between marginally Dirichlet-distributed random vectors thereby allowing explicit modeling of correlations between base classifiers or experts. The proposed model naturally accommodates the classic Independent Opinion Pool and other independent fusion algorithms as special cases. It is evaluated by uncertainty reduction and correctness of fusion on synthetic and real-world data sets. We show that a change in performance of the fused classifier due to uncertainty reduction can be Bayes optimal even for highly correlated base classifiers.,,,,,,"Rothkopf, Constantin/0000-0002-5636-0801",,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072702015,0
C,"Xiao, S; Jiang, ZF; Yang, S",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Xiao, Shuai; Jiang, Zaifan; Yang, Shuang",,,Tile Networks: Learning Optimal Geometric Layout for Whole-page Recommendation,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,Finding optimal configurations in a geometric space is a key challenge in many technological disciplines. Current approaches either rely heavily on human domain expertise and are difficult to scale. In this paper we show it is possible to solve configuration optimization problems for whole-page recommendation using reinforcement learning. The proposed Tile Networks is a neural architecture that optimizes 2D geometric configurations by arranging items on proper positions. Empirical results on real dataset demonstrate its superior performance compared to traditional learning to rank approaches and recent deep models.,,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852302033,0
C,"Agrawal, M; Lang, H; Offin, M; Gazit, L; Sontag, D",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Agrawal, Monica; Lang, Hunter; Offin, Michael; Gazit, Lior; Sontag, David",,,Leveraging Time Irreversibility with Order-Contrastive Pre-training,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Label-scarce, high-dimensional domains such as healthcare present a challenge for modern machine learning techniques. To overcome the difficulties posed by a lack of labeled data, we explore an order-contrastive method for self-supervised pre-training on longitudinal data. We sample pairs of time segments, switch the order for half of them, and train a model to predict whether a given pair is in the correct order. Intuitively, the ordering task allows the model to attend to the least time-reversible features (for example, features that indicate progression of a chronic disease). The same features are often useful for downstream tasks of interest. To quantify this, we study a simple theoretical setting where we prove a finite-sample guarantee for the downstream error of a representation learned with order-contrastive pre-training. Empirically, in synthetic and longitudinal healthcare settings, we demonstrate the effectiveness of order-contrastive pre-training in the small-data regime over supervised learning and other self-supervised pre-training baselines. Our results indicate that pre-training methods designed for particular classes of distributions and downstream tasks can improve the performance of self-supervised learning.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072702017,0
C,"Bao, H; Shimada, T; Xu, LY; Sato, I; Sugiyama, M",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Bao, Han; Shimada, Takuya; Xu, Liyuan; Sato, Issei; Sugiyama, Masashi",,,Pairwise Supervision Can Provably Elicit a Decision Boundary,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Similarity learning is a general problem to elicit useful representations by predicting the relationship between a pair of patterns. This problem is related to various important preprocessing tasks such as metric learning, kernel learning, and contrastive learning. A classifier built upon the representations is expected to perform well in downstream classification; however, little theory has been given in literature so far and thereby the relationship between similarity and classification has remained elusive. Therefore, we tackle a fundamental question: can similarity information provably leads a model to perform well in downstream classification? In this paper, we reveal that a product-type formulation of similarity learning is strongly related to an objective of binary classification. We further show that these two different problems are explicitly connected by an excess risk bound. Consequently, our results elucidate that similarity learning is capable of solving binary classification by directly eliciting a decision boundary.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072702030,0
C,"Chen, TY; Sun, YJ; Xiao, Q; Yin, WT",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Chen, Tianyi; Sun, Yuejiao; Xiao, Quan; Yin, Wotao",,,A Single-Timescale Method for Stochastic Bilevel Optimization,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Stochastic bilevel optimization generalizes the classic stochastic optimization from the minimization of a single objective to the minimization of an objective function that depends on the solution of another optimization problem. Recently, bilevel optimization is regaining popularity in emerging machine learning applications such as hyper-parameter optimization and model-agnostic meta learning. To solve this class of optimization problems, existing methods require either double-loop or two-timescale updates, which are sometimes less efficient. This paper develops a new optimization method for a class of stochastic bilevel problems that we term Single-Timescale stochAstic BiLevEl optimization (STABLE) method. STABLE runs in a single loop fashion, and uses a single-timescale update with a fixed batch size. To achieve an epsilon-stationary point of the bilevel problem, STABLE requires O (epsilon(-2)) samples in total; and to achieve an epsilon-optimal solution in the strongly convex case, STABLE requires O (epsilon(-1)) samples. To the best of our knowledge, when STABLE was proposed, it is the first bilevel optimization algorithm achieving the same order of sample complexity as SGD for single-level stochastic optimization.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072702024,0
C,"Cheng, XW; Jaggi, S; Zhou, QQ",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Cheng, Xiwei; Jaggi, Sidharth; Zhou, Qiaoqiao",,,Generalized Group Testing,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"In the problem of classical group testing one aims to identify a small subset (of size d) diseased individuals/defective items in a large population (of size n). This process is based on a minimal number of suitably-designed group tests on subsets of items, where the test outcome is positive iff the given test contains at least one defective item. Motivated by physical considerations, we consider a generalized setting that includes as special cases multiple other group-testing-like models in the literature. In our setting, which subsumes as special cases a variety of noiseless and noisy group-testing models in the literature, the test outcome is positive with probability f(x), where x is the number of defectives tested in a pool, and f(center dot) is an arbitrary monotonically increasing (stochastic) test function. Our main contributions are as follows. 1. We present a non-adaptive scheme that with probability 1 - epsilon identifies all defective items. Our scheme requires at most O(H(f)dlog(n/epsilon)) tests, where H(f) is a suitably defined sensitivity parameter of f(center dot), and is never larger than O(d(1+o(1))), but may be substantially smaller for many f(center dot). 2. We argue that any non-adaptive group testing scheme needs at least Omega((1 - epsilon)h(f)dlog(n/d)) tests to ensure reliable recovery. Here h(f) >= 1 is a suitably defined concentration parameter of f(center dot). 3. We prove that our sample-complexity bounds for generalized group testing are information-theoretically near-optimal for a variety of sparse-recovery group-testing models in the literature. That is, for any noisy test function f(center dot) (i.e., 0 < f(0) < f(d) < 1), and for a variety of (one-sided) noiseless test functions f(center dot) (i.e., either f(0)=0, or f(d)=1, or both) studied in the literature we show that H(f)/h(f) is an element of Theta(1). As a by-product we tightly characterize the heretofore open information-theoretic sample-complexity for the well-studied model of threshold group-testing. For general (near)-noiseless test functions f(center dot) we show that H(f)/h(f) is an element of O(d(1+o(1))). We also demonstrate a natural test-function f(center dot) whose sample complexity scales extremally as Theta(d(2)log(n)), rather than Theta(dlog(n)) as in the case of classical group-testing. Some of our techniques may be of independent interest - in particular our achievability requires a delicate saddle-point approximation, and our impossibility proof relies on a novel bound relating the mutual information of pair of random variables with the mean and variance of a specific function, and we derive novel structural results about monotone functions.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852305014,0
C,"Cyffers, E; Bellet, A",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Cyffers, Edwige; Bellet, Aurelien",,,Privacy Amplification by Decentralization,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Analyzing data owned by several parties while achieving a good trade-off between utility and privacy is a key challenge in federated learning and analytics. In this work, we introduce a novel relaxation of local differential privacy (LDP) that naturally arises in fully decentralized algorithms, i.e., when participants exchange information by communicating along the edges of a network graph without central coordinator. This relaxation, that we call network DP, captures the fact that users have only a local view of the system. To show the relevance of network DP, we study a decentralized model of computation where a token performs a walk on the network graph and is updated sequentially by the party who receives it. For tasks such as real summation, histogram computation and optimization with gradient descent, we propose simple algorithms on ring and complete topologies. We prove that the privacy-utility trade-offs of our algorithms under network DP significantly improve upon what is achievable under LDP, and often match the utility of the trusted curator model. Our results show for the first time that formal privacy gains can be obtained from full decentralization. We also provide experiments to illustrate the improved utility of our approach for decentralized training with stochastic gradient descent.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072705018,0
C,"Faury, L; Abeille, M; Jun, KS; Calauzenes, C",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Faury, Louis; Abeille, Marc; Jun, Kwang-Sung; Calauzenes, Clement",,,Jointly Efficient and Optimal Algorithms for Logistic Bandits,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Logistic Bandits have recently undergone careful scrutiny by virtue of their combined theoretical and practical relevance. This research effort delivered statistically efficient algorithms, improving the regret of previous strategies by exponentially large factors. Such algorithms are however strikingly costly as they require Omega(t) operations at each round. On the other hand, a different line of research focused on computational efficiency (O(1) per-round cost), but at the cost of letting go of the aforementioned exponential improvements. Obtaining the best of both world is unfortunately not a matter of marrying both approaches. Instead we introduce a new learning procedure for Logistic Bandits. It yields confidence sets which sufficient statistics can be easily maintained online without sacrificing statistical tightness. Combined with efficient planning mechanisms we design fast algorithms which regret performance still match the problem-dependent lower-bound of Abeille et al. (2021). To the best of our knowledge, those are the first Logistic Bandit algorithms that simultaneously enjoy statistical and computational efficiency.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,546,580,,,,,,,,,,,,,,,,WOS:000828072700023,0
C,"Hoffmann, MP; Braun, T; Moller, R",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Hoffmann, Moritz P.; Braun, Tanya; Moeller, Ralf",,,Lifted Division for Lifted Hugin Belief Propagation,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"The lifted junction tree algorithm (LJT) is an inference algorithm that allows for tractable inference regarding domain sizes. To answer multiple queries efficiently, it decomposes a first-order input model into a first-order junction tree. During inference, degrees of belief are propagated through the tree. This propagation significantly contributes to the runtime complexity not just of LJT but of any tree-based inference algorithm. We present a lifted propagation scheme based on the so-called Hugin scheme whose runtime complexity is independent of the degree of the tree. Thereby, lifted Hugin can achieve asymptotic speed improvements over the existing lifted Shafer-Shenoy propagation. An empirical evaluation confirms these results.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,6501,6510,,,,,,,,,,,,,,,,WOS:000841852300040,0
C,"Lan, QF; Tosatto, S; Farrahi, H; Mahmood, AR",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Lan, Qingfong; Tosatto, Samuele; Farrahi, Homayoon; Mahmood, A. Rupam",,,Model-free Policy Learning with Reward Gradients,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Despite the increasing popularity of policy gradient methods, they are yet to be widely utilized in sample-scarce applications, such as robotics. The sample efficiency could be improved by making best usage of available information. As a key component in reinforcement learning, the reward function is usually devised carefully to guide the agent. Hence, the reward function is usually known, allowing access to not only scalar reward signals but also reward gradients. To benefit from reward gradients, previous works require the knowledge of environment dynamics, which are hard to obtain. In this work, we develop the Reward Policy Gradient estimator, a novel approach that integrates reward gradients without learning a model. Bypassing the model dynamics allows our estimator to achieve a better bias-variance trade-off, which results in a higher sample efficiency, as shown in the empirical analysis. Our method also boosts the performance of Proximal Policy Optimization on different MuJoCo control tasks. (1)",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072704013,0
C,"Le Lan, C; Tu, S; Oberman, A; Agarwal, R; Bellemare, M",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Le Lan, Charline; Tu, Stephen; Oberman, Adam; Agarwal, Rishabh; Bellemare, Marc",,,On the Generalization of Representations in Reinforcement Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"In reinforcement learning, state representations are used to tractably deal with large problem spaces. State representations serve both to approximate the value function with few parameters, but also to generalize to newly encountered states. Their features may be learned implicitly (as part of a neural network) or explicitly (for example, the successor representation of Dayan (1993)). While the approximation properties of representations are reasonably well-understood, a precise characterization of how and when these representations generalize is lacking. In this work, we address this gap and provide an informative bound on the generalization error arising from a specific state representation. This bound is based on the notion of effective dimension which measures the degree to which knowing the value at one state informs the value at other states. Our bound applies to any state representation and quantifies the natural tension between representations that generalize well and those that approximate well. We complement our theoretical results with an empirical survey of classic representation learning methods from the literature and results on the Arcade Learning Environment, and find that the generalization behaviour of learned representations is well-explained by their effective dimension. [GRAPHICS] .",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072704009,0
C,"Li, CJ; Yu, YD; Loizou, N; Gidel, G; Ma, Y; Le Roux, N; Jordan, MI",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Li, Chris Junchi; Yu, Yaodong; Loizou, Nicolas; Gidel, Gauthier; Ma, Yi; Le Roux, Nicolas; Jordan, Michael, I",,,On the Convergence of Stochastic Extragradient for Bilinear Games using Restarted Iteration Averaging,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We study the stochastic bilinear minimax optimization problem, presenting an analysis of the same-sample Stochastic ExtraGradient (SEG) method with constant step size, and presenting variations of the method that yield favorable convergence. In sharp contrasts with the basic SEG method whose last iterate only contracts to a fixed neighborhood of the Nash equilibrium, SEG augmented with iteration averaging provably converges to the Nash equilibrium under the same standard settings, and such a rate is further improved by incorporating a scheduled restarting procedure. In the interpolation setting where noise vanishes at the Nash equilibrium, we achieve an optimal convergence rate up to tight constants. We present numerical experiments that validate our theoretical findings and demonstrate the effectiveness of the SEG method when equipped with iteration averaging and restarting.",,,,,"LI, chris/HDO-6232-2022",,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852304015,0
C,"Liu, JC; Zhong, CD; Seltzer, M; Rudin, C",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Liu, Jiachang; Zhong, Chudi; Seltzer, Margo; Rudin, Cynthia",,,Fast Sparse Classification for Generalized Linear and Additive Models,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We present fast classification techniques for sparse generalized linear and additive models. These techniques can handle thousands of features and thousands of observations in minutes, even in the presence of many highly correlated features. For fast sparse logistic regression, our computational speed-up over other best-subset search techniques owes to linear and quadratic surrogate cuts for the logistic loss that allow us to efficiently screen features for elimination, as well as use of a priority queue that favors a more uniform exploration of features. As an alternative to the logistic loss, we propose the exponential loss, which permits an analytical solution to the line search at each iteration. Our algorithms are generally 2 to 5 times faster than previous approaches. They produce interpretable models that have accuracy comparable to black box models on challenging datasets.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,35601052,,,,,WOS:000841852303030,0
C,"Liu, LT; Garg, N; Borgs, C",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Liu, Lydia T.; Garg, Nikhil; Borgs, Christian",,,Strategic ranking,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Strategic classification studies the design of a classifier robust to the manipulation of input by strategic individuals. However, the existing literature does not consider the effect of competition among individuals as induced by the algorithm design. Motivated by constrained allocation settings such as college admissions, we introduce strategic ranking, in which the (designed) individual reward depends on an applicant's post-effort rank in a measurement of interest. Our results illustrate how competition among applicants affects the resulting equilibria and model insights. We analyze how various ranking reward designs, belonging to a family of step functions, trade off applicant, school, and societal utility, as well as how ranking design counters inequities arising from disparate access to resources. In particular, we find that randomization in the reward design can mitigate two measures of disparate impact, welfare gap and access.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072702025,0
C,"Neklyudov, K; Welling, M",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Neklyudov, Kirill; Welling, Max",,,Orbital MCMC,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Markov Chain Monte Carlo (MCMC) algorithms ubiquitously employ complex deterministic transformations to generate proposal points that are then filtered by the MetropolisHastings-Green (MHG) test. However, the condition of the target measure invariance puts restrictions on the design of these transformations. In this paper, we first derive the acceptance test for the stochastic Markov kernel considering arbitrary deterministic maps as proposal generators. When applied to the transformations with orbits of period two (involutions), the test reduces to the MHG test. Based on the derived test we propose two practical algorithms: one operates by constructing periodic orbits from any diffeomorphism, another on contractions of the state space (such as optimization trajectories). Finally, we perform an empirical study demonstrating the practical advantages of both kernels.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,5790,5814,,,,,,,,,,,,,,,,WOS:000841852300009,0
C,"Pawelczyk, M; Agarwal, C; Joshi, S; Upadhyay, S; Lakkaraju, H",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Pawelczyk, Martin; Agarwal, Chirag; Joshi, Shalmali; Upadhyay, Sohini; Lakkaraju, Himabindu",,,Exploring Counterfactual Explanations Through the Lens of Adversarial Examples: A Theoretical and Empirical Analysis,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"As machine learning (ML) models become more widely deployed in high-stakes applications, counterfactual explanations have emerged as key tools for providing actionable model explanations in practice. Despite the growing popularity of counterfactual explanations, a deeper understanding of these explanations is still lacking. In this work, we systematically analyze counterfactual explanations through the lens of adversarial examples. We do so by formalizing the similarities between popular counterfactual explanation and adversarial example generation methods identifying conditions when they are equivalent. We then derive the upper bounds on the distances between the solutions output by counterfactual explanation and adversarial example generation methods, which we validate on several real world data sets. By establishing these theoretical and empirical similarities between counterfactual explanations and adversarial examples, our work raises fundamental questions about the design and development of existing counterfactual explanation algorithms.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072704030,0
C,"Sander, ME; Ablin, P; Blondel, M; Peyre, G",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Sander, Michael E.; Ablin, Pierre; Blondel, Mathieu; Peyre, Gabriel",,,Sinkformers: Transformers with Doubly Stochastic Attention,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Attention based models such as Transformers involve pairwise interactions between data points, modeled with a learnable attention matrix. Importantly, this attention matrix is normalized with the SoftMax operator, which makes it row-wise stochastic. In this paper, we propose instead to use Sinkhorn's algorithm to make attention matrices doubly stochastic. We call the resulting model a Sinkformer. We show that the row-wise stochastic attention matrices in classical Transformers get close to doubly stochastic matrices as the number of epochs increases, justifying the use of Sinkhorn normalization as an informative prior. On the theoretical side, we show that, unlike the SoftMax operation, this normalization makes it possible to understand the iterations of self-attention modules as a discretized gradient-flow for the Wasserstein metric. We also show in the infinite number of samples limit that, when rescaling both attention matrices and depth, Sinkformers operate a heat diffusion. On the experimental side, we show that Sinkformers enhance model accuracy in vision and natural language processing tasks. In particular, on 3D shapes classification, Sinkformers lead to a significant improvement.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072703025,0
C,"Shi, Z; Eryilmaz, A",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Shi, Zai; Eryilmaz, Atilla",,,A Bayesian Approach for Stochastic Continuum-armed Bandit with Long-term Constraints,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Despite many valuable advances in the domain of online convex optimization over the last decade, many machine learning and networking problems of interest do not fit into that framework due to their nonconvex objectives and the presence of constraints. This motivates us in this paper to go beyond convexity and study the problem of stochastic continuum-armed bandit with long-term constraints. For noiseless observations of constraint functions, we propose a generic method using a Bayesian approach based on a class of penalty functions, and prove that it can achieve a sublinear regret with respect to the global optimum and a sublinear constraint violation (CV), which can match the best results of previous methods. Additionally, we propose another method to deal with the case where constraint functions are observed with noise, which can achieve a sublinear regret and a sublinear CV with more assumptions. Finally, we use two experiments to compare our methods with two benchmark methods in online optimization and Bayesian optimization, which demonstrates the advantages of our algorithms.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852302034,0
C,"Vakilian, A; Yalciner, M",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Vakilian, Ali; Yalciner, Mustafa",,,Improved Approximation Algorithms for Individually Fair Clustering,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We consider the k-clustering problem with l(p)-norm cost, which includes k-median, kmeans and k-center, under an individual notion of fairness proposed by Jung et al. [2020]: given a set of points P of size n, a set of k centers induces a fair clustering if every point in P has a center among its n/k closest neighbors. Mahabadi and Vakilian [2020] presented a (p(o(P)), 7)-bicriteria approximation for fair clustering with l(p)-norm cost: every point finds a center within distance at most 7 times its distance to its (n/k)-th closest neighbor and the l(p)-norm cost of the solution is at most p(o(P)) times the cost of an optimal fair solution. In this work, for any epsilon > 0, we present an improved (16(P) + epsilon, 3)-bicriteria for this problem. Moreover, for p = 1 (k-median) and p = infinity (k-center), we present improved cost-approximation factors 7.081 + epsilon and 3 + epsilon respectively. To achieve our guarantees, we extend the framework of [Charikar et al., 2002, Swam3T, 2016] and devise a 16P-approximation algorithm for the facility location with l(p)-norm cost under matroid constraint which might be of an independent interest. Besides, our approach suggests a reduction from our individually fair clustering to a clustering with a group fairness requirement proposed by Kleindessner et al. [2019], which is essentially the median matroid problem [Krishnaswamy et al., 2011].",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852303010,0
C,"Wang, ZL; Wagenmaker, A; Jamieson, K",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Wang, Zhenlin; Wagenmaker, Andrew; Jamieson, Kevin",,,Best Arm Identification with Safety Constraints,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"The best arm identification problem in the multi-armed bandit setting is an excellent model of many real-world decision-making problems, yet it fails to capture the fact that in the real-world, safety constraints often must be met while learning. In this work we study the question of best-arm identification in safety-critical settings, where the goal of the agent is to find the best safe option out of many, while exploring in a way that guarantees certain, initially unknown safety constraints are met. We first analyze this problem in the setting where the reward and safety constraint takes a linear structure, and show nearly matching upper and lower bounds. We then analyze a much more general version of the problem where we only assume the reward and safety constraint can be modeled by monotonic functions, and propose an algorithm in this setting which is guaranteed to learn safely. We conclude with experimental results demonstrating the effectiveness of our approaches in scenarios such as safely identifying the best drug out of many in order to treat an illness.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852303023,0
C,"Wu, Y; Zhou, DR; Gu, QQ",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Wu, Yue; Zhou, Dongruo; Gu, Quanquan",,,Nearly Minimax Optimal Regret for Learning Infinite-horizon Average-reward MDPs with Linear Function Approximation,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We study reinforcement learning in an infinite-horizon average-reward setting with linear function approximation for linear mixture Markov decision processes (MDPs), where the transition probability function of the underlying MDP admits a linear form over a feature mapping of the current state, action, and next state. We propose a new algorithm UCRL2-VTR, which can be seen as an extension of the UCRL2 algorithm with linear function approximation. We show that UCRL2-VTR with Bernstein-type bonus can achieve a regret of (O) over tilde (d root DT), where d is the dimension of the feature mapping, T is the horizon, and D is the diameter of the MDP. We also prove a matching lower bound (Omega) over tilde (d root DT), which suggests that the proposed UCRL2-VTR is minimax optimal up to logarithmic factors. To the best of our knowledge, our algorithm is the first nearly minimax optimal RL algorithm with function approximation in the infinite-horizon average-reward setting.",,,,,"Zhou, Dongruo/GYJ-3503-2022",,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072703042,0
C,"Zheng, E; Yu, Q; Li, R; Shi, PC; Haake, A",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Zheng, Ervine; Yu, Qi; Li, Rui; Shi, Pengcheng; Haake, Anne",,,Dual-Level Adaptive Information Filtering for Interactive Image Segmentation,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Image segmentation can be performed interactively by accepting user annotations to refine the segmentation. It seeks frequent feedback from humans, and the model is updated with a smaller batch of data in each iteration of the feedback loop. Such a training paradigm requires effective information filtering to guide the model so that it can encode vital information and avoid overfitting due to limited data and inherent heterogeneity and noises thereof. We propose an adaptive interactive segmentation framework to support user interaction while introducing dual-level information filtering to train a robust model. The framework integrates an encoder-decoder architecture with a style-aware augmentation module that applies augmentation to feature maps and customizes the segmentation prediction for different latent styles. It also applies a systematic label softening strategy to generate uncertainty-aware soft labels for model updates. Experiments on both medical and natural image segmentation tasks demonstrate the effectiveness of the proposed framework.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852301013,0
C,"Ardywibowo, R; Boluki, S; Wang, ZY; Mortazavi, B; Huang, S; Qian, XN",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Ardywibowo, Randy; Boluki, Shahin; Wang, Zhangyang; Mortazavi, Bobak; Huang, Shuai; Qian, Xiaoning",,,VFDS: Variational Foresight Dynamic Selection in Bayesian Neural Networks for Efficient Human Activity Recognition,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"In many machine learning tasks, input features with varying degrees of predictive capability are acquired at varying costs. In order to optimize the performance-cost trade-o., one would select features to observe a priori. However, given the changing context with previous observations, the subset of predictive features to select may change dynamically. Therefore, we face the challenging new problem of foresight dynamic selection (FDS): finding a dynamic and light-weight policy to decide which features to observe next, before actually observing them, for overall performance-cost trade-o.s. To tackle FDS, this paper proposes a Bayesian learning framework of Variational Foresight Dynamic Selection (VFDS). VFDS learns a policy that selects the next feature subset to observe, by optimizing a variational Bayesian objective that characterizes the trade-o. between model performance and feature cost. At its core is an implicit variational distribution on binary gates that are dependent on previous observations, which will select the next subset of features to observe. We apply VFDS on the Human Activity Recognition (HAR) task where the performance-cost trade-o. is critical in its practice. Extensive results demonstrate that VFDS selects di.erent features under changing contexts, notably saving sensory costs while maintaining or improving the HAR accuracy. Moreover, the features that VFDS dynamically select are shown to be interpretable and associated with the di.erent activity types. We will release the code.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072701019,0
C,"Boffi, NM; Tu, S; Slotine, JJ",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Boffi, Nicholas M.; Tu, Stephen; Slotine, Jean-Jacques",,,The role of optimization geometry in single neuron learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Recent numerical experiments have demonstrated that the choice of optimization geometry used during training can impact generalization performance when learning expressive nonlinear model classes such as deep neural networks. These observations have important implications for modern deep learning, but remain poorly understood due to the difficulty of the associated nonconvex optimization. Towards an understanding of this phenomenon, we analyze a family of pseudogradient methods for learning generalized linear models under the square loss - a simplified problem containing both nonlinearity in the model parameters and nonconvexity of the optimization which admits a single neuron as a special case. We prove nonasymptotic bounds on the generalization error that sharply characterize how the interplay between the optimization geometry and the feature space geometry sets the out-of-sample performance of the learned model. Experimentally, selecting the optimization geometry as suggested by our theory leads to improved performance in generalized linear model estimation problems such as nonlinear and nonconvex variants of sparse vector recovery and low-rank matrix sensing.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852306003,0
C,"Diouane, Y; Lucchi, A; Patil, V",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Diouane, Youssef; Lucchi, Aurelien; Patil, Vihang",,,A Globally Convergent Evolutionary Strategy for Stochastic Constrained Optimization with Applications to Reinforcement Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Evolutionary strategies have recently been shown to achieve competing levels of performance for complex optimization problems in reinforcement learning. In such problems, one often needs to optimize an objective function subject to a set of constraints, including for instance constraints on the entropy of a policy or to restrict the possible set of actions or states accessible to an agent. Convergence guarantees for evolutionary strategies to optimize stochastic constrained problems are however lacking in the literature. In this work, we address this problem by designing a novel optimization algorithm with a sufficient decrease mechanism that ensures convergence and that is based only on estimates of the functions. We demonstrate the applicability of this algorithm on two types of experiments: i) a control task for maximizing rewards and ii) maximizing rewards subject to a non-relaxable set of constraints.",,,,,"Diouane, Youssef/AAN-4161-2020","Diouane, Youssef/0000-0002-6609-7330; Lucchi, Aurelien/0000-0001-7015-2710",,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,836,859,,,,,,,,,,,,,,,,WOS:000828072700035,0
C,"Kpotufe, S; Yuan, G; Zhao, YF",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Kpotufe, Samory; Yuan, Gan; Zhao, Yunfan",,,Nuances in Margin Conditions Determine Gains in Active Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We consider nonparametric classification with smooth regression functions, where it is well known that notions of margin in E[Y vertical bar X] determine fast or slow rates in both active and passive learning. Here we elucidate a striking distinction between the two settings. Namely, we show that some seemingly benign nuances in notions of margin somehow involving the uniqueness of the Bayes classifier, and which have no apparent effect on rates in passive learning determine whether or not any active learner can outperform passive learning rates. In particular, for Audibert-Tsybakov's margin condition (allowing general situations with non-unique Bayes classifiers), no active learner can gain over passive learning in commonly studied settings where the marginal on X is near uniform. Our results thus negate the usual intuition from past literature that active rates should generally improve over passive rates in non-parametric settings.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852302023,0
C,"Kveton, B; Meshi, O; Qin, Z; Zoghi, M",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Kveton, Branislav; Meshi, Ofer; Qin, Zhen; Zoghi, Masrour",,,On the Value of Prior in Online Learning to Rank,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"This paper addresses the cold-start problem in online learning to rank (OLTR). We show both theoretically and empirically that priors improve the quality of ranked lists presented to users interactively based on user feedback. These priors can come in the form of unbiased estimates of the relevance of the ranked items, or more practically, can be obtained from offline-learned models. Our experiments show the effectiveness of priors in improving the short-term regret of tabular OLTR algorithms, based on Thompson sampling and BayesUCB.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852301014,0
C,"Lengerich, B; Xing, EP; Caruana, R",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Lengerich, Benjamin; Xing, Eric P.; Caruana, Rich",,,Dropout as a Regularizer of Interaction Effects,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We examine Dropout through the perspective of interactions. This view provides a symmetry to explain Dropout: given N variables, there are ((N)(k)) possible sets of k variables to form an interaction (i.e. O(N-k)); conversely, the probability an interaction of k variables survives Dropout at rate p is (1-p)(k) (decaying with k). These rates effectively cancel, and so Dropout regularizes against higher-order interactions. We prove this perspective analytically and empirically. This perspective of Dropout as a regularizer against interaction effects has several practical implications: (1) higher Dropout rates should be used when we need stronger regularization against spurious high-order interactions, (2) caution should be exercised when interpreting Dropout-based explanations and uncertainty measures, and (3) networks trained with Input Dropout are biased estimators. We also compare Dropout to other regularizers and find that it is difficult to obtain the same selective pressure against high-order interactions with these methods.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852302001,0
C,"Maheshwari, C; Chiu, CY; Sastry, S; Ratliff, L; Mazumdar, E",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Maheshwari, Chinmay; Chiu, Chih-Yuan; Sastry, Shankar; Ratliff, Lillian; Mazumdar, Eric",,,Zeroth-Order Methods for Convex-Concave Minmax Problems: Applications to Decision-Dependent Risk Minimization,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Min-max optimization is emerging as a key framework for analyzing problems of robustness to strategically and adversarially generated data. We propose the random reshuffling-based gradient-free Optimistic Gradient Descent-Ascent algorithm for solving convex-concave min-max problems with finite sum structure. We prove that the algorithm enjoys the same convergence rate as that of zeroth-order algorithms for convex minimization problems. We deploy the algorithm to solve the distributionally robust strategic classification problem, where gradient information is not readily available, by reformulating the latter into a finite-dimensional convex concave min-max problem. Through illustrative simulations, we observe that our proposed approach learns models that are simultaneously robust against adversarial distribution shifts and strategic decisions from the data sources, and outperforms existing methods from the strategic classification literature.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852301007,0
C,"Mutny, M; Krause, A",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Mutny, Mojmir; Krause, Andreas",,,Sensing Cox Processes via Posterior Sampling and Positive Bases,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We study adaptive sensing of Cox point processes, a widely used model from spatial statistics. We introduce three tasks: maximization of captured events, search for the maximum of the intensity function and learning level sets of the intensity function. We model the intensity function as a sample from a truncated Gaussian process, represented in a specially constructed positive basis. In this basis, the positivity constraint on the intensity function has a simple form. We show how the minimal description positive basis can be adapted to the covariance kernel, to non-stationarity and make connections to common positive bases from prior works. Our adaptive sensing algorithms use Langevin dynamics and are based on posterior sampling (Cox-TnomPsoN) and top-two posterior sampling (ToP2) principles. With latter, the difference between samples serves as a surrogate to the uncertainty. We demonstrate the approach using examples from environmental monitoring and crime rate modeling, and compare it to the classical Bayesian experimental design approach.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852301019,0
C,"Stimper, V; Scholkopf, B; Hernandez-Lobato, JM",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Stimper, Vincent; Schoelkopf, Bernhard; Hernandez-Lobato, Jose Miguel",,,Resampling Base Distributions of Normalizing Flows,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Normalizing flows are a popular class of models for approximating probability distributions. However, their invertible nature limits their ability to model target distributions whose support have a complex topological structure, such as Boltzmann distributions. Several procedures have been proposed to solve this problem but many of them sacrifice invertibility and, thereby, tractability of the log-likelihood as well as other desirable properties. To address these limitations, we introduce a base distribution for normalizing flows based on learned rejection sampling, allowing the resulting normalizing flow to model complicated distributions without giving up bijectivity. Furthermore, we develop suitable learning algorithms using both maximizing the log-likelihood and the optimization of the Kullback-Leibler divergence, and apply them to various sample problems, i.e. approximating 2D densities, density estimation of tabular data, image generation, and modeling Boltzmann distributions. In these experiments our method is competitive with or outperforms the baselines.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072705002,0
C,"Tao, YM; Wu, YL; Zhao, P; Wang, D",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Tao, Youming; Wu, Yulian; Zhao, Peng; Wang, Di",,,Optimal Rates of (Locally) Differentially Private Heavy-tailed Multi-Armed Bandits,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"In this paper we investigate the problem of stochastic multi-armed bandits (MAB) in the (local) differential privacy (DP/LDP) model. Unlike previous results that assume bounded/sub-Gaussian reward distributions, we focus on the setting where each arm's reward distribution only has (1+v)-th moment with some v is an element of (0, 1]. In the first part, we study the problem in the central epsilon-DP model. We first provide a near-optimal result by developing a private and robust Upper Confidence Bound (UCB) algorithm. Then, we improve the result via a private and robust version of the Successive Elimination (SE) algorithm. Finally, we establish the lower bound to show that the instance-dependent regret of our improved algorithm is optimal. In the second part, we study the problem in the epsilon-LDP model. We propose an algorithm that can be seen as locally private and robust version of SE algorithm, which provably achieves (near) optimal rates for both instance-dependent and instance-independent regret. Our results reveal differences between the problem of private MAB with bounded/sub-Gaussian rewards and heavy-tailed rewards. To achieve these (near) optimal rates, we develop several new hard instances and private robust estimators as byproducts, which might be used to other related problems. Finally, experiments also support our theoretical findings and show the effectiveness of our algorithms.",,,,,,"Wu, Yulian/0000-0002-7187-2856",,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072701028,0
C,"Yu, PL; Ding, T; Bach, SH",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Yu, Peilin; Ding, Tiffany; Bach, Stephen H.",,,Learning from Multiple Noisy Partial Labelers,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Programmatic weak supervision creates models without hand-labeled training data by combining the outputs of heuristic labelers. Existing frameworks make the restrictive assumption that labelers output a single class label. Enabling users to create partial labelers that output subsets of possible class labels would greatly expand the expressivity of programmatic weak supervision. We introduce this capability by defining a probabilistic generative model that can estimate the underlying accuracies of multiple noisy partial labelers without ground truth labels. We show how to scale up learning, for example learning on 100k examples in one minute, a 300x speed up compared to a naive implementation. We also prove that this class of models is generically identifiable up to label swapping under mild conditions. We evaluate our framework on three text classification and six object classification tasks. On text tasks, adding partial labels increases average accuracy by 8.6 percentage points. On image tasks, we show that partial labels allow us to approach some zero-shot object classification problems with programmatic weak supervision by using class attributes as partial labelers. On these tasks, our framework has accuracy comparable to recent embedding-based zero-shot learning methods, while using only pre-trained attribute detectors.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852305026,0
C,"Zhu, YL; Nowak, R",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Zhu, Yinglun; Nowak, Robert",,,Pareto Optimal Model Selection in Linear Bandits,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We study model selection in linear bandits, where the learner must adapt to the dimension (denoted by d(*)) of the smallest hypothesis class containing the true linear model while balancing exploration and exploitation. Previous papers provide various guarantees for this model selection problem, but have limitations; i.e., the analysis requires favorable conditions that allow for inexpensive statistical testing to locate the right hypothesis class or are based on the idea of corralling multiple base algorithms, which often performs relatively poorly in practice. These works also mainly focus on upper bounds. In this paper, we establish the first lower bound for the model selection problem. Our lower bound implies that, even with a fixed action set, adaptation to the unknown dimension d, comes at a cost: There is no algorithm that can achieve the regret bound (O) over tilde(root d*T) simultaneously for all values of d(*). We propose Pareto optimal algorithms that match the lower bound. Empirical evaluations show that our algorithm enjoys superior performance compared to existing ones.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852301010,0
C,"Aggarwal, A; Kasiviswanathan, SP; Xu, ZK; Feyisetan, O; Teissier, N",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Aggarwal, Abhinav; Kasiviswanathan, Shiva Prasad; Xu, Zekun; Feyisetan, Oluwaseyi; Teissier, Nathanael",,,Reconstructing Test Labels From Noisy Loss Functions,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Machine learning classifiers rely on loss functions for performance evaluation, often on a private (hidden) dataset. In a recent line of research, label inference was introduced as the problem of reconstructing the ground truth labels of this private dataset from just the (possibly perturbed) cross-entropy loss function values evaluated at chosen prediction vectors (without any other access to the hidden dataset). In this paper, we formally study the necessary and sufficient conditions under which label inference is possible from any (noisy) loss function value. Using tools from analytical number theory, we show that a broad class of commonly used loss functions, including general Bregman divergence-based losses and multiclass cross-entropy with common activation functions like sigmoid and soft-max, it is possible to design label inference attacks that succeed even for arbitrary noise levels and using only a single query from the adversary. We formally study the computational complexity of label inference and show that while in general, designing adversarial prediction vectors for these attacks is co-NP-hard, once we have these vectors, the attacks can also be carried out through a lightweight augmentation to any neural network model, making them look benign and hard to detect. The observations in this paper provide a deeper understanding of the vulnerabilities inherent in modern machine learning and could be used for designing future trustworthy ML.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852303002,0
C,"Bello, K; Ke, CY; Honorio, J",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Bello, Kevin; Ke, Chuyang; Honorio, Jean",,,A View of Exact Inference in Graphs from the Degree-4 Sum-of-Squares Hierarchy,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Performing inference in graphs is a common task within several machine learning problems, e.g., image segmentation, community detection, among others. For a given undirected connected graph, we tackle the statistical problem of exactly recovering an unknown ground-truth binary labeling of the nodes from a single corrupted observation of each edge. Such problem can be formulated as a quadratic combinatorial optimization problem over the boolean hypercube, where it has been shown before that one can (with high probability and in polynomial time) exactly recover the ground-truth labeling of graphs that have an isoperimetric number that grows with respect to the number of nodes (e.g., complete graphs, regular expanders). In this work, we apply a powerful hierarchy of relaxations, known as the sum-of-squares (SoS) hierarchy, to the combinatorial problem. Motivated by empirical evidence on the improvement in exact recoverability, we center our attention on the degree-4 SoS relaxation and set out to understand the origin of such improvement from a graph theoretical perspective. We show that the solution of the dual of the relaxed problem is related to finding edge weights of the Johnson and Kneser graphs, where the weights fulfill the SoS constraints and intuitively allow the input graph to increase its algebraic connectivity. Finally, as byproduct of our analysis, we derive a novel Cheeger-type lower bound for the algebraic connectivity of graphs with signed edge weights.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,640,654,,,,,,,,,,,,,,,,WOS:000828072700027,0
C,"Chau, SL; Gonzalez, J; Sejdinovic, D",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Chau, Siu Lun; Gonzalez, Javier; Sejdinovic, Dino",,,Learning Inconsistent Preferences with Gaussian Processes,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We revisit widely used preferential Gaussian processes (PGP) by Chu and Ghahramani [2005] and challenge their modelling assumption that imposes rankability of data items via latent utility function values. We propose a generalisation of PGP which can capture more expressive latent preferential structures in the data and thus be used to model inconsistent preferences, i.e. where transitivity is violated, or to discover clusters of comparable items via spectral decomposition of the learned preference functions. We also consider the properties of associated covariance kernel functions and its reproducing kernel Hilbert Space (RKHS), giving a simple construction that satisfies universality in the space of preference functions. Finally, we provide an extensive set of numerical experiments on simulated and real-world datasets showcasing the competitiveness of our proposed method with state-of-the-art. Our experimental findings support the conjecture that violations of rankability are ubiquitous in real-world preferential data.",,,,,,"Sejdinovic, Dino/0000-0001-5547-9213",,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072702014,0
C,"Chhaya, R; Dasgupta, A; Choudhari, J; Shit, S",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Chhaya, Rachit; Dasgupta, Anirban; Choudhari, Jayesh; Shit, Supratim",,,On Coresets for Fair Regression and Individually Fair Clustering,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"In this paper we present coresets for Fair Regression with Statistical Parity (SP) constraints and for Individually Fair Clustering. Due to the fairness constraints, the classical coreset definition is not enough for these problems. We first define coresets for both the problems. We show that to obtain such coresets, it is sufficient to sample points based on the probabilities dependent on combination of sensitivity score and a carefully chosen term according to the fairness constraints. We give provable guarantees with relative error in preserving the cost and a small additive error in preserving fairness constraints for both problems. Since our coresets are much smaller in size as compared to n, the number of points, they can give huge benefits in computational costs (from polynomial to polylogarithmic in n), especially when n >= d, where d is the input dimension. We support our theoretical claims with experimental evaluations.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852304005,0
C,"Deng, YY; Kamani, MM; Mahdavi, M",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Deng, Yuyang; Kamani, Mohammad Mahdi; Mahdavi, Mehrdad",,,Local SGD Optimizes Overparameterized Neural Networks in Polynomial Time,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"In this paper we prove that Local (S)GD (or FedAvg) can optimize deep neural networks with Rectified Linear Unit (ReLU) activation function in polynomial time. Despite the established convergence theory of Local SGD on optimizing general smooth functions in communication-efficient distributed optimization, its convergence on non-smooth ReLU networks still eludes full theoretical understanding. The key property used in many Local SGD analysis on smooth function is gradient Lipschitzness, so that the gradient on local models will not drift far away from that on averaged model. However, this decent property does not hold in networks with non-smooth ReLU activation function. We show that, even though ReLU network does not admit gradient Lipschitzness property, the difference between gradients on local models and average model will not change too much, under the dynamics of Local SGD. We validate our theoretical results via extensive experiments. This work is the first to show the convergence of Local SGD on non-smooth functions, and will shed lights on the optimization theory of federated training of deep neural networks.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852301012,0
C,"Gales, S; Sethuraman, S; Jun, KS",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Gales, Spencer (Brady); Sethuraman, Sunder; Jun, Kwang-Sung",,,Norm-Agnostic Linear Bandits,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Linear bandits have a wide variety of applications including recommendation systems yet they make one strong assumption: the algorithms must know an upper bound S on the norm of the unknown parameter theta* that governs the reward generation. Such an assumption forces the practitioner to guess S involved in the confidence bound, leaving no choice but to wish that parallel to theta*parallel to <= S is true to guarantee that the regret will be low. In this paper, we propose novel algorithms that do not require such knowledge for the first time. Specifically, we propose two algorithms and analyze their regret bounds: one for the changing arm set setting and the other for the fixed arm set setting. Our regret bound for the former shows that the price of not knowing S does not affect the leading term in the regret bound and inflates only the lower order term. For the latter, we do not pay any price in the regret for now knowing S. Our numerical experiments show standard algorithms assuming knowledge of S can fail catastrophically when parallel to theta*parallel to <= S is not true whereas our algorithms enjoy low regret.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,73,91,,,,,,,,,,,,,,,,WOS:000828072700005,0
C,"Karlsson, RKA; Willbo, M; Hussain, Z; Krishnan, RG; Sontag, D; Johansson, FD",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Karlsson, Rickard K. A.; Willbo, Martin; Hussain, Zeshan; Krishnan, Rahul G.; Sontag, David; Johansson, Fredrik D.",,,Using Time-Series Privileged Information for Provably Efficient Learning of Prediction Models,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We study prediction of future outcomes with supervised models that use privileged information during learning. The privileged information comprises samples of time series observed between the baseline time of prediction and the future outcome; this information is only available at training time which differs from the traditional supervised learning. Our question is when using this privileged data leads to more sample-efficient learning of models that use only baseline data for predictions at test time. We give an algorithm for this setting and prove that when the time series are drawn from a non-stationary Gaussian-linear dynamical system of fixed horizon, learning with privileged information is more efficient than learning without it. On synthetic data, we test the limits of our algorithm and theory, both when our assumptions hold and when they are violated. On three diverse real-world datasets, we show that our approach is generally preferable to classical learning, particularly when data is scarce. Finally, we relate our estimator to a distillation approach both theoretically and empirically. [GRAPHICS] .",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072705022,0
C,"Le, T; Bui, A; Le, T; Zhao, H; Montague, P; Tran, Q; Phung, D",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Le, Trung; Bui, Anh; Tue Le; Zhao, He; Montague, Paul; Quan Tran; Dinh Phung",,,On Global-view Based Defense via Adversarial Attack and Defense Risk Guaranteed Bounds,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"It is well-known that deep neural networks (DNNs) are susceptible to adversarial attacks, which presents the most severe fragility of deep learning system. Despite achieving impressive performance, most of the current state-of-the-art classifiers remain highly vulnerable to crafted imperceptible, adversarial perturbations. Recent research attempts to understand neural network attack and defense have become increasingly urgent and important. While rapid progress has been made on this front, there is still an important theoretical gap in achieving guaranteed bounds on attack/defense models, leaving uncertainty in the certified guarantees of these models. To this end, we systematically address this problem in this paper. More specifically, we formulate attack and defense in a generic setting where there exists a family of adversaries (i.e., attackers) for attacking a family of classifiers (i.e., defenders). We develop a novel class of f-divergences suitable for measuring divergence among multiple distributions. This equips us to study the interactions between attackers and defenders in a countervailing game where we formulate a joint risk on attack and defense schemes. This is followed by our key results on guaranteed upper and lower bounds on this risk that can provide a better understanding of the behaviors of those parties from the attack and defense perspectives, thereby having important implications to both attack and defense sides. Finally, benefited from our theory, we propose an empirical approach that bases on global view to defend against adversarial attacks. The experimental results conducted on benchmark datasets show that the global-view if exploited appropriately can help to improve adversarial robustness.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852305043,0
C,"McRae, AD; Karnik, S; Davenport, MA; Muthukumar, V",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"McRae, Andrew D.; Karnik, Santhosh; Davenport, Mark A.; Muthukumar, Vidya",,,Harmless interpolation in regression and classification with structured features,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Overparametrized neural networks tend to perfectly fit noisy training data yet generalize well on test data. Inspired by this empirical observation, recent work has sought to understand this phenomenon of benign overfitting or harmless interpolation in the much simpler linear model. Previous theoretical work critically assumes that either the data features are statistically independent or the input data is high-dimensional; this precludes general nonparametric settings with structured feature maps. In this paper, we present a general and flexible framework for upper bounding regression and classification risk in a reproducing kernel Hilbert space. A key contribution is that our framework describes precise sufficient conditions on the data Gram matrix under which harmless interpolation occurs. Our results recover prior independent-features results (with a much simpler analysis), but they furthermore show that harmless interpolation can occur in more general settings such as features that are a bounded orthonormal system. Furthermore, our results show an asymptotic separation between classification and regression performance in a manner that was previously only shown for Gaussian features.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,5853,5875,,,,,,,,,,,,,,,,WOS:000841852300011,0
C,"Mohtashami, A; Jaggi, M; Stich, SU",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Mohtashami, Amirkeivan; Jaggi, Martin; Stich, Sebastian U.",,,Masked Training of Neural Networks with Partial Gradients,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"State-of-the-art training algorithms for deep learning models are based on stochastic gradient descent (SGD). Recently, many variations have been explored: perturbing parameters for better accuracy (such as in Extra-gradient), limiting SGD updates to a subset of parameters for increased efficiency (such as meProp) or a combination of both (such as Dropout). However, the convergence of these methods is often not studied in theory. We propose a unified theoretical framework to study such SGD variants-encompassing the aforementioned algorithms and additionally a broad variety of methods used for communication efficient training or model compression. Our insights can be used as a guide to improve the efficiency of such methods and facilitate generalization to new applications. As an example, we tackle the task of jointly training networks, a version of which (limited to sub-networks) is used to create Slimmable Networks. By training a low-rank Transformer jointly with a standard one we obtain superior performance than when it is trained separately.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,5876,5890,,,,,,,,,,,,,,,,WOS:000841852300012,0
C,"Nguyen, TD; Trippe, BL; Broderick, T",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Nguyen, Tin D.; Trippe, Brian L.; Broderick, Tamara",,,"Many Processors, Little Time: MCMC for Partitions via Optimal Transport Couplings","INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Markov chain Monte Carlo (MCMC) methods are often used in clustering since they guarantee asymptotically exact expectations in the infinite-time limit. In finite time, though, slow mixing often leads to poor performance. Modern computing environments offer massive parallelism, but naive implementations of parallel MCMC can exhibit substantial bias. In MCMC samplers of continuous random variables, Markov chain couplings can overcome bias. But these approaches depend crucially on paired chains meetings after a small number of transitions. We show that straightforward applications of existing coupling ideas to discrete clustering variables fail to meet quickly. This failure arises from the label-switching problem: semantically equivalent cluster relabelings impede fast meeting of coupled chains. We instead consider chains as exploring the space of partitions rather than partitions' (arbitrary) labelings. Using a metric on the partition space, we formulate a practical algorithm using optimal transport couplings. Our theory confirms our method is accurate and efficient. In experiments ranging from clustering of genes or seeds to graph colorings, we show the benefits of our coupling in the highly parallel, time-limited regime.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072703024,0
C,"Romanov, E; Ordentlich, O",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Romanov, Elad; Ordentlich, Or",,,Spiked Covariance Estimation from Modulo-Reduced Measurements,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Consider the rank-1 spiked model: X = root nu xi u + Z, where nu is the spike in- tensity, u is an element of Sk-1 is an unknown direction and xi similar to N(0,1), Z similar to N(0, I). Motivated by recent advances in analog-to-digital conversion, we study the problem of recovering u is an element of S(k-1 )from n i.i.d. modulo-reduced measurements Y = [X] mod Delta, focusing on the high-dimensional regime (k >> 1). We develop and analyze an algorithm that, for most directions u and nu = poly(k), estimates u to high accuracy using n = poly(k) measurements, provided that Delta greater than or similar to root log k. Up to constants, our algorithm accurately estimates u at the smallest possible Delta that allows (in an information-theoretic sense) to recover X from Y. A key step in our analysis involves estimating the probability that a line segment of length approximate to root nu in a random direction u passes near a point in the lattice Delta Z(k). Numerical experiments show that the developed algorithm performs well even in a non-asymptotic setting.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072701016,0
C,"Sivill, T; Flach, P",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Sivill, Torty; Flach, Peter",,,"LIMESegment: Meaningful, Realistic Time Series Explanations","INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"LIME (Locally Interpretable Model-Agnostic Explanations) has become a popular way of generating explanations for tabular, image and natural language models, providing insight into why an instance was given a particular classification. In this paper we adapt LIME to time series classification, an under-explored area with existing approaches failing to account for the structure of this kind of data. We frame the non-trivial challenge of adapting LIME to time series classification as the following open questions: What is a meaningful interpretable representation of a time series?, How does one realistically perturb a time series? and What is a local neighbourhood around a time series?. We propose solutions to all three questions and combine them into a novel time series explanation framework called LIMESegment, which outperforms existing adaptations of LIME to time series on a variety of classification tasks.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072703021,0
C,"Tenzer, Y; Dror, O; Nadler, B; Bilal, E; Kluger, Y",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Tenzer, Yaniv; Dror, Omer; Nadler, Boaz; Bilal, Erhan; Kluger, Yuval",,,Crowdsourcing Regression: A Spectral Approach,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Merging the predictions of multiple experts is a frequent task. When ground-truth response values are available, this merging is often based on the estimated accuracies of the experts. In various applications, however, the only available information are the experts' predictions on unlabeled test data, which do not allow to directly estimate their accuracies. Moreover, simple merging schemes such as majority voting in classification or the ensemble mean or median in regression, are clearly sub-optimal when some experts are more accurate than others. Focusing on regression tasks, in this work we propose U-PCR, a framework for unsupervised ensemble regression. Specifically, we develop spectral-based methods that under mild assumptions and in the absence of ground truth data, are able to estimate the mean squared error of the different experts and combine their predictions to a more accurate meta-learner. We provide theoretical support for U-PCR as well as empirical evidence for the validity of its underlying assumptions. On a variety of regression problems, we illustrate the improved accuracy of U-PCR over various unsupervised merging strategies. Finally, we also illustrate its applicability to unsupervised multi-class ensemble learning.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072705015,0
C,"Tian, L; So, AMC",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Tian, Lai; So, Anthony Man-Cho",,,Computing D-Stationary Points of rho-Margin Loss SVM,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"This paper is concerned with the algorithmic aspects of sharper stationarity of a nonconvex, nonsmooth, Clarke irregular machine learning model. We study the SVM problem with a rho-margin loss function, which is the margin theory generalization bound of SVM introduced in the learning theory textbook by Mohri et al. [2018], and has been extensively studied in operations research, statistics, and machine learning communities. However, due to its nonconvex, nonsmooth, and irregular nature, none of the existing optimization methods can efficiently compute a d(irectional)-stationary point, which turns out to be also a local minimum, for the rho-margin loss SVM problem. After a detailed discussion of various nonsmooth stationarity notions, we propose a highly efficient nonconvex semi-proximal ADMM-based scheme that provably computes d-stationary points and enjoys a local linear convergence rate. We report concrete examples to demonstrate the necessity of our assumptions. Numerical results verify the effectiveness of the new algorithm and complement our theoretical results.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072703038,0
C,"Xiao, CJ; Lee, I; Dai, B; Schuurmans, D; Szepesvari, C",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Xiao, Chenjun; Lee, Ilbin; Dai, Bo; Schuurmans, Dale; Szepesvari, Csaba",,,The Curse of Passive Data Collection in Batch Reinforcement Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"In high stake applications, active experimentation may be considered too risky and thus data are often collected passively. While in simple cases, such as in bandits, passive and active data collection are similarly effective, the price of passive sampling can be much higher when collecting data from a system with controlled states. The main focus of the current paper is the characterization of this price. For example, when learning in episodic finite state-action Markov decision processes (MDPs) with S states and A actions, we show that even with the best (but passively chosen) logging policy, Omega(A(min(S-1,)(H))/epsilon(2)) episodes are necessary (and sufficient) to obtain an epsilon-optimal policy, where H is the length of episodes. Note that this shows that the sample complexity blows up exponentially compared to the case of active data collection, a result which is not unexpected, but, as far as we know, have not been published beforehand and perhaps the form of the exact expression is a little surprising. We also extend these results in various directions, such as other criteria or learning in the presence of function approximation, with similar conclusions. A remarkable feature of our result is the sharp characterization of the exponent that appears, which is critical for understanding what makes passive learning hard.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852302036,0
C,"Yeh, RA; Hu, YT; Hasegawa-Johnson, M; Schwing, AG",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Yeh, Raymond A.; Hu, Yuan-Ting; Hasegawa-Johnson, Mark; Schwing, Alexander G.",,,Equivariance Discovery by Learned Parameter-Sharing,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Designing equivariance as an inductive bias into deep-nets has been a prominent approach to build effective models, e.g., a convolutional neural network incorporates translation equivariance. However, incorporating these inductive biases requires knowledge about the equivariance properties of the data, which may not be available, e.g., when encountering a new domain. To address this, we study how to discover interpretable equivariances from data. Specifically, we formulate this discovery process as an optimization problem over a model's parameter-sharing schemes. We propose to use the partition distance to empirically quantify the accuracy of the recovered equivariance. Also, we theoretically analyze the method for Gaussian data and provide a bound on the mean squared gap between the studied discovery scheme and the oracle scheme. Empirically, we show that the approach recovers known equivariances, such as permutations and shifts, on sum of numbers and spatially-invariant data.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072701027,0
C,"Zhu, JJ; Kouridi, C; Nemmour, Y; Scholkopf, B",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Zhu, Jia-Jie; Kouridi, Christina; Nemmour, Yassine; Schoelkopf, Bernhard",,,Adversarially Robust Kernel Smoothing,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We propose a scalable robust learning algorithm combining kernel smoothing and robust optimization. Our method is motivated by the convex analysis perspective of distributionally robust optimization based on probability metrics, such as the Wasserstein distance and the maximum mean discrepancy. We adapt the integral operator using supremal convolution in convex analysis to form a novel function majorant used for enforcing robustness. Our method is simple in form and applies to general loss functions and machine learning models. Exploiting a connection with optimal transport, we prove theoretical guarantees for certified robustness under distribution shift. Furthermore, we report experiments with general machine learning models, such as deep neural networks, to demonstrate competitive performance with the state-of-the-art certifiable robust learning algorithms based on the Wasserstein distance.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072705005,0
C,"Chewi, S; Gerber, P; Lu, C; Le Gouic, T; Rigollet, P",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Chewi, Sinho; Gerber, Patrik; Lu, Chen; Le Gouic, Thibaut; Rigollet, Philippe",,,Rejection sampling from shape-constrained distributions in sublinear time,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We consider the task of generating exact samples from a target distribution, known up to normalization, over a finite alphabet. The classical algorithm for this task is rejection sampling, and although it has been used in practice for decades, there is surprisingly little study of its fundamental limitations. In this work, we study the query complexity of rejection sampling in a minimax framework for various classes of discrete distributions. Our results provide new algorithms for sampling whose complexity scales sublinearly with the alphabet size. When applied to adversarial bandits, we show that a slight modification of the EXP3 algorithm reduces the per-iteration complexity from O(K) to O(log(K) log(K/delta)) with probability 1 - delta, where K is the number of arms.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072702013,0
C,"Dytso, A; Goldenbaum, M; Poor, HV; Shamai, S",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Dytso, Alex; Goldenbaum, Mario; Poor, H. Vincent; Shamai (Shitz), Shlomo",,,A Dimensionality Reduction Method for Finding Least Favorable Priors with a Focus on Bregman Divergence,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"A common way of characterizing minimax estimators in point estimation is by moving the problem into the Bayesian estimation domain and finding a least favorable prior distribution. The Bayesian estimator induced by a least favorable prior, under mild conditions, is then known to be minimax. However, finding least favorable distributions can be challenging due to inherent optimization over the space of probability distributions, which is infinite-dimensional. This paper develops a dimensionality reduction method that allows us to move the optimization to a finite-dimensional setting with an explicit bound on the dimension. The benefit of this dimensionality reduction is that it permits the use of popular algorithms such as projected gradient ascent to find least favorable priors. Throughout the paper, in order to make progress on the problem, we restrict ourselves to Bayesian risks induced by a relatively large class of loss functions, namely Bregman divergences.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852302021,0
C,"Eldowa, K; Bisi, L; Restelli, M",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Eldowa, Khaled; Bisi, Lorenzo; Restelli, Marcello",,,Finite Sample Analysis of Mean-Volatility Actor-Critic for Risk-Averse Reinforcement Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"The goal in the standard reinforcement learning problem is to find a policy that optimizes the expected return. However, such an objective is not adequate in a lot of real-life applications, like finance, where controlling the uncertainty of the outcome is imperative. The mean-volatility objective penalizes, through a tunable parameter, policies with high variance of the per-step reward. An interesting property of this objective is that it admits simple linear Bellman equations that resemble, up to a reward transformation, those of the risk-neutral case. However, the required reward transformation is policy-dependent, and requires the (usually unknown) expected return of the used policy. In this work, we propose two general methods for policy evaluation under the mean-volatility objective: the direct method and the factored method. We then extend recent results for finite sample analysis in the risk-neutral actor-critic setting to the mean-volatility case. Our analysis shows that the sample complexity to attain an epsilon-accurate stationary point is the same as that of the risk-neutral version, using either policy evaluation method for training the critic. Finally, we carry out experiments to test the proposed methods in a simple environment that exhibits some trade-off between optimality, in expectation, and uncertainty of outcome.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852304026,0
C,"Moskovitz, T; Arbel, M; Parker-Holder, J; Pacchiano, A",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Moskovitz, Ted; Arbel, Michael; Parker-Holder, Jack; Pacchiano, Aldo",,,Towards an Understanding of Default Policies in Multitask Policy Optimization,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Much of the recent success of deep reinforcement learning has been driven by regularized policy optimization (RPO) algorithms with strong performance across multiple domains. In this family of methods, agents are trained to maximize cumulative reward while penalizing deviation in behavior from some reference, or default policy. In addition to empirical success, there is a strong theoretical foundation for understanding RPO methods applied to single tasks, with connections to natural gradient, trust region, and variational approaches. However, there is limited formal understanding of desirable properties for default policies in the multitask setting, an increasingly important domain as the field shifts towards training more generally capable agents. Here, we take a first step towards filling this gap by formally linking the quality of the default policy to its effect on optimization. Using these results, we then derive a principled RPO algorithm for multitask learning with strong performance guarantees.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852305009,0
C,"Park, Y; Kan, K; Maddix, DC; Gasthaus, J; Aubet, FX; Wang, YY",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Park, Youngsuk; Kan, Kelvin; Maddix, Danielle C.; Gasthaus, Jan; Aubet, Francois-Xavier; Wang, Yuyang",,,Learning Quantile Functions without Quantile Crossing for Distribution-free Time Series Forecasting,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Quantile regression is an effective technique to quantify uncertainty, fit challenging underlying distributions, and often provide full probabilistic predictions through joint learnings over multiple quantile levels. A common drawback of these joint quantile regressions, however, is quantile crossing, which violates the desirable monotone property of the conditional quantile function. In this work, we propose the Incremental (Spline) Quantile Functions I(S)QF, a flexible and efficient distribution-free quantile estimation framework that resolves quantile crossing with a simple neural network layer. Moreover, I(S)QF inter/extrapolate to predict arbitrary quantile levels that differ from the underlying training ones. Equipped with the analytical evaluation of the continuous ranked probability score of I(S)QF representations, we apply our methods to NN-based time series forecasting cases, where the savings of the expensive re-training costs for non-trained quantile levels is particularly significant. We also provide a generalization error analysis of our proposed approaches under the sequence-to-sequence setting. Lastly, extensive experiments demonstrate the improvement of consistency and accuracy errors over other baselines.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852302024,0
C,"Roy, S; Choudhury, JR; Dutta, S",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Roy, Sarbojit; Choudhury, Jyotishka Ray; Dutta, Subhajit",,,"On Some Fast And Robust Classifiers For High Dimension, Low Sample Size Data","INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"In high dimension, low sample size (HDLSS) settings, distance concentration phenomena affects the performance of several popular classifiers which are based on Euclidean distances. The behaviour of these classifiers in high dimensions is completely governed by the first and second order moments of the underlying class distributions. Moreover, the classifiers become useless for such HDLSS data when the first two moments of the competing distributions are equal, or when the moments do not exist. In this work, we propose robust, computationally efficient and tuning-free classifiers applicable in the HDLSS scenario. As the data dimension increases, these classifiers yield perfect classification if the one-dimensional marginals of the underlying distributions are different. We establish strong theoretical properties for the proposed classifiers in ultrahigh-dimensional settings. Numerical experiments with a wide variety of simulated examples and analysis of real data sets exhibit clear and convincing advantages over existing methods.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852304021,0
C,"Sasaki, H; Hirayama, J; Kanamori, T",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Sasaki, Hiroaki; Hirayama, Jun-ichiro; Kanamori, Takafumi",,,Mode estimation on matrix manifolds: Convergence and robustness,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Data on matrix manifolds are ubiquitous on a wide range of research fields. The key issue is estimation of the modes (i.e., maxima) of the probability density function underlying the data. For instance, local modes (i.e., local maxima) can be used for clustering, while the global mode (i.e., the global maximum) is a robust alternative to the Frechet mean. Previously, to estimate the modes, an iterative method has been proposed based on a Riemannian gradient estimator and empirically showed the superior performance in clustering (Ashizawa et al., 2017). However, it has not been theoretically investigated if the iterative method is able to capture the modes based on the gradient estimator. In this paper, we propose simple iterative methods for mode estimation on matrix manifolds based on the Euclidean metric. A key contribution is to perform theoretical analysis and establish sufficient conditions for the monotonic ascending and convergence of the proposed iterative methods. In addition, for the previous method, we prove the monotonic ascending property towards a mode. Thus, our work can be also regarded as compensating for the lack of theoretical analysis in the previous method. Furthermore, the robustness of the iterative methods is theoretically investigated in terms of the breakdown point. Finally, the proposed methods are experimentally demonstrated to work well in clustering and robust mode estimation on matrix manifolds.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852302020,0
C,"Silva, JF; Tobar, F",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Silva, Jorge F.; Tobar, Felipe",,,On the Interplay between Information Loss and Operation Loss in Representations for Classification,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Information-theoretic measures have been widely adopted in the design of features for learning and decision problems. Inspired by this, we look at the relationship between i) a weak form of information loss in the Shannon sense and ii) operational loss in the minimum probability of error (MPE) sense when considering a family of lossy continuous representations of an observation. Our first result offers a lower bound on a weak form of information loss as a function of its respective operation loss when adopting a discrete lossy representation (quantization) instead of the original raw observation. From this, our main result shows that a specific form of vanishing information loss (a weak notion of asymptotic informational sufficiency) implies a vanishing MPE loss (or asymptotic operational sufficiency) when considering a family of lossy continuous representations. Our theoretical findings support the observation that the selection of feature representations that attempt to capture informational sufficiency is appropriate for learning, but this design principle is a rather conservative if the intended goal is achieving MPE in classification. On this last point, we discuss about studying weak forms of informational sufficiencies to achieve operational sufficiency in learning settings.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072704043,0
C,"Thekumparampil, KK; He, NA; Oh, S",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Thekumparampil, Kiran Koshy; He, Niao; Oh, Sewoong",,,Lifted Primal-Dual Method for Bilinearly Coupled Smooth Minimax Optimization,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We study the bilinearly coupled minimax problem: min(x) max(y) f (x) + < y, Ax > - h(y), where f and h are both strongly convex smooth functions and admit first-order gradient oracles. Surprisingly, no known first-order algorithms have hitherto achieved the lower complexity bound of Omega((root Lx/mu x + parallel to A parallel to/root mu(x)mu(y) + root L-y/mu(y)) log(1/epsilon)) for solving this problem up to an epsilon primal-dual gap in the general parameter regime, where L-x, L-y, mu(x), mu(y) are the corresponding smoothness and strongly convexity constants. We close this gap by devising the first optimal algorithm, the Lifted Primal-Dual (LPD) method. Our method lifts the objective into an extended form that allows both the smooth terms and the bilinear term to be handled optimally and seamlessly with the same primal-dual framework. Besides optimality, our method yields a desirably simple single-loop algorithm that uses only one gradient oracle call per iteration. Moreover, when f is just convex, the same algorithm applied to a smoothed objective achieves the nearly optimal iteration complexity. We also provide a direct single-loop algorithm, using the LPD method, that achieves the iteration complexity of O(root L-x/epsilon + parallel to A parallel to/root mu(y)epsilon + root L-y/epsilon). Numerical experiments on quadratic minimax problems and policy evaluation problems further demonstrate the fast convergence of our algorithm in practice.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072704016,0
C,"Wu, L",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Wu, Lei",,,Learning a Single Neuron for Non-monotonic Activation Functions,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We study the problem of learning a single neuron x -> sigma(w(T) x) with gradient descent (GD). All the existing positive results are limited to the case where sigma is monotonic. However, it is recently observed that non-monotonic activation functions outperform the traditional monotonic ones in many applications. To fill this gap, we establish learnability without assuming monotonicity. Specifically, when the input distribution is the standard Gaussian, we show that mild conditions on sigma (e.g., sigma has a dominating linear part) are sufficient to guarantee the learnability in polynomial time and polynomial samples. Moreover, with a stronger assumption on the activation function, the condition of input distribution can be relaxed to a non-degeneracy of the marginal distribution. We remark that our conditions on sigma are satisfied by practical non-monotonic activation functions, such as SiLU/Swish and GELU. We also discuss how our positive results are related to existing negative results on training two-layer neural networks.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072704011,0
C,"Wu, Y; Inkpen, D; El-Roby, A",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Wu, Yuan; Inkpen, Diana; El-Roby, Ahmed",,,Co-Regularized Adversarial Learning for Multi-Domain Text Classification,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Multi-domain text classification (MDTC) aims to leverage all available resources from multiple domains to learn a predictive model that can generalize well on these domains. Recently, many MDTC methods adopt adversarial learning, shared-private paradigm, and entropy minimization to yield state-of-the-art results. However, these approaches face three issues: (1) Minimizing domain divergence can not fully guarantee the success of domain alignment; (2) Aligning marginal feature distributions can not fully guarantee the discriminability of the learned features; (3) Standard entropy minimization may make the predictions on unlabeled data over-confident, deteriorating the discriminability of the learned features. In order to address the above issues, we propose a co-regularized adversarial learning (CRAL) mechanism for MDTC. This approach constructs two diverse shared latent spaces, performs domain alignment in each of them, and punishes the disagreements of these two alignments with respect to the predictions on unlabeled data. Moreover, virtual adversarial training (VAT) with entropy minimization is incorporated to impose consistency regularization to the CRAL method. Experiments show that our model outperforms state-of-the-art methods on two MDTC benchmarks.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852301006,0
C,"Ablin, P; Peyre, G",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Ablin, Pierre; Peyre, Gabriel",,,Fast and accurate optimization on the orthogonal manifold without retraction,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We consider the problem of minimizing a function over the manifold of orthogonal matrices. The majority of algorithms for this problem compute a direction in the tangent space, and then use a retraction to move in that direction while staying on the manifold. Unfortunately, the numerical computation of retractions on the orthogonal manifold always involves some expensive linear algebra operation, such as matrix inversion, exponential or square-root. These operations quickly become expensive as the dimension of the matrices grows. To bypass this limitation, we propose the landing algorithm which does not use retractions. The algorithm is not constrained to stay on the manifold but its evolution is driven by a potential energy which progressively attracts it towards the manifold. One iteration of the landing algorithm only involves matrix multiplications, which makes it cheap compared to its retraction counterparts. We provide an analysis of the convergence of the algorithm, and demonstrate its promises on large-scale and deep learning problems, where it is faster and less prone to numerical errors than retraction-based methods.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,5636,5657,,,,,,,,,,,,,,,,WOS:000841852300003,0
C,"Camuto, A; Willetts, M",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Camuto, Alexander; Willetts, Matthew",,,Variational Autoencoders: A Harmonic Perspective,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"In this work we study Variational Autoencoders (VAEs) from the perspective of harmonic analysis. By viewing a VAE's latent space as a Gaussian Space, a variety of measure space, we derive a series of results that show that the encoder variance of a VAE controls the frequency content of the functions parameterised by the VAE encoder and decoder neural networks. In particular we demonstrate that larger encoder variances reduce the high frequency content of these functions. Our analysis allows us to show that increasing this variance effectively induces a soft Lipschitz constraint on the decoder network of a VAE, which is a core contributor to the adversarial robustness of VAEs. We further demonstrate that adding Gaussian noise to the input of a VAE allows us to more finely control the frequency content and the Lipschitz constant of the VAE encoder networks. Finally, we show that the KL term of the VAE loss serves as single point of action for modulating the frequency content of both encoder and decoder networks; whereby upweighting this term decreases the high-frequency content of both networks. To support our theoretical analysis we run experiments using VAEs with small fully-connected neural networks and with larger convolutional networks, demonstrating empirically that our theory holds for a variety of neural network architectures.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072704031,0
C,"Jeong, Y; Lee, D; An, G; Son, C; Song, HO",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Jeong, Yeonwoo; Lee, Deokjae; An, Gaon; Son, Changyong; Song, Hyun Oh",,,Optimal channel selection with discrete QCQP,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Reducing the high computational cost of large convolutional neural networks is crucial when deploying the networks to resource-constrained environments. We first show the greedy approach of recent channel pruning methods ignores the inherent quadratic coupling between channels in the neighboring layers and cannot safely remove inactive weights during the pruning procedure. Furthermore, due to these inactive weights, the greedy methods cannot. guarantee to satisfy the given resource constraints and deviate with the true objective. In this regard, we propose a novel channel selection method that optimally selects channels via discrete QCQP, which provably prevents any inactive weights and guarantees to meet the resource constraints tightly in terms of FLOPs, memory usage, and network size. We also propose a quadratic model that accurately estimates the actual inference time of the pruned network, which allows us to adopt inference time as a resource constraint option. Furthermore, we generalize our method to extend the selection granularity beyond channels and handle non-sequential connections. Our experiments on CIFAR-10 and ImageNet show our proposed pruning method outperforms other fixed-importance channel pruning methods on various network architectures.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852302015,0
C,"Kallus, N; Zhou, A",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Kallus, Nathan; Zhou, Angela",,,Stateful Offline Contextual Policy Evaluation and Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We study off-policy evaluation and learning from sequential data in a structured class of Markov decision processes that arise from repeated interactions with an exogenous sequence of arrivals with contexts, which generate unknown individual-level responses to agent actions that induce known transitions. This is a relevant model, for example, for dynamic personalized pricing and other operations management problems in the presence of potentially high-dimensional user types. The individual-level response is not causally affected by the state variable. In this setting, we adapt doubly-robust estimation in the single-timestep setting to the sequential setting so that a state-dependent policy can be learned even from a single timestep's worth of data. We introduce a marginal MDP model and study an algorithm for off-policy learning, which can be viewed as fitted value iteration in the marginal MDP. We also provide structural results on when errors in the response model leads to the persistence, rather than attenuation, of error over time. In simulations, we show that the advantages of doubly-robust estimation in the single time-step setting, via unbiased and lower-variance estimation, can directly translate to improved out-of-sample policy performance. This structure-specific analysis sheds light on the underlying structure on a class of problems, operations research/management problems, often heralded as a real-world domain for offline RL, which are in fact qualitatively easier.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852305031,0
C,"Lale, S; Azizzadenesheli, K; Hassibi, B; Anandkumar, A",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Lale, Sahin; Azizzadenesheli, Kamyar; Hassibi, Babak; Anandkumar, Anima",,,Reinforcement Learning with Fast Stabilization in Linear Dynamical Systems,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"In this work, we study model-based reinforcement learning (RL) in unknown stabilizable linear dynamical systems. When learning a dynamical system, one needs to stabilize the unknown dynamics in order to avoid system blow-ups. We propose an algorithm that certifies fast stabilization of the underlying system by effectively exploring the environment with an improved exploration strategy. We show that the proposed algorithm attains (O) over bar(root T) regret after T time steps of agent-environment interaction. We also show that the regret of the proposed algorithm has only a polynomial dependence in the problem dimensions, which gives an exponential improvement over the prior methods. Our improved exploration method is simple, yet efficient, and it combines a sophisticated exploration policy in RL with an isotropic exploration strategy to achieve fast stabilization and improved regret. We empirically demonstrate that the proposed algorithm outperforms other popular methods in several adaptive control tasks.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072705019,0
C,"Li, Q; Wai, HT",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Li, Qiang; Wai, Hoi-To",,,State Dependent Performative Prediction with Stochastic Approximation,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"This paper studies the performative prediction problem which optimizes a stochastic loss function with data distribution that depends on the decision variable. We consider a setting where the agent(s) provides samples adapted to both the learner's and agent's previous states. The samples are then used by the learner to update his/her state to optimize a loss function. Such closed loop update dynamics is studied as a state dependent stochastic approximation (SA) algorithm, which is shown to find a fixed point known as the performative stable solution. Our setting captures the unforgetful nature and reliance on past experiences of agents. Our contributions are three-fold. First, we present a framework for modeling state dependent performative prediction with biased stochastic gradients driven by a controlled Markov chain whose transition probability depends on the learner's state. Second, we present a new finite-time performance analysis of the SA algorithm. We show that the expected squared distance to the performative stable solution decreases as O(1/k), where k is the iteration number. Third, numerical experiments verify our findings.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072703012,0
C,"Munteanu, A; Omlor, S; Peters, C",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Munteanu, Alexander; Omlor, Simon; Peters, Christian",,,p-Generalized Probit Regression and Scalable Maximum Likelihood Estimation via Sketching and Coresets,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We study the p-generalized probit regression model, which is a generalized linear model for binary responses. It extends the standard probit model by replacing its link function, the standard normal cdf, by a p-generalized normal distribution for p is an element of[1, infinity). The p-generalized normal distributions (Subbotin, 1923) are of special interest in statistical modeling because they fit much more flexibly to data. Their tail behavior can be controlled by choice of the parameter p, which influences the model's sensitivity to outliers. Special cases include the Laplace, the Gaussian, and the uniform distributions. We further show how the maximum likelihood estimator for p-generalized probit regression can be approximated efficiently up to a factor of (1 + epsilon) on large data by combining sketching techniques with importance subsampling to obtain a small data summary called coreset.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072702006,0
C,"Vaswani, S; Bachem, O; Totaro, S; Muller, R; Garg, S; Geist, M; Machado, MC; Castro, PS; Le Roux, N",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Vaswani, Sharan; Bachem, Olivier; Totaro, Simone; Mueller, Robert; Garg, Shivam; Geist, Matthieu; Machado, Marlos C.; Castro, Pablo Samuel; Le Roux, Nicolas",,,A general class of surrogate functions for stable and efficient reinforcement learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Common policy gradient methods rely on the maximization of a sequence of surrogate functions. In recent years, many such surrogate functions have been proposed, most without strong theoretical guarantees, leading to algorithms such as TRPO, PPO or MPO. Rather than design yet another surrogate function, we instead propose a general framework (FMA-PG) based on functional mirror ascent that gives rise to an entire family of surrogate functions. We construct surrogate functions that enable policy improvement guarantees, a property not shared by most existing surrogate functions. Crucially, these guarantees hold regardless of the choice of policy parameterization. Moreover, a particular instantiation of FMA-PG recovers important implementation heuristics (e.g., using forward vs reverse KL divergence) resulting in a variant of TRPO with additional desirable properties. Via experiments on simple reinforcement learning problems, we evaluate the algorithms instantiated by FMA-PG. The proposed framework also suggests an improved variant of PPO, whose robustness and efficiency we empirically demonstrate on the MuJoCo suite.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852303004,0
C,"Vieillard, N; Andrychowicz, M; Ftaichuk, A; Pietquin, O; Geist, M",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Vieillard, Nino; Andrychowicz, Marcin; Ftaichuk, Anton; Pietquin, Olivier; Geist, Matthieu",,,Implicitly Regularized RL with Implicit Q-values,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"The Q-function is a central quantity in many Reinforcement Learning (RL) algorithms for which RL agents behave following a (soft)greedy policy w.r.t. to Q. It is a powerful tool that allows action selection without a model of the environment and even without explicitly modeling the policy. Yet, this scheme can only be used in discrete action tasks, with small numbers of actions, as the softmax over actions cannot be computed exactly otherwise. More specifically, the usage of function approximation to deal with continuous action spaces in modern actor-critic architectures intrinsically prevents the exact computation of a softmax. We propose to alleviate this issue by parametrizing the Q-function implicitly, as the sum of a log-policy and a value function. We use the resulting parametrization to derive a practical off-policy deep RL algorithm, suitable for large action spaces, and that enforces the softmax relation between the policy and the Q-value. We provide a theoretical analysis of our algorithm: from an Approximate Dynamic Programming perspective, we show its equivalence to a regularized version of value iteration, accounting for both entropy and Kullback-Leibler regularization, and that enjoys beneficial error propagation results. We then evaluate our algorithm on classic control tasks, where its results compete with state-of-the-art methods.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072701020,0
C,"Wang, XL; Wang, P; So, AMC",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Wang, Xiaolu; Wang, Peng; So, Anthony Man-Cho",,,Exact Community Recovery over Signed Graphs,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Signed graphs encode similarity and dissimilarity relationships among different entities with positive and negative edges. In this paper, we study the problem of community recovery over signed graphs generated by the signed stochastic block model (SSBM) with two equal-sized communities. Our approach is based on the maximum likelihood estimation (MLE) of the SSBM. Unlike many existing approaches, our formulation reveals that the positive and negative edges of a signed graph should be treated unequally. We then propose a simple two-stage iterative algorithm for solving the regularized MLE. It is shown that in the logarithmic degree regime, the proposed algorithm can exactly recover the underlying communities in nearly-linear time at the information-theoretic limit. Numerical results on both synthetic and real data are reported to validate and complement our theoretical developments and demonstrate the efficacy of the proposed method.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852304009,0
C,"Wang, YJ; Izbicki, M",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Wang, Yujie; Izbicki, Mike",,,The Tree Loss: Improving Generalization with Many Classes,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Multi-class classification problems often have many semantically similar classes. For example, 90 of ImageNet's 1000 classes are for different breeds of dog. We should expect that these semantically similar classes will have similar parameter vectors, but the standard cross entropy loss does not enforce this constraint. We introduce the tree loss as a drop-in replacement for the cross entropy loss. The tree loss re-parameterizes the parameter matrix in order to guarantee that semantically similar classes will have similar parameter vectors. Using simple properties of stochastic gradient descent, we show that the tree loss's generalization error is asymptotically better than the cross entropy loss's. We then validate these theoretical results on synthetic data, image data (CIFAR100, ImageNet), and text data (Twitter).",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,6121,6133,,,,,,,,,,,,,,,,WOS:000841852300024,0
C,"Catalina, A; Burkner, P; Vehtari, A",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Catalina, Alejandro; Burkner, Paul; Vehtari, Aki",,,Projection Predictive Inference for Generalized Linear and Additive Multilevel Models,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Projection predictive inference is a decision theoretic Bayesian approach that decouples model estimation from decision making. Given a reference model previously built including all variables present in the data, projection predictive inference projects its posterior onto a constrained space of a subset of variables. Variable selection is then performed by sequentially adding relevant variables until predictive performance is satisfactory. Previously, projection predictive inference has been demonstrated only for generalized linear models (GLMs) and Gaussian processes (GPs) where it showed superior performance to competing variable selection procedures. In this work, we extend projection predictive inference to support variable and structure selection for generalized linear multilevel models (GLMMs) and generalized additive multilevel models (GAMMs). Our simulative and real-world experiments demonstrate that our method can drastically reduce the model complexity required to reach reference predictive performance and achieve good frequency properties.",,,,,,"Vehtari, Aki/0000-0003-2164-9469",,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072704024,0
C,"Fasoulakis, M; Markakis, E; Pantazis, Y; Varsos, C",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Fasoulakis, Michail; Markakis, Evangelos; Pantazis, Yannis; Varsos, Constantinos",,,Forward Looking Best-Response Multiplicative Weights Update Methods for Bilinear Zero-sum Games,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Our work focuses on extra gradient learning algorithms for finding Nash equilibria in bilinear zero-sum games. The proposed method, which can be formally considered as a variant of Optimistic Mirror Descent (Mertikopoulos et al., 2019), uses a large learning rate for the intermediate gradient step which essentially leads to computing (approximate) best response strategies against the profile of the previous iteration. Although counter-intuitive at first sight due to the irrationally large, for an iterative algorithm, intermediate learning step, we prove that the method guarantees last-iterate convergence to an equilibrium. Particularly, we show that the algorithm reaches first an eta(1/rho)-approximate Nash equilibrium, with rho > 1, by decreasing the Kullback-Leibler divergence of each iterate by at least Omega(eta(1+1/rho)), for sufficiently small learning rate eta, until the method becomes a contracting map, and converges to the exact equilibrium. Furthermore, we perform experimental comparisons with the optimistic variant of the multiplicative weights update method, by (Daskalakis and Panageas, 2019) and show that our algorithm has significant practical potential since it offersoffers substantial gains in terms of accelerated convergence.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852305027,0
C,"Hoffman, MD; Sountsov, P",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Hoffman, Matthew D.; Sountsov, Pavel",,,Tuning-Free Generalized Hamiltonian Monte Carlo,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Hamiltonian Monte Carlo (HMC) has become a go-to family of Markov chain Monte Carlo (MCMC) algorithms for Bayesian inference problems, in part because we have good procedures for automatically tuning its parameters. Much less attention has been paid to automatic tuning of generalized HMC (GHMC), in which the auxiliary momentum vector is partially updated frequently instead of being completely resampled infrequently. Since GHMC spreads progress over many iterations, it is not straightforward to tune GHMC based on quantities typically used to tune HMC such as average acceptance rate and squared jumped distance. In this work, we propose an ensemble-chain adaptation (ECA) algorithm for GHMC that automatically selects values for all of GHMC's tunable parameters each iteration based on statistics collected from a population of many chains. This algorithm is designed to make good use of SIMD hardware accelerators such as GPUs, allowing most chains to be updated in parallel each iteration. Unlike typical adaptive-MCMC algorithms, our ECA algorithm does not perturb the chain's stationary distribution, and therefore does not need to be frozen after warmup. Empirically, we find that the proposed algorithm quickly converges to its stationary distribution, producing accurate estimates of posterior expectations with relatively few gradient evaluations.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852302010,0
C,"Lai, JL; Domke, J; Sheldon, D",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Lai, Jinlin; Domke, Justin; Sheldon, Daniel",,,Variational Marginal Particle Filters,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Variational inference for state space models (SSMs) is known to be hard in general. Recent works focus on deriving variational objectives for SSMs from unbiased sequential Monte Carlo estimators. We reveal that the marginal particle filter is obtained from sequential Monte Carlo by applying Rao-Blackwellization operations, which sacrifices the trajectory information for reduced variance and differentiability. We propose the variational marginal particle filter (VMPF), which is a differentiable and reparameterizable variational filtering objective for SSMs based on an unbiased estimator. We find that VMPF with biased gradients gives tighter bounds than previous objectives, and the unbiased reparameterization gradients are sometimes beneficial.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,875,895,,,,,,,,,,,,,,,,WOS:000828072700037,0
C,"Lalchand, V; Ravuri, A; Lawrence, ND",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Lalchand, Vidhi; Ravuri, Aditya; Lawrence, Neil D.",,,Generalised Gaussian Process Latent Variable Models (GPLVM) with Stochastic Variational Inference,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Gaussian process latent variable models (GPLVM) are a flexible and non-linear approach to dimensionality reduction, extending classical Gaussian processes to an unsupervised learning context. The Bayesian incarnation of the GPLVM [Titsias and Lawrence, 2010] uses a variational framework, where the posterior over latent variables is approximated by a well-behaved variational family, a factorised Gaussian yielding a tractable lower bound. However, the non-factorisability of the lower bound prevents truly scalable inference. In this work, we study the doubly stochastic formulation of the Bayesian GPLVM model amenable with minibatch training. We show how this framework is compatible with different latent variable formulations and perform experiments to compare a suite of models. Further, we demonstrate how we can train in the presence of massively missing data and obtain high-fidelity reconstructions.We demonstrate the model's performance by benchmarking against the canonical sparse GPLVM for high dimensional data examples.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852302012,0
C,"Letham, B; Guan, P; Tymms, C; Bakshy, E; Shvartsman, M",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Letham, Benjamin; Guan, Phillip; Tymms, Chase; Bakshy, Eytan; Shvartsman, Michael",,,Look-Ahead Acquisition Functions for Bernoulli Level Set Estimation,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Level set estimation (LSE) is the problem of identifying regions where an unknown function takes values above or below a specified threshold. Active sampling strategies for efficient LSE have primarily been studied in continuous-valued functions. Motivated by applications in human psychophysics where common experimental designs produce binary responses, we study LSE active sampling with Bernoulli outcomes. With Gaussian process classification surrogate models, the look-ahead model posteriors used by state-of-the-art continuous-output methods are intractable. However, we derive analytic expressions for look-ahead posteriors of sublevel set membership, and show how these lead to analytic expressions for a class of look-ahead LSE acquisition functions, including information-based methods. Benchmark experiments show the importance of considering the global look-ahead impact on the entire posterior. We demonstrate a clear benefit to using this new class of acquisition functions on benchmark problems, and on a challenging real-world task of estimating a high-dimensional contrast sensitivity function.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852302039,0
C,"Mason, B; Camilleri, R; Mukherjee, S; Jamieson, K; Nowak, R; Jain, L",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Mason, Blake; Camilleri, Romain; Mukherjee, Subhojyoti; Jamieson, Kevin; Nowak, Robert; Jain, Lalit",,,Nearly Optimal Algorithms for Level Set Estimation,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"The level set estimation problem seeks to find all points in a domain chi where the value of an unknown function f : chi -> R exceeds a threshold alpha. The estimation is based on noisy function evaluations that may be acquired at sequentially and adaptively chosen locations in chi. The threshold value a can either be explicit and provided a priori, or implicit and defined relative to the optimal function value, i.e. alpha = (1 - epsilon)f(x(*)) for a given epsilon > 0 where f (x(*)) is the maximal function value and is unknown. In this work we provide a new approach to the level set estimation problem by relating it to recent adaptive experimental design methods for linear bandits in the Reproducing Kernel Hilbert Space (RKHS) setting. We assume that can be approximated by a function in the RKHS up to an unknown misspecification and provide novel algorithms for both the implicit and explicit cases in this setting with strong theoretical guarantees. Moreover, in the linear (kernel) setting, we show that our bounds are nearly optimal, namely, our upper bounds match existing lower bounds for threshold linear bandits. To our knowledge this work provides the first instance-dependent, non-asymptotic upper bounds on sample complexity of level-set estimation that match information theoretic lower bounds.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852302004,0
C,"Mutti, M; Del Col, S; Restelli, M",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Mutti, Mirco; Del Col, Stefano; Restelli, Marcello",,,Reward-Free Policy Space Compression for Reinforcement Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"In reinforcement learning, we encode the potential behaviors of an agent interacting with an environment into an infinite set of policies, the policy space, typically represented by a family of parametric functions. Dealing with such a policy space is a hefty challenge, which often causes sample and computation inefficiencies. However, we argue that a limited number of policies are actually relevant when we also account for the structure of the environment and of the policy parameterization, as many of them would induce very similar interactions, i.e., state-action distributions. In this paper, we seek for a reward-free compression of the policy space into a finite set of representative policies, such that, given any policy pi, the minimum Renyi divergence between the state-action distributions of the representative policies and the state-action distribution of pi is bounded. We show that this compression of the policy space can be formulated as a set cover problem, and it is inherently NP-hard. Nonetheless, we propose a game-theoretic reformulation for which a locally optimal solution can be efficiently found by iteratively stretching the compressed space to cover an adversarial policy. Finally, we provide an empirical evaluation to illustrate the compression procedure in simple domains, and its ripple effects in reinforcement learning.",,,,,,"Restelli, Marcello/0000-0002-6322-1076",,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072703013,0
C,"Panaganti, K; Kalathil, D",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Panaganti, Kishan; Kalathil, Dileep",,,Sample Complexity of Robust Reinforcement Learning with a Generative Model,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"The Robust Markov Decision Process (RMDP) framework focuses on designing control policies that are robust against the parameter uncertainties due to the mismatches between the simulator model and real-world settings. An RMDP problem is typically formulated as a max-min problem, where the objective is to find the policy that maximizes the value function for the worst possible model that lies in an uncertainty set around a nominal model. The standard robust dynamic programming approach requires the knowledge of the nominal model for computing the optimal robust policy. In this work, we propose a model-based reinforcement learning (RL) algorithm for learning an e-optimal robust policy when the nominal model is unknown. We consider three different forms of uncertainty sets, characterized by the total variation distance, chi-square divergence, and KL divergence. For each of these uncertainty sets, we give a precise characterization of the sample complexity of our proposed algorithm. In addition to the sample complexity results, we also present a formal analytical argument on the benefit of using robust policies. Finally, we demonstrate the performance of our algorithm on two benchmark problems.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852304004,0
C,"Sanford, C; Chatziafratis, V",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Sanford, Clayton; Chatziafratis, Vaggos",,,Expressivity of Neural Networks via Chaotic Itineraries beyond Sharkovsky's Theorem,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Given a target function f, how large must a neural network be in order to approximate f? Recent works examine this basic question on neural network expressivity from the lens of dynamical systems and provide novel depth-vs-width tradeoffs for a large family of functions f. They suggest that such tradeoffs are governed by the existence of periodic points or cycles in f. Our work, by further deploying dynamical systems concepts, illuminates a more subtle connection between periodicity and expressivity: we prove that periodic points alone lead to suboptimal depth-width tradeoffs and we improve upon them by demonstrating that certain chaotic itineraries give stronger exponential tradeoffs, even in regimes where previous analyses only imply polynomial gaps. Contrary to prior works, our bounds are nearly-optimal, tighten as the period increases, and handle strong notions of inapproximability (e.g., constant L 1 error). More broadly, we identify a phase transition to the chaotic regime that exactly coincides with an abrupt shift in other notions of function complexity, including VC-dimension and topological entropy.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852304001,0
C,"Tan, YS; Agarwal, A; Yu, B",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Tan, Yan Shuo; Agarwal, Abhineet; Yu, Bin",,,A cautionary tale on fitting decision trees to data from additive models: generalization lower bounds,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Decision trees are important both as interpretable models amenable to high-stakes decision-making, and as building blocks of ensemble methods such as random forests and gradient boosting. Their statistical properties, however, are not well understood. The most cited prior works have focused on deriving pointwise consistency guarantees for CART in a classical nonparametric regression setting. We take a different approach, and advocate studying the generalization performance of decision trees with respect to different generative regression models. This allows us to elicit their inductive bias, that is, the assumptions the algorithms make (or do not make) to generalize to new data, thereby guiding practitioners on when and how to apply these methods. In this paper, we focus on sparse additive generative models, which have both low statistical complexity and some nonparametric flexibility. We prove a sharp squared error generalization lower bound for a large class of decision tree algorithms fitted to sparse additive models with C-1 component functions. This bound is surprisingly much worse than the minimax rate for estimating such sparse additive models. The inefficiency is due not to greediness, but to the loss in power for detecting global structure when we average responses solely over each leaf, an observation that suggests opportunities to improve tree-based algorithms, for example, by hierarchical shrinkage. To prove these bounds, we develop new technical machinery, establishing a novel connection between decision tree estimation and rate-distortion theory, a sub-field of information theory.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852304008,0
C,"van der Zander, B; Wienobst, M; Blaser, M; Liskiewicz, M",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"van der Zander, Benito; Wienoebst, Marcel; Blaeser, Markus; Liskiewicz, Maciej",,,Identification in Tree-shaped Linear Structural Causal Models,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Linear structural equation models represent direct causal effects as directed edges and confounding factors as bidirected edges. An open problem is to identify the causal parameters from correlations between the nodes. We investigate models, whose directed component forms a tree, and show that there, besides classical instrumental variables, missing cycles of bidirected edges can be used to identify the model. They can yield systems of quadratic equations that we explicitly solve to obtain one or two solutions for the causal parameters of adjacent directed edges. We show how multiple missing cycles can be combined to obtain a unique solution. This results in an algorithm that can identify instances that previously required approaches based on Grobner bases, which have doubly-exponential time complexity in the number of structural parameters.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852301009,0
C,"Wang, J; Gao, R; Xie, Y",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Wang, Jie; Gao, Rui; Xie, Yao",,,Two-Sample Test with Kernel Projected Wasserstein Distance,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We develop a kernel projected Wasserstein distance for the two-sample test, an essential building block in statistics and machine learning: given two sets of samples, to determine whether they are from the same distribution. This method operates by finding the nonlinear mapping in the data space which maximizes the distance between projected distributions. In contrast to existing works about projected Wasserstein distance, the proposed method circumvents the curse of dimensionality more efficiently. We present practical algorithms for computing this distance function together with the non-asymptotic uncertainty quantification of empirical estimates. Numerical examples validate our theoretical results and demonstrate good performance of the proposed method.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852302019,0
C,"Wu, CE; Masoomi, A; Gretton, A; Dy, J",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Wu, Chieh; Masoomi, Aria; Gretton, Arthur; Dy, Jennifer",,,Deep Layer-wise Networks Have Closed-Form Weights,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"There is currently a debate within the neuroscience community over the likelihood of the brain performing backpropagation (BP). To better mimic the brain, training a network one layer at a time with only a single forward pass has been proposed as an alternative to bypass BP; we refer to these networks as layer-wise networks. We continue the work on layer-wise networks by answering two outstanding questions. First, do they have a closed-form solution? Second, how do we know when to stop adding more layers? This work proves that the Kernel Mean Embedding is the closed-form weight that achieves the network global optimum while driving these networks to converge towards a highly desirable kernel for classification; we call it the Neural Indicator Kernel.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,188,225,,,,,,,,,,,,,,,,WOS:000828072700010,0
C,"Cardoso, AR; Rogers, R",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Cardoso, Adrian Rivera; Rogers, Ryan",,,Differentially Private Histograms under Continual Observation: Streaming Selection into the Unknown,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We generalize the continuous observation privacy setting from Dwork et al. (2010a) and Chan et al. (2011) by allowing each event in a stream to be a subset of some (possibly unknown) universe of items. We design differentially private (DP) algorithms for histograms in several settings, including top-k selection, with privacy loss that scales with polylog(T), where T is the maximum length of the input stream. We present a meta-algorithm that can use existing one-shot top-k private algorithms as a subroutine to continuously release DP histograms from a stream. Further, we present more practical DP algorithms for two settings: 1) continuously releasing the top-k counts from a histogram over a known domain when an event can consist of an arbitrary number of items, and 2) continuously releasing histograms over an unknown domain when an event has few items.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072702020,0
C,"Chatalic, A; Carratino, L; De Vito, E; Rosasco, L",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Chatalic, Antoine; Carratino, Luigi; De Vito, Ernesto; Rosasco, Lorenzo",,,Mean Nystrom Embeddings for Adaptive Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Compressive learning is an approach to efficient large scale learning based on compressing an entire dataset to a single mean embedding (the sketch), i.e. a vector of generalized moments. The learning task is then approximately solved a S an inverse problem using an adapted parametric model. Previous works in this context have focused on sketches obtained by averaging random features, that while universal can be poorly adapted to the problem at hand. In this paper, we propose and study the idea of performing sketching based on data-dependent Nystrom approximation. From a theoretical perspective we prove that the excess risk can be controlled under a geometric assumption relating the parametric model used to learn from the sketch and the covariance operator as sociated to the task at hand. Empirically, we show for k-means clustering and Gaussian modeling that for a fixed sketch size, Nystrom sketches indeed outperform those built with random features.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852304018,0
C,"Cherief-Abdellatif, BE; Shi, YY; Doucet, ARN; Guedj, BAM",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Cherief-Abdellatif, Badr-Eddine; Shi, Yuyang; Doucet, Arnaud; Guedj, Benjamin",,,On PAC-Bayesian reconstruction guarantees for VAEs,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Despite its wide use and empirical successes, the theoretical understanding and study of the behaviour and performance of the variational autoencoder (VAE) have only emerged in the past few years. We contribute to this recent line of work by analysing the VAE's reconstruction ability for unseen test data, leveraging arguments from the PAC-Bayes theory. We provide generalisation bounds on the theoretical reconstruction error, and provide insights on the regularisation effect of VAE objectives. We illustrate our theoretical results with supporting experiments on classical benchmark datasets.",,,,,,"Guedj, Benjamin/0000-0003-1237-7430",,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072703007,0
C,"Cisneros-Velarde, P; Bullo, F",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Cisneros-Velarde, Pedro; Bullo, Francesco",,,A Contraction Theory Approach to Optimization Algorithms from Acceleration Flows,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Much recent interest has focused on the design of optimization algorithms from the discretization of an associated optimization flow, i.e., a system of differential equations (ODEs) whose trajectories solve an associated optimization problem. Such a design approach poses an important problem: how to find a principled methodology to design and discretize appropriate ODEs. This paper aims to provide a solution to this problem through the use of contraction theory. We first introduce general mathematical results that explain how contraction theory guarantees the stability of the implicit and explicit Euler integration methods. Then, we propose a novel system of ODEs, namely the Accelerated-Contracting-Nesterov flow, and use contraction theory to establish it is an optimization flow with exponential convergence rate, from which the linear convergence rate of its associated optimization algorithm is immediately established. Remarkably, a simple explicit Euler discretization of this flow corresponds to the Nesterov acceleration method. Finally, we present how our approach leads to performance guarantees in the design of optimization algorithms for time-varying optimization problems.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072701017,0
C,"Emmenegger, N; Kyng, R; Zehmakan, AN",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Emmenegger, Nicolas; Kyng, Rasmus; Zehmakan, Ahad N.",,,On the Oracle Complexity of Higher-Order Smooth Non-Convex Finite-Sum Optimization,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We prove lower bounds for higher-order methods in smooth non-convex finite-sum optimization. Our contribution is threefold: We first show that a deterministic algorithm cannot profit from the finite-sum structure of the objective, and that simulating a pth-order regularized method on the whole function by constructing exact gradient information is optimal up to constant factors. We further show lower bounds for randomized algorithms and compare them with the best known upper bounds. To address some gaps between the bounds, we propose a new second-order smoothness assumption that can be seen as an analogue of the first-order mean-squared smoothness assumption. We prove that it is sufficient to ensure state-ofthe-art convergence guarantees, while allowing for a sharper lower bound.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852305012,0
C,"Farhadi, A; Hajiaghayi, M; Shi, E",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Farhadi, Alireza; Hajiaghayi, MohammadTaghi; Shi, Elaine",,,Differentially Private Densest Subgraph,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Given a graph, the densest subgraph problem asks for a set of vertices such that the average degree among these vertices is maximized. Densest subgraph has numerous applications in learning, e.g., community detection in social networks, link spam detection, correlation mining, bioinformatics, and so on. Although there are efficient algorithms that output either exact or approximate solutions to the densest subgraph problem, existing algorithms may violate the privacy of the individuals in the network, e.g., leaking the existence/non-existence of edges. In this paper, we study the densest subgraph problem in the framework of differential privacy, and we derive upper and lower bounds for this problem. We show that there exists a linear-time 6-differentially private algorithm that finds a 2-approximation of the densest subgraph with an extra poly-logarithmic additive error. Our algorithm not only reports the approximate density of the densest subgraph, but also reports the vertices that form the dense subgraph. Our upper bound almost matches the famous 2-approximation by Charikar both in performance and in approximation ratio, but we additionally achieve differential privacy. In comparison with Charikar's algorithm, our algorithm has an extra poly-logarithmic additive error. We partly justify the additive error with a new lower bound, showing that for any differentially private algorithm that provides a constant-factor approximation, a sub-logarithmic additive error is inherent. We also practically study our differentially private algorithm on real-world graphs, and we show that in practice the algorithm finds a solution which is very close to the optimal.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852306005,0
C,"Garcelon, E; Perchet, V; Pirotta, M",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Garcelon, Evrard; Perchet, Vianney; Pirotta, Matteo",,,Encrypted Linear Contextual Bandit,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Contextual bandit is a general framework for online learning in sequential decision-making problems that has found application in a wide range of domains, including recommendation systems, online advertising, and clinical trials. A critical aspect of bandit methods is that they require to observe the contexts -i.e., individual or group-level data- and rewards in order to solve the sequential problem. The large deployment in industrial applications has increased interest in methods that preserve the users' privacy. In this paper, we introduce a privacy-preserving bandit framework based on homomorphic encryptionwhich allows computations using encrypted data. The algorithm only observes encrypted information (contexts and rewards) and has no ability to decrypt it. Leveraging the properties of homomorphic encryption, we show that despite the complexity of the setting, it is possible to solve linear contextual bandits over encrypted data with a (O) over tilde (d root T) regret bound in any linear contextual bandit problem, while keeping data encrypted.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072702026,0
C,"Ginart, AA; Zhang, MJ; Zou, J",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Ginart, Antonio A.; Zhang, Martin Jinye; Zou, James",,,MLDemon: Deployment Monitoring for Machine Learning Systems,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Post-deployment monitoring of ML systems is critical for ensuring reliability, especially as new user inputs can differ from the training distribution. Here we propose a novel approach, MLDEMON, for ML DEployment MONitoring. MLDEMON integrates both unlabeled data and a small amount of on-demand labels to produce a real-time estimate of the ML model's current performance on a given data stream. Subject to budget constraints, MLDEMON decides when to acquire additional, potentially costly, expert supervised labels to verify the model. On temporal datasets with diverse distribution drifts and models, MLDEMON outperforms existing approaches. Moreover, we provide theoretical analysis to show that MLDEMON is minimax rate optimal for a broad class of distribution drifts.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072704003,0
C,"Jose, ST; Park, S; Simeone, O",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Jose, Sharu Theresa; Park, Sangwoo; Simeone, Osvaldo",,,Information-Theoretic Analysis of Epistemic Uncertainty in Bayesian Meta-learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"The overall predictive uncertainty of a trained predictor can be decomposed into separate contributions due to epistemic and aleatoric uncertainty. Under a Bayesian formulation, assuming a well-specified model, the two contributions can be exactly expressed (for the log-loss) or bounded (for more general losses) in terms of information-theoretic quantities (Xu and Raginsky, 2020). This paper addresses the study of epistemic uncertainty within an information-theoretic framework in the broader setting of Bayesian meta-learning. A general hierarchical Bayesian model is assumed in which hyperparameters determine the per-task priors of the model parameters. Exact characterizations (for the log-loss) and bounds (for more general losses) are derived for the epistemic uncertainty - quantified by the minimum excess meta-risk (MEMR) - of optimal meta-learning rules. This characterization is leveraged to bring insights into the dependence of the epistemic, uncertainty on the number of tasks and on the amount of per-task training data. Experiments are presented that use the proposed information-theoretic bounds, evaluated via neural mutual information estimators, to compare the performance of conventional learning and meta-learning as the number of meta-learning tasks increases.",,,,,,"Jose, Sharu Theresa/0000-0001-8872-3462",,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852304013,0
C,"Li, SJ; Lopez-Garcia, M; Lawrence, ND; Cutillo, L",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Li, Sijia; Lopez-Garcia, Martin; Lawrence, Neil D.; Cutillo, Luisa",,,Two-way Sparse Network Inference for Count Data,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Classically, statistical datasets have a larger number of data points than features (n > p). The standard model of classical statistics caters for the case where data points are considered conditionally independent given the parameters. However, for n approximate to p or p > n such models are poorly determined. Kalaitzis et al. (2013) introduced the Bigraphical Lasso, an estimator for sparse precision matrices based on the Cartesian product of graphs. Unfortunately, the original Bigraphical Lasso algorithm is not applicable in case of large p and n due to memory requirements. We exploit eigenvalue decomposition of the Cartesian product graph to present a more efficient version of the algorithm which reduces memory requirements from O(n(2)p(2)) to O(n(2) + p(2)). Many datasets in different application fields, such as biology, medicine and social science, come with count data, for which Gaussian based models are not applicable. Our multiway network inference approach can be used for discrete data. Our methodology accounts for the dependencies across both instances and features, reduces the computational complexity for high dimensional data and enables to deal with both discrete and continuous data. Numerical studies on both synthetic and real datasets are presented to showcase the performance of our method.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852305018,0
C,"Ling, ZN; Zhou, F; Wei, M; Zhang, QS",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Ling, Zenan; Zhou, Fan; Wei, Meng; Zhang, Quanshi",,,Exploring Image Regions Not Well Encoded by an INN,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"This paper proposes a method to clarify image regions that are not well encoded by an invertible neural network (INN), i.e. image regions that significantly decrease the likelihood of the input image. The proposed method can diagnose the limitation of the representation capacity of an INN. Given an input image, our method extracts image regions, which are not well encoded, by maximizing the likelihood of the image. We explicitly model the distribution of not-well-encoded regions. A metric is proposed to evaluate the extraction of the not-well-encoded regions. Finally, we use the proposed method to analyze several state-of-the-art INNs trained on various benchmark datasets.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,483,509,,,,,,,,,,,,,,,,WOS:000828072700020,0
C,"Mejia, SHG; Kirschbaum, E; Janzing, D",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Mejia, Sergio Hernan Garrido; Kirschbaum, Elke; Janzing, Dominik",,,Obtaining Causal Information by Merging Datasets with MAXENT,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"The investigation of the question which treatment has a causal effect on a target variable? is of particular relevance in a large number of scientific disciplines. This challenging task becomes even more difficult if not all treatment variables were or even cannot be observed jointly with the target variable. In this paper, we discuss how causal knowledge can be obtained without having observed all variables jointly, but by merging the statistical information from different datasets. We show how the maximum entropy principle can be used to identify edges among random variables when assuming causal sufficiency and an extended version of faithfulness, and when only subsets of the variables have been observed jointly.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,581,603,,,,,,,,,,,,,,,,WOS:000828072700024,0
C,"Panda, A; Mahloujifar, S; Bhagoji, AN; Chakraborty, S; Mittal, P",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Panda, Ashwinee; Mahloujifar, Saeed; Bhagoji, Arjun N.; Chakraborty, Supriyo; Mittal, Prateek",,,SparseFed: Mitigating Model Poisoning Attacks in Federated Learning with Sparsification,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Federated learning is inherently vulnerable to model poisoning attacks because its decentralized nature allows attackers to participate with compromised devices. In model poisoning attacks, the attacker reduces the model's performance on targeted sub-tasks (e.g. classifying planes as birds) by uploading poisoned updates. In this report we introduce SparseFed, a novel defense that uses global top-k update sparsification and device-level gradient clipping to mitigate model poisoning attacks. We propose a theoretical framework for analyzing the robustness of defenses against poisoning attacks, and provide robustness and convergence analysis of our algorithm. To validate its empirical efficacy we conduct an open-source evaluation at scale across multiple benchmark datasets for computer vision and federated learning.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852302003,0
C,"Shirzad, H; Hajimirsadeghi, H; Abdi, AH; Mori, G",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Shirzad, Hamed; Hajimirsadeghi, Hossein; Abdi, Amir H.; Mori, Greg",,,TD-GEN: Graph Generation Using Tree Decomposition,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We propose TD-GEN, a graph generation framework based on tree decomposition, and introduce a reduced upper bound on the maximum number of decisions needed for graph generation. The framework includes a permutation invariant tree generation model which forms the backbone of graph generation. Tree nodes are supernodes, each representing a cluster of nodes in the graph. Graph nodes and edges are incrementally generated inside the clusters by traversing the tree supernodes, respecting the structure of the tree decomposition, and following node sharing decisions between the clusters. Further, we discuss the shortcomings of the standard evaluation criteria based on statistical properties of the generated graphs. We propose to compare the generalizability of models based on expected likelihood. Empirical results on a variety of standard graph generation datasets demonstrate the superior performance of our method.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072705024,0
C,"Nguyen-Duc, T; Le, T; Zhao, H; Cai, JF; Phung, D",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Thanh Nguyen-Duc; Trung Le; Zhao, He; Cai, Jianfei; Dinh Phung",,,Particle-based Adversarial Local Distribution Regularization,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Adversarial training defense (ATD) and virtual adversarial training (VAT) are the two most effective methods to improve model robustness against attacks and model generalization. While ATD is usually applied in robust machine learning, VAT is used in semi-supervised learning and domain adaption. In this paper, we introduce a novel adversarial local distribution regularization. The adversarial local distribution is defined by a set of all adversarial examples within a ball constraint given a natural input. We illustrate this regularization is a general form of previous methods (e.g., PGD, TRADES, VAT and VADA). We conduct comprehensive experiments on MNIST, SVHN and CIFAR10 to illustrate that our method outperforms well-known methods such as PGD, TRADES and ADT in robust machine learning, VAT in semi-supervised learning and VADA in domain adaption. Our implementation is on Github: https://github.com/ PotatoThanh/ALD-Regularization.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072705014,0
C,"van der Hoeven, D; Cesa-Bianchi, N",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"van der Hoeven, Dirk; Cesa-Bianchi, Nicolo",,,Nonstochastic Bandits and Experts with Arm-Dependent Delays,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We study nonstochastic bandits and experts in a delayed setting where delays depend on both time and arms. While the setting in which delays only depend on time has been extensively studied, the arm-dependent delay setting better captures real-world applications at the cost of introducing new technical challenges. In the full information (experts) setting, we design an algorithm with a first-order regret bound that reveals an interesting trade-off between delays and losses. We prove a similar first-order regret bound also for the bandit setting, when the learner is allowed to observe how many losses are missing. These are the first bounds in the delayed setting that depend on the losses and delays of the best arm only. When in the bandit setting no information other than the losses is observed, we still manage to prove a regret bound through a modification to the algorithm of Zimmert and Seldin (2020). Our analyses hinge on a novel bound on the drift, measuring how much better an algorithm can perform when given a look-ahead of one round.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072702004,0
C,"Wang, YJ; Lin, L; Chen, JH",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Wang, Yujia; Lin, Lu; Chen, Jinghui",,,Communication-Compressed Adaptive Gradient Method for Distributed Nonconvex Optimization,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Due to the explosion in the size of the training datasets, distributed learning has received growing interest in recent years. One of the major bottlenecks is the large communication cost between the central server and the local workers. While error feedback compression has been proven to be successful in reducing communication costs with stochastic gradient descent (SGD), there are much fewer attempts in building communication-efficient adaptive gradient methods with provable guarantees, which are widely used in training large-scale machine learning models. In this paper, we propose a new communication-compressed AMSGrad for distributed nonconvex optimization problem, which is provably efficient. Our proposed distributed learning framework features an effective gradient compression strategy and a worker-side model update design. We prove that the proposed communication-efficient distributed adaptive gradient method converges to the first-order stationary point with the same iteration complexity as uncompressed vanilla AMSGrad in the stochastic nonconvex optimization setting. Experiments on various benchmarks back up our theory.",,,,,,"Chen, Jinghui/0000-0002-1486-4526",,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,6292,6320,,,,,,,,,,,,,,,,WOS:000841852300032,0
C,"Weinberger, E; Beebe-Wang, N; Lee, SI",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Weinberger, Ethan; Beebe-Wang, Nicasia; Lee, Su-In",,,Moment Matching Deep Contrastive Latent Variable Models,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"In the contrastive analysis (CA) setting, machine learning practitioners are specifically interested in discovering patterns that are enriched in a target dataset as compared to a background dataset generated from sources of variation irrelevant to the task at hand. For example, a biomedical data analyst may seek to understand variations in genomic data only present among patients with a given disease as opposed to those also present in healthy control subjects. Such scenarios have motivated the development of contrastive latent variable models to isolate variations unique to these target datasets from those shared across the target and background datasets, with current state of the art models based on the variational autoencoder (VAE) framework. However, previously proposed models do not explicitly enforce the constraints on latent variables underlying CA, potentially leading to the undesirable leakage of information between the two sets of latent variables. Here we propose the moment matching contrastive VAE (MM-cVAE), a reformulation of the VAE for CA that uses the maximum mean discrepancy to explicitly enforce two crucial latent variable constraints underlying CA. On three challenging CA tasks we find that our method outperforms the previous state-of-the-art both qualitatively and on a set of quantitative metrics.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072702018,0
C,"Wild, V; Wynne, G",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Wild, Veit; Wynne, George",,,Variational Gaussian Processes: A Functional Analysis View,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,Variational Gaussian process (GP) approximations have become a standard tool in fast GP inference. This technique requires a user to select variational features to increase efficiency. So far the common choices in the literature are disparate and lacking generality. We propose to view the GP as lying in a Banach space which then facilitates a unified perspective. This is used to understand the relationship between existing features and to draw a connection between kernel ridge regression and variational GP approximations.,,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072705004,0
C,"Zhou, ZY; Gong, ZY; Ravikumar, P; Inouye, DI",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Zhou, Zeyu; Gong, Ziyu; Ravikumar, Pradeep; Inouye, David I.",,,Iterative Alignment Flows,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"The unsupervised task of aligning two or more distributions in a shared latent space has many applications including fair representations, batch effect mitigation, and unsupervised domain adaptation. Existing flow-based approaches estimate multiple flows independently, which is equivalent to learning multiple full generative models. Other approaches require adversarial learning, which can be computationally expensive and challenging to optimize. Thus, we aim to jointly align multiple distributions while avoiding adversarial learning. Inspired by efficient alignment algorithms from optimal transport (OT) theory for univariate distributions, we develop a simple iterative method to build deep and expressive flows. Our method decouples each iteration into two subproblems: 1) form a variational approximation of a distribution divergence and 2) minimize this variational approximation via closed-form invertible alignment maps based on known OT results. Our empirical results give evidence that this iterative algorithm achieves competitive distribution alignment at low computational cost while being able to naturally handle more than two distributions.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,6409,6444,,,,,,,,,,,,,,,,WOS:000841852300037,0
C,"Chevallier, A; Cazals, F; Fearnhead, P",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Chevallier, Augustin; Cazals, Frederic; Fearnhead, Paul",,,Efficient computation of the volume of a polytope in high-dimensions using Piecewise Deterministic Markov Processes,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Computing the volume of a polytope in high dimensions is computationally challenging but has wide applications. Current state-of-the-art algorithms to compute such volumes rely on efficient sampling of a Gaussian distribution restricted to the polytope, using e.g. Hamiltonian Monte Carlo. We present a new sampling strategy that uses a Piecewise Deterministic Markov Process. Like Hamiltonian Monte Carlo, this new method involves simulating trajectories of a non-reversible process and inherits similar good mixing properties. However, importantly, the process can be simulated more easily due to its piecewise linear trajectories and this leads to a reduction of the computational cost by a factor of the dimension of the space. Our experiments indicate that our method is numerically robust and is one order of magnitude faster (or better) than existing methods using Hamiltonian Monte Carlo. On a single core processor, we report computational time of a few minutes up to dimension 500.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852304030,0
C,"Engelhard, M; Henao, R",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Engelhard, Matthew; Henao, Ricardo",,,Disentangling Whether from When in a Neural Mixture Cure Model for Failure Time Data,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"The mixture cure model allows failure probability to be estimated separately from failure timing in settings wherein failure never occurs in a subset of the population. In this paper, we draw on insights from representation learning and causal inference to develop a neural network based mixture cure model that is free of distributional assumptions, yielding improved prediction of failure timing, yet still effectively disentangles information about failure timing from information about failure probability. Our approach also mitigates effects of selection biases in the observation of failure and censoring times on estimation of the failure density and censoring density, respectively. Results suggest this approach could be applied to distinguish factors predicting failure occurrence versus timing and mitigate biases in real-world observational datasets.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,35937033,,,,,WOS:000841852304003,0
C,"Jain, SA; Shah, R; Gupta, S; Mehta, D; Nair, I; Vora, J; Khyalia, S; Das, S; Ribeiro, VJ; Kalyanakrishnan, S",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Jain, Shubham Anand; Shah, Rohan; Gupta, Sanit; Mehta, Denil; Nair, Inderjeet; Vora, Jian; Khyalia, Sushil; Das, Sourav; Ribeiro, Vinay J.; Kalyanakrishnan, Shivaram",,,PAC Mode Estimation using PPR Martingale Confidence Sequences,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We consider the problem of correctly identifying the mode of a discrete distribution P with sufficiently high probability by observing a sequence of i.i.d. samples drawn from P. This problem reduces to the estimation of a single parameter when P has a support set of size K = 2. After noting that this special case is tackled very well by prior-posteriorratio (PPR) martingale confidence sequences (Waudby-Smith and Ramdas, 2020), we propose a generalisation to mode estimation, in which P may take K >= 2 values. To begin, we show that the one-versus-one principle to generalise from K = 2 to K >= 2 classes is more efficient than the one-versus-rest alternative. We then prove that our resulting stopping rule, denoted PPR-1v1, is asymptotically optimal (as the mistake probability is taken to 0). PPR-1v1 is parameter-free and computationally light, and incurs significantly fewer samples than competitors even in the non-asymptotic regime. We demonstrate its gains in two practical applications of sampling: election forecasting and verification of smart contracts in blockchains.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,5815,5852,,,,,,,,,,,,,,,,WOS:000841852300010,0
C,"Kassraie, P; Krause, A",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Kassraie, Parnian; Krause, Andreas",,,Neural Contextual Bandits without Regret,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Contextual bandits are a rich model for sequential decision making given side information, with important applications, e.g., in recommender systems. We propose novel algorithms for contextual bandits harnessing neural networks to approximate the unknown reward function. We resolve the open problem of proving sublinear regret bounds in this setting for general context sequences, considering both fully-connected and convolutional networks. To this end, we first analyze NTK-UCB, a kernelized bandit optimization algorithm employing the Neural Tangent Kernel (NTK), and bound its regret in terms of the NTK maximum information gain gamma(T), a complexity parameter capturing the difficulty of learning. Our bounds on gamma(T) for the NTK may be of independent interest. We then introduce our neural network based algorithm NN-UCB, and show that its regret closely tracks that of NTK-UCB. Under broad non-parametric assumptions about the reward function, our approach converges to the optimal policy at a (O) over tilde (T-1/2d) rate, where d is the dimension of the context.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,240,278,,,,,,,,,,,,,,,,WOS:000828072700012,0
C,"Rendsburg, L; Kristiadi, A; Hennig, P; von Luxburg, U",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Rendsburg, Luca; Kristiadi, Agustinus; Hennig, Philipp; von Luxburg, Ulrike",,,Discovering Inductive Bias with Gibbs Priors: A Diagnostic Tool for Approximate Bayesian Inference,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Full Bayesian posteriors are rarely analytically tractable, which is why real-world Bayesian inference heavily relies on approximate techniques. Approximations generally differ from the true posterior and require diagnostic tools to assess whether the inference can still be trusted. We investigate a new approach to diagnosing approximate inference: the approximation mismatch is attributed to a change in the inductive bias by treating the approximations as exact and reverse-engineering the corresponding prior. We show that the problem is more complicated than it appears to be at first glance, because the solution generally depends on the observation. By reframing the problem in terms of incompatible conditional distributions we arrive at a natural solution: the Gibbs prior. The resulting diagnostic is based on pseudo-Gibbs sampling, which is widely applicable and easy to implement. We illustrate how the Gibbs prior can be used to discover the inductive bias in a controlled Gaussian setting and for a variety of Bayesian models and approximations.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072701026,0
C,"Wang, F; Padilla, OHM; Yu, Y; Rinaldo, A",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Wang, Fan; Padilla, Oscar Hernan Madrid; Yu, Yi; Rinaldo, Alessandro",,,Denoising and Change Point Localisation in Piecewise-Constant High-Dimensional Regression Coefficients,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We study the theoretical properties of the fused lasso procedure originally proposed by Tibshirani et al. (2005) in the context of a linear regression model in which the regression coefficient are totally ordered and assumed to be sparse and piecewise constant. Despite its popularity, to the best of our knowledge, estimation error bounds in high-dimensional settings have only been obtained for the simple case in which the design matrix is the identity matrix. We formulate a novel restricted isometry condition on the design matrix that is tailored to the fused lasso estimator and derive estimation bounds for both the constrained version of the fused lasso assuming dense coefficients and for its penalised version. We observe that the estimation error can be dominated by either the lasso or the fused lasso rate, depending on whether the number of non-zero coefficient is larger than the number of piecewise constant segments. Finally, we devise a post-processing procedure to recover the piecewise-constant pattern of the coefficients. Extensive numerical experiments support our theoretical findings.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072704017,0
C,"Wang, G; Donhauser, K; Yang, F",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Wang, Guillaume; Donhauser, Konstantin; Yang, Fanny",,,Tight bounds for minimum l(1)-norm interpolation of noisy data,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We provide matching upper and lower bounds of order sigma(2)/log(d/n) for the prediction error of the minimum l(1)-norm interpolator, a.k.a. basis pursuit. Our result is tight up to negligible terms when d >> n, and is the first to imply asymptotic consistency of noisy minimum-norm interpolation for isotropic features and sparse ground truths. Our work complements the literature on benign overfitting for minimum l(2)-norm interpolation, where asymptotic consistency can be achieved only when the features are effectively low-dimensional.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852305005,0
C,"Xu, WK",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Xu, Wenkai",,,Standardisation-function Kernel Stein Discrepancy: A Unifying View on Kernel Stein Discrepancy Tests for Goodness-of-fit,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Non-parametric goodness-of-fit testing procedures based on kernel Stein discrepancies (KSD) are promising approaches to validate general unnormalised distributions in various scenarios. Existing works focused on studying kernel choices to boost test performances. However, the choices of (non-unique) Stein operators also have considerable effect on the test performances. Inspired by the standardisation technique that was originally developed to better derive approximation properties for normal distributions, we present a unifying framework, called standardisation-function kernel Stein discrepancy (Sf-KSD), to study different Stein operators in KSD-based tests for goodness-of-fit. We derive explicitly how the proposed framework relates to existing KSD-based tests and show that Sf-KSD can be used as a guide to develop novel kernel-based non-parametric tests on complex data scenarios, e.g. truncated distributions or compositional data. Experimental results demonstrate that the proposed tests control type-I error well and achieve higher test power than existing approaches.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072701029,0
C,"Yang, MJ; Dai, B; Nachum, O; Tucker, G; Schuurmans, D",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Yang, Mengjiao; Dai, Bo; Nachum, Ofir; Tucker, George; Schuurmans, Dale",,,Offline Policy Selection under Uncertainty,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"The presence of uncertainty in policy evaluation significantly complicates the process of policy ranking and selection in real-world settings. We formally consider offline policy selection as learning preferences over a set of policy prospects given a fixed experience dataset. While one can select or rank policies based on point estimates of their expected values or high-confidence intervals, access to the full distribution over one's belief of the policy value enables more flexible selection algorithms under a wider range of downstream evaluation metrics. We propose a Bayesian approach for estimating this belief distribution in terms of posteriors of distribution correction ratios derived from stochastic constraints. Empirically, despite being Bayesian, the credible intervals obtained are competitive with state-of-the-art frequentist approaches in confidence interval estimation. More importantly, we show how the belief distribution may be used to rank policies with respect to arbitrary downstream policy selection metrics, and empirically demonstrate that this selection procedure significantly outperforms existing approaches, such as ranking policies according to mean or high-confidence lower bound value estimates.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072704020,0
C,"Zhou, Q; Smith, A",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Zhou, Quan; Smith, Aaron",,,Rapid Convergence of Informed Importance Tempering,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Informed Markov chain Monte Carlo (MCMC) methods have been proposed as scalable solutions to Bayesian posterior computation on high-dimensional discrete state spaces, but theoretical results about their convergence behavior in general settings are lacking. In this article, we propose a class of MCMC schemes called informed importance tempering (IIT), which combine importance sampling and informed local proposals, and derive generally applicable spectral gap bounds for IIT estimators. Our theory shows that IIT samplers have remarkable scalability when the target posterior distribution concentrates on a small set. Further, both our theory and numerical experiments demonstrate that the informed proposal should be chosen with caution: the performance of some proposals may be very sensitive to the shape of the target distribution. We find that the square-root proposal weighting scheme tends to perform well in most settings.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852305019,0
C,"Bhattacharyya, A; Choo, D; Gajjala, R; Gayen, S; Wang, YH",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Bhattacharyya, Arnab; Choo, Davin; Gajjala, Rishikesh; Gayen, Sutanu; Wang, Yuhao",,,Learning Sparse Fixed-Structure Gaussian Bayesian Networks,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Gaussian Bayesian networks are widely used to model causal interactions among continuous variables. In this work, we study the problem of learning a fixed-structure Gaussian Bayesian network up to a bounded error in total variation distance. We analyze the commonly used node-wise least squares regression LeastSquares and prove that it has the near-optimal sample complexity. We also study a couple of new algorithms for the problem: BatchAvgLeastSquares takes the average of several batches of least squares solutions at each node, so that one can interpolate between the batch size and the number of batches. We show that BatchAvgLeastSquares has near-optimal sample complexity. CauchyEst takes the median of solutions to several batches of linear systems at each node. We show that the algorithm specialized to polytrees, CauchyEstTree, has near-optimal sample complexity. Experimentally, LeastSquares performs best for uncontaminated data generated from the given DAG. However, with data contamination or DAG misspecification, CauchyEst, CauchyEstTree and BatchAvgLeastSquares perform significantly better.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852303033,0
C,"Chen, YA; Natarajan, S; Ruozzi, N",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Chen, Yugiao; Natarajan, Sriraam; Ruozzi, Nicholas",,,Relational Neural Markov Random Fields,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Statistical Relational Learning (SRL) models have attracted significant attention due to their ability to model complex data while handling uncertainty. However, most of these models have been restricted to discrete domains owing to the complexity of inference in continuous domains. In this work, we introduce Relational Neural Markov Random Fields (RN-MRFs) that allow handling of complex relational hybrid domains, i.e., those that include discrete and continuous quantities, and we propose a maximum pseudolikelihood estimation-based learning algorithm with importance sampling for training the neural potential parameters. The key advantage of our approach is that it makes minimal data distributional assumptions and can seamlessly embed human knowledge through potentials or relational rules. Our empirical evaluations across diverse domains, such as image processing and relational object mapping, demonstrate its practical utility.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852302029,0
C,"Coker, B; Bruinsma, WP; Burt, DR; Pan, WW; Doshi-Velez, F",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Coker, Beau; Bruinsma, Wessel P.; Burt, David R.; Pan, Weiwei; Doshi-Velez, Finale",,,Wide Mean-Field Bayesian Neural Networks Ignore the Data,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Bayesian neural networks (BNNs) combine the expressive power of deep learning with the advantages of Bayesian formalism. In recent years, the analysis of wide, deep BNNs has provided theoretical insight into their priors and posteriors. However, we have no analogous insight into their posteriors under approximate inference. In this work, we show that mean-field variational inference entirely fails to model the data when the network width is large and the activation function is odd. Specifically, for fully-connected BNNs with odd activation functions and a homoscedastic Gaussian likelihood, we show that the optimal mean-field variational posterior predictive (i.e., function space) distribution converges to the prior predictive distribution as the width tends to infinity. We generalize aspects of this result to other likelihoods. Our theoretical results are suggestive of underfitting behavior previously observered in BNNs. While our convergence bounds are non-asymptotic and constants in our analysis can be computed, they are currently too loose to be applicable in standard training regimes. Finally, we show that the optimal approximate posterior need not tend to the prior if the activation function is not odd, showing that our statements cannot be generalized arbitrarily.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072705017,0
C,"Ghosh, S; Birrell, PJ; De Angelis, D",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Ghosh, Sanmitra; Birrell, Paul J.; De Angelis, Daniela",,,Differentiable Bayesian inference of SDE parameters using a pathwise series expansion of Brownian motion,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"By invoking a pathwise series expansion of Brownian motion, we propose to approximate a stochastic differential equation (SDE) with an ordinary differential equation (ODE). This allows us to reformulate Bayesian inference for a SDE as the parameter estimation task for an ODE. Unlike a nonlinear SDE, the likelihood for an ODE model is tractable and its gradient can be obtained using adjoint sensitivity analysis. This reformulation allows us to use an efficient sampler, such as NUTS, that rely on the gradient of the log posterior. Applying the reparameterisation trick, variational inference can also be used for the same estimation task. We illustrate the proposed method on a variety of SDE models. We obtain similar parameter estimates when compared to data augmentation techniques.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852305021,0
C,"Gustafsson, FK; Danelljan, M; Schon, TB",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Gustafsson, Fredrik K.; Danelljan, Martin; Schon, Thomas B.",,,Learning Proposals for Practical Energy-Based Regression,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Energy-based models (EBMs) have experienced a resurgence within machine learning in recent years, including as a promising alternative for probabilistic regression. However, energy-based regression requires a proposal distribution to be manually designed for training, and an initial estimate has to be provided at test-time. We address both of these issues by introducing a conceptually simple method to automatically learn an effective proposal distribution, which is parameterized by a separate network head. To this end, we derive a surprising result, leading to a unified training objective that jointly minimizes the KL divergence from the proposal to the EBM, and the negative log-likelihood of the EBM. At test-time, we can then employ importance sampling with the trained proposal to efficiently evaluate the learned EBM and produce standalone predictions. Furthermore, we utilize our derived training objective to learn mixture density networks (MDNs) with a jointly trained energy-based teacher, consistently outperforming conventional MDN training on four real-world regression tasks within computer vision. Code is available at https://github.com/fregu856/ebms_proposals.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072704035,0
C,"Ha, J; Kim, G",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Ha, Junsoo; Kim, Gunhee",,,On Convergence of Lookahead in Smooth Games,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"A key challenge in smooth games is that there is no general guarantee for gradient methods to converge to an equilibrium. Recently, Chavdarova et al. (2021) reported a promising empirical observation that Lookahead (Zhang et al., 2019) significantly improves GAN training. While promising, few theoretical guarantees has been studied for Lookahead in smooth games. In this work, we establish the first convergence guarantees of Lookahead for smooth games. We present a spectral analysis and provide a geometric explanation of how and when it actually improves the convergence around a stationary point. Based on the analysis, we derive sufficient conditions for Lookahead to stabilize or accelerate the local convergence in smooth games. Our study reveals that Lookahead provides a general mechanism for stabilization and acceleration in smooth games.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072704034,0
C,"Indyk, P; Mallmann-Trenn, F; Mitrovic, S; Rubinfeld, R",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Indyk, Piotr; Mallmann-Trenn, Frederik; Mitrovic, Slobodan; Rubinfeld, Ronitt",,,Online Page Migration with ML Advice,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We consider online algorithms for the page migration problem that use predictions, potentially imperfect, to improve their performance. The best known online algorithms for this problem, due to Westbrook'94 and Bienkowski et al'17, have competitive ratios strictly bounded away from 1. In contrast, we show that if the algorithm is given a prediction of the input sequence, then it can achieve a competitive ratio that tends to 1 as the prediction error rate tends to 0. Specifically, the competitive ratio is equal to 1+O(q), where q is the prediction error rate. We also design a fallback option that ensures that the competitive ratio of the algorithm for any input sequence is at most O(1/q). Our result adds to the recent body of work that uses machine learning to improve the performance of \classic algorithms.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072701033,0
C,"Le, K; Nguyen, H; Nguyen, K; Pham, T; Ho, N",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Khang Le; Huy Nguyen; Khai Nguyen; Tung Pham; Ho, Nhat",,,On Multimarginal Partial Optimal Transport: Equivalent Forms and Computational Complexity,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We study the multimarginal partial optimal transport (POT) problem between m discrete (unbalanced) measures with at most n supports. We first prove that we can obtain two equivalent forms of the multimarginal POT problem in terms of the multimarginal optimal transport problem via novel extensions of cost tensors. The first equivalent form is derived under the assumptions that the total masses of each measure are sufficiently close while the second equivalent form does not require any conditions on these masses but at the price of more sophisticated extended cost tensor. Our proof techniques for obtaining these equivalent forms rely on novel procedures of moving masses in graph theory to push transportation plan into appropriate regions. Finally, based on the equivalent forms, we develop an optimization algorithm, named the ApproxMPOT algorithm, that builds upon the Sinkhorn algorithm for solving the entropic regularized multimarginal optimal transport. We demonstrate that the ApproxMPOT algorithm can approximate the optimal value of multimarginal POT problem with a computational complexity upper bound of the order (O) over tilde (m(3)(n + 1)(m)/epsilon(2)) where epsilon > 0 stands for the desired tolerance.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072704021,0
C,"Opolka, FL; Zhi, YC; Lio, P; Dong, XW",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Opolka, Felix L.; Zhi, Yin-Cong; Lio, Pietro; Dong, Xiaowen",,,Adaptive Gaussian Processes on Graphs via Spectral Graph Wavelets,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Graph-based models require aggregating information in the graph from neighbourhoods of different sizes. In particular, when the data exhibit varying levels of smoothness on the graph, a multi-scale approach is required to capture the relevant information. In this work, we propose a Gaussian process model using spectral graph wavelets, which can naturally aggregate neighbourhood information at different scales. Through maximum likelihood optimisation of the model hyperparameters, the wavelets automatically adapt to the different frequencies in the data, and as a result our model goes beyond capturing low frequency information. We achieve scalability to larger graphs by using a spectrum-adaptive polynomial approximation of the filter function, which is designed to yield a low approximation error in dense areas of the graph spectrum. Synthetic and real-world experiments demonstrate the ability of our model to infer scales accurately and produce competitive performances against state-of-the-art models in graph-based learning tasks.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072704041,0
C,"Schwobel, P; Jorgensen, M; Ober, SW; van der Wilk, M",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Schwobel, Pola; Jorgensen, Martin; Ober, Sebastian W.; van der Wilk, Mark",,,Last Layer Marginal Likelihood for Invariance Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Data augmentation is often used to incorporate inductive biases into models. Traditionally, these are hand-crafted and tuned with cross validation. The Bayesian paradigm for model selection provides a path towards end-to-end learning of invariances using only the training data, by optimising the marginal likelihood. Computing the marginal likelihood is hard for neural networks, but success with tractable approaches that compute the marginal likelihood for the last layer only raises the question of whether this convenient approach might be employed for learning invariances. We show partial success on standard benchmarks, in the low-data regime and on a medical imaging dataset by designing a custom optimisation routine. Introducing a new lower bound to the marginal likelihood allows us to perform inference for a larger class of likelihood functions than before. On the other hand, we demonstrate failure modes on the CIFAR10 dataset, where the last layer approximation is not sufficient due to the increased complexity of our neural network. Our results indicate that once more sophisticated approximations become available the marginal likelihood is a promising approach for invariance learning in neural networks.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072703027,0
C,"Yeh, CK; Lee, KY; Liu, F; Ravikumar, P",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Yeh, Chih-Kuan; Lee, Kuan-Yun; Liu, Frederick; Ravikumar, Pradeep",,,Threading the Needle of On and Off-Manifold Value Functions for Shapley Explanations,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"A popular explainable AI (XAI) approach to quantify feature importance of a given model is via Shapley values. These Shapley values arose in cooperative games, and hence a critical ingredient to compute these in an XAI context is a so-called value function, that computes the value of a subset of features, and which connects machine learning models to cooperative games. There are many possible choices for such value functions, which broadly fall into two categories: on-manifold and off-manifold value functions, which take an observational and an interventional viewpoint respectively. Both these classes however have their respective flaws, where on-manifold value functions violate key axiomatic properties and are computationally expensive while off-manifold value functions pay less heed to the data manifold and evaluate the model on regions for which it wasn't trained. Thus, there is no consensus on which class of value functions to use. In this paper, we show that in addition to these existing issues, both classes of value functions are prone to adversarial manipulations on low density regions. We formalize the desiderata of value functions that respect both the model and the data manifold in a set of axioms and are robust to perturbation on off-manifold regions, and show that there exists a unique value function that satisfies these axioms, which we term the Joint Baseline value function, and the resulting Shapley value the Joint Baseline Shapley (JBshap), and validate the effectiveness of JBshap in experiments.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072701025,0
C,"Zheng, JJ; D'Amour, A; Franks, A",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Zheng, Jiajing; D'Amour, Alexander; Franks, Alexander",,,Bayesian Inference and Partial Identification in Multi-Treatment Causal Inference with Unobserved Confounding,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"In causal estimation problems, the parameter of interest is often only partially identified, implying that the parameter cannot be recovered exactly, even with infinite data. Here, we study Bayesian inference for partially identified treatment effects in multi-treatment causal inference problems with unobserved confounding. In principle, inferring the partially identified treatment effects is natural under the Bayesian paradigm, but the results can be highly sensitive to parameterization and prior specification, often in surprising ways. It is thus essential to understand which aspects of the conclusions about treatment effects are driven entirely by the prior specification. We use a so-called transparent parameterization to contextualize the effects of more interpretable scientifically motivated prior specifications on the multiple effects. We demonstrate our analysis in an example quantifying the effects of gene expression levels on mouse obesity.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072703030,0
C,"Zhou, KW; Tian, L; So, AMC; Cheng, J",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Zhou, Kaiwen; Tian, Lai; So, Anthony Man-Cho; Cheng, James",,,Practical Schemes for Finding Near-Stationary Points of Convex Finite-Sums,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"In convex optimization, the problem of finding near-stationary points has not been adequately studied yet, unlike other optimality measures such as the function value. Even in the deterministic case, the optimal method (OGM-G, due to Kim and Fessler (2021)) has just been discovered recently. In this work, we conduct a systematic study of algorithmic techniques for finding near-stationary points of convex finite-sums. Our main contributions are several algorithmic discoveries: (1) we discover a memory-saving variant of OGM-G based on the performance estimation problem approach (Drori and Teboulle, 2014); (2) we design a new accelerated SVRG variant that can simultaneously achieve fast rates for minimizing both the gradient norm and function value; (3) we propose an adaptively regularized accelerated SVRG variant, which does not require the knowledge of some unknown initial constants and achieves near-optimal complexities. We put an emphasis on the simplicity and practicality of the new schemes, which could facilitate future work.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072703034,0
C,"Zhu, R; Kveton, B",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Zhu, Rong; Kveton, Branislav",,,Random Effect Bandits,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"This paper studies regret minimization in a multi-armed bandit. It is well known that side information, such as the prior distribution of arm means in Thompson sampling, can improve the statistical efficiency of the bandit algorithm. While the prior is a blessing when correctly specified, it is a curse when misspecified. To address this issue, we introduce the assumption of a random-effect model to bandits. In this model, the mean arm rewards are drawn independently from an unknown distribution, which we estimate. We derive a random-effect estimator of the arm means, analyze its uncertainty, and design a UCB algorithm ReUCB that uses it. We analyze ReUCB and derive an upper bound on its n-round Bayes regret, which improves upon not using the random-effect structure. Our experiments show that ReUCB can outperform Thompson sampling, without knowing the prior distribution of arm means.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072703009,0
C,"Babay, A; Dinitz, M; Srinivasan, A; Tsepenekas, L; Vullikanti, A",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Babay, Amy; Dinitz, Michael; Srinivasan, Aravind; Tsepenekas, Leonidas; Vullikanti, Anil",,,Controlling Epidemic Spread using Probabilistic Diffusion Models on Networks,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"The spread of an epidemic is often modeled by an SIR random process on a social network graph. The MININFEDGE problem for optimal social distancing involves minimizing the expected number of infections, when we are allowed to break at most B edges; similarly the MININFNODE problem involves removing at most B vertices. These are fundamental problems in epidemiology and network science. While a number of heuristics have been considered, the complexity of these problems remains generally open. In this paper, we present two bicriteria approximation algorithms for MININFEDGE, which give the first non-trivial approximations for this problem. The first is based on the cut sparsification result of Karger (1999), and works when the transmission probabilities are not too small. The second is a Sample Average Approximation (SAA) based algorithm, which we analyze for the Chung-Lu random graph model. We also extend some of our results to tackle the MININFNODE problem.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852306008,0
C,"Barrett, B; Camuto, A; Willetts, M; Rainforth, T",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Barrett, Ben; Camuto, Alexander; Willetts, Matthew; Rainforth, Tom",,,Certifiably Robust Variational Autoencoders,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We introduce an approach for training variational autoencoders (VAEs) that are certifiably robust to adversarial attack. Specifically, we first derive actionable bounds on the minimal size of an input perturbation required to change a VAE's reconstruction by more than an allowed amount, with these bounds depending on certain key parameters such as the Lipschitz constants of the encoder and decoder. We then show how these parameters can be controlled, thereby providing a mechanism to ensure a priori that a VAE will attain a desired level of robustness. Moreover, we extend this to a complete practical approach for training such VAEs to ensure our criteria are met. Critically, our method allows one to specify a desired level of robustness upfront and then train a VAE that is guaranteed to achieve this robustness. We further demonstrate that these Lipschitz-constrained VAEs are more robust to attack than standard VAEs in practice.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072703033,0
C,"Bunne, C; Meng-Papaxanthos, L; Krause, A; Cuturi, M",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Bunne, Charlotte; Meng-Papaxanthos, Laetitia; Krause, Andreas; Cuturi, Marco",,,Proximal Optimal Transport Modeling of Population Dynamics,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We propose a new approach to model the collective dynamics of a population of particles evolving with time. As is often the case in challenging scientific applications, notably single-cell genomics, measuring features for these particles requires destroying them. As a result, the population can only be monitored with periodic snapshots, obtained by sampling a few particles that are sacrificed in exchange for measurements. Given only access to these snapshots, can we reconstruct likely individual trajectories for all other particles? We propose to model these trajectories as collective realizations of a causal Jordan-Kinderlehrer-Otto (JKO) flow of measures: The JKO scheme posits that the new configuration taken by a population at time t + 1 is one that trades off an improvement, in the sense that it decreases an energy, while remaining close (in Wasserstein distance) to the previous configuration observed at t. In order to learn such an energy using only snapshots, we propose JKONET, a neural architecture that computes (in end-to-end differentiable fashion) the JKO flow given a parametric energy and initial configuration of points. We demonstrate the good performance and robustness of the JKONET fitting procedure, compared to a more direct forward method.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,6511,6528,,,,,,,,,,,,,,,,WOS:000841852300041,0
C,"Cen, SH; Shah, D",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Cen, Sarah H.; Shah, Devavrat",,,"Regret, stability & fairness in matching markets with bandit learners","INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Making an informed decision for example, when choosing a career or housing requires knowledge about the available options. Such knowledge is generally acquired through costly trial and error, but this learning process can be disrupted by competition. In this work, we study how competition affects the long-term outcomes of individuals as they learn. We build on a line of work that models this setting as a two-sided matching market with bandit learners. A recent result in this area states that it is impossible to simultaneously guarantee two natural desiderata: stability and low optimal regret for all agents. Resource-allocating platforms can point to this result as a justification for assigning good long-term outcomes to some agents and poor ones to others. We show that this impossibility need not hold true. In particular, by modeling two additional components of competition namely, costs and transfers we prove that it is possible to simultaneously guarantee four desiderata: stability, low optimal regret, fairness in the distribution of regret, and high social welfare.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852303017,0
C,"Chen, YZ; Zhang, SZ; Low, BKH",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Chen, Yizhou; Zhang, Shizhuo; Low, Bryan Kian Hsiang",,,Near-Optimal Task Selection for Meta-Learning with Mutual Information and Online Variational Bayesian Unlearning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"This paper addresses the problem of active task selection which involves selecting the most informative tasks for meta-learning. We propose a novel active task selection criterion based on the mutual information between latent task vectors. Unfortunately, such a criterion scales poorly in the number of candidate tasks when optimized. To resolve this issue, we exploit the submodularity property of our new criterion for devising the first active task selection algorithm for meta-learning with a near-optimal performance guarantee. To further improve our efficiency, we propose an online variant of the Stein variational gradient descent to perform fast belief updates of the meta-parameters via maintaining a set of forward (and backward) particles when learning (or unlearning) from each selected task. We empirically demonstrate the performance of our proposed algorithm on real-world datasets.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852303022,0
C,"Curry, MJ; Lyi, U; Goldstein, T; Dickerson, JP",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Curry, Michael J.; Lyi, Uro; Goldstein, Tom; Dickerson, John P.",,,Learning Revenue-Maximizing Auctions With Differentiable Matching,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We propose a new architecture to approximately learn incentive compatible, revenue-maximizing auctions from sampled valuations. Our architecture uses the Sinkhorn algorithm to perform a differentiable bipartite matching which allows the network to learn strategyproof revenue-maximizing mechanisms in settings not learnable by the previous Regret-Net architecture. In particular, our architecture is able to learn mechanisms in settings without free disposal where each bidder must be allocated exactly some number of items. In experiments, we show our approach successfully recovers multiple known optimal mechanisms and high-revenue, low-regret mechanisms in larger settings where the optimal mechanism is unknown.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,6062,6073,,,,,,,,,,,,,,,,WOS:000841852300021,0
C,"De Brouwer, E; Hernandez, JG; Hyland, S",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"De Brouwer, Edward; Gonzalez Hernandez, Javier; Hyland, Stephanie",,,Predicting the impact of treatments over time with uncertainty aware neural differential equations.,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Predicting the impact of treatments from observational data only still represents a major challenge despite recent significant advances in time series modeling. Treatment assignments are usually correlated with the predictors of the response, resulting in a lack of data support for counterfactual predictions and therefore in poor quality estimates. Developments in causal inference have lead to methods addressing this confounding by requiring a minimum level of overlap. However, overlap is difficult to assess and usually not satisfied in practice. In this work, we propose Counterfactual ODE (CF-ODE), a novel method to predict the impact of treatments continuously over time using Neural Ordinary Differential Equations equipped with uncertainty estimates. This allows to specifically assess which treatment outcomes can be reliably predicted. We demonstrate over several longitudinal data sets that CF-ODE provides more accurate predictions and more reliable uncertainty estimates than previously available methods.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072704036,0
C,"Izzo, Z; Zou, J; Ying, LX",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Izzo, Zachary; Zou, James; Ying, Lexing",,,How to Learn when Data Gradually Reacts to Your Model,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"A recent line of work has focused on training machine learning (ML) models in the performative setting, i.e. when the data distribution reacts to the deployed model. The goal in this setting is to learn a model which both induces a favorable data distribution and performs well on the induced distribution, thereby minimizing the test loss. Previous work on finding an optimal model assumes that the data distribution immediately adapts to the deployed model. In practice, however, this may not be the case, as the population may take time to adapt to the model. In many applications, the data distribution depends on both the currently deployed ML model and on the state that the population was in before the model was deployed. In this work, we propose a new algorithm, Stateful Performative Gradient Descent (Stateful PerfGD), for minimizing the performative loss even in the presence of these effects. We provide theoretical guarantees for the convergence of Stateful PerfGD. Our experiments confirm that Stateful PerfGD substantially outperforms previous state-of-the-art methods.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072704004,0
C,"Jin, H; Peng, Y; Yang, WH; Wang, SS; Zhang, ZH",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Jin, Hao; Peng, Yang; Yang, Wenhao; Wang, Shusen; Zhang, Zhihua",,,Federated Reinforcement Learning with Environment Heterogeneity,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We study a Federated Reinforcement Learning (FedRL) problem in which n agents collaboratively learn a single policy without sharing the trajectories they collected during agent-environment interaction. We stress the constraint of environment heterogeneity, which means n environments corresponding to these n agents have different state transitions. To obtain a value function or a policy function which optimizes the overall performance in all environments, we propose two federated RL algorithms, QAvg and PAvg. We theoretically prove that these algorithms converge to suboptimal solutions, while such sub-optimality depends on how heterogeneous these n environments are. Moreover, we propose a heuristic that achieves personalization by embedding the n environments into n vectors. The personalization heuristic not only improves the training but also allows for better generalization to new environments.",,,,,"Yang, wenhao/GXG-5965-2022",,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,18,37,,,,,,,,,,,,,,,,WOS:000828072700002,0
C,"Laroche, R; des Combes, RT",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Laroche, Romain; des Combes, Remi Tachet",,,Beyond the Policy Gradient Theorem for Efficient Policy Updates in Actor-Critic Algorithms,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"In Reinforcement Learning, the optimal action at a given state is dependent on policy decisions at subsequent states. As a consequence, the learning targets evolve with time and the policy optimization process must be efficient at unlearning what it previously learnt. In this paper, we discover that the policy gradient theorem prescribes policy updates that are slow to unlearn because of their structural symmetry with respect to the value target. To increase the unlearning speed, we study a novel policy update: the gradient of the cross-entropy loss with respect to the action maximizing q, but find that such updates may lead to a decrease in value. Consequently, we introduce a modified policy update devoid of that flaw, and prove its guarantees of convergence to global optimality in O(t(-1)) under classic assumptions. Further, we assess standard policy updates and our cross-entropy policy updates along six analytical dimensions. Finally, we empirically validate our theoretical findings.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,5658,5688,,,,,,,,,,,,,,,,WOS:000841852300004,0
C,"Li, CAH; Wang, HN",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Li, Chuanhao; Wang, Hongning",,,Asynchronous Upper Confidence Bound Algorithms for Federated Linear Bandits,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Linear contextual bandit is a popular online learning problem. It has been mostly studied in centralized learning settings. With the surging demand of large-scale decentralized model learning, e.g., federated learning, how to retain regret minimization while reducing communication cost becomes an open challenge. In this paper, we study linear contextual bandit in a federated learning setting. We propose a general framework with asynchronous model update and communication for a collection of homogeneous clients and heterogeneous clients, respectively. Rigorous theoretical analysis is provided about the regret and communication cost under this distributed learning framework; and extensive empirical evaluations demonstrate the effectiveness of our solution.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,6529,6553,,,,,,,,,,,,,,,,WOS:000841852300042,0
C,"Makar, M; Ben Packer; Moldovan, D; Blalock, D; Halpern, Y; D'Amour, A",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Makar, Maggie; Ben Packer; Moldovan, Dan; Blalock, Davis; Halpern, Yoni; D'Amour, Alexander",,,Causally Motivated Shortcut Removal Using Auxiliary Labels,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Shortcut learning, in which models make use of easy-to-represent but unstable associations, is a major failure mode for robust machine learning. We study a flexible, causally-motivated approach to training robust predictors by discouraging the use of specific shortcuts, focusing on a common setting where a robust predictor could achieve optimal iid generalization in principle, but is overshadowed by a shortcut predictor in practice. Our approach uses auxiliary labels, typically available at training time, to enforce conditional independences implied by the causal graph. We show both theoretically and empirically that causally-motivated regularization schemes (a) lead to more robust estimators that generalize well under distribution shift, and (b) have better finite sample efficiency compared to usual regularization schemes, even when no shortcut is present. Our analysis highlights important theoretical properties of training techniques commonly used in the causal inference, fairness, and disentanglement literatures.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,739,766,,,,,,,,,,,,,,,,WOS:000828072700031,0
C,"Ng, I; Zhang, K",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Ng, Ignavier; Zhang, Kun",,,Towards Federated Bayesian Network Structure Learning with Continuous Optimization,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Traditionally, Bayesian network structure learning is often carried out at a central site, in which all data is gathered. However, in practice, data may be distributed across different parties (e.g., companies, devices) who intend to collectively learn a Bayesian network, but are not willing to disclose information related to their data owing to privacy or security concerns. In this work, we present a federated learning approach to estimate the structure of Bayesian network from data that is horizontally partitioned across different parties. We develop a distributed structure learning method based on continuous optimization, using the alternating direction method of multipliers (ADMM), such that only the model parameters have to be exchanged during the optimization process. We demonstrate the flexibility of our approach by adopting it for both linear and nonlinear cases. Experimental results on synthetic and real datasets show that it achieves an improved performance over the other methods, especially when there is a relatively large number of clients and each has a limited sample size.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852302022,0
C,"Nitanda, A; Wu, D; Suzuki, T",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Nitanda, Atsushi; Wu, Denny; Suzuki, Taiji",,,Convex Analysis of the Mean Field Langevin Dynamics,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"As an example of the nonlinear Fokker-Planck equation, the mean field Langevin dynamics recently attracts attention due to its connection to (noisy) gradient descent on infinitely wide neural networks in the mean field regime, and hence the convergence property of the dynamics is of great theoretical interest. In this work, we give a concise and self-contained convergence rate analysis of the mean field Langevin dynamics with respect to the (regularized) objective function in both continuous and discrete time settings. The key ingredient of our proof is a proximal Gibbs distribution p(q) associated with the dynamics, which, in combination with techniques in Vempala and Wibisono (2019), allows us to develop a simple convergence theory parallel to classical results in convex optimization. Furthermore, we reveal that p(q) connects to the duality gap in the empirical risk minimization setting, which enables efficient empirical evaluation of the algorithm convergence.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852304012,0
C,"Qian, X; Islamov, R; Safaryan, M; Richtarik, P",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Qian, Xun; Islamov, Rustem; Safaryan, Mher; Richtarik, Peter",,,Basis Matters: Better Communication-Efficient Second Order Methods for Federated Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Recent advances in distributed optimization have shown that Newton-type methods with proper communication compression mechanisms can guarantee fast local rates and low communication cost compared to first order methods. We discover that the communication cost of these methods can be further reduced, sometimes dramatically so, with a surprisingly simple trick: Basis Learn (BL). The idea is to transform the usual representation of the local Hessians via a change of basis in the space of matrices and apply compression tools to the new representation. To demonstrate the potential of using custom bases, we design a new Newton-type method (BL1), which reduces communication cost via both BL technique and bidirectional compression mechanism. Furthermore, we present two alternative extensions (BL2 and BL3) to partial participation to accommodate federated learning applications. We prove local linear and superlinear rates independent of the condition number. Finally, we support our claims with numerical experiments by comparing several first and second order methods.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,680,720,,,,,,,,,,,,,,,,WOS:000828072700029,0
C,"Slowik, A; Bottou, L",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Slowik, Agnieszka; Bottou, Leon",,,On Distributionally Robust Optimization and Data Rebalancing,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Machine learning systems based on minimizing average error have been shown to perform inconsistently across notable subsets of the data, which is not exposed by a low average error for the entire dataset. Distributionally Robust Optimization (DRO) seemingly addresses this problem by minimizing the worst expected risk across subpopulations. We establish theoretical results that clarify the relation between DRO and the optimization of the same loss averaged on an adequately weighted training dataset. The results cover finite and infinite number of training distributions, as well as convex and non-convex loss functions. An implication of our results is that for each DRO problem there exists a data distribution such that learning this distribution is equivalent to solving the DRO problem. Yet, important problems that DRO seeks to address (for instance, adversarial robustness and fighting bias) cannot be reduced to finding the one 'unbiased' dataset. Our discussion section addresses this important discrepancy.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072701015,0
C,"Tong, T; Ma, C; Prater-Bennette, A; Tripp, E; Chi, YJ",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Tong, Tian; Ma, Cong; Prater-Bennette, Ashley; Tripp, Erin; Chi, Yuejie",,,Scaling and Scalability: Provable Nonconvex Low-Rank Tensor Completion,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Tensors, which provide a powerful and flexible model for representing multi-attribute data and multi-way interactions, play an indispensable role in modern data science across various fields in science and engineering. A fundamental task is tensor completion, which aims to faithfully recover the tensor from a small subset of its entries in a statistically and computationally efficient manner. Harnessing the low-rank structure of tensors in the Tucker decomposition, this paper develops a scaled gradient descent (ScaledGD) algorithm to directly recover the tensor factors with tailored spectral initializations, and shows that it provably converges at a linear rate independent of the condition number of the ground truth tensor for tensor completion as soon as the sample size is above the order of n(3/2) ignoring other parameter dependencies, where n is the dimension of the tensor. To the best of our knowledge, ScaledGD is the first algorithm that achieves near-optimal statistical and computational complexities simultaneously for low-rank tensor completion with the Tucker decomposition. Our algorithm highlights the power of appropriate preconditioning in accelerating nonconvex statistical estimation, where the iteration-varying preconditioners promote desirable invariance properties of the trajectory with respect to the underlying symmetry in low-rank tensor factorization.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072702029,0
C,"Wang, GH; Yang, M; Zhang, LJ; Yang, TB",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Wang, Guanghui; Yang, Ming; Zhang, Lijun; Yang, Tianbao",,,Momentum Accelerates the Convergence of Stochastic AUPRC Maximization,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"In this paper, we study stochastic optimization of areas under precision-recall curves (AUPRC), which is widely used for combating imbalanced classification tasks. Although a few methods have been proposed for maximizing AUPRC, stochastic optimization of AUPRC with convergence guarantee remains an undeveloped territory. A state-of-the-art complexity is O(1/epsilon(5)) for finding an o-stationary solution. In this paper, we further improve the stochastic optimization of AURPC by (i) developing novel stochastic momentum methods with a better iteration complexity of O(1/epsilon(4)) for finding an epsilon-stationary solution; and (ii) designing a novel family of stochastic adaptive methods with the same iteration complexity, which enjoy faster convergence in practice. To this end, we propose two innovative techniques that are critical for improving the convergence: (i) the biased estimators for tracking individual ranking scores are updated in a randomized coordinate-wise manner; and (ii) a momentum update is used on top of the stochastic gradient estimator for tracking the gradient of the objective. The novel analysis of Adam-style updates is also one main contribution. Extensive experiments on various data sets demonstrate the effectiveness of the proposed algorithms. Of independent interest, the proposed stochastic momentum and adaptive algorithms are also applicable to a class of two-level stochastic dependent compositional optimization problems.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072703037,0
C,"Wang, R; Xu, WL",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Wang, Rui; Xu, Wangli",,,On a Connection Between Fast and Sparse Oblivious Subspace Embeddings,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Fast Johnson-Lindenstrauss Transform (FJLT) and Sparse Johnson-Lindenstrauss Transform (SJLT) are two important oblivious subspace embeddings. So far, the developments of these two methods are almost orthogonal. In this work, we propose an iterative algorithm for oblivious subspace embedding which makes a connection between these two methods. The proposed method is built upon an iterative implementation of FJLT and is equipped with several theoretically motivated modifications. One important strategy we adopt is the early stopping strategy. On the one hand, the early stopping strategy makes our algorithm fast. On the other hand, it results in a sparse embedding matrix. As a result, the proposed algorithm is not only faster than the FJLT, but also faster than the SJLT with the same degree of sparsity. We present a general theoretical framework to analyze the embedding property of sparse embedding methods, which is used to prove the embedding property of the proposed method. This framework is also of independent interest. Lastly, we conduct numerical experiments to verify the good performance of the proposed algorithm.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852305001,0
C,"Winter, V; Dinari, O; Freifeld, O",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Winter, Vlad; Dinari, Or; Freifeld, Oren",,,Common Failure Modes of Subcluster-based Sampling in Dirichlet Process Gaussian Mixture Models - and a Deep-learning Solution,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"The Dirichlet Process Gaussian Mixture Model (DPGMM) is often used to cluster data when the number of clusters is unknown. One main DPGMM inference paradigm relies on sampling. Here we consider a known state-of-art sampler (proposed by Chang and Fisher III (2013) and improved by Dinari et al. (2019)), analyze its failure modes, and show how to improve it, often drastically. Concretely, in that sampler, whenever a new cluster is formed it is augmented with two subclusters whose labels are initialized at random. Upon their evolution, the subclusters serve to propose a split of the parent cluster. We show that the random initialization is often problematic and hurts the otherwise-effective sampler. Specifically, we demonstrate that this initialization tends to lead to poor split proposals and/or too many iterations before a desired split is accepted. This slows convergence and can damage the clustering. As a remedy, we propose two drop-in-replacement options for the subcluster-initialization subroutine. The first is an intuitive heuristic while the second is based on deep learning. We show that the proposed approach yields better splits, which in turn translate to substantial improvements in performance, results, and stability. Our code is publicly available.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072701023,0
C,"Acharya, A; Hashemi, A; Jain, P; Sanghavi, S; Dhillon, I; Topcu, U",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Acharya, Anish; Hashemi, Abolfazl; Jain, Prateek; Sanghavi, Sujay; Dhillon, Inderjit; Topcu, Ufuk",,,Robust Training in High Dimensions via Block Coordinate Geometric Median Descent,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Geometric median (GM) is a classical method in statistics for achieving robust estimation of the uncorrupted data; under gross corruption, it achieves the optimal breakdown point of 1/2. However, its computational complexity makes it infeasible for robustifying stochastic gradient descent (SGD) in high-dimensional optimization problems. In this paper, we show that by applying GM to only a judiciously chosen block of coordinates at a time and using a memory mechanism, one can retain the breakdown point of 1/2 for smooth non-convex problems, with non-asymptotic convergence rates comparable to the SGD with GM while resulting in significant speedup in training. We further validate the run-time and robustness of our approach empirically on several popular deep learning tasks. Code available at: https://github.com/anishacharya/BGMD.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852305030,0
C,"Arvanitidis, G; Gonzalez-Duque, M; Pouplin, A; Kalatzis, D; Hauberg, S",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Arvanitidis, Georgios; Gonzalez-Duque, Miguel; Pouplin, Alison; Kalatzis, Dimitris; Hauberg, Soren",,,Pulling back information geometry,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Latent space geometry has shown itself to provide a rich and rigorous framework for interacting with the latent variables of deep generative models. The existing theory, however, relies on the decoder being a Gaussian distribution as its simple reparametrization allows us to interpret the generating process as a random projection of a deterministic manifold. Consequently, this approach breaks down when applied to decoders that are not as easily reparametrized. We here propose to use the Fisher-Rao metric associated with the space of decoder distributions as a reference metric, which we pull back to the latent space. We show that we can achieve meaningful latent geometries for a wide range of decoder distributions for which the previous theory was not applicable, opening the door to 'black box' latent geometries.",,,,,"Hauberg, Soren/L-2104-2016","Hauberg, Soren/0000-0001-7223-877X",,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072704044,0
C,"Balzano, L",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Balzano, Laura",,,On the equivalence of Oja's algorithm and GROUSE,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"The analysis of streaming PCA has gained significant traction through the analysis of an early simple variant: Oja's algorithm, which implements online projected gradient descent for the trace objective. Several other streaming PCA algorithms have been developed, each with their own performance guarantees or empirical studies, and the question arises whether there is a relationship between the algorithms. We show that the Grassmannian Rank-One Subspace Estimation (GROUSE) algorithm is indeed equivalent to Oja's algorithm in the sense that, at each iteration, given a step size for one of the algorithms, we may construct a step size for the other algorithm that results in an identical update. This allows us to apply all results on one algorithm to the other. In particular, we have (1) better global convergence guarantees of GROUSE to the global minimizer of the PCA objective with full data; and (2) local convergence guarantees for Oja's algorithm with incomplete or compressed data.",,,,,,"Balzano, Laura/0000-0003-2914-123X",,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852301021,0
C,"Bartler, A; Buhler, A; Wiewel, F; Dobler, M; Yang, B",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Bartler, Alexander; Buehler, Andre; Wiewel, Felix; Doebler, Mario; Yang, Bin",,,MT3: Meta Test-Time Training for Self-Supervised Test-Time Adaption,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"An unresolved problem in Deep Learning is the ability of neural networks to cope with domain shifts during test-time, imposed by commonly fixing network parameters after training. Our proposed method Meta Test-Time Training (MT3), however, breaks this paradigm and enables adaption at test-time. We combine meta-learning, self-supervision and test-time training to learn to adapt to unseen test distributions. By minimizing the self-supervised loss, we learn task-specific model parameters for different tasks. A meta-model is optimized such that its adaption to the different task-specific models leads to higher performance on those tasks. During test-time a single unlabeled image is sufficient to adapt the meta-model parameters. This is achieved by minimizing only the self-supervised loss component resulting in a better prediction for that image. Our approach significantly improves the state-of-the-art results on the CIFAR-10-Corrupted image classification benchmark.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072703008,0
C,"Bosch, N; Tronarp, F; Hennig, P",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Bosch, Nathanael; Tronarp, Filip; Hennig, Philipp",,,Pick-and-Mix Information Operators for Probabilistic ODE Solvers,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Probabilistic numerical solvers for ordinary differential equations compute posterior distributions over the solution of an initial value problem via Bayesian inference. In this paper, we leverage their probabilistic formulation to seamlessly include additional information as general likelihood terms. We show that second-order differential equations should be directly provided to the solver, instead of transforming the problem to first order. Additionally, by including higher-order information or physical conservation laws in the model, solutions become more accurate and more physically meaningful. Lastly, we demonstrate the utility of flexible information operators by solving differential-algebraic equations. In conclusion, the probabilistic formulation of numerical solvers offers a flexible way to incorporate various types of information, thus improving the resulting solutions.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852304025,0
C,"Brown, R; Schmerling, E; Azizan, N; Pavone, M",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Brown, Robin; Schmerling, Edward; Azizan, Navid; Pavone, Marco",,,A Unified View of SDP-based Neural Network Verification through Completely Positive Programming,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Verifying that input-output relationships of a neural network conform to prescribed operational specifications is a key enabler towards deploying these networks in safety-critical applications. Semidefinite programming (SDP)-based approaches to Rectified Linear Unit (ReLU) network verification transcribe this problem into an optimization problem, where the accuracy of any such formulation reflects the level of fidelity in how the neural network computation is represented, as well as the relaxations of intractable constraints. While the literature contains much progress on improving the tightness of SDP formulations while maintaining tractability, comparatively little work has been devoted to the other extreme, i.e., how to most accurately capture the original verification problem before SDP relaxation. In this work, we develop an exact, convex formulation of verification as a completely positive program (CPP), and provide analysis showing that our formulation is minimal the removal of any constraint fundamentally misrepresents the neural network computation. We leverage our formulation to provide a unifying view of existing approaches, and give insight into the source of large relaxation gaps observed in some cases.",,,,,,"Pavone, Marco/0000-0002-0206-4337",,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852303031,0
C,"Dong, HL; Roy, C; Rahman, T; Gogate, V; Ruozzi, N",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Dong, Hailiang; Roy, Chiradeep; Rahman, Tahrima; Gogate, Vibhav; Ruozzi, Nicholas",,,Conditionally Tractable Density Estimation using Neural Networks,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Tractable models such as cutset networks and sum-product networks (SPNs) have become increasingly popular because they have superior predictive performance. Among them, cutset networks, which model the mechanics of Pearl's cutset conditioning algorithm, demonstrate great scalability and prediction accuracy. Existing research on cutset networks has mainly focused on discrete domains, and the best mechanism to extend cutset networks to continuous domains is unclear. We propose one possible alternative to cutset networks that models the full joint distribution as the product of a local, complex distribution over a small subset of variables and a fully tractable conditional distribution whose parameters are controlled using a neural network. This model admits exact inference when all variables in the local distribution are observed, and although the model is not fully tractable in general, we show that cutset sampling can be employed to efficiently generate accurate predictions in practice. We show that our model performs comparably or better than existing competitors through a variety of prediction tasks on real datasets.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852301017,0
C,"Dyer, J; Cannon, P; Schmon, SM",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Dyer, Joel; Cannon, Patrick; Schmon, Sebastian M.",,,Amortised Likelihood-free Inference for Expensive Time-series Simulators with Signatured Ratio Estimation,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Simulation models of complex dynamics in the natural and social sciences commonly lack a tractable likelihood function, rendering traditional likelihood-based statistical inference impossible. Recent advances in machine learning have introduced novel algorithms for estimating otherwise intractable likelihood functions using a likelihood ratio trick based on binary classifiers. Consequently, efficient likelihood approximations can be obtained whenever good probabilistic classifiers can be constructed. We propose a kernel classifier for sequential data using path signatures based on the recently introduced signature kernel. We demonstrate that the representative power of signatures yields a highly performant classifier, even in the crucially important case where sample numbers are low. In such scenarios, our approach can outperform sophisticated neural networks for common posterior inference tasks.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852305029,0
C,"Garreau, D",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Garreau, Damien",,,How to scale hyperparameters for quickshift image segmentation,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Quickshift is a popular algorithm for image segmentation, used as a preprocessing step in many applications. Unfortunately, it is quite challenging to understand the hyperparameters' influence on the number and shape of superpixels produced by the method. In this paper, we study theoretically a slightly modified version of the quickshift algorithm, with a particular emphasis on homogeneous image patches with i.i.d. pixel noise and sharp boundaries between such patches. Leveraging this analysis, we derive a simple heuristic to scale quickshift hyperparameters with respect to the image size, which we check empirically.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072705016,0
C,"Lin, ZJ; Astudillo, R; Frazier, PI; Bakshy, E",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Lin, Zhiyuan Jerry; Astudillo, Raul; Frazier, Peter, I; Bakshy, Eytan",,,Preference Exploration for Efficient Bayesian Optimization with Multiple Outcomes,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We consider Bayesian optimization of expensive-to-evaluate experiments that generate vector-valued outcomes over which a decision-maker (DM) has preferences. These preferences are encoded by a utility function that is not known in closed form but can be estimated by asking the DM to express preferences over pairs of outcome vectors. To address this problem, we develop Bayesian optimization with preference exploration, a novel framework that alternates between interactive real-time preference learning with the DM via pairwise comparisons between outcomes, and Bayesian optimization with a learned compositional model of DM utility and outcomes. Within this framework, we propose preference exploration strategies specifically designed for this task, and demonstrate their performance via extensive simulation studies.",,,,,,"Frazier, Peter/0000-0002-3501-3341",,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072704014,0
C,"Olmin, A; Lindsten, F",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Olmin, Amanda; Lindsten, Fredrik",,,Robustness and Reliability When Training With Noisy Labels,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Labelling of data for supervised learning can be costly and time-consuming and the risk of incorporating label noise in large data sets is imminent. When training a flexible discriminative model using a strictly proper loss, such noise will inevitably shift the solution towards the conditional distribution over noisy labels. Nevertheless, while deep neural networks have proven capable of fitting random labels, regularisation and the use of robust loss functions empirically mitigate the effects of label noise. However, such observations concern robustness in accuracy, which is insufficient if reliable uncertainty quantification is critical. We demonstrate this by analysing the properties of the conditional distribution over noisy labels for an input-dependent noise model. In addition, we evaluate the set of robust loss functions characterised by noise-insensitive, asymptotic risk minimisers. We find that strictly proper and robust loss functions both offer asymptotic robustness in accuracy, but neither guarantee that the final model is calibrated. Moreover, even with robust loss functions, overfitting is an issue in practice. With these results, we aim to explain observed robustness of common training practices, such as early stopping, to label noise. In addition, we aim to encourage the development of new noise-robust algorithms that not only preserve accuracy but that also ensure reliability.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,922,942,,,,,,,,,,,,,,,,WOS:000828072700039,0
C,"Ono, H; Minami, K; Hino, H",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Ono, Hajime; Minami, Kazuhiro; Hino, Hideitsu",,,One-bit Submission for Locally Private Quasi-MLE: Its Asymptotic Normality and Limitation,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Local differential privacy (LDP) is an information-theoretic privacy definition suitable for statistical surveys that involve an untrusted data curator. An LDP version of quasi-maximum likelihood estimator (QMLE) has been developed, but the existing method to build LDP QMLE is difficult to implement for a large-scale survey system in the real world due to long waiting time, expensive communication cost, and the boundedness assumption of derivative of a log-likelihood function. We provided an alternative LDP protocol without those issues, which is potentially much easily deployable to a large-scale survey. We also provided sufficient conditions for the consistency and asymptotic normality and limitations of our protocol. Our protocol is less burdensome for the users, and the theoretical guarantees cover more realistic cases than those for the existing method.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072702035,0
C,"Pavez, E",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Pavez, Eduardo",,,Laplacian Constrained Precision Matrix Estimation: Existence and High Dimensional Consistency,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"This paper considers the problem of estimating high dimensional Laplacian constrained precision matrices by minimizing Stein's loss. We obtain a necessary and sufficient condition for existence of this estimator, that consists on checking whether a certain data dependent graph is connected. We also prove consistency in the high dimensional setting under the symmetrized Stein loss. We show that the error rate does not depend on the graph sparsity, or other type of structure, and that Laplacian constraints are sufficient for high dimensional consistency. Our proofs exploit properties of graph Laplacians, the matrix tree theorem, and a characterization of the proposed estimator based on effective graph resistances. We validate our theoretical claims with numerical experiments.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852304010,0
C,"Pedrood, B; Domeniconi, C; Lackey, KB",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Pedrood, Bahman; Domeniconi, Carlotta; Lackey, Kathryn B.",,,Hypergraph Simultaneous Generators,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Generative models for affiliation networks condition the edges on the membership of their nodes to communities. The problem of community detection under these models is addressed by inferring the membership parameters from the network structure. Current models make several unrealistic assumptions to make the inference feasible, and are mostly designed to work on regular graphs that cannot handle multi-way connections between nodes. While the models designed for hypergraphs attempt to capture the latter, they add further strict assumptions on the structure and size of hyperedges and are usually computationally intractable for real data. This paper proposes an efficient probabilistic generative model for detecting overlapping communities that process hyperedges without any changes or restrictions on their size. Our model represents the entire state space of the hyperedges, which is exponential in the number of nodes. We develop a mathematical computation reduction scheme that reduces the inference time to linear in the volume of the hypergraph without sacrificing precision. Our experimental results validate the effectiveness and scalability of our model and demonstrate the superiority of our approach over state-of-the-art community detection methods.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852305028,0
C,"Rindt, D; Hu, R; Steinsaltz, D; Sejdinovic, D",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Rindt, David; Hu, Robert; Steinsaltz, David; Sejdinovic, Dino",,,Survival Regression with Proper Scoring Rules and Monotonic Neural Networks,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We consider frequently used scoring rules for right-censored survival regression models such as time-dependent concordance, survival-CRPS, integrated Brier score and integrated binomial log-likelihood, and prove that neither of them is a proper scoring rule. This means that the true survival distribution may be scored worse than incorrect distributions, leading to inaccurate estimation. We prove that, in contrast to these scores, the right-censored log-likelihood is a proper scoring rule, i.e., the highest expected score is achieved by the true distribution. Despite this, modern feed-forward neural-network-based survival regression models are unable to train and validate directly on the right-censored log-likelihood, due to its intractability, and resort to the aforementioned alternatives, i.e., non-proper scoring rules. We therefore propose a simple novel survival regression method capable of directly optimizing log-likelihood using a monotonic restriction on the time-dependent weights, coined SurvivalMonotonic-net (SuMo-net). SuMo-net achieves state-of-the-art log-likelihood scores across several datasets with 20-100x computational speedup on inference over existing state-of-the-art neural methods, and is readily applicable to datasets with several million observations.",,,,,,"Sejdinovic, Dino/0000-0001-5547-9213",,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072701011,0
C,"Sejourne, T; Vialard, FX; Peyre, G",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Sejourne, Thibault; Vialard, Francois-Xavier; Peyre, Gabriel",,,Faster Unbalanced Optimal Transport: Translation invariant Sinkhorn and 1-D Frank-Wolfe,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Unbalanced optimal transport (UOT) extends optimal transport (OT) to take into account mass variations when comparing distributions. This is crucial for successful ML applications of OT, as it makes it robust to data normalization and outliers. The baseline algorithm is Sinkhorn, but its convergence speed might be significantly slower for UOT than for OT. In this work, we identify the cause for this deficiency, namely the lack of a global normalization of the iterates, which equivalently corresponds to a translation of the dual OT potentials. Our first contribution leverages this idea to develop a provably accelerated Sinkhorn algorithm (coined translation invariant Sinkhorn) for UOT, bridging the computational gap with OT. Our second contribution focuses on 1D UOT and proposes a Frank-Wolfe solver applied to this translation invariant formulation. The linear oracle of each step amounts to solving a 1-D OT problem, resulting in a linear time complexity per iteration. Our last contribution extends this method to the computation of UOT barycenter of 1-D measures. Numerical simulations showcase the convergence speed improvement brought by these three approaches.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072705006,0
C,"Tarbouriech, J; Domingues, OD; Menard, P; Pirotta, M; Valko, M; Lazaric, A",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Tarbouriech, Jean; Domingues, Omar Darwiche; Menard, Pierre; Pirotta, Matteo; Valko, Michal; Lazaric, Alessandro",,,Adaptive Multi-Goal Exploration,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We introduce a generic strategy for provably efficient multi-goal exploration. It relies on ADAGOAL, a novel goal selection scheme that leverages a measure of uncertainty in reaching states to adaptively target goals that are neither too difficult nor too easy. We show how ADAGOAL can be used to tackle the objective of learning an e-optimal goal-conditioned policy for the (initially unknown) set of goal states that are reachable within L steps in expectation from a reference state s(0) in a reward-free Markov decision process. In the tabular case with S states and A actions, our algorithm requires (O) over tilde (L(3)SA epsilon(-2)) exploration steps, which is nearly minimax optimal. We also readily instantiate ADAGoAL in linear mixture Markov decision processes, yielding the first goal-oriented PAC guarantee with linear function approximation. Beyond its strong theoretical guarantees, we anchor ADAGOAL in goal-conditioned deep reinforcement learning, both conceptually and empirically, by connecting its idea of selecting uncertain goals to maximizing value ensemble disagreement.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852301036,0
C,"Truong, LV",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Truong, Lan, V",,,On Linear Model with Markov Signal Priors,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"In this paper, we estimate free energy, average mutual information, and minimum mean square error (MMSE) of a linear model under the assumption that the source is generated by a Markov chain. Our estimates are based on the replica method in statistical physics. We show that under the MMSE estimator, the linear model with Markov sources is decoupled into single-input AWGN channels with state information available at both encoder and decoder where the state distribution follows the stationary distribution of the stochastic matrix of Markov chains. Numerical results show that the free energies and MSEs obtained via the replica method are closely approximate to their counterparts via MCMC simulations.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,38,53,,,,,,,,,,,,,,,,WOS:000828072700003,0
C,"Tzeng, RC; Wang, PA; Adriaens, F; Gionis, A; Lu, CJ",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Tzeng, Ruo-Chun; Wang, Po-An; Adriaens, Florian; Gionis, Aristides; Lu, Chi-Jen",,,Improved analysis of randomized SVD for top-eigenvector approximation,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Computing the top eigenvectors of a matrix is a problem of fundamental interest to various fields. While the majority of the literature has focused on analyzing the reconstruction error of low-rank matrices associated with the retrieved eigenvectors, in many applications one is interested in finding one vector with high Rayleigh quotient. In this paper we study the problem of approximating the top-eigenvector. Given a symmetric matrix A with largest eigenvalue lambda(1), our goal is to find a vector (u) over cap that approximates the leading eigenvector u(1) with high accuracy, as measured by the ratio R((u) over cap) = lambda(-1)(1) (u) over cap (T)A (u) over cap/(u) over cap (T)(u) over cap. We present a novel analysis of the randomized SVD algorithm of Halko et al. (2011b) and derive tight bounds in many cases of interest. Notably, this is the first work that provides non-trivial bounds for approximating the ratio R((u) over cap) using randomized SVD with any number of iterations. Our theoretical analysis is complemented with a thorough experimental study that confirms the efficiency and accuracy of the method.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072702005,0
C,"Vono, M; Plassier, V; Durmus, A; Dieuleveut, A; Moulines, E",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Vono, Maxime; Plassier, Vincent; Durmus, Alain; Dieuleveut, Aymeric; Moulines, Eric",,,QLSD: Quantised Langevin Stochastic Dynamics for Bayesian Federated Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"The objective of Federated Learning (FL) is to perform statistical inference for data which are decentralised and stored locally on networked clients. FL raises many constraints which include privacy and data ownership, communication overhead, statistical heterogeneity, and partial client participation. In this paper, we address these problems in the framework of the Bayesian paradigm. To this end, we propose a novel federated Markov Chain Monte Carlo algorithm, referred to as Quantised Langevin Stochastic Dynamics which may be seen as an extension to the FL setting of Stochastic Gradient Langevin Dynamics, which handles the communication bottleneck using gradient compression. To improve performance, we then introduce variance reduction techniques, which lead to two improved versions coined QLSD* and QLSD(++). We give both non-asymptotic and asymptotic convergence guarantees for the proposed algorithms. We illustrate their performances using various Bayesian Federated Learning benchmarks.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,6459,6500,,,,,,,,,,,,,,,,WOS:000841852300039,0
C,"Wood, D; Mu, TT; Brown, G",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Wood, Danny; Mu, Tingting; Brown, Gavin",,,Bias-Variance Decompositions for Margin Losses,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We introduce a novel bias-variance decomposition for a range of strictly convex margin losses, including the logistic loss (minimized by the classic LogitBoost algorithm), as well as the squared margin loss and canonical boosting loss. Furthermore, we show that, for all strictly convex margin losses, the expected risk decomposes into the risk of a central model and a term quantifying variation in the functional margin with respect to variations in the training data. These decompositions provide a diagnostic tool for practitioners to understand model overfitting/underfitting, and have implications for additive ensemble models-for example, when our bias-variance decomposition holds, there is a corresponding ambiguity decomposition, which can be used to quantify model diversity.",,,,,,"Brown, Gavin/0000-0003-2261-9018",,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072702002,0
C,"Yun, JH; Lozano, AC; Yang, EO",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Yun, Jihun; Lozano, Aurelie C.; Yang, Eunho",,,AdaBlock: SGD with Practical Block Diagonal Matrix Adaptation for Deep Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We introduce ADABLOCK, a class of adaptive gradient methods that extends popular approaches such as ADAM by adopting the simple and natural idea of using block-diagonal matrix adaption to effectively utilize structural characteristics of deep learning architectures. Unlike other quadratic or blockdiagonal approaches, ADABLOCK has complete freedom to select block-diagonal groups, providing a wider trade-off applicable even to extremely high-dimensional problems. We provide convergence and generalization error bounds for ADABLOCK, and study both theoretically and empirically the impact of the block size on the bounds and advantages over usual diagonal approaches. In addition, we propose a randomized layer-wise variant of ADABLOCK to further reduce computations and memory footprint, and devise an efficient spectrum-clipping scheme for ADABLOCK to benefit from SGD's superior generalization performance. Extensive experiments on several deep learning tasks demonstrate the benefits of block diagonal adaptation compared to adaptive diagonal methods, vanilla SGD, as well as modified versions of full-matrix adaptation.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072702028,0
C,"Zhang, GD; Wang, YH; Lessard, L; Grosse, R",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Zhang, Guodong; Wang, Yuanhao; Lessard, Laurent; Grosse, Roger",,,Near-optimal Local Convergence of Alternating Gradient Descent-Ascent for Minimax Optimization,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Smooth minimax games often proceed by simultaneous or alternating gradient updates. Although algorithms with alternating updates are commonly used in practice, the majority of existing theoretical analyses focus on simultaneous algorithms for convenience of analysis. In this paper, we study alternating gradient descent-ascent (Alt-GDA) in minimax games and show that Alt-GDA is superior to its simultaneous counterpart (SimGDA) in many settings. We prove that AltGDA achieves a near-optimal local convergence rate for strongly convex-strongly concave (SCSC) problems while Sim-GDA converges at a much slower rate. To our knowledge, this is the first result of any setting showing that Alt-GDA converges faster than Sim-GDA by more than a constant. We further adapt the theory of integral quadratic constraints (IQC) and show that Alt-GDA attains the same rate globally for a subclass of SCSC minimax problems. Empirically, we demonstrate that alternating updates speed up GAN training significantly and the use of optimism only helps for simultaneous algorithms.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852302005,0
C,"Zhu, RH; Kveton, B",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Zhu, Ruihao; Kveton, Branislav",,,Safe Optimal Design with Applications in Off-Policy Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Motivated by practical needs in online experimentation and off-policy learning, we study the problem of safe optimal design, where we develop a data logging policy that efficiently explores while achieving competitive rewards with a baseline production policy. We first show, perhaps surprisingly, that a common practice of mixing the production policy with uniform exploration, despite being safe, is sub-optimal in maximizing information gain. Then we propose a safe optimal logging policy for the case when no side information about the actions' expected rewards is available. We improve upon this design by considering side information and also extend both approaches to a large number of actions with a linear reward model. We analyze how our data logging policies impact errors in off-policy learning. Finally, we empirically validate the benefit of our designs by conducting extensive experiments.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072702022,0
C,"Zuo, JH; Liu, XT; Joe-Wong, C; Lui, JCS; Chen, W",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Zuo, Jinhang; Liu, Xutong; Joe-Wong, Carlee; Lui, John C. S.; Chen, Wei",,,Online Competitive Influence Maximization,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Online influence maximization has attracted much attention as a way to maximize influence spread through a social network while learning the values of unknown network parameters. Most previous works focus on single-item diffusion. In this paper, we introduce a new Online Competitive Influence Maximization (OCIM) problem, where two competing items (e.g., products, news stories) propagate in the same network and influence probabilities on edges are unknown. We adopt a combinatorial multi-armed bandit (CMAB) framework for OCIM, but unlike the non-competitive setting, the important monotonicity property (influence spread in- creases when influence probabilities on edges increase) no longer holds due to the competitive nature of propagation, which brings a significant new challenge to the problem. We provide a nontrivial proof showing that the Triggering Probability Modulated (TPM) condition for CMAB still holds in OCIM, which is instrumental for our proposed algorithms OCIM-TS and OCIM-OFU to achieve sublinear Bayesian and frequentist regret, respectively. We also design an OCIM-ETC algorithm that requires less feedback and easier offline computation, at the expense of a worse frequentist regret bound. Experimental evaluations demonstrate the effectiveness of our algorithms.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852306001,0
C,,,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Agterberg, Joshua; Sulam, Jeremias",,,Supplementary Material: Entrywise Recovery Guarantees for Sparse PCA via Sparsistent Algorithms,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"This supplementary material contains all the proofs of our main results. Appendix A contains the proof of Theorem 1, Appendix B contains the proofs of additional lemmas needed en route the the proof of the main theorem, and Appendix C contains additional background material on Orlicz norms, concentration inequalities, and subspace perturbation theory.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852301004,0
C,"Agarwal, A; Khanna, S; Patil, P",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Agarwal, Arpit; Khanna, Sanjeev; Patil, Prathamesh",,,PAC Top-k Identification under SST in Limited Rounds,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We consider the problem of finding top-k items from a set of n items using actively chosen pairwise comparisons. This problem has been widely studied in machine learning and has widespread applications in recommendation systems, sports, social choice etc. Motivated by applications where there can be a substantial delay between requesting comparisons and receiving feedback, we consider an active/adaptive learning setting where the algorithm uses limited rounds of parallel interaction with the feedback generating oracle. We study this problem under the strong stochastic transitivity (SST) noise model which is a widely studied ranking model and captures many applications. A special case of this model is the noisy comparison model for which it was recently shown that O(n log k) comparisons and log* n rounds of adaptivity are sufficient to find the set of top-k items (Cohen-Addad et al., 2020; Braverman et al., 2019). Under the more general SST model, it is known that O(n) comparisons and O(n) rounds are sufficient to find a PAC top-1 item (Falahatgar et al., 2017a,b), however, not much seems to be known for general k, even given unbounded rounds of adaptivity. We first show that C2 (nk) comparisons are necessary for PAC top-k identification under SST even with unbounded adaptivity, establishing that this problem is strictly harder under SST than it is for the noisy comparison model. Our main contribution is to show that the 2-round query complexity for this problem is (circle minus) over tilde (n(4/3) + nk), and to show that just 3 rounds are sufficient to obtain a nearly optimal query complexity of (circle minus) over tilde (nk). We further show that our 3-round result can be improved by a log(n) factor using 2 log* n + 4 rounds.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852301011,0
C,"Agarwal, C; Zitnik, M; Lakkaraju, H",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Agarwal, Chirag; Zitnik, Marinka; Lakkaraju, Himabindu",,,Probing GNN Explainers: A Rigorous Theoretical and Empirical Analysis of GNN Explanation Methods,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"As Graph Neural Networks (GNNs) are increasingly being employed in critical real-world applications, several methods have been proposed in recent literature to explain the predictions of these models. However, there has been little to no work on systematically analyzing the reliability of these methods. Here, we introduce the first-ever theoretical analysis of the reliability of state-of-the-art GNN explanation methods. More specifically, we theoretically analyze the behavior of various state-of-the-art GNN explanation methods with respect to several desirable properties (e.g., faithfulness, stability, and fairness preservation) and establish upper bounds on the violation of these properties. We also empirically validate our theoretical results using extensive experimentation with nine real-world graph datasets. Our empirical results further shed light on several interesting insights about the behavior of state-of-the-art GNN explanation methods.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852303018,0
C,"Aminian, G; Abroshan, M; Khalili, MM; Toni, L; Rodrigues, MRD",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Aminian, Gholamali; Abroshan, Mahed; Khalili, Mohammad Mahdi; Toni, Laura; Rodrigues, Miguel R. D.",,,An Information-theoretical Approach to Semi-supervised Learning under Covariate-shift,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"A common assumption in semi-supervised learning is that the labeled, unlabeled, and test data are drawn from the same distribution. However, this assumption is not satisfied in many applications. In many scenarios, the data is collected sequentially (e.g., healthcare) and the distribution of the data may change over time often exhibiting so-called covariate shifts. In this paper, we propose an approach for semi-supervised learning algorithms that is capable of addressing this issue. Our framework also recovers some popular methods, including entropy minimization and pseudo-labeling. We provide new information-theoretical based generalization error upper bounds inspired by our novel framework. Our bounds are applicable to both general semi-supervised learning and the covariate-shift scenario. Finally, we show numerically that our method outperforms previous approaches proposed for semi-supervised learning under the covariate shift.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852301038,0
C,"Ding, YH; Zhang, JZ; Lavaei, J",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Ding, Yuhao; Zhang, Junzi; Lavaei, Javad",,,On the Global Optimum Convergence of Momentum-based Policy Gradient,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Policy gradient (PG) methods are popular and efficient for large-scale reinforcement learning due to their relative stability and incremental nature. In recent years, the empirical success of PG methods has led to the development of a theoretical foundation for these methods. In this work, we generalize this line of research by establishing the first set of global convergence results of stochastic PG methods with momentum terms, which have been demonstrated to be efficient recipes for improving PG methods. We study both the soft-max and the Fishernon-degenerate policy parametrizations, and show that adding a momentum term improves the global optimality sample complexities of vanilla PG methods by (O) over tilde(epsilon(-1.5)) and (O) over tilde(epsilon(-1)), respectively, where epsilon > 0 is the target tolerance. Our results for the generic Fishernon-degenerate policy parametrizations also provide the first single-loop and finite-batch PG algorithm achieving an (O) over tilde (epsilon(-3)) global optimality sample complexity. Finally, as a byproduct, our analyses provide general tools for deriving the global convergence rates of stochastic PG methods, which can be readily applied and extended to other PG estimators under the two parametrizations.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072701044,0
C,"Duran-Martin, G; Kara, A; Murphy, K",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Duran-Martin, Gerardo; Kara, Aleyna; Murphy, Kevin",,,Efficient Online Bayesian Inference for Neural Bandits,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"In this paper we present a new algorithm for online (sequential) inference in Bayesian neural networks, and show its suitability for tackling contextual bandit problems. The key idea is to combine the extended Kalman filter (which locally linearizes the likelihood function at each time step) with a (learned or random) low-dimensional affine subspace for the parameters; the use of a subspace enables us to scale our algorithm to models with similar to 1M parameters. While most other neural bandit methods need to store the entire past dataset in order to avoid the problem of catastrophic forgetting, our approach uses constant memory. This is possible because we represent uncertainty about all the parameters in the model, not just the final linear layer. We show good results on the Deep Bayesian Bandit Showdown benchmark, as well as MNIST and a recommender system.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,6002,6021,,,,,,,,,,,,,,,,WOS:000841852300018,0
C,"Korba, A; Portier, F",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Korba, Anna; Portier, Francois",,,Adaptive Importance Sampling meets Mirror Descent: a Bias-Variance Tradeoff,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Adaptive importance sampling is a widely spread Monte Carlo technique that uses a re-weighting strategy to iteratively estimate the so-called target distribution. A major drawback of adaptive importance sampling is the large variance of the weights which is known to badly impact the accuracy of the estimates. This paper investigates a regularization strategy whose basic principle is to raise the importance weights at a certain power. This regularization parameter, that might evolve between zero and one during the algorithm, is shown (i) to balance between the bias and the variance and (ii) to be connected to the mirror descent framework. Using a kernel density estimate to build the sampling policy, the uniform convergence is established under mild conditions. Finally, several practical ways to choose the regularization parameter are discussed and the benefits of the proposed approach are illustrated empirically.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852306002,0
C,"Muller, R; Pacchiano, A",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Mueller, Robert; Pacchiano, Aldo",,,Meta Learning MDPs with Linear Transition Models,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We study meta-learning in Markov Decision Processes (MDP) with linear transition models in the undiscounted episodic setting. Under a task sharedness metric based on model proximity we study task families characterized by a distribution over models specified by a bias term and a variance component. We then propose BUC-MatrixRL, a version of the UC-Matrix RL algorithm (Yang and Wang, 2019) and show it can meaningfully leverage a set of sampled training tasks to quickly solve a test task sampled from the same task distribution by learning an estimator of the bias parameter of the task distribution. The analysis leverages and extends results in the learning to learn linear regression and linear bandit setting to the more general case of MDP's with linear transition models. We prove that compared to learning the tasks in isolation, BUC-Matrix RL provides significant improvements in the transfer regret for high bias low variance task distributions.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,5928,5948,,,,,,,,,,,,,,,,WOS:000841852300015,0
C,"Narang, A; Faulkner, E; Drusvyatskiy, D; Fazel, M; Ratliff, LJ",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Narang, Adhyyan; Faulkner, Evan; Drusvyatskiy, Dmitriy; Fazel, Maryam; Ratliff, Lillian J.",,,Learning in Stochastic Monotone Games with Decision-Dependent Data,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Learning problems commonly exhibit an interesting feedback mechanism wherein the population data reacts to competing decision makers' actions. This paper formulates a new game theoretic framework for this phenomenon, called multi-player performative prediction. We establish transparent sufficient conditions for strong monotonicity of the game and use them to develop algorithms for finding Nash equilibria. We investigate derivative free methods and adaptive gradient algorithms wherein each player alternates between learning a parametric description of their distribution and gradient steps on the empirical risk. Synthetic and semi-synthetic numerical experiments illustrate the results.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,5891,5912,,,,,,,,,,,,,,,,WOS:000841852300013,0
C,"Ng, I; Lachapelle, S; Ke, NR; Lacoste-Julien, S; Zhang, K",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Ng, Ignavier; Lachapelle, Sebastien; Ke, Nan Rosemary; Lacoste-Julien, Simon; Zhang, Kun",,,On the Convergence of Continuous Constrained Optimization for Structure Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Recently, structure learning of directed acyclic graphs (DAGs) has been formulated as a continuous optimization problem by leveraging an algebraic characterization of acyclicity. The constrained problem is solved using the augmented Lagrangian method (ALM) which is often preferred to the quadratic penalty method (QPM) by virtue of its standard convergence result that does not require the penalty coefficient to go to infinity, hence avoiding ill-conditioning. However, the convergence properties of these methods for structure learning, including whether they are guaranteed to return a DAG solution, remain unclear, which might limit their practical applications. In this work, we examine the convergence of ALM and QPM for structure learning in the linear, nonlinear, and confounded cases. We show that the standard convergence result of ALM does not hold in these settings, and demonstrate empirically that its behavior is akin to that of the QPM which is prone to ill-conditioning. We further establish the convergence guarantee of QPM to a DAG solution, under mild conditions. Lastly, we connect our theoretical results with existing approaches to help resolve the convergence issue, and verify our findings in light of an empirical comparison of them.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852302026,0
C,"Rakotomamonjy, A; Flamary, R; Gasso, G; Salmon, J",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Rakotomamonjy, Alain; Flamary, Remi; Gasso, Gilles; Salmon, Joseph",,,Convergent Working Set Algorithm for Lasso with Non-Convex Sparse Regularizers,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Non-convex sparse regularizers are common tools for learning with high-dimensional data. For accelerating convergence of a Lasso problem using those regularizers, a working set strategy addresses the optimization problem through an iterative algorithm by gradually incrementing the number of variables to optimize until the identification of the solution support. We propose in this paper the first Lasso working set algorithm for non-convex sparse regularizers with convergence guarantees. The algorithm, named FireWorks, is based on a non-convex reformulation of a recent duality-based approach and leverages on the geometry of the residuals. We provide theoretical guarantees showing that convergence is preserved even when the inner solver is inexact, under sufficient decay of the error across iterations. Experimental results demonstrate strong computational gain when using our working set strategy compared to full problem solvers for both block-coordinate descent or a proximal gradient solver.",,,,,"GASSO, Gilles/HCI-8911-2022",,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072705013,0
C,"Saad, FA; Cusumano-Towner, M; Mansinghka, VK",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Saad, Feras A.; Cusumano-Towner, Marco; Mansinghka, Vikash K.",,,Estimators of Entropy and Information via Inference in Probabilistic Models,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Estimating information-theoretic quantities such as entropy and mutual information is central to many problems in statistics and machine learning, but challenging in high dimensions. This paper presents estimators of entropy via inference (EEVI), which deliver upper and lower bounds on many information quantities for arbitrary variables in a probabilistic generative model. These estimators use importance sampling with proposal distribution families that include amortized variational inference and sequential Monte Carlo, which can be tailored to the target model and used to squeeze true information values with high accuracy. We present several theoretical properties of EEVI and demonstrate scalability and efficacy on two problems from the medical domain: (i) in an expert system for diagnosing liver disorders, we rank medical tests according to how informative they are about latent diseases, given a pattern of observed symptoms and patient attributes; and (ii) in a differential equation model of carbohydrate metabolism, we find optimal times to take blood glucose measurements that maximize information about a diabetic patient's insulin sensitivity, given their meal and medication schedule.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,5604,5621,,,,,,,,,,,,,,,,WOS:000841852300001,0
C,"Tonekaboni, S; Li, CL; Arik, SO; Goldenberg, A; Pfister, T",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Tonekaboni, Sana; Li, Chun-Liang; Arik, Sercan O.; Goldenberg, Anna; Pfister, Tomas",,,Decoupling Local and Global Representations of Time Series,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Real-world time series data are often generated from several sources of variation. Learning representations that capture the factors contributing to this variability enables a better understanding of the data via its underlying generative process and improves performance on downstream machine learning tasks. This paper proposes a novel generative approach for learning representations for the global and local factors of variation in time series. The local representation of each sample models non-stationarity over time with a stochastic process prior, and the global representation of the sample encodes the time-independent characteristics. To encourage decoupling between the representations, we introduce counterfactual regularization that minimizes the mutual information between the two variables. In experiments, we demonstrate successful recovery of the true local and global variability factors on simulated data, and show that representations learned using our method yield superior performance on downstream tasks on real-world datasets. We believe that the proposed way of defining representations is beneficial for data modelling and yields better insights into the complexity of real-world data.(1)",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852303007,0
C,"Wang, Z; Xing, W; Kirby, RM; Zhe, SD",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Wang, Zheng; Xing, Wei; Kirby, Robert M.; Zhe, Shandian",,,Physics Informed Deep Kernel Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Deep kernel learning is a promising combination of deep neural networks and nonparametric function estimation. However, as a data driven approach, the performance of deep kernel learning can still be restricted by scarce or insufficient data, especially in extrapolation tasks. To address these limitations, we propose Physics Informed Deep Kernel Learning (PI-DKL) that exploits physics knowledge represented by differential equations with latent sources. Specifically, we use the posterior function sample of the Gaussian process as the surrogate for the solution of the differential equation, and construct a generative component to integrate the equation in a principled Bayesian hybrid framework. For efficient and effective inference, we marginalize out the latent variables in the joint probability and derive a collapsed model evidence lower bound (ELBO), based on which we develop a stochastic model estimation algorithm. Our ELBO can be viewed as a nice, interpretable posterior regularization objective. On synthetic datasets and real-world applications, we show the advantage of our approach in both prediction accuracy and uncertainty quantification. The code is available at https://github.com/GregDobby/PIDKL.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072701012,0
C,"Wei, HH; Liu, X; Ying, L",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Wei, Honghao; Liu, Xin; Ying, Lei",,,Triple-Q: A Model-Free Algorithm for Constrained Reinforcement Learning with Sublinear Regret and Zero Constraint Violation,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"This paper presents the first model-free, simulator-free reinforcement learning algorithm for Constrained Markov Decision Processes (CMDPs) with sublinear regret and zero constraint violation. The algorithm is named Triple-Q because it includes three key components: a Q-function (also called action-value function) for the cumulative reward, a Q-function for the cumulative utility for the constraint, and a virtual-Queue that (over)estimates the cumulative constraint violation. Under Triple-Q, at each step, an action is chosen based on the pseudo-Q-value that is a combination of the three Q values. The algorithm updates the reward and utility Q-values with learning rates that depend on the visit counts to the corresponding (state, action) pairs and are periodically reset. In the episodic CMDP setting, Triple-Q achieves (O) over tilde (1/delta H(4)S(1/2)A(1/2)K(4/5)) regret(1), where K is the total number of episodes, H is the number of steps in each episode, S is the number of states, A is the number of actions, and delta is Slater's constant. Furthermore, Triple-Q guarantees zero constraint violation, both on expectation and with a high probability, when K is sufficiently large. Finally, the computational complexity of Triple-Q is similar to SARSA for unconstrained MDPs, and is computationally efficient.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072703017,0
C,"Xu, WN; Chen, RTQ; Li, XC; Duvenaud, D",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Xu, Winnie; Chen, Ricky T. Q.; Li, Xuechen; Duvenaud, David",,,Infinitely Deep Bayesian Neural Networks with Stochastic Differential Equations,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We perform scalable approximate inference in continuous-depth Bayesian neural networks. In this model class, uncertainty about separate weights in each layer gives hidden units that follow a stochastic differential equation. We demonstrate gradient-based stochastic variational inference in this infinite-parameter setting, producing arbitrarily-flexible approximate posteriors. We also derive a novel gradient estimator that approaches zero variance as the approximate posterior over weights approaches the true posterior. This approach brings continuous-depth Bayesian neural nets to a competitive comparison against discrete-depth alternatives, while inheriting the memory-efficient training and tunable precision of Neural ODEs.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,721,738,,,,,,,,,,,,,,,,WOS:000828072700030,0
C,"Zhang, WJ; Zhang, YK; Hu, XL; Goswami, M; Chen, C; Metaxas, D",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Zhang, Wenjia; Zhang, Yikai; Hu, Xiaolin; Goswami, Mayank; Chen, Chao; Metaxas, Dimitris",,,A Manifold View of Adversarial Risk,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"The adversarial risk of a machine learning model has been widely studied. Most previous works assume that the data lies in the whole ambient space. We propose to take a new angle and take the manifold assumption into consideration. Assuming data lies in a manifold, we investigate two new types of adversarial risk, the normal adversarial risk due to perturbation along normal direction, and the in-manifold adversarial risk due to perturbation within the manifold. We prove that the classic adversarial risk can be bounded from both sides using the normal and in-manifold adversarial risks. We also show with a surprisingly pessimistic case that the standard adversarial risk can be nonzero even when both normal and in-manifold risks are zero. We finalize the paper with empirical studies supporting our theoretical results. Our results suggest the possibility of improving the robustness of a classifier by only focusing on the normal adversarial risk.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852306006,0
C,"Zhou, RD; Tian, C",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Zhou, Ruida; Tian, Chao",,,Approximate Top-m Arm Identification with Heterogeneous Reward Variances,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We study the effect of reward variance heterogeneity in the approximate top-m arm identification setting. In this setting, the reward for the i-th arm follows a sigma(2)(i)-subGaussian distribution, and the agent needs to incorporate this knowledge to minimize the expected number of arm pulls to identify m arms with the largest means within error epsilon out of the n arms, with probability at least 1 - delta. We show that the worst-case sample complexity of this problem is Theta(Sigma(n)(i=1) sigma(2)(i)/epsilon(2) ln 1/delta + Sigma(i is an element of Gm) sigma(2)(i)/epsilon(2) ln(m) + Sigma(j is an element of Gl) sigma(2)(j)/epsilon(2) Ent(sigma(2)(Gr))), where G(m), G(l), G(r) are certain specific subsets of the overall arm set {1, 2, ..., n}, and Ent(.) is an entropy-like function which measures the heterogeneity of the variance proxies. The upper bound of the complexity is obtained using a divide-and-conquer style algorithm, while the matching lower bound relies on the study of a dual formulation.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852301040,0
C,"Bruinsma, WP; Tegner, M; Turner, RE",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Bruinsma, Wessel P.; Tegner, Martin; Turner, Richard E.",,,Modelling Non-Smooth Signals with Complex Spectral Structure,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"The Gaussian Process Convolution Model (GPCM; Tobar et al., 2015a) is a model for signals with complex spectral structure. A significant limitation of the GPCM is that it assumes a rapidly decaying spectrum: it can only model smooth signals. Moreover, inference in the GPCM currently requires (1) a mean-field assumption, resulting in poorly calibrated uncertainties, and (2) a tedious variational optimisation of large co-variance matrices. We redesign the GPCM model to induce a richer distribution over the spectrum with relaxed assumptions about smoothness: the Causal Gaussian Process Convolution Model (CGPCM) introduces a causality assumption into the GPCM, and the Rough Gaussian Process Convolution Model (RGPCM) can be interpreted as a Bayesian nonparametric generalisation of the fractional Ornstein{Uhlenbeck process. We also propose a more effective variational inference scheme, going beyond the meanfield assumption: we design a Gibbs sampler which directly samples from the optimal variational solution, circumventing any variational optimisation entirely. The proposed variations of the GPCM are validated in experiments on synthetic and real-world data, showing promising results.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072705012,0
C,"Clerico, E; Deligiannidis, G; Doucet, A",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Clerico, Eugenio; Deligiannidis, George; Doucet, Arnaud",,,Conditionally Gaussian PAC-Bayes,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Recent studies have empirically investigated different methods to train stochastic neural networks on a classification task by optimising a PAC-Bayesian bound via stochastic gradient descent. Most of these procedures need to replace the misclassification error with a surrogate loss, leading to a mismatch between the optimisation objective and the actual generalisation bound. The present paper proposes a novel training algorithm that optimises the PAC-Bayesian bound, without relying on any surrogate loss. Empirical results show that this approach outperforms currently available PAC-Bayesian training methods.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072702016,0
C,"Deng, ZH; Devic, S; Juba, B",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Deng, Zihao; Devic, Siddartha; Juba, Brendan",,,Polynomial Time Reinforcement Learning in Factored State MDPs with Linear Value Functions,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Many reinforcement learning (RL) environments in practice feature enormous state spaces that may be described compactly by a factored structure, that may be modeled by Factored Markov Decision Processes (FMDPs). We present the first polynomial-time algorithm for RL in Factored State MDPs (generalizing FMDPs) that neither relies on an oracle planner nor requires a linear transition model; it only requires a linear value function with a suitable local basis with respect to the factorization, permitting efficient variable elimination. With this assumption, we can solve this family of Factored State MDPs in polynomial time by constructing an efficient separation oracle for convex optimization. Importantly, and in contrast to prior work on FMDPs, we do not assume that the transitions on various factors are conditionally independent.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852305036,0
C,"Esfandiari, H; Mirrokni, V; Syed, U; Vassilvitskii, S",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Esfandiari, Hossein; Mirrokni, Vahab; Syed, Umar; Vassilvitskii, Sergei",,,Label differential privacy via clustering,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We present new mechanisms for label differential privacy, a relaxation of differentially private machine learning that only protects the privacy of the labels in the training set. Our mechanisms cluster the examples in the training set using their (non-private) feature vectors, randomly re-sample each label from examples in the same cluster, and output a training set with noisy labels as well as a modified version of the true loss function. We prove that when the clusters are both large and homogeneous, the model that minimizes the modified loss on the noisy training set converges to small excess risk at a rate that is comparable to the rate for non-private learning. We also describe a learning problem in which large clusters are necessary to achieve both strong privacy and either good precision or good recall. Our experiments show that randomizing the labels within each cluster significantly improves the privacy vs. accuracy trade-off compared to applying uniform randomized response to the labels, and also compared to learning a model via DP-SGD.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852301023,0
C,"Hamid, S; Schulze, S; Osborne, MA; Roberts, SJ",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Hamid, Saad; Schulze, Sebastian; Osborne, Michael A.; Roberts, Stephen J.",,,Marginalising over Stationary Kernels with Bayesian Quadrature,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Marginalising over families of Gaussian Process kernels produces flexible model classes with well-calibrated uncertainty estimates. Existing approaches require likelihood evaluations of many kernels, rendering them prohibitively expensive for larger datasets. We propose a Bayesian Quadrature scheme to make this marginalisation more efficient and thereby more practical. Through use of the maximum mean discrepancies between distributions, we define a kernel over kernels that captures invariances between Spectral Mixture (SM) Kernels. Kernel samples are selected by generalising an information-theoretic acquisition function for warped Bayesian Quadrature. We show that our framework achieves more accurate predictions with better calibrated uncertainty than state-of-the-art baselines, especially when given limited (wall-clock) time budgets.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852304014,0
C,"Hartmann, M; Girolami, M; Klami, A",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Hartmann, Marcelo; Girolami, Mark; Klami, Arto",,,Lagrangian Manifold Monte Carlo on Monge Patches,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"The efficiency of Markov Chain Monte Carlo (MCMC) depends on how the underlying geometry of the problem is taken into account. For distributions with strongly varying curvature, Riemannian metrics help in efficient exploration of the target distribution. Unfortunately, they have significant computational overhead due to e.g. repeated inversion of the metric tensor, and current geometric MCMC methods using the Fisher information matrix to induce the manifold are in practice slow. We propose a new alternative Riemannian metric for MCMC, by embedding the target distribution into a higher-dimensional Euclidean space as a Monge patch and using the induced metric determined by direct geometric reasoning. Our metric only requires first-order gradient information and has fast inverse and determinants, and allows reducing the computational complexity of individual iterations from cubic to quadratic in the problem dimensionality. We demonstrate how Lagrangian Monte Carlo in this metric efficiently explores the target distributions.",,,,,"Klami, Arto/E-7227-2012","Klami, Arto/0000-0002-7950-1355",,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072704039,0
C,"Heckel, R",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Heckel, Reinhard",,,Provable Continual Learning via Sketched Jacobian Approximations,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"An important problem in machine learning is the ability to learn tasks in a sequential manner. If trained with standard first-order methods most models forget previously learned tasks when trained on a new task, which is often referred to as catastrophic forgetting. A popular approach to overcome forgetting is to regularize the loss function by penalizing models that perform poorly on previous tasks. For example, elastic weight consolidation (EWC) regularizes with a quadratic form involving a diagonal matrix build based on past data. While EWC works very well for some setups, we show that, even under otherwise ideal conditions, it can provably suffer catastrophic forgetting if the diagonal matrix is a poor approximation of the Hessian matrix of previous tasks. We propose a simple approach to overcome this: Regularizing training of a new task with sketches of the Jacobian matrix of past data. This provably enables overcoming catastrophic forgetting for linear models and for wide neural networks, at the cost of memory. The overarching goal of this paper is to provided insights on when regularization-based continual learning algorithms work and under what memory costs.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852304040,0
C,"Liu, Y; Zhou, YF; Li, P; Hu, FF",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Liu, Yang; Zhou, Yifan; Li, Ping; Hu, Feifang",,,Adaptive A/B Test on Networks with Cluster Structures,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Units in online A/B tests are often involved in social networks. Thus, their outcomes may depend on the treatment of their neighbors. Many of such networks exhibit certain cluster structures allowing the use of these features in the design to reduce the bias from network interference. When the average treatment effect (ATE) is considered from the individual perspective, conditions for the valid estimation restrict the use of these features in the design. We show that such restrictions can be alleviated if the ATE from the cluster perspective is considered. Using an illustrative example, we further show that the weights employed by the Horvitz-Thompson estimator may not appropriately accommodate the network structure, and purely relying on graph-cluster randomization may generate very unbalanced cluster-treated structures across the treatment arms. The measures of such structures for one cluster may depend on the treatment of other clusters and pose a great challenge for the design of A/B tests. To address these issues, we propose a rerandomized-adaptive randomization to balance the clusters and a cluster-adjusted estimator to alleviate the problem of the weights. Numerical studies are conducted to demonstrate the usage of the proposed procedure.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852305015,0
C,"Mazumdar, A; Pal, S",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Mazumdar, Arya; Pal, Soumyabrata",,,On Learning Mixture Models with Sparse Parameters,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Mixture models are widely used to fit complex and multimodal datasets. In this paper we study mixtures with high dimensional sparse latent parameter vectors and consider the problem of support recovery of those vectors. While parameter learning in mixture models is well-studied, the sparsity constraint remains relatively unexplored. Sparsity of parameter vectors is a natural constraint in variety of settings, and support recovery is a major step towards parameter estimation. We provide efficient algorithms for support recovery that have a logarithmic sample complexity dependence on the dimensionality of the latent space. Our algorithms are quite general, namely they are applicable to 1) mixtures of many different canonical distributions including Uniform, Poisson, Laplace, Gaussians, etc. 2) Mixtures of linear regressions and linear classifiers with Gaussian covariates under different assumptions on the unknown parameters. In most of these settings, our results are the first guarantees on this problem while in the rest, we provide significant improvements on existing results in certain regimes.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852303026,0
C,"Milionis, J; Kalavasis, A; Fotakis, D; Ioannidis, S",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Milionis, Jason; Kalavasis, Alkis; Fotakis, Dimitris; Ioannidis, Stratis",,,Differentially Private Regression with Unbounded Covariates,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We provide computationally efficient, differentially private algorithms for the classical regression settings of Least Squares Fitting, Binary Regression and Linear Regression with unbounded covariates. Prior to our work, privacy constraints in such regression settings were studied under strong a priori bounds on covariates. We consider the case of Gaussian marginals and extend recent differentially private techniques on mean and covariance estimation (Kamath et al., 2019; Karwa and Vadhan, 2018) to the sub-gaussian regime. We provide a novel technical analysis yielding differentially private algorithms for the above classical regression settings. Through the case of Binary Regression, we capture the fundamental and widely-studied models of logistic regression and linearly-separable SVMs, learning an unbiased estimate of the true regression vector, up to a scaling factor.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072703016,0
C,"Naderiparizi, S; Scibior, A; Munk, A; Ghadiri, M; Baydin, AG; Gram-Hansen, B; de Witt, CS; Zinkov, R; Torr, P; Rainforth, T; Teh, YW; Wood, F",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Naderiparizi, Saeid; Scibior, Adam; Munk, Andreas; Ghadiri, Mehrdad; Baydin, Atilim Gunes; Gram-Hansen, Bradley; de Witt, Christian Schroeder; Zinkov, Robert; Torr, Philip; Rainforth, Tom; Teh, Yee Whye; Wood, Frank",,,Amortized Rejection Sampling in Universal Probabilistic Programming,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,Naive approaches to amortized inference in probabilistic programs with unbounded loops can produce estimators with infinite variance. This is particularly true of importance sampling inference in programs that explicitly include rejection sampling as part of the user-programmed generative procedure. In this paper we develop a new and efficient amortized importance sampling estimator. We prove finite variance of our estimator and empirically demonstrate our method's correctness and efficiency compared to existing alternatives on generative programs containing rejection sampling loops and discuss how to implement our method in a generic probabilistic programming framework.,,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852302035,0
C,"Nguyen, J; Malik, K; Zhan, HY; Yousefpour, A; Rabbat, M; Malek, M; Huba, D",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Nguyen, John; Malik, Kshitiz; Zhan, Hongyuan; Yousefpour, Ashkan; Rabbat, Michael; Malek, Mani; Huba, Dzmitry",,,Federated Learning with Buffered Asynchronous Aggregation,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Scalability and privacy are two critical concerns for cross-device federated learning (FL) systems. In this work, we identify that synchronous FL - synchronized aggregation of client updates in FL - cannot scale efficiently beyond a few hundred clients training in parallel. It leads to diminishing returns in model performance and training speed, analogous to large-batch training. On the other hand, asynchronous aggregation of client updates in FL (i.e., asynchronous FL) alleviates the scalability issue. However, aggregating individual client updates is incompatible with Secure Aggregation, which could result in an undesirable level of privacy for the system. To address these concerns, we propose a novel buffered asynchronous aggregation method, FedBuff, that is agnostic to the choice of optimizer, and combines the best properties of synchronous and asynchronous FL. We empirically demonstrate that FedBuff is 3.3x more efficient than synchronous FL and up to 2.5x more efficient than asynchronous FL, while being compatible with privacy-preserving technologies such as Secure Aggregation and differential privacy. We provide theoretical convergence guarantees in a smooth non-convex setting. Finally, we show that under differentially private training, FedBuff can outperform FedAvgM at low privacy settings and achieve the same utility for higher privacy settings.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072703029,0
C,"Patil, P; Rinaldo, A; Tibshirani, RJ",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Patil, Pratik; Rinaldo, Alessandro; Tibshirani, Ryan J.",,,Estimating Functionals of the Out-of-Sample Error Distribution in High-Dimensional Ridge Regression,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We study the problem of estimating the distribution of the out-of-sample prediction error associated with ridge regression. In contrast, the traditional object of study is the uncentered second moment of this distribution (the mean squared prediction error), which can be estimated using cross-validation methods. We show that both generalized and leave-one-out cross-validation (GCV and LOOCV) for ridge regression can be suitably extended to estimate the full error distribution. This is still possible in a high-dimensional setting where the ridge regularization parameter is zero. In an asymptotic framework in which the feature dimension and sample size grow proportionally, we prove that almost surely, with respect to the training data, our estimators (extensions of GCV and LOOCV) converge weakly to the true out-of-sample error distribution. This result requires mild assumptions on the response and feature distributions. We also establish a more general result that allows us to estimate certain functionals of the error distribution, both linear and nonlinear. This yields various applications, including consistent estimation of the quantiles of the out-of-sample error distribution, which gives rise to prediction intervals with asymptotically exact coverage conditional on the training data.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,6087,6120,,,,,,,,,,,,,,,,WOS:000841852300023,0
C,"Peleg, A; Pearl, N; Meir, R",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Peleg, Amit; Pearl, Naama; Meir, Ron",,,Metalearning Linear Bandits by Prior Update,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Fully Bayesian approaches to sequential decision-making assume that problem parameters are generated from a known prior. In practice, such information is often lacking. This problem is exacerbated in setups with partial information, where a misspecified prior may lead to poor exploration and performance. In this work we prove, in the context of stochastic linear bandits and Gaussian priors, that as long as the prior is sufficiently close to the true prior, the performance of the applied algorithm is close to that of the algorithm that uses the true prior. Furthermore, we address the task of learning the prior through metalearning, where a learner updates her estimate of the prior across multiple task instances in order to improve performance on future tasks. We provide an algorithm and regret bounds, demonstrate its effectiveness in comparison to an algorithm that knows the correct prior, and support our theoretical results empirically. Our theoretical results hold for a broad class of algorithms, including Thompson Sampling and Information Directed Sampling.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072702040,0
C,"Poignard, B; Naylor, P; Climento-Gonzalez, H; Yamada, M",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Poignard, Benjamin; Naylor, Peter; Climento-Gonzalez, Hector; Yamada, Makoto",,,Feature screening with kernel knockoffs,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"This article analyses three feature screening procedures: Kendall's Tau and Spearman Rho (TR), Hilbert-Schmidt Independence Criterion (HSIC) and conditional Maximum Mean Discrepancy (cMMD), where the latter is a modified version of the standard MMD for categorical classification. These association measures are not based on any specific underlying model, such as the linear regression. We provide the conditions for which the sure independence screening (SIS) property is satisfied under a lower bound assumption on the minimum signal strength of the association measure. The SIS property for the HSIC and cMMD is established for given bounded and symmetric kernels. Within the high-dimensional setting, we propose a two-step approach to control the false discovery rate (FDR) using the knockoff filtering. The performances of the association measures are assessed through simulated and real data experiments and compared with existing competing screening methods.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072702001,0
C,"Rath, P; Hughes, MC",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Rath, Preetish; Hughes, Michael C.",,,Optimizing Early Warning Classifiers to Control False Alarms via a Minimum Precision Constraint,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Early warning prediction systems can suffer from high false alarm rates that limit utility, especially in settings with high class imbalance such as healthcare. Despite the widespread need to control false alarms, the dominant classifier training paradigm remains minimizing cross entropy, a loss function which does not treat false alarms differently than other types of mistakes. While existing efforts often try to reduce false alarms by post-hoc threshold selection after training, we suggest a comprehensive solution by changing the loss function used to train the classifier. Our proposed objective maximizes recall while enforcing a constraint requiring precision to exceed a specified value. We make our objective tractable for gradient-based optimization by developing tight sigmoidal bounds on the counts needed to compute precision and recall. Our objective is applicable to any classifier trainable via gradient descent, including linear models and neural networks. When predicting mortality risk across two large hospital datasets, we show how our method satisfies a desired constraint on false alarms while achieving better recall than alternatives.",,,,,,"Hughes, Michael/0000-0003-4859-7400",,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072705001,0
C,"Ruppert, J; Aleksandrova, M; Engel, T",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Ruppert, Jean; Aleksandrova, Marharyta; Engel, Thomas",,,k-Pareto Optimality-Based Sorting with Maximization of Choice,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Topological sorting(1) is an important technique in numerous practical applications, such as information retrieval, recommender systems, optimization, etc. In this paper, we introduce a problem of generalized topological sorting with maximization of choice, that is, of choosing a subset of items of a predefined size that contains the maximum number of equally preferable options (items) with respect to a dominance relation. We formulate this problem in a very abstract form and prove that sorting by k-Pareto optimality yields a valid solution. Next, we show that the proposed theory can be useful in practice. We apply it during the selection step of genetic optimization and demonstrate that the resulting algorithm outperforms existing state-of-the-art approaches such as NSGA-II and NSGA-III. We also demonstrate that the provided general formulation allows discovering interesting relationships and applying the developed theory to different applications.",,,,,,"Ruppert, Jean/0000-0002-9514-1230",,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072701008,0
C,"Zhang, XH; Blanchet, J; Ghosh, S; Squillante, MS",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Zhang, Xuhui; Blanchet, Jose; Ghosh, Soumyadip; Squillante, Mark S.",,,A Class of Geometric Structures in Transfer Learning: Minimax Bounds and Optimality,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We study the problem of transfer learning, observing that previous efforts to understand its information-theoretic limits do not fully exploit the geometric structure of the source and target domains. In contrast, our study first illustrates the benefits of incorporating a natural geometric structure within a linear regression model, which corresponds to the generalized eigenvalue problem formed by the Gram matrices of both domains. We next establish a finite-sample minimax lower bound, propose a refined model interpolation estimator that enjoys a matching upper bound, and then extend our framework to multiple source domains and generalized linear models. Surprisingly, as long as information is available on the distance between the source and target parameters, negative-transfer does not occur. Simulation studies show that our proposed interpolation estimator outperforms state-of-the-art transfer learning methods in both moderate- and high-dimensional settings.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072703039,0
C,"Baby, D; Wang, YX",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Baby, Dheeraj; Wang, Yu-Xiang",,,Optimal Dynamic Regret in Proper Online Learning with Strongly Convex Losses and Beyond,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We study the framework of universal dynamic regret minimization with strongly convex losses. We answer an open problem in (Baby and Wang, 2021) by showing that in a proper learning setup, Strongly Adaptive algorithms can achieve the near optimal dynamic regret of (O) over tilde (d(1/3)n(1/3) TV[u(1:n)](2/3) V d) against any comparator sequence u(1); : : :; u(n) simultaneously, where n is the time horizon and TV[u1:n] is the Total Variation of comparator. These results are facilitated by exploiting a number of new structures imposed by the KKT conditions that were not considered in (Baby and Wang, 2021) which also lead to other improvements over their results such as: (a) handling non-smooth losses and (b) improving the dimension dependence on regret. Further, we also derive near optimal dynamic regret rates for the special case of proper online learning with exp-concave losses and an L1 constrained decision set.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072701040,0
C,"Bhattacharyya, A; Gayen, S; Kandasamy, S; Raval, V; Vinodchandran, NV",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Bhattacharyya, Arnab; Gayen, Sutanu; Kandasamy, Saravanan; Raval, Vedant; Vinodchandran, N. V.",,,Efficient Interventional Distribution Learning in the PAC Framework,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We consider the problem of efficiently inferring interventional distributions in a causal Bayesian network from a finite number of observations. Let P be a causal model on a set V of observable variables on a given causal graph G. For sets X, Y subset of V, and setting x to X, P-x (Y) denotes the interventional distribution on Y with respect to an intervention x to variables X. Shpitser and Pearl (AAAI 2006), building on the work of Tian and Pearl (AAAI 2001), proved that the ID algorithm is sound and complete for recovering P-x (Y) from observations. We give the first provably efficient version of the ID algorithm. In particular, under natural assumptions, we give a polynomialtime algorithm that on input a causal graph G on observable variables V, a setting x of a set X subset of V of bounded size, outputs succinct descriptions of both an evaluator and a generator for a distribution (P) over cap that is epsilon-close (in total variation distance) to P-x (Y) where Y = V \ X, if P-x (Y) is identifiable. We also show that when Y is an arbitrary subset of V \ X, there is no efficient algorithm that outputs an evaluator of a distribution that is E-close to P-x (Y) unless all problems that have statistical zero-knowledge proofs, including the Graph Isomorphism problem, have efficient randomized algorithms.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852301043,0
C,"Chakrabarti, D; Dickerson, JP; Esmaeili, SA; Srinivasan, A; Tsepenekas, L",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Chakrabarti, Darshan; Dickerson, John P.; Esmaeili, Seyed A.; Srinivasan, Aravind; Tsepenekas, Leonidas",,,A New Notion of Individually Fair Clustering: alpha-Equitable k-Center,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Clustering is a fundamental problem in unsupervised machine learning, and due to its numerous societal implications fair variants of it have recently received significant attention. In this work we introduce a novel definition of individual fairness for clustering problems. Specifically, in our model, each point j has a set of other points S-j that it perceives as similar to itself, and it feels that it is being fairly treated if the quality of service it receives in the solution is alpha-close (in a multiplicative sense, for some given alpha >= 1) to that of the points in S-j. We begin our study by answering questions regarding the combinatorial structure of the problem, namely for what values of a the problem is well-defined, and what the behavior of the Price of Fairness (PoF) for it is. For the well-defined region of alpha, we provide efficient and easily-implementable approximation algorithms for the k-center objective, which in certain cases also enjoy bounded-PoF guarantees. We finally complement our analysis by an extensive suite of experiments that validates the effectiveness of our theoretical results.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,6387,6408,,,,,,,,,,,,,,,,WOS:000841852300036,0
C,"de Souza, DA; Mesquita, D; Kaski, S; Acerbi, L",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"de Souza, Daniel Augusto; Mesquita, Diego; Kaski, Samuel; Acerbi, Luigi",,,Parallel MCMC Without Embarrassing Failures,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Embarrassingly parallel Markov Chain Monte Carlo (MCMC) exploits parallel computing to scale Bayesian inference to large datasets by using a two-step approach. First, MCMC is run in parallel on (sub)posteriors defined on data partitions. Then, a server combines local results. While efficient, this framework is very sensitive to the quality of subposterior sampling. Common sampling problems such as missing modes or misrepresentation of low-density regions are amplified - instead of being corrected - in the combination phase, leading to catastrophic failures. In this work, we propose a novel combination strategy to mitigate this issue. Our strategy, Parallel Active Inference (PAI), leverages Gaussian Process (GP) surrogate modeling and active learning. After fitting GPs to subposteriors, PAI (i) shares information between GP surrogates to cover missing modes; and (ii) uses active sampling to individually refine subposterior approximations. We validate PAI in challenging benchmarks, including heavy-tailed and multi-modal posteriors and a real-world application to computational neuroscience. Empirical results show that PAI succeeds where previous methods catastrophically fail, with a small communication overhead.",,,,,"Kaski, Samuel/B-6684-2008","Kaski, Samuel/0000-0003-1925-9154",,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072701039,0
C,"Dou, ZH; Yang, ZR; Wang, ZR; Du, SS",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Dou, Zehao; Yang, Zhuoran; Wang, Zhaoran; Du, Simon S.",,,Gap-Dependent Bounds for Two-Player Markov Games,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"As one of the most popular methods in the field of reinforcement learning, Q-learning has received increasing attention. Recently, there have been more theoretical works on the regret bound of algorithms that belong to the Q-learning class in different settings. In this paper, we analyze the cumulative regret when conducting Nash Q-learning algorithm on 2-player turn-based stochastic Markov games (2-TBSG), and propose the first gap dependent logarithmic upper bounds in the episodic tabular setting. This bound matches the lower bound only up to a horizon term. Furthermore, we extend the conclusion to the discounted game setting with infinite horizon and propose a similar gap dependent logarithmic regret bound. In addition, under the linear MDP assumption, we obtain another logarithmic regret for 2-TBSG, in both centralized and independent settings.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,432,455,,,,,,,,,,,,,,,,WOS:000828072700018,0
C,"Guo, WS; Agrawal, KK; Grover, A; Muthukumar, V; Pananjady, A",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Guo, Wenshuo; Agrawal, Kumar Krishna; Grover, Aditya; Muthukumar, Vidya; Pananjady, Ashwin",,,Learning from an Exploring Demonstrator: Optimal Reward Estimation for Bandits,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We introduce the inverse bandit problem of estimating the rewards of a multi-armed bandit instance from observing the learning process of a low-regret demonstrator. Existing approaches to the related problem of inverse reinforcement learning assume the execution of an optimal policy, and thereby suffer from an identifiability issue. In contrast, we propose to leverage the demonstrator's behavior en route to optimality, and in particular, the exploration phase, for reward estimation. We begin by establishing a general information-theoretic lower bound under this paradigm that applies to any demonstrator algorithm, which characterizes a fundamental tradeoff between reward estimation and the amount of exploration of the demonstrator. Then, we develop simple and efficient reward estimators for upper-confidence-based demonstrator algorithms that attain the optimal tradeoff, showing in particular that consistent reward estimation-free of identifiability-issues is possible under our paradigm. Extensive simulations on both synthetic and semi-synthetic data corroborate our theoretical results.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,6357,6386,,,,,,,,,,,,,,,,WOS:000841852300035,0
C,"Hong, J; Kveton, B; Zaheer, M; Ghavamzadeh, M; Boutilier, C",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Hong, Joey; Kveton, Branislav; Zaheer, Manzil; Ghavamzadeh, Mohammad; Boutilier, Craig",,,Thompson Sampling with a Mixture Prior,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We study Thompson sampling (TS) in online decision making, where the uncertain environment is sampled from a mixture distribution. This is relevant in multi-task learning, where a learning agent faces different classes of problems. We incorporate this structure in a natural way by initializing TS with a mixture prior, and call the resulting algorithm MixTS. To analyze MixTS, we develop a novel and general proof technique for analyzing the concentration of mixture distributions. We use it to prove Bayes regret bounds for MixTS in both linear bandits and finite-horizon reinforcement learning. Our bounds capture the structure of the prior, depend on the number of mixture components and their widths. We also demonstrate the empirical effectiveness of MixTS in synthetic and real-world experiments.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852302002,0
C,"Karppa, M; Aumuller, M; Pagh, R",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Karppa, Matti; Aumueller, Martin; Pagh, Rasmus",,,DEANN: Speeding up Kernel-Density Estimation using Approximate Nearest Neighbor Search,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Kernel Density Estimation (KDE) is a non-parametric method for estimating the shape of a density function, given a set of samples from the distribution. Recently, locality-sensitive hashing, originally proposed as a tool for nearest neighbor search, has been shown to enable fast KDE data structures. However, these approaches do not take advantage of the many other advances that have been made in algorithms for nearest neighbor algorithms. We present an algorithm called Density Estimation from Approximate Nearest Neighbors (DEANN) where we apply Approximate Nearest Neighbor (ANN) algorithms as a black box subroutine to compute an unbiased KDE. The idea is to find points that have a large contribution to the KDE using ANN, compute their contribution exactly, and approximate the remainder with Random Sampling (RS). We present a theoretical argument that supports the idea that an ANN subroutine can speed up the evaluation. Furthermore, we provide a C++ implementation with a Python interface that can make use of an arbitrary ANN implementation as a subroutine for kernel density estimation. We show empirically that our implementation outperforms state of the art implementations in all high dimensional datasets we considered, and matches the performance of RS in cases where the ANN yield no gains in performance.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072703010,0
C,"Li, SB; Wang, Z; Kirby, RM; Zhe, S",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Li, Shibo; Wang, Zheng; Kirby, Robert M.; Zhe, Shandian",,,Deep Multi-Fidelity Active Learning of High-Dimensional Outputs,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Many applications, such as in physical simulation and engineering design, demand we estimate functions with high-dimensional outputs. To reduce the expensive cost of generating training examples, we usually choose several fidelities to enable a cost/quality trade-off. In this paper, we consider the active learning task to automatically identify the fidelities and training inputs to query new examples so as to achieve the best learning benefitcost ratio. To this end, we propose DMFAL, a Deep Multi-Fidelity Active Learning approach. We first develop a deep neural network based multi-fidelity model for high-dimensional outputs, which can flexibly capture strong complex correlations across the outputs and fidelities to enhance the learning of the target function. We then propose a mutual information based acquisition function that extends the predictive entropy principle. To overcome the computational challenges caused by large output dimensions, we use the multi-variate delta method and moment-matching to estimate the output posterior, and Weinstein-Aronszajn identity to calculate and optimize the acquisition function. We show the advantage of our method in several applications of computational physics and engineering design. The code is available at https://github.com/shib0li/DMFAL.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072701035,0
C,"Meanti, G; Carratino, L; De Vito, E; Rosasco, L",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Meanti, Giacomo; Carratino, Luigi; De Vito, Ernesto; Rosasco, Lorenzo",,,Efficient Hyperparameter Tuning for Large Scale Kernel Ridge Regression,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Kernel methods provide a principled approach to nonparametric learning. While their basic implementations scale poorly to large problems, recent advances showed that approximate solvers can efficiently handle massive datasets. A shortcoming of these solutions is that hyperparameter tuning is not taken care of, and left for the user to perform. Hyperparameters are crucial in practice and the lack of automated tuning greatly hinders efficiency and usability. In this paper, we work to fill in this gap focusing on kernel ridge regression based on the Nystrom approximation. After reviewing and contrasting a number of hyperparameter tuning strategies, we propose a complexity regularization criterion based on a data dependent penalty, and discuss its efficient optimization. Then, we proceed to a careful and extensive empirical evaluation highlighting strengths and weaknesses of the different tuning strategies. Our analysis shows the benefit of the proposed approach, that we hence incorporate in a library for large scale kernel methods to derive adaptively tuned solutions.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852301001,0
C,"Nikitin, A; John, ST; Solin, A; Kaski, S",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Nikitin, Alexander; John, S. T.; Solin, Arno; Kaski, Samuel",,,Non-separable Spatio-temporal Graph Kernels via SPDEs,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Gaussian processes (GPs) provide a principled and direct approach for inference and learning on graphs. However, the lack of justified graph kernels for spatio-temporal modelling has held back their use in graph problems. We leverage an explicit link between stochastic partial di.erential equations (SPDEs) and GPs on graphs, introduce a framework for deriving graph kernels via SPDEs, and derive non-separable spatio-temporal graph kernels that capture interaction across space and time. We formulate the graph kernels for the stochastic heat equation and wave equation. We show that by providing novel tools for spatio-temporal GP modelling on graphs, we outperform pre-existing graph kernels in realworld applications that feature di.usion, oscillation, and other complicated interactions.",,,,,"Kaski, Samuel/B-6684-2008; Solin, Arno/G-6859-2012","Kaski, Samuel/0000-0003-1925-9154; Solin, Arno/0000-0002-0958-7886",,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852305008,0
C,"Noble, M; Bellet, A; Dieuleveut, A",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Noble, Maxence; Bellet, Aurelien; Dieuleveut, Aymeric",,,Differentially Private Federated Learning on Heterogeneous Data,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Federated Learning (FL) is a paradigm for large-scale distributed learning which faces two key challenges: (i) training efficiently from highly heterogeneous user data, and (ii) protecting the privacy of participating users. In this work, we propose a novel FL approach (DP-SCAFFOLD) to tackle these two challenges together by incorporating Differential Privacy (DP) constraints into the popular SCAFFOLD algorithm. We focus on the challenging setting where users communicate with a honest-but-curious server without any trusted intermediary, which requires to ensure privacy not only towards a third party observing the final model but also towards the server itself. Using advanced results from DP theory and optimization, we establish the convergence of our algorithm for convex and non-convex objectives. Our paper clearly highlights the trade-off between utility and privacy and demonstrates the superiority of DP-SCAFFOLD over the state-of-the-art algorithm DP-FedAvg when the number of local updates and the level of heterogeneity grows. Our numerical results confirm our analysis and show that DP-SCAFFOLD provides significant gains in practice.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852304029,0
C,"Opolka, FL; Lio, P",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Opolka, Felix L.; Lio, Pietro",,,Bayesian Link Prediction with Deep Graph Convolutional Gaussian Processes,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Link prediction aims to reveal missing edges in a graph. We introduce a deep graph convolutional Gaussian process model for this task, which addresses recent challenges in graph machine learning with oversmoothing and overfitting. Using simplified graph convolutions, we transform a Gaussian process to leverage the topological information of the graph domain. To scale the Gaussian process model to larger graphs, we introduce a variational inducing point method that places pseudo-inputs on a graph-structured domain. Multiple Gaussian processes are assembled into a hierarchy whose structure allows skipping convolutions and thus counteracting oversmoothing. The proposed model represents the first Gaussian process for link prediction that makes use of both node features and topological information. We evaluate our model on multiple graph data sets with up to thousands of nodes and report consistent improvements over competitive link prediction approaches.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072704042,0
C,"Tam, DSH; Xie, SY; Lau, WC",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Tam, Da Sun Handason; Xie, Siyue; Lau, Wing Cheong",,,GraphAdaMix: Enhancing Node Representations with Graph Adaptive Mixtures,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Graph Neural Networks (GNNs) are the current state-of-the-art models in learning node representations for many predictive tasks on graphs. Typically, GNNs reuses the same set of model parameters across all nodes in the graph to improve the training efficiency and exploit the translationally-invariant properties in many datasets. However, the parameter sharing scheme prevents GNNs from distinguishing two nodes having the same local structure and that the translation invariance property may not exhibit in real-world graphs. In this paper, we present Graph Adaptive Mixtures (GraphAdaMix), a novel approach for learning node representations in a graph by introducing multiple independent GNN models and a trainable mixture distribution for each node. GraphAdaMix can adapt to tasks with different settings. Specifically, for semi-supervised tasks, we op- timize GraphAdaMix using the Expectation-Maximization (EM) algorithm, while in un-supervised settings, GraphAdaMix is trained following the paradigm of contrastive learning. We evaluate GraphAdaMix on ten benchmark datasets with extensive experiments. GraphAdaMix is demonstrated to consistently boost state-of-the-art GNN variants in semi-supervised and unsupervised node classification tasks. The code of GraphAdaMix is available online(1).",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852304019,0
C,"Tukan, M; Wu, X; Zhou, S; Braverman, V; Feldman, D",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Tukan, Murad; Wu, Xuan; Zhou, Samson; Braverman, Vladimir; Feldman, Dan",,,New Coresets for Projective Clustering and Applications,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"(j, k)-projective clustering is the natural generalization of the family of k-clustering and j -subspace clustering problems. Given a set of points P in R-d, the goal is to find k flats of dimension j, i.e., affine subspaces, that best fit P under a given distance measure. In this paper, we propose the first algorithm that returns an L-infinity coreset of size polynomial in d. Moreover, we give the first strong coreset construction for general M-estimator regression. Specifically, we show that our construction provides efficient coreset constructions for Cauchy, Welsch, Huber, Geman-McClure, Tukey, L-1 - L-2, and Fair regression, as well as general concave and power-bounded loss functions. Finally, we provide experimental results based on real-world datasets, showing the efficacy of our approach.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072705020,0
C,"Vial, D; Parulekar, A; Shakkottai, S; Srikant, R",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Vial, Daniel; Parulekar, Advait; Shakkottai, Sanjay; Srikant, R.",,,Improved Algorithms for Misspecified Linear Markov Decision Processes,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"For the misspecified linear Markov decision process (MLMDP) model of Jin et al. (2020), we propose an algorithm with three desirable properties. (P1) Its regret after K episodes scales as K max{epsilon(mis); epsilon(tol)}, where epsilon(mis) is the degree of misspecification and epsilon(tol) is a user-specified error tolerance. (P2) Its space and per-episode time complexities are bounded as K -> infinity. (P3) It does not require epsilon(mis) as input. To our knowledge, this is the first algorithm satisfying all three properties. For concrete choices of epsilon(tol), we also improve existing regret bounds (up to log factors) while achieving either (P2) or (P3) (existing algorithms satisfy neither). At a high level, our algorithm generalizes (to MLMDPs) and refines the Sup-Lin-UCB algorithm, which Takemura et al. (2021) recently showed satisfies (P3) for contextual bandits. We also provide an intuitive interpretation of their result, which informs the design of our algorithm.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072704037,0
C,"Zhu, YQ; Dong, JS; Wang, YX",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Zhu, Yuqing; Dong, Jinshuo; Wang, Yu-Xiang",,,Optimal Accounting of Differential Privacy via Characteristic Function,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Characterizing the privacy degradation over compositions, i.e., privacy accounting, is a fundamental topic in differential privacy (DP) with many applications to differentially private machine learning and federated learning. We propose a unification of recent advances (Renyi DP, privacy profiles, f-DP and the PLD formalism) via the characteristic function (phi-function) of a certain dominating privacy loss random variable. We show that our approach allows natural adaptive composition like Renyi DP, provides exactly tight privacy accounting like PLD, and can be (often losslessly) converted to privacy profile and f-DP, thus providing (epsilon, delta)-DP guarantees and interpretable tradeoff functions. Algorithmically, we propose an analytical Fourier accountant that represents the complex logarithm of phi-functions symbolically and uses Gaussian quadrature for numerical computation. On several popular DP mechanisms and their subsampled counterparts, we demonstrate the flexibility and tightness of our approach in theory and experiments.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072704040,0
C,"Agterberg, J; Sulam, J",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Agterberg, Joshua; Sulam, Jeremias",,,Entrywise Recovery Guarantees for Sparse PCA via Sparsistent Algorithms,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Sparse Principal Component Analysis (PCA) is a prevalent tool across a plethora of subfields of applied statistics. While several results have characterized the recovery error of the principal eigenvectors, these are typically in spectral or Frobenius norms. In this paper, we provide entrywise l(2,infinity) bounds for Sparse PCA under a general high-dimensional subgaussian design. In particular, our results hold for any algorithm that selects the correct support with high probability, those that are sparsistent. Our bound improves upon known results by providing a finer characterization of the estimation error, and our proof uses techniques recently developed for entrywise subspace perturbation theory.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852301003,0
C,"Barisal, A; Chunduru, R; Data, D; Prabhakaran, M",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Barisal, Arran; Chunduru, Rahul; Data, Deepesh; Prabhakaran, Manoj",,,Flexible Accuracy for Differential Privacy,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Differential Privacy (DP) has become a gold standard in privacy-preserving data analysis. While it provides one of the most rigorous notions of privacy, there are many settings where its applicability is limited. Our main contribution is in augmenting differential privacy with Flexible Accuracy, which allows small distortions in the input (e.g., dropping outliers) before measuring accuracy of the output, allowing one to extend DP mechanisms to high-sensitivity functions. We present mechanisms that can help in achieving this notion for functions that had no meaningful differentially private mechanisms previously. In particular, we illustrate an application to differentially private histograms, which in turn yields mechanisms for revealing the support of a dataset or the extremal values in the data. Analyses of our constructions exploit new versatile composition theorems that facilitate modular design. All the above extensions use our new definitional framework, which is in terms of lossy Wasserstein distance - a 2-parameter error measure for distributions. This may be of independent interest.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072703041,0
C,"Ben Taieb, S",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Ben Taieb, Souhaib",,,Learning Quantile Functions for Temporal Point Processes with Recurrent Neural Splines,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We can build flexible predictive models for rich continuous-time event data by combining the framework of temporal point processes (TPP) with (recurrent) neural networks. We propose a new neural parametrization for TPPs based on the conditional quantile function. Specifically, we use a flexible monotonic rational-quadratic spline to learn a smooth continuous quantile function. Conditioning on historical events is achieved through a recurrent neural network. This novel parametrization provides a flexible yet tractable TPP model with multiple advantages, such as analytical sampling and closedform expressions for quantiles and prediction intervals. While neural TPP models are often trained using maximum likelihood estimation, we consider the more robust continuous ranked probability score (CRPS). We additionally derive a closed-form expression for the CRPS of our model. Finally, we demonstrate that the proposed model achieves state-of-the-art performance in standard prediction tasks on both synthetic and real-world event data.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072703015,0
C,"Chen, ZW; Maguluri, ST",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Chen, Zaiwei; Maguluri, Siva Theja",,,Sample Complexity of Policy-Based Methods under Off-Policy Sampling and Linear Function Approximation,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"In this work, we study policy-based methods for solving the reinforcement learning problem, where off-policy sampling and linear function approximation are employed for policy evaluation, and various policy update rules (including natural policy gradient) are considered for policy improvement. To solve the policy evaluation sub-problem in the presence of the deadly triad, we propose a generic algorithm framework of multi-step TD-learning with generalized importance sampling ratios, which includes two specific algorithms: the lambda-averaged Q-trace and the two-sided Q-trace. The generic algorithm is single time-scale, has provable finitesample guarantees, and overcomes the high variance issue in off-policy learning. As for the policy improvement, we provide a universal analysis that establishes geometric convergence of various policy update rules, which leads to an overall (O) over tilde(epsilon(-2)) sample complexity.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852305032,0
C,"Dinari, O; Freifeld, O",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Dinari, Or; Freifeld, Oren",,,Sampling in Dirichlet Process Mixture Models for Clustering Streaming Data,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Practical tools for clustering streaming data must be fast enough to handle the arrival rate of the observations. Typically, they also must adapt on the fly to possible lack of stationarity; i.e., the data statistics may be time-dependent due to various forms of drifts, changes in the number of clusters, etc. The Dirichlet Process Mixture Model (DPMM), whose Bayesian nonparametric nature allows it to adapt its complexity to the data, seems a natural choice for the streaming-data case. In its classical formulation, however, the DPMM cannot capture common types of drifts in the data statistics. Moreover, and regardless of that limitation, existing methods for online DPMM inference are too slow to handle rapid data streams. In this work we propose adapting both the DPMM and a known DPMM sampling-based non-streaming inference method for streaming-data clustering. We demonstrate the utility of the proposed method on several challenging settings, where it obtains state-of-the-art results while being on par with other methods in terms of speed.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,818,835,,,,,,,,,,,,,,,,WOS:000828072700034,0
C,"Fried, S; Wolfer, G",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Fried, Sela; Wolfer, Geoffrey",,,Identity Testing of Reversible Markov Chains,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We consider the problem of identity testing of Markov chain transition matrices based on a single trajectory of observations under the distance notion introduced by Daskalakis et al. (2018a) and further analyzed by Cherapanamjeri and Bartlett (2019). Both works made the restrictive assumption that the Markov chains under consideration are symmetric. In this work we relax the symmetry assumption and show that it is possible to perform identity testing under the much weaker assumption of reversibility, provided that the stationary distributions of the reference and of the unknown Markov chains are close under a distance notion related to the separation distance. Additionally, we provide intuition on the distance notion of Daskalakis et al. (2018a) by showing how it behaves under several natural operations. In particular, we address some of their open questions.",,,,,,"Wolfer, Geoffrey/0000-0002-5388-7640",,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,798,817,,,,,,,,,,,,,,,,WOS:000828072700033,0
C,"Hsiao, V; Nau, D; Dechter, R",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Hsiao, Vincent; Nau, Dana; Dechter, Rina",,,Fast Fourier Transform Reductions for Bayesian Network Inference,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Bayesian Networks are useful for analyzing the properties of systems with large populations of interacting agents (e.g., in social modeling applications and distributed service applications). These networks typically have large functions (CPTs), making exact inference intractable. However, often these models have additive symmetry. In this paper we show how summation-based CPTs, especially in the presence of symmetry, can be computed efficiently through the usage of the Fast Fourier Transform (FFT). In particular, we propose an efficient method using the FFT for reducing the size of Conditional Probability Tables (CPTs) in Bayesian Networks with summation-based causal independence (CI). We show how to apply it directly towards the acceleration of Bucket Elimination, and we subsequently provide experimental results demonstrating the computational speedup provided by our method.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,6445,6458,,,,,,,,,,,,,,,,WOS:000841852300038,0
C,"Koyama, K; Kiritoshi, K; Okawachi, T; Izumitani, T",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Koyama, Kazuki; Kiritoshi, Keisuke; Okawachi, Tomomi; Izumitani, Tomonori",,,Effective Nonlinear Feature Selection Method based on HSIC Lasso and with Variational Inference,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"HSIC Lasso is one of the most effective sparse nonlinear feature selection methods based on the Hilbert-Schmidt independence criterion. We propose an adaptive nonlinear feature selection method, which is based on the HSIC Lasso, that uses a stochastic model with a family of super-Gaussian prior distributions for sparsity enhancement. The method includes easily implementable closed-form update equations that are derived approximately from variational inference and can handle high-dimensional and large datasets. We applied the method to several synthetic datasets and real-world datasets and verified its effectiveness regarding redundancy, computational complexity, and classification and prediction accuracy using the selected features. The results indicate that the method can more effectively remove irrelevant features, leaving only relevant features. In certain problem settings, the method assigned non-zero importance only to the actually relevant features. This is an important characteristic for practical use.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852304038,0
C,"Liu, YR; Qiao, XH; Lam, J",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Liu, Yirui; Qiao, Xinghao; Lam, Jessica",,,CATVI: Conditional and Adaptively Truncated Variational Inference for Hierarchical Bayesian Nonparametric Models,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Current variational inference methods for hierarchical Bayesian nonparametric models can neither characterize the correlation structure among latent variables due to the mean-field setting, nor infer the true posterior dimension because of the universal truncation. To overcome these limitations, we propose the conditional and adaptively truncated variational inference method (CATVI) by maximizing the nonparametric evidence lower bound and integrating Monte Carlo into the variational inference framework. CATVI enjoys several advantages over traditional methods, including a smaller divergence between variational and true posteriors, reduced risk of underfitting or overfitting, and improved prediction accuracy. Empirical studies on three large datasets reveal that CATVI applied in Bayesian nonparametric topic models substantially outperforms competing models, providing lower perplexity and clearer topic-words clustering.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072703032,0
C,"Nava, E; Mutny, M; Krause, A",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Nava, Elvis; Mutny, Mojmir; Krause, Andreas",,,Diversified Sampling for Batched Bayesian Optimization with Determinantal Point Processes,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"In Bayesian Optimization (BO) we study black-box function optimization with noisy point evaluations and Bayesian priors. Convergence of BO can be greatly sped up by batching, where multiple evaluations of the black-box function are performed in a single round. The main difficulty in this setting is to propose at the same time diverse and informative batches of evaluation points. In this work, we introduce DPP-Batch Bayesian Optimization (DPP-BBO), a universal framework for inducing batch diversity in sampling based BO by leveraging the repulsive properties of Determinantal Point Processes (DPP) to naturally diversify the batch sampling procedure. We illustrate this framework by formulating DPP-Thompson Sampling (DPP-TS) as a variant of the popular Thompson Sampling (TS) algorithm and introducing a Markov Chain Monte Carlo procedure to sample from it. We then prove novel Bayesian simple regret bounds for both classical batched TS as well as our counterpart DPP-TS, with the latter bound being tighter. Our real-world, as well as synthetic, experiments demonstrate improved performance of DPP-BBO over classical batching methods with Gaussian process and Cox process models.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852301022,0
C,"Raj, A; Joulani, P; Gyorgy, A; Szepesvari, C",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Raj, Anant; Joulani, Pooria; Gyorgy, Andras; Szepesvari, Csaba",,,"Faster Rates, Adaptive Algorithms, and Finite-Time Bounds for Linear Composition Optimization and Gradient TD Learning","INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Gradient temporal difference (GTD) algorithms are provably convergent policy evaluation methods for off-policy reinforcement learning. Despite much progress, proper tuning of the stochastic approximation methods used to solve the resulting saddle point optimization problem requires the knowledge of several (unknown) problem-dependent parameters. In this paper we apply adaptive step-size tuning strategies to greatly reduce this dependence on prior knowledge, and provide algorithms with adaptive convergence guarantees. In addition, we use the underlying refined analysis technique to obtain new O(1/T) rates that do not depend on the strong-convexity parameter of the problem, and also apply to the Markov noise setting, as well as the unbounded i.i.d. noise setting.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852301028,0
C,"Saket, R; Raghuveer, A; Ravindran, B",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Saket, Rishi; Raghuveer, Aravindan; Ravindran, Balaraman",,,On Combining Bags to Better Learn from Label Proportions,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"In the framework of learning from label proportions (LLP) the goal is to learn a good instance-level label predictor from the observed label proportions of bags of instances. Most of the LLP algorithms either explicitly or implicitly assume the nature of bag distributions with respect to the actual labels and instances, or cleverly adapt supervised learning techniques to suit LLP. In practical applications however, the scale and nature of data could render such assumptions invalid and the many of the algorithms impractical. In this paper we address the hard problem of solving LLP with provable error bounds while being bag distribution agnostic and model agnostic. We first propose the concept of generalized bags, an extension of bags and then devise an algorithm to combine bag distributions, if possible, into good generalized bag distributions. We show that (w.h.p) any classifier optimizing the squared Euclidean label-proportion loss on such a generalized bag distribution is guaranteed to minimize the instance-level loss as well. The predictive quality of our method is experimentally evaluated and it equals or betters the previous methods on pseudo-synthetic and real-world datasets.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,5913,5927,,,,,,,,,,,,,,,,WOS:000841852300014,0
C,"Titsias, MK; Shi, JX",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Titsias, Michalis K.; Shi, Jiaxin",,,Double Control Variates for Gradient Estimation in Discrete Latent Variable Models,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Stochastic gradient-based optimization for discrete latent variable models is challenging due to the high variance of gradients. We introduce a variance reduction technique for score function estimators that makes use of double control variates. These control variates act on top of a main control variate, and try to further reduce the variance of the overall estimator. We develop a double control variate for the REINFORCE leave-one-out estimator using Taylor expansions. For training discrete latent variable models, such as variational autoencoders with binary latent variables, our approach adds no extra computational cost compared to standard training with the REINFORCE leave-one-out estimator. We apply our method to challenging high-dimensional toy examples and for training variational autoencoders with binary latent variables. We show that our estimator can have lower variance compared to other state-of-the-art estimators.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,6134,6151,,,,,,,,,,,,,,,,WOS:000841852300025,0
C,"Weinstein, EN; Amin, AN; Grathwohl, W; Kassler, D; Disset, J; Marks, DS",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Weinstein, Eli N.; Amin, Alan N.; Grathwohl, Will; Kassler, Daniel; Disset, Jean; Marks, Debora S.",,,Optimal Design of Stochastic DNA Synthesis Protocols based on Generative Sequence Models,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Generative probabilistic models of biological sequences have widespread existing and potential applications in analyzing, predicting and designing proteins, RNA and genomes. To test the predictions of such a model experimentally, the standard approach is to draw samples, and then synthesize each sample individually in the laboratory. However, often orders of magnitude more sequences can be experimentally assayed than can be affordably synthesized individually. In this article, we propose instead to use stochastic synthesis methods, such as mixed nucleotides or trimers. We describe a black-box algorithm for optimizing stochastic synthesis protocols to produce approximate samples from any target generative model. We establish theoretical bounds on the method's performance, and validate it in simulation using held-out sequence-to-function predictors trained on real experimental data. We show that using optimized stochastic synthesis protocols in place of individual synthesis can increase the number of hits in protein engineering efforts by orders of magnitude, e.g. from zero to a thousand.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852301039,0
C,"Behne, JK; Reeves, G",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Behne, Joshua K.; Reeves, Galen",,,Fundamental limits for rank-one matrix estimation with groupwise heteroskedasticity,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Low-rank matrix recovery problems involving high-dimensional and heterogeneous data appear in applications throughout statistics and machine learning. The contribution of this paper is to establish the fundamental limits of recovery for a broad class of these problems. In particular, we study the problem of estimating a rank-one matrix from Gaussian observations where different blocks of the matrix are observed under different noise levels. In the setting where the number of blocks is fixed while the number of variables tends to infinity, we prove asymptotically exact formulas for the minimum mean-squared error in estimating both the matrix and underlying factors. These results are based on a novel reduction from the low-rank matrix tensor product model (with homogeneous noise) to a rank-one model with heteroskedastic noise. As an application of our main result, we show that recently proposed methods based on applying principal component analysis (PCA) to weighted combinations of the data are optimal in some settings but sub-optimal in others. We also provide numerical results comparing our asymptotic formulas with the performance of methods based on weighted PCA, gradient descent, and approximate message passing.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852303005,0
C,"Bharadwaj, A; Cormode, G",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Bharadwaj, Akash; Cormode, Graham",,,Sample-and-threshold differential privacy: Histograms and applications,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Federated analytics seeks to compute accurate statistics from data distributed across users' devices while providing a suitable privacy guarantee and being practically feasible to implement and scale. In this paper, we show how a strong (epsilon,delta)-Differential Privacy (DP) guarantee can be achieved for the fundamental problem of histogram generation in a federated setting, via a highly practical sampling-based procedure that does not add noise to disclosed data. Given the ubiquity of sampling in practice, we thus obtain a DP guarantee almost for free, avoid overestimating histogram counts, and allow easy reasoning about how privacy guarantees may obscure minorities and outliers. Using such histograms, related problems such as heavy hitters and quantiles can be answered with provable error and privacy guarantees. Experimental results show that our sample-andthreshold approach is accurate and scalable.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072701022,0
C,"Challu, C; Jiang, PH; Wu, YN; Callot, L",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Challu, Cristian; Jiang, Peihong; Wu, Ying Nian; Callot, Laurent",,,Deep Generative model with Hierarchical Latent Factors for Time Series Anomaly Detection,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Multivariate time series anomaly detection has become an active area of research in recent years, with Deep Learning models outperforming previous approaches on benchmark datasets. Among reconstruction-based models, most previous work has focused on Variational Autoencoders and Generative Adversarial Networks. This work presents DGHL, a new family of generative models for time series anomaly detection, trained by maximizing the observed likelihood by posterior sampling and alternating backpropagation. A top-down Convolution Network maps a novel hierarchical latent space to time series windows, exploiting temporal dynamics to encode information efficiently. Despite relying on posterior sampling, it is computationally more efficient than current approaches, with up to 10x shorter training times than RNN based models. Our method outperformed current state-of-the-art models on four popular benchmark datasets. Finally, DGHL is robust to variable features between entities and accurate even with large proportions of missing values, settings with increasing relevance with the advent of IoT. We demonstrate the superior robustness of DGHL with novel occlusion experiments in this literature. Our code is available at https://github.com/cchallu/dghl.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072701032,0
C,"Chen, GH; Li, XM; Yu, MG",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Chen, Guanhua; Li, Xiaomao; Yu, Menggang",,,Policy Learning for Optimal Individualized Dose Intervals,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We study the problem of learning individualized dose intervals using observational data. There are very few previous works for policy learning with continuous treatment, and all of them focused on recommending an optimal dose rather than an optimal dose interval. In this paper, we propose a new method to estimate such an optimal dose interval, named probability dose interval (PDI). The potential outcomes for doses in the PDI are guaranteed better than a pre-specified threshold with a given probability (e.g., 50%). The associated nonconvex optimization problem can be efficiently solved by the Difference-of-Convex functions (DC) algorithm. We prove that our estimated policy is consistent, and its risk converges to that of the best-in-class policy at a root-n rate. Numerical simulations show the advantage of the proposed method over outcome modeling based benchmarks. We further demonstrate the performance of our method in determining individualized Hemoglobin A1c (HbA1c) control intervals for elderly patients with diabetes.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072701034,0
C,"Chen, LS; Chen, TY",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Chen, Lisha; Chen, Tianyi",,,"Is Bayesian Model-Agnostic Meta Learning Better than Model-Agnostic Meta Learning, Provably?","INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Meta learning aims at learning a model that can quickly adapt to unseen tasks. Widely used meta learning methods include mode-lagnostic meta learning (MAML), implicit MAML, Bayesian MAML. Thanks to its ability of modeling uncertainty, Bayesian MAML often has advantageous empirical performance. However, the theoretical understanding of Bayesian MAML is still limited, especially on questions such as if and when Bayesian MAML has provably better performance than MAML. In this paper, we aim to provide theoretical justifications for Bayesian MAML's advantageous performance by comparing the meta test risks of MAML and Bayesian MAML. In the meta linear regression, under both the distribution agnostic and linear centroid cases, we have established that Bayesian MAML indeed has provably lower meta test risks than MAML. We verify our theoretical results through experiments, the code of which is available at https://github.com/lishachen/Bayesian-MAML-vs-MAML.",,,,,"Chen, Lisha/HGC-6247-2022",,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072701037,0
C,"Cho, YJ; Wang, JY; Joshi, G",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Cho, Yae Jee; Wang, Jianyu; Joshi, Gauri",,,Towards Understanding Biased Client Selection in Federated Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Federated learning is a distributed optimization paradigm that enables a large number of resource-limited client nodes to cooperatively train a model without data sharing. Previous works analyzed the convergence of federated learning by accounting of data heterogeneity, communication/computation limitations, and partial client participation. However, most assume unbiased client participation, where clients are selected such that the aggregated model update is unbiased. In our work, we present the convergence analysis of federated learning with biased client selection and quantify how the bias affects convergence speed. We show that biasing client selection towards clients with higher local loss yields faster error convergence. From this insight, we propose POWER-OF-CHOICE, a communication- and computation-efficient client selection framework that flexibly spans the trade-off between convergence speed and solution bias. Extensive experiments demonstrate that POWER-OF-CHOICE can converge up to 3x faster and give 10% higher test accuracy than the baseline random selection.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852304036,0
C,"Fong, E; Lehmann, B",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Fong, Edwin; Lehmann, Brieuc",,,A Predictive Approach to Bayesian Nonparametric Survival Analysis,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Bayesian nonparametric methods are a popular choice for analysing survival data due to their ability to flexibly model the distribution of survival times. These methods typically employ a nonparametric prior on the survival function that is conjugate with respect to right-censored data. Eliciting these priors, particularly in the presence of covariates, can be challenging and inference typically relies on computationally intensive Markov chain Monte Carlo schemes. In this paper, we build on recent work that recasts Bayesian inference as assigning a predictive distribution on the unseen values of a population conditional on the observed samples, thus avoiding the need to specify a complex prior. We describe a copula-based predictive update which admits a scalable sequential importance sampling algorithm to perform inference that properly accounts for right-censoring. We provide theoretical justification through an extension of Doob's consistency theorem and illustrate the method on a number of simulated and real data sets, including an example with covariates. Our approach enables analysts to perform Bayesian nonpararnetric inference through only the specification of a predictive distribution.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852301020,0
C,"Frei, S; Zou, DF; Chen, ZX; Gu, QQ",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Frei, Spencer; Zou, Difan; Chen, Zixiang; Gu, Quanquan",,,Self-training Converts Weak Learners to Strong Learners in Mixture Models,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We consider a binary classification problem when the data comes from a mixture of two rotationally symmetric distributions satisfying concentration and anti-concentration properties enjoyed by log-concave distributions among others. We show that there exists a universal constant C-err > 0 such that if a pseudolabeler beta(Pl) can achieve classification error at most C-err, then for any epsilon > 0, an iterative self-training algorithm initialized at beta(0) := beta(Pl) using pseudolabels (y) over cap = sgn(<beta(t), x >) and using at most (O) over tilde (d/epsilon(2)) unlabeled examples suffices to learn the Bayes-optimal classifier up to epsilon error, where d is the ambient dimension. That is, self-training converts weak learners to strong learners using only unlabeled examples. We additionally show that by running gradient descent on the logistic loss one can obtain a pseudolabeler beta(Pl) with classification error C-err using only O(d) labeled examples (i.e., independent of epsilon). Together our results imply that mixture models can be learned to within epsilon of the Bayes-optimal accuracy using at most O(d) labeled examples and (O) over tilde (d/epsilon(2)) unlabeled examples by way of a semi-supervised self-training algorithm.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852302018,0
C,"Gorbunov, E; Berard, H; Gidel, G; Loizou, N",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Gorbunov, Eduard; Berard, Hugo; Gidel, Gauthier; Loizou, Nicolas",,,Stochastic Extragradient: General Analysis and Improved Rates,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"The Stochastic Extragradient (SEG) method is one of the most popular algorithms for solving min-max optimization and variational inequalities problems (VIP) appearing in various machine learning tasks. However, several important questions regarding the convergence properties of SEG are still open, including the sampling of stochastic gradients, mini-batching, convergence guarantees for the monotone finite-sum variational inequalities with possibly non-monotone terms, and others. To address these questions, in this paper, we develop a novel theoretical framework that allows us to analyze several variants of SEG in a unified manner. Besides standard setups, like Same-Sample SEG under Lipschitzness and monotonicity or Independent-Samples SEG under uniformly bounded variance, our approach allows us to analyze variants of SEG that were never explicitly considered in the literature before. Notably, we analyze SEG with arbitrary sampling which includes importance sampling and various mini-batching strategies as special cases. Our rates for the new variants of SEG outperform the current state-of-the-art convergence guarantees and rely on less restrictive assumptions.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852302013,0
C,"Holland, MJ; Haress, E",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Holland, Matthew J.; Haress, El Mehdi",,,Spectral risk-based learning using unbounded losses,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,,,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072701042,0
C,"Hu, JX; Wang, MY",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Hu, Jiaxin; Wang, Miaoyan",,,Multiway Spherical Clustering via Degree-Corrected Tensor Block Models,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We consider the problem of multiway clustering in the presence of unknown degree heterogeneity. Such data problems arise commonly in applications such as recommendation system, neuroimaging, community detection, and hypergraph partitions in social networks. The allowance of degree heterogeneity provides great flexibility in clustering models, but the extra complexity poses significant challenges in both statistics and computation. Here, we develop a degree-corrected tensor block model with estimation accuracy guarantees. We present the phase transition of clustering performance based on the notion of angle separability, and we characterize three signal-to-noise regimes corresponding to different statistical-computational behaviors. In particular, we demonstrate that an intrinsic statistical-to-computational gap emerges only for tensors of order three or greater. Further, we develop an efficient polynomialtime algorithm that provably achieves exact clustering under mild signal conditions. The efficacy of our procedure is demonstrated through both simulations and analyses of Peru Legislation dataset.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072701006,0
C,"Kim, B; Seo, J",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Kim, Beomsu; Seo, Junghoon",,,Semi-Implicit Hybrid Gradient Methods with Application to Adversarial Robustness,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Adversarial examples, crafted by adding imperceptible perturbations to natural inputs, can easily fool deep neural networks (DNNs). One of the most successful methods for training adversarially robust DNNs is solving a nonconvex-nonconcave minimax problem with an adversarial training (AT) algorithm. However, among the many AT algorithms, only Dynamic AT (DAT) and You Only Propagate Once (YOPO) is guaranteed to converge to a stationary point with rate O(1/K-1/2). In this work, we generalize the stochastic primal-dual hybrid gradient algorithm to develop semi-implicit hybrid gradient methods (SI-HGs) for finding stationary points of nonconvex-nonconcave minimax problems. SI-HGs have the convergence rate O(1/K), which improves upon the rate O(1/K-1/2) of DAT and YOPO. We devise a practical variant of SI-HGs, and show that it outperforms other AT algorithms in terms of convergence speed and robustness.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072704023,0
C,"Levine, A; Feizi, S",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Levine, Alexander; Feizi, Soheil",,,Provable Adversarial Robustness for Fractional l(p) Threat Models,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"In recent years, researchers have extensively studied adversarial robustness in a variety of threat models, including l(0), l(1) , l(2) , and l(infinity) norm bounded adversarial attacks. However, attacks bounded by fractional l(p) norms (quasi-norms defined by the l(p) distance with 0 < p < 1) have yet to be thoroughly considered. We proactively propose a defense with several desirable properties: it provides provable (certified) robustness, scales to ImageNet, and yields deterministic (rather than high-probability) certified guarantees when applied to quantized data (e.g., images). Our technique for fractional l(p) robustness constructs expressive, deep classifiers that are globally Lipschitz with respect to the l(p)(p), metric, for any 0 < p < 1. However, our method is even more general: we can construct classifiers which are globally Lipschitz with respect to any metric defined as the sum of concave functions of components. Our approach builds on a recent work, Levine and Feizi (2021), which provides a provable defense against l(1) attacks. However, we demonstrate that our proposed guarantees are highly nonvacuous, compared to the trivial solution of using (Levine and Feizi, 2021) directly and applying norm inequalities.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852304020,0
C,"Macdonald, J; Waldchen, S",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Macdonald, Jan; Waeldchen, Stephan",,,A Complete Characterisation of ReLU-Invariant Distributions,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We give a complete characterisation of families of probability distributions that are invariant under the action of ReLU neural network layers (in the same way that the family of Gaussian distributions is invariant to affine linear transformations). The need for such families arises during the training of Bayesian networks or the analysis of trained neural networks, e.g., in the context of uncertainty quantification (UQ) or explainable artificial intelligence (XAI). We prove that no invariant parametrised family of distributions can exist unless at least one of the following three restrictions holds: First, the network layers have a width of one, which is unreasonable for practical neural networks. Second, the probability measures in the family have finite support, which basically amounts to sampling distributions. Third, the parametrisation of the family is not locally Lipschitz continuous, which excludes all computationally feasible families. Finally, we show that these restrictions are individually necessary. For each of the three cases we can construct an invariant family exploiting exactly one of the restrictions but not the other two.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072701024,0
C,"Stephenson, WT; Ghosh, S; Nguyen, TD; Yurochkin, M; Deshpande, SK; Broderick, T",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Stephenson, William T.; Ghosh, Soumya; Nguyen, Tin D.; Yurochkin, Mikhail; Deshpande, Sameer K.; Broderick, Tamara",,,Measuring the robustness of Gaussian processes to kernel choice,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Gaussian processes (GPs) are used to make medical and scientific decisions, including in cardiac care and monitoring of atmospheric carbon dioxide levels. Notably, the choice of GP kernel is often somewhat arbitrary. In particular, uncountably many kernels typically align with qualitative prior knowledge (e.g. function smoothness or stationarity). But in practice, data analysts choose among a handful of convenient standard kernels (e.g. squared exponential). In the present work, we ask: Would decisions made with a GP differ under other, qualitatively interchangeable kernels? We show how to answer this question by solving a constrained optimization problem over a finite-dimensional space. We can then use standard optimizers to identify substantive changes in relevant decisions made with a GP. We demonstrate in both synthetic and real-world examples that decisions made with a GP can exhibit non-robustness to kernel choice, even when prior draws are qualitatively interchangeable to a user.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072703018,0
C,"Tsai, CP; Prasad, A; Balakrishnan, S; Ravikumar, P",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Tsai, Che-Ping; Prasad, Adarsh; Balakrishnan, Sivaraman; Ravikumar, Pradeep",,,Heavy-tailed Streaming Statistical Estimation,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We consider the task of heavy-tailed statistical estimation given streaming p-dimensional samples. This could also be viewed as stochastic optimization under heavy-tailed distributions, with an additional O(p) space complexity constraint. We design a clipped stochastic gradient descent algorithm and provide an improved analysis, under a more nuanced condition on the noise of the stochastic gradients, which we show is critical when analyzing stochastic optimization problems arising from general statistical estimation problems. Our results guarantee convergence not just in expectation but with exponential concentration, and moreover does so using O(1) batch size. We provide consequences of our results for mean estimation and linear regression. Finally, we provide empirical corroboration of our results and algorithms via synthetic experiments for mean estimation and linear regression.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072701014,0
C,"Ying, DH; Ding, YH; Lavaei, J",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Ying, Donghao; Ding, Yuhao; Lavaei, Javad",,,A Dual Approach to Constrained Markov Decision Processes with Entropy Regularization,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We study entropy-regularized constrained Markov decision processes (CMDPs) under the soft-max parameterization, in which an agent aims to maximize the entropy-regularized value function while satisfying constraints on the expected total utility. By leveraging the entropy regularization, our theoretical analysis shows that its Lagrangian dual function is smooth and the Lagrangian duality gap can be decomposed into the primal optimality gap and the constraint violation. Furthermore, we propose an accelerated dual-descent method for entropy-regularized CMDPs. We prove that our method achieves the global convergence rate (O) over tilde (1/T) for both the optimality gap and the constraint violation for entropy-regularized CMDPs. A discussion about a linear convergence rate for CMDPs with a single constraint is also provided.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072701043,0
C,"Yu, Y; Padilla, OHM; Rinaldo, A",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Yu, Yi; Padilla, Oscar Hernan Madrid; Rinaldo, Alessandro",,,Optimal Partition Recovery in General Graphs,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We consider a graph-structured change point problem in which we observe a random vector with piece-wise constant but otherwise unknown mean and whose independent, sub-Gaussian coordinates correspond to the n nodes of a fixed graph. We are interested in the localisation task of recovering the partition of the nodes associated to the constancy regions of the mean vector or, equivalently, of estimating the cut separating the sub-graphs over which the mean remains constant. Although graph-valued signals of this type have been previously studied in the literature for the different tasks of testing for the presence of an anomalous cluster and of estimating the mean vector, no localisation results are known outside the classical case of chain graphs. When the partition S consists of only two elements, we characterise the difficulty of the localisation problem in terms of four key parameters: the maximal noise variance sigma(2), the size Delta of the smaller element of the partition, the magnitude kappa of the difference in the signal values across contiguous elements of the partition and the sum of the effective resistance edge weights vertical bar partial derivative(r)(S)vertical bar of the corresponding cut - a graph theoretic quantity quantifying the size of the partition boundary. In particular, we demonstrate an information theoretical lower bound implying that, in the low signal-to-noise ratio regime kappa(2)Delta sigma(-2)vertical bar partial derivative(r)(S)vertical bar(-1) less than or similar to 1, no consistent estimator of the true partition exists. On the other hand, when kappa(2)Delta sigma (2)vertical bar partial derivative(r)(S)vertical bar(-1) greater than or similar to zeta(n) log{r(vertical bar E vertical bar)}, with r(vertical bar E vertical bar) being the sum of effective resistance weighted edges and zeta(n) being any diverging sequence in n, we show that a polynomial-time, approximate l(0)-penalised least squared estimator delivers a localisation error - measured by the symmetric difference between the true and estimated partition - of order kappa(-2)sigma(2)vertical bar partial derivative(r)(S)vertical bar log{r(vertical bar E vertical bar)}. Aside from the log{r(vertical bar E vertical bar)} term, this rate is minimax optimal. Finally, we provide discussions on the localisation error for more general partitions of unknown sizes.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072704018,0
C,"Altamirano, M; Tobar, F",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Altamirano, Matias; Tobar, Felipe",,,Nonstationary multi-output Gaussian processes via harmonizable spectral mixtures,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Kernel design for Multi-output Gaussian Processes (MOGP) has received increased attention recently. In particular, the Multi-Output Spectral Mixture kernel (MOSM) approach has been praised as a general model in the sense that it extends other approaches such as Linear Model of Corregionalization, Intrinsic Corregionalization Model and Cross-Spectral Mixture. MOSM relies on Cramer's theorem to parametrise the power spectral densities (PSD) as a Gaussian mixture, thus, having a structural restriction: by assuming the existence of a PSD, the method is only suited for multi-output stationary applications. We develop a nonstationary extension of MOSM by proposing the family of harmonizable kernels for MOGPs, a class of kernels that contains both stationary and a vast majority of nonstationary processes. A main contribution of the proposed harmonizable kernels is that they automatically identify a possible nonstationary behaviour meaning that practitioners do not need to choose between stationary or non-stationary kernels. The proposed method is first validated on synthetic data with the purpose of illustrating the key properties of our approach, and then compared to existing MOGP methods on two real-world settings from finance and electroencephalography.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072703014,0
C,"Arvanitidis, G; Georgiev, B; Scholkopf, B",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Arvanitidis, Georgios; Georgiev, Bogdan; Scholkopf, Bernhard",,,A prior-based approximate latent Riemannian metric,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Stochastic generative models enable us to capture the geometric structure of a data manifold lying in a high dimensional space through a Riemannian metric in the latent space. However, its practical use is rather limited mainly due to inevitable functionality problems and computational complexity. In this work we propose a surrogate conformal Riemannian metric in the latent space of a generative model that is simple, efficient and robust. This metric is based on a learnable prior that we propose to learn using a basic energy-based model. We theoretically analyze the behavior of the proposed metric and show that it is sensible to use in practice. We demonstrate experimentally the efficiency and robustness, as well as the behavior of the new approximate metric. Also, we show the applicability of the proposed methodology for data analysis in the life sciences.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072704033,0
C,"Baudry, D; Russac, Y; Kaufmann, E",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Baudry, Dorian; Russac, Yoan; Kaufmann, Emilie",,,Efficient Algorithms for Extreme Bandits,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"In this paper, we contribute to the Extreme Bandit problem, a variant of Multi-Armed Bandits in which the learner seeks to collect the largest possible reward. We first study the concentration of the maximum of i.i.d random variables under mild assumptions on the tail of the rewards distributions. This analysis motivates the introduction of Quantile of Maxima (QoMax). The properties of QoMax are sufficient to build an Explore-Then-Commit (ETC) strategy, QoMax-ETC, achieving strong asymptotic guarantees despite its simplicity. We then propose and analyze a more adaptive, anytime algorithm, QoMax-SDA, which combines QoMax with a subsampling method recently introduced by Baudry et al. (2021). Both algorithms are more efficient than existing approaches in two aspects (1) they lead to better empirical performance (2) they enjoy a significant reduction of the memory and time complexities.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072702012,0
C,"Benzing, F",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Benzing, Frederik",,,Unifying Importance Based Regularisation Methods for Continual Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Continual Learning addresses the challenge of learning a number of different tasks sequentially. The goal of maintaining knowledge of earlier tasks without re-accessing them starkly conflicts with standard SGD training for artificial neural networks. An influential method to tackle this problem without storing old data are so-called regularisation approaches. They measure the importance of each parameter for solving a given task and subsequently protect important parameters from large changes. In the literature, three ways to measure parameter importance have been put forward and they have inspired a large body of follow-up work. Here, we present strong theoretical and empirical evidence that these three methods, Elastic Weight Consolidation (EWC), Synaptic Intelligence (SI) and Memory Aware Synapses (MAS), are surprisingly similar and are all linked to the same theoretical quantity. Concretely, we show that, despite stemming from very different motivations, both SI and MAS approximate the square root of the Fisher Information, with the Fisher being the theoretically justified basis of EWC. Moreover, we show that for SI the relation to the Fisher - and in fact its performance - is due to a previously unknown bias. On top of uncovering unknown similarities and unifying regularisation approaches, we also demonstrate that our insights enable practical performance improvements for large batch training.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072702019,0
C,"Choi, K; Meng, CL; Song, Y; Ermon, S",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Choi, Kristy; Meng, Chenlin; Song, Yang; Ermon, Stefano",,,Density Ratio Estimation via Infinitesimal Classification,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Density ratio estimation (DRE) is a fundamental machine learning technique for comparing two probability distributions. However, existing methods struggle in high-dimensional settings, as it is difficult to accurately compare probability distributions based on finite samples. In this work we propose DRE-infinity, a divide-and-conquer approach to reduce DRE to a series of easier subproblems. Inspired by Monte Carlo methods, we smoothly interpolate between the two distributions via an infinite continuum of intermediate bridge distributions. We then estimate the instantaneous rate of change of the bridge distributions indexed by time (the time score)-a quantity defined analogously to data (Stein) scores-with a novel time score matching objective. Crucially, the learned time scores can then be integrated to compute the desired density ratio. In addition, we show that traditional (Stein) scores can be used to obtain integration paths that connect regions of high density in both distributions, improving performance in practice. Empirically, we demonstrate that our approach performs well on downstream tasks such as mutual information estimation and energy-based modeling on complex, high-dimensional datasets.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072702027,0
C,"Cohen-Addad, V; Esencayi, Y; Fan, CL; Gaboradi, M; Li, S; Wang, D",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Cohen-Addad, Vincent; Esencayi, Yunus; Fan, Chenglin; Gaboradi, Marco; Li, Shi; Wang, Di",,,On Facility Location Problem in the Local Differential Privacy Model,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We study the facility location problem under the constraints imposed by local differential privacy (LDP). Recently, Gupta et al. (2010) and Esencayi et al. (2019) proposed lower and upper bounds for the problem on the central differential privacy (DP) model where a trusted curator first collects all data and processes it. In this paper, we focus on the LDP model, where we protect a client's participation in the facility location instance. Under the HST metric, we show that there is a non-interactive epsilon-LDP algorithm achieving O(n(1/4)/epsilon(2))-approximation ratio, where n is the size of the metric. On the negative side, we show a lower bound of Omega(n(1/4)/root epsilon) on the approximation ratio for any non-interactive epsilon-LDP algorithm. Thus, our results are tight up to a polynomial factor of epsilon. Moreover, unlike previous results, our results generalize to non-uniform facility costs.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072704001,0
C,"Danks, D; Yau, C",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Danks, Dominic; Yau, Christopher",,,Derivative-Based Neural Modelling of Cumulative Distribution Functions for Survival Analysis,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Survival models - particularly those able to account for patient comorbidities via competing risks analysis - offer valuable prognostic information to clinicians making critical decisions and represent a growing area of application for machine learning approaches. However, current methods typically involve restrictive parameterisations, discretisation of time or the modelling of only one event cause. In this paper, we highlight how general cumulative distribution functions can be naturally expressed via neural network-based ordinary differential equations and how this observation can be utilised in survival analysis. In particular, we present DeSurv, a neural derivative-based approach capable of avoiding the aforementioned restrictions and flexibly modelling competing-risk survival data in continuous time. We apply DeSury to both single-risk and competing-risk synthetic and real-world datasets and obtain results which compare favourably with current state-of-the-art models.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852301031,0
C,"Ding, Q; Hsieh, CJ; Sharpnack, J",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Ding, Qin; Hsieh, Cho-Jui; Sharpnack, James",,,Robust Stochastic Linear Contextual Bandits Under Adversarial Attacks,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Stochastic linear contextual bandit algorithms have substantial applications in practice, such as recommender systems, online advertising, clinical trials, etc. Recent works show that optimal bandit algorithms are vulnerable to adversarial attacks and can fail completely in the presence of attacks. Existing robust bandit algorithms only work for the non-contextual setting under the attack of rewards and cannot improve the robustness in the general and popular contextual bandit environment. In addition, none of the existing methods can defend against attacked context. In this work, we provide the first robust bandit algorithm for stochastic linear contextual bandit setting under a fully adaptive and omniscient attack with sub-linear regret. Our algorithm not only works under the attack of rewards, but also under attacked context. Moreover, it does not need any information about the attack budget or the particular form of the attack. We provide theoretical guarantees for our proposed algorithm and show by experiments that our proposed algorithm improves the robustness against various kinds of popular attacks.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852301025,0
C,"Dupont, E; Teh, YW; Doucet, A",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Dupont, Emilien; Teh, Yee Whye; Doucet, Arnaud",,,Generative Models as Distributions of Functions,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Generative models are typically trained on grid-like data such as images. As a result, the size of these models usually scales directly with the underlying grid resolution. In this paper, we abandon discretized grids and instead parameterize individual data points by continuous functions. We then build generative models by learning distributions over such functions. By treating data points as functions, we can abstract away from the specific type of data we train on and construct models that are agnostic to discretization. To train our model, we use an adversarial approach with a discriminator that acts on continuous signals. Through experiments on a wide variety of data modalities including images, 3D shapes and climate data, we demonstrate that our model can learn rich distributions of functions independently of data type and resolution.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072703004,0
C,"Hoang, TN; Deoras, A; Zhao, T; Li, J; Karypis, G",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Hoang, Trong Nghia; Deoras, Anoop; Zhao, Tong; Li, Jin; Karypis, George",,,Learning Personalized Item-to-Item Recommendation Metric via Implicit Feedback,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"This paper studies the item-to-item recommendation problem in recommender systems from a new perspective of metric learning via implicit feedback. We develop and investigate a personalizable deep metric model that captures both the internal contents of items and how they were interacted with by users. There are two key challenges in learning such model. First, there is no explicit similarity annotation, which deviates from the assumption of most metric learning methods. Second, these approaches ignore the fact that items are often represented by multiple sources of meta data and different users use different combinations of these sources to form their own notion of similarity. To address these challenges, we develop a new metric representation embedded as kernel parameters of a probabilistic model. This helps express the correlation between items that a user has interacted with, which can be used to predict user interaction with new items. Our approach hinges on the intuition that similar items induce similar interactions from the same user, thus fitting a metric-parameterized model to predict an implicit feedback signal could indirectly guide it towards finding the most suitable metric for each user. To this end, we also analyze how and when the proposed method is effective from a theoretical lens. Its empirical effectiveness is also demonstrated on several real-world datasets.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072701005,0
C,"Hong, J; Kveton, B; Zaheer, M; Ghavamzadeh, M",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Hong, Joey; Kveton, Branislav; Zaheer, Manzil; Ghavamzadeh, Mohammad",,,Hierarchical Bayesian Bandits,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Meta-, multi-task, and federated learning can be all viewed as solving similar tasks, drawn from a distribution that reflects task similarities. We provide a unified view of all these problems, as learning to act in a hierarchical Bayesian bandit. We propose and analyze a natural hierarchical Thompson sampling algorithm (HierTS) for this class of problems. Our regret bounds hold for many variants of the problems, including when the tasks are solved sequentially or in parallel; and show that the regret decreases with a more informative prior. Our proofs rely on a novel total variance decomposition that can be applied beyond our models. Our theory is complemented by experiments, which show that the hierarchy helps with knowledge sharing among the tasks. This confirms that hierarchical Bayesian bandits are a universal and statistically-efficient tool for learning to act with similar bandit tasks.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852302007,0
C,"Huang, A; Liu, LQ; Lipton, ZC; Azizzadenesheli, K",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Huang, Audrey; Liu Leqi; Lipton, Zachary C.; Azizzadenesheli, Kamyar",,,Off-Policy Risk Assessment for Markov Decision Processes,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Addressing such diverse ends as mitigating safety risks, aligning agent behavior with human preferences, and improving the efficiency of learning, an emerging line of reinforcement learning research addresses the entire distribution of returns and various risk functionals that depend upon it. In the contextual bandit setting, recently work on off-policy risk assessment estimates the target policy's CDF of returns, providing finite sample guarantees that extend to (and hold simultaneously over) plugin estimates of an arbitrarily large set of risk functionals. In this paper, we lift OPRA to Markov decision processes (MDPs), where importance sampling (IS) CDF estimators suffer high variance on longer trajectories due to vanishing (and exploding) importance weights. To mitigate these problems, we incorporate model-based estimation to develop the first doubly robust (DR) estimator for the CDF of returns in MDPs. The DR estimator enjoys significantly less variance and, when the model is well specified, achieves the Cramer-Rao variance lower bound. Moreover, for many risk functionals, the downstream estimates enjoy both lower bias and lower variance. Additionally, we derive the first minimax lower bounds for off-policy CDF and risk estimation, which match our error bounds up to a constant. Finally, we demonstrate the efficacy of our DR CDF estimates experimentally on several different environments.",,,,,"Liu, Leqi/HGA-0678-2022",,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072705007,0
C,"Kim, H; Cho, S; Kim, D; Ok, J",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Kim, Hoyoung; Cho, Seunghyuk; Kim, Dongwoo; Ok, Jungseul",,,Robust Deep Learning from Crowds with Belief Propagation,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Crowdsourcing systems enable us to collect large-scale dataset, but inherently suffer from noisy labels of low-paid workers. We address the inference and learning problems using such a crowdsourced dataset with noise. Due to the nature of sparsity in crowdsourcing, it is critical to exploit both probabilistic model to capture worker prior and neural network to extract task feature despite risks from wrong prior and overfitted feature in practice. We hence establish a neural-powered Bayesian framework, from which we devise deepMF and deepBP with different choice of variational approximation methods, mean field (MF) and belief propagation (BP), respectively. This provides a unified view of existing methods, which are special cases of deepMF with different priors. In addition, our empirical study suggests that deepBP is a new approach, which is more robust against wrong prior, feature overfitting and extreme workers thanks to the more sophisticated BP than MF.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072702037,0
C,"Kim, J; Choi, S",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Kim, Jungtaek; Choi, Seungjin",,,On Uncertainty Estimation by Tree-based Surrogate Models in Sequential Model-based Optimization,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Sequential model-based optimization sequentially selects a candidate point by constructing a surrogate model with the history of evaluations, to solve a black-box optimization problem. Gaussian process (GP) regression is a popular choice as a surrogate model, because of its capability of calculating prediction uncertainty analytically. On the other hand, an ensemble of randomized trees is another option and has practical merits over GPs due to its scalability and easiness of handling continuous/discrete mixed variables. In this paper we revisit various ensembles of randomized trees to investigate their behavior in the perspective of prediction uncertainty estimation. Then, we propose a new way of constructing an ensemble of randomized trees, referred to as BwO forest, where bagging with oversampling is employed to construct boot-strapped samples that are used to build randomized trees with random splitting. Experimental results demonstrate the validity and good performance of BwO forest over existing tree-based models in various circumstances.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072704019,0
C,"Kleindessner, M; Samadi, S; Zafar, MB; Kenthapadi, K; Russell, C",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Kleindessner, Matthaus; Samadi, Samira; Zafar, Muhammad Bilal; Kenthapadi, Krishnaram; Russell, Chris",,,Pairwise Fairness for Ordinal Regression,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We initiate the study of fairness for ordinal regression. We adapt two fairness notions previously considered in fair ranking and propose a strategy for training a predictor that is approximately fair according to either notion. Our predictor has the form of a threshold model, composed of a scoring function and a set of thresholds, and our strategy is based on a reduction to fair binary classification for learning the scoring function and local search for choosing the thresholds. We provide generalization guarantees on the error and fairness violation of our predictor, and we illustrate the effectiveness of our approach in extensive experiments.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072703020,0
C,"Liu, TY; Li, Y; Zhou, EL; Zhao, T",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Liu, Tianyi; Li, Yan; Zhou, Enlu; Zhao, Tuo",,,"Noise Regularizes Over-parameterized Rank One Matrix Recovery, Provably","INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"We investigate the role of noise in optimization algorithms for learning overparameterized models. Specifically, we consider the recovery of a rank one matrix Y* is an element of R-dxd from a noisy observation Y using an over-parameterization model. We parameterize the rank one matrix Y* by XX inverted perpendicular, where X is an element of R-dxd. We then show that under mild conditions, the estimator, obtained by the randomly perturbed gradient descent algorithm using the square loss function, attains a mean square error of O(sigma(2)/d), where sigma(2) is the variance of the observational noise. In contrast, the estimator obtained by gradient descent without random perturbation only attains a mean square error of O(sigma(2)). Our result partially justifies the implicit regularization effect of noise when learning over-parameterized models, and provides new understandings of training over-parameterized neural networks.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072702036,0
C,"Morningstar, WR; Alemi, AA; Dillon, JV",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Morningstar, Warren R.; Alemi, Alexander A.; Dillon, Joshua, V",,,PAC(m)-Bayes Narrowing the Empirical Risk Gap in the Misspecified Bayesian Regime,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"The Bayesian posterior minimizes the inferential risk which itself bounds the predictive risk. This bound is tight when the likelihood and prior are well-specified. However since misspecification induces a gap, the Bayesian posterior predictive distribution may have poor generalization performance. This work develops a multi-sample loss (PAC(m)) which can close the gap by spanning a trade-off between the two risks. The loss is computationally favorable and offers PAC generalization guarantees. Empirical study demonstrates improvement to the predictive distribution.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852302030,0
C,"Nguyen, V; Dastidar, KG; Granitzer, M; Siblini, W",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Nguyen, Van Bach; Dastidar, Kanishka Ghosh; Granitzer, Michael; Siblini, Wissam",,,The Importance of Future Information in Credit Card Fraud Detection,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Fraud detection systems (FDS) mainly perform two tasks: (i) real-time detection while the payment is being processed and (ii) posterior detection to block the card retrospectively and avoid further frauds. Since human verification is often necessary and the payment processing time is limited, the second task manages the largest volume of transactions. In the literature, fraud detection challenges and algorithms performance are widely studied but the very formulation of the problem is never disrupted: it aims at predicting if a transaction is fraudulent based on its characteristics and the past transactions of the cardholder. Yet, in posterior detection, verification often takes days, so new payments on the card become available before a decision is taken. This is our motivation to propose a new paradigm: posterior fraud detection with future information. We start by providing evidence of the on-time availability of subsequent transactions, usable as extra context to improve detection. We then design a Bidirectional LSTM to make use of these transactions. On a real-world dataset with over 30 million transactions, it achieves higher performance than a regular LSTM, which is the state-of-the-art classifier for fraud detection that only uses the past context. We also introduce new metrics to show that the proposal catches more frauds, more compromised cards, and based on their earliest frauds. We believe that future works on this new paradigm will have a significant impact on the detection of compromised cards.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852304027,0
C,"Nietert, S; Cummings, R; Goldfeld, Z",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Nietert, Sloan; Cummings, Rachel; Goldfeld, Ziv",,,"Outlier-Robust Optimal Transport: Duality, Structure, and Statistical Analysis","INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"The Wasserstein distance, rooted in optimal transport (OT) theory, is a popular discrepancy measure between probability distributions with various applications to statistics and machine learning. Despite their rich structure and demonstrated utility, Wasserstein distances are sensitive to outliers in the considered distributions, which hinders applicability in practice. We propose a new outlier-robust Wasserstein distance W-p(epsilon) which allows for epsilon outlier mass to be removed from each contaminated distribution. Under standard moment assumptions, W-p(epsilon) is shown to be minimax optimal for robust estimation under the Huber epsilon-contamination model. Our formulation of this robust distance amounts to a highly regular optimization problem that lends itself better for analysis compared to previously considered frameworks. Leveraging this, we conduct a thorough theoretical study of W-p(epsilon), encompassing robustness guarantees, characterization of optimal perturbations, regularity, duality, and statistical estimation. In particular, by decoupling the optimization variables, we arrive at a simple dual form for W(p)(epsilon )that can be implemented via an elementary modification to standard, duality-based OT solvers. We illustrate the virtues of our framework via applications to generative modeling with contaminated datasets.",,,,,,"Nietert, Sloan/0000-0001-5741-4296",,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852306011,0
C,"Nix, AF; Shrinivasan, S; Walker, EY; Sinz, FH",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Nix, Arne F.; Shrinivasan, Suhas; Walker, Edgar Y.; Sinz, Fabian H.",,,Can Functional Transfer Methods Capture Simple Inductive Biases?,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Transferring knowledge embedded in trained neural networks is a core problem in areas like model compression and continual learning. Among knowledge transfer approaches, functional transfer methods such as knowledge distillation and representational distance learning are particularly promising, since they allow for transferring knowledge across different architectures and tasks. Considering various characteristics of networks that are desirable to transfer, equivariance is a notable property that enables a network to capture valuable relationships in the data. We assess existing functional transfer methods on their ability to transfer equivariance and empirically show that they fail to even transfer shift equivariance, one of the simplest equivariances. Further theoretical analysis demonstrates that representational similarity methods, in fact, cannot guarantee the transfer of the intended equivariance. Motivated by these findings, we develop a novel transfer method that learns an equivariance model from a given teacher network and encourages the student network to acquire the same equivariance, via regularization. Experiments show that our method successfully transfers equivariance even in cases where highly restrictive methods, such as directly matching student and teacher representations, fail.(1)",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852305011,0
C,"Rando, M; Carratino, L; Villa, S; Rosasco, L",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Rando, Marco; Carratino, Luigi; Villa, Silvia; Rosasco, Lorenzo",,,Ada-BKB: Scalable Gaussian Process Optimization on Continuous Domains by Adaptive Discretization,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Gaussian process optimization is a successful class of algorithms(e.g. GP-UCB) to optimize a black-box function through sequential evaluations. However, for functions with continuous domains, Gaussian process optimization has to rely on either a fixed discretization of the space, or the solution of a non-convex optimization subproblem at each evaluation. The first approach can negatively affect performance, while the second approach requires a heavy computational burden. A third option, only recently theoretically studied, is to adaptively discretize the function domain. Even though this approach avoids the extra non-convex optimization costs, the overall computational complexity is still prohibitive. An algorithm such as GP-UCB has a runtime of O(T-4), where T is the number of iterations. In this paper, we introduce Ada-BKB (Adaptive Budgeted Kernelized Bandit), a no-regret Gaussian process optimization algorithm for functions on continuous domains, that provably runs in O(T(2)d(eff)(2)), where d(eff) is the effective dimension of the explored space, and which is typically much smaller than T. We corroborate our theoretical findings with experiments on synthetic non-convex functions and on the real-world problem of hyperparameter optimization, confirming the good practical performances of the proposed approach.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852301035,0
C,"Sawhney, R; Agarwal, S; Neerkaje, AT; Pathak, K",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Sawhney, Ramit; Agarwal, Shivam; Neerkaje, Atula Tejaswi; Pathak, Kapil",,,Orthogonal Multi-Manifold Enriching of Directed Networks,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Directed Acyclic Graphs and trees are widely prevalent in several real-world applications. These hierarchical structures show intriguing properties such as scale-free and bipartite nature, with line-grained ternporal irregularities among nodes. Building on advances in geometrical deep learning, we explore a time-aware neural network to model trees and Directed Acyclic Graphs in multiple Riemannian manifolds of varying curvatures. To jointly utilize the strength of these manifolds, we propose Multi-Manifold Recursive Interaction Learning (MRIL) on Directed Acyclic Graphs where we introduce an inter-manifold learning mechanism that recursively enriches each manifold with representations from sibling manifolds. We propose the integration of the Stiefel orthogonality constraint which stabilizes the training process in Riemannian manifolds. Through a series of quantitative and exploratory experiments, we show that our method achieves competitive performance and converges much faster on data spanning several domains.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,6074,6086,,,,,,,,,,,,,,,,WOS:000841852300022,0
C,"Sreenivasan, K; Rajput, S; Sohn, JY; Papailiopoulos, D",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Sreenivasan, Kartik; Rajput, Shashank; Sohn, Jy-yong; Papailiopoulos, Dimitris",,,Finding Nearly Everything within Random Binary Networks,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"A recent work by Ramanujan et al. (2020) provides significant empirical evidence that sufficiently overparameterized, random neural networks contain untrained subnetworks that achieve state-of-the-art accuracy on several predictive tasks. A follow-up line of theoretical work provides justification of these findings by proving that slightly overparameterized neural networks, with commonly used continuous-valued random initializations can indeed be pruned to approximate any target network. In this work, we show that the amplitude of those random weights does not even matter. We prove that any target network of width d and depth l can be approximated up to arbitrary accuracy epsilon by simply pruning a random network of binary {+/- 1} weights that is wider and deeper than the target network only by a polylogarithmic factor of d; l and epsilon.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000828072703026,0
C,"Xu, JY; Wang, YX",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Xu, Jianyu; Wang, Yu-Xiang",,,Towards Agnostic Feature-based Dynamic Pricing: Linear Policies vs Linear Valuation with Unknown Noise,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"In feature-based dynamic pricing, a seller sets appropriate prices for a sequence of products (described by feature vectors) on the fly by learning from the binary outcomes of previous sales sessions (Sold if valuation >= price, and Not Sold otherwise). Existing works either assume noiseless linear valuation or precisely-known noise distribution, which limits the applicability of those algorithms in practice when these assumptions are hard to verify. In this work, we study two more agnostic models: (a) a linear policy problem where we aim at competing with the best linear pricing policy while making no assumptions on the data, and (b) a linear noisy valuation problem where the random valuation is linear plus an unknown and assumption-free noise. For the former model, we show a (Theta) over tilde (d(2/3) T-2/3) minimax regret up to logarithmic factors. For the latter model, we present an algorithm that achieves an (O) over tilde (T-3/4) regret, and improve the best-known lower bound from Omega(T-3/5) to (Omega) over tilde (T-2/3). These results demonstrate that no-regret learning is possible for feature-based dynamic pricing under weak assumptions, but also reveal a disappointing fact that the seemingly richer pricing feedback is not significantly more useful than the bandit-feedback in regret reduction.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852304007,0
C,"Xu, YZ; Yadlowsky, S",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Xu, Yizhe; Yadlowsky, Steve",,,Calibration Error for Heterogeneous Treatment Effects,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Recently, many researchers have advanced data-driven methods for modeling heterogeneous treatment effects (HTEs). Even still, estimation of HTEs is a difficult task these methods frequently over- or under-estimate the treatment effects, leading to poor calibration of the resulting models. However, while many methods exist for evaluating the calibration of prediction and classification models, formal approaches to assess the calibration of HTE models are limited to the calibration slope. In this paper, we define an analogue of the (l(2)) expected calibration error for HTEs, and propose a robust estimator. Our approach is motivated by doubly robust treatment effect estimators, making it unbiased, and resilient to confounding, overfitting, and high-dimensionality issues. Furthermore, our method is straightforward to adapt to many structures under which treatment effects can be identified, including randomized trials, observational studies, and survival analysis. We illustrate how to use our proposed metric to evaluate the calibration of learned HTE models through the application to the CRITEOUPLIFT Trial.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852303029,0
C,"Zhuang, YB; Chen, XH; Yang, Y",,"Camps-Valls, G; Ruiz, FJR; Valera, I",,"Zhuang, Yubo; Chen, Xiaohui; Yang, Yun",,,Sketch-and-Lift: Scalable Subsampled Semidefinite Program for K-means Clustering,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 151",Proceedings of Machine Learning Research,,,,International Conference on Artificial Intelligence and Statistics,"MAR 28-30, 2022",ELECTR NETWORK,,,,,"Semidefinite programming (SDP) is a powerful tool for tackling a wide range of computationally hard problems such as clustering. Despite the high accuracy, semidefinite programs are often too slow in practice with poor scalability on large (or even moderate) datasets. In this paper, we introduce a linear time complexity algorithm for approximating an SDP relaxed K-means clustering. The proposed sketch-and-lift (SL) approach solves an SDP on a subsampled dataset and then propagates the solution to all data points by a nearest-centroid rounding procedure. It is shown that the SL approach enjoys a similar exact recovery threshold as the K-means SDP on the full dataset, which is known to be information-theoretically tight under the Gaussian mixture model. The SL method can be made adaptive with enhanced theoretic properties when the cluster sizes are unbalanced. Our simulation experiments demonstrate that the statistical accuracy of the proposed method outperforms state-of-the-art fast clustering algorithms without sacrificing too much computational efficiency, and is comparable to the original K-means SDP with substantially reduced runtime.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2022,151,,,,,,,,,,,,,,,,,,,,,,,WOS:000841852303027,0
C,"Abeille, M; Faury, L; Calauzenes, C",,"Banerjee, A; Fukumizu, K",,"Abeille, Marc; Faury, Louis; Calauzenes, Clement",,,Instance-Wise Minimax-Optimal Algorithms for Logistic Bandits,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Logistic Bandits have recently attracted substantial attention, by providing an uncluttered yet challenging framework for understanding the impact of non-linearity in parametrized bandits. It was shown by Faury et al. (2020) that the learning-theoretic difficulties of Logistic Bandits can be embodied by a large (sometimes prohibitively) problem-dependent constant kappa, characterizing the magnitude of the reward's non-linearity. In this paper we introduce a novel algorithm for which we provide a refined analysis. This allows for a better characterization of the effect of non-linearity and yields improved problem-dependent guarantees. In most favorable cases this leads to a regret upper-bound scaling as (O) over tilde (d root T/kappa), which dramatically improves over the (O) over tilde (d root T + kappa) state-of-the-art guarantees. We prove that this rate is minimax-optimal by deriving a Omega(d root T/kappa) problem-dependent lower-bound. Our analysis identifies two regimes (permanent and transitory) of the regret, which ultimately re-conciliates (Faury et al., 2020) with the Bayesian approach of Dong et al. (2019). In contrast to previous works, we find that in the permanent regime non-linearity can dramatically ease the exploration-exploitation trade-off. While it also impacts the length of the transitory phase in a problem-dependent fashion, we show that this impact is mild in most reasonable configurations.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804045,0
C,"Ahuja, K; Shanmugam, K; Dhurandhar, A",,"Banerjee, A; Fukumizu, K",,"Ahuja, Kartik; Shanmugam, Karthikeyan; Dhurandhar, Amit",,,Linear Regression Games: Convergence Guarantees to Approximate Out-Of-Distribution Solutions,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Recently, invariant risk minimization (IRM) [Arjovsky et al., 2019] was proposed as a promising solution to address out-of-distribution (OOD) generalization. In [Ahuja et al., 2020], it was shown that solving for the Nash equilibria of a new class of ensemble-games is equivalent to solving IRM. In this work, we extend the framework in [Ahuja et al., 2020] for linear regressions by projecting the ensemble-game on an l(infinity) ball. We show that such projections help achieve non-trivial OOD guarantees despite not achieving perfect invariance. For linear models with confounders, we prove that Nash equilibria of these games are closer to the ideal OOD solutions than the standard empirical risk minimization (ERM) and we also provide learning algorithms that provably converge to these Nash Equilibria. Empirical comparisons of the proposed approach with the state-of-the-art show consistent gains in achieving OOD solutions in several settings involving anti-causal variables and confounders.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801055,0
C,"Anagnostides, I; Gouleakis, T; Marashian, A",,"Banerjee, A; Fukumizu, K",,"Anagnostides, Ioannis; Gouleakis, Themis; Marashian, Ali",,,Robust Learning under Strong Noise via SQs,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"This work provides several new insights on the robustness of Kearns' statistical query framework against challenging label-noise models. First, we build on a recent result by Chen et al. (2020) which showed noise tolerance of distribution-independently evolvable concept classes under Massart noise. Specifically, we extend their characterization to more general noise models, including the Tsybakov model which considerably generalizes the Massart condition by allowing the flipping probability to be arbitrarily close to 1/2 for a subset of the domain. As a corollary, we employ an evolutionary algorithm by Kanade et al. (2010) to obtain the first polynomial time algorithm with arbitrarily small excess error for learning linear threshold functions over any spherically symmetric distribution in the presence of spherically symmetric Tsybakov noise. Moreover, we posit access to a stronger oracle, in which for every labeled example we additionally obtain its flipping probability. In this model, we show that every SQ learnable class admits an efficient learning algorithm with OPT+subset of misclassification error for a broad class of noise models. This setting substantially generalizes the widely-studied problem of classification under RCN with known noise rate, and corresponds to a non-convex optimization problem even when the noise function - i.e. the flipping probabilities of all points -is known in advance.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804058,0
C,"Bhaskara, A; Ruwanpathirana, K; Wijewardena, M",,"Banerjee, A; Fukumizu, K",,"Bhaskara, Aditya; Ruwanpathirana, Kanchana; Wijewardena, Maheshakya",,,Principal Component Regression with Semirandom Observations via Matrix Completion,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Principal Component Regression (PCR) is a popular method for prediction from data, and is one way to address the so-called multi-collinearity problem in regression. It was shown recently that algorithms for PCR such as hard singular value thresholding (HSVT) are also quite robust, in that they can handle data that has missing or noisy covariates. However, such spectral approaches require strong distributional assumptions on which entries are observed. Specifically, every covariate is assumed to be observed with probability (exactly) p, for some value of p. Our goal in this work is to weaken this requirement, and as a step towards this, we study a \semi-random model. In this model, every covariate is revealed with probability p, and then an adversary comes in and reveals additional covariates. While the model seems intuitively easier, it is well known that algorithms such as HSVT perform poorly. Our approach is based on studying the closely related problem of Noisy Matrix Completion in a semi-random setting. By considering a new semidefinite programming relaxation, we develop new guarantees for matrix completion, which is our core technical contribution.",,,,,,"RUWANPATHIRANA, ARAVINDA KANCHANA/0000-0002-9890-1071",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803023,0
C,"Chen, YF; Yang, Y",,"Banerjee, A; Fukumizu, K",,"Chen, Yifan; Yang, Yun",,,Fast Statistical Leverage Score Approximation in Kernel Ridge Regression,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Nystrom approximation is a fast randomized method that rapidly solves kernel ridge regression (KRR) problems through subsampling the n-by-n empirical kernel matrix appearing in the objective function. However, the performance of such a sub-sampling method heavily relies on correctly estimating the statistical leverage scores for forming the sampling distribution, which can be as costly as solving the original KRR. In this work, we propose a linear time (modulo polylog terms) algorithm to accurately approximate the statistical leverage scores in the stationary-kernel-based KRR with theoretical guarantees. Particularly, by analyzing the first-order condition of the KRR objective, we derive an analytic formula, which depends on both the input distribution and the spectral density of stationary kernels, for capturing the non-uniformity of the statistical leverage scores. Numerical experiments demonstrate that with the same prediction accuracy our method is orders of magnitude more efficient than existing methods in selecting the representative sub-samples in the Nystrom approximation.",,,,,"Chen, Yi/HIR-2608-2022",,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803053,0
C,"Domingues, OD; Menard, P; Pirotta, M; Kaufmann, E; Valko, M",,"Banerjee, A; Fukumizu, K",,"Domingues, Omar D.; Menard, Pierre; Pirotta, Matteo; Kaufmann, Emilie; Valko, Michal",,,A Kernel-Based Approach to Non-Stationary Reinforcement Learning in Metric Spaces,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"In this work, we propose KeRNS: an algorithm for episodic reinforcement learning in non-stationary Markov Decision Processes (MDPs) whose state-action set is endowed with a metric. Using a non-parametric model of the MDP built with time-dependent kernels, we prove a regret bound that scales with the covering dimension of the state-action space and the total variation of the MDP with time, which quantifies its level of non-stationarity. Our method generalizes previous approaches based on sliding windows and exponential discounting used to handle changing environments. We further propose a practical implementation of KeRNS, we analyze its regret and validate it experimentally.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804028,0
C,"Dvinskikh, D; Tiapkin, D",,"Banerjee, A; Fukumizu, K",,"Dvinskikh, Darina; Tiapkin, Daniil",,,Improved Complexity Bounds in Wasserstein Barycenter Problem,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"In this paper, we focus on computational aspects of the Wasserstein barycenter problem. We propose two algorithms to compute Wasserstein barycenters of m discrete measures of size n with accuracy epsilon. The first algorithm, based on mirror prox with a specific norm, meets the complexity of celebrated accelerated iterative Bregman projections (IBP), namely (O) over tilde (mn(2) root n/epsilon), however, with no limitations in contrast to the (accelerated) IBP, which is numerically unstable under small regularization parameter. The second algorithm, based on area-convexity and dual extrapolation, improves the previously best-known convergence rates for the Wasserstein barycenter problem enjoying (O) over tilde (mn(2)/epsilon) complexity.",,,,,,"Tiapkin, Daniil/0000-0002-8832-7926",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802013,0
C,"Ginart, AA; Zhang, E; Kwon, Y; Zou, J",,"Banerjee, A; Fukumizu, K",,"Ginart, Antonio A.; Zhang, Eva; Kwon, Yongchan; Zou, James",,,Competing AI: How does competition feedback affect machine learning?,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"This papers studies how competition affects machine learning (ML) predictors. As ML becomes more ubiquitous, it is often deployed by companies to compete over customers. For example, digital platforms like Yelp use ML to predict user preference and make recommendations. A service that is more often queried by users, perhaps because it more accurately anticipates user preferences, is also more likely to obtain additional user data (e.g. in the form of a Yelp review). Thus, competing predictors cause feedback loops whereby a predictor's performance impacts what training data it receives and biases its predictions over time. We introduce a flexible model of competing ML predictors that enables both rapid experimentation and theoretical tractability. We show with empirical and mathematical analysis that competition causes predictors to specialize for specific sub-populations at the cost of worse performance over the general population. We further analyze the impact of predictor specialization on the overall prediction quality experienced by users. We show that having too few or too many competing predictors in a market can hurt the overall prediction quality. Our theory is complemented by experiments on several real datasets using popular learning algorithms, such as neural networks and nearest neighbor methods.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802008,0
C,"Iwazaki, S; Inatsu, Y; Takeuchi, I",,"Banerjee, A; Fukumizu, K",,"Iwazaki, Shogo; Inatsu, Yu; Takeuchi, Ichiro",,,Mean-Variance Analysis in Bayesian Optimization under Uncertainty,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We consider active learning (AL) in an uncertain environment in which trade-off between multiple risk measures need to be considered. As an AL problem in such an uncertain environment, we study Mean-Variance Analysis in Bayesian Optimization (MVA-BO) setting. Mean-variance analysis was developed in the field of financial engineering and has been used to make decisions that take into account the trade-off between the average and variance of investment uncertainty. In this paper, we specifically focus on BO setting with an uncertain component and consider multi-task, multi-objective, and constrained optimization scenarios for the mean-variance trade-off of the uncertain component. When the target blackbox function is modeled by Gaussian Process (GP), we derive the bounds of the two risk measures and propose AL algorithm for each of the above three scenarios based on the risk measure bounds. We show the effectiveness of the proposed AL algorithms through theoretical analysis and numerical experiments.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801022,0
C,"Li, YK; Wang, YN; Chen, X; Zhou, Y",,"Banerjee, A; Fukumizu, K",,"Li, Yingkai; Wang, Yining; Chen, Xi; Zhou, Yuan",,,Tight Regret Bounds for Infinite-armed Linear Contextual Bandits,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Linear contextual bandit is an important class of sequential decision making problems with a wide range of applications to recommender systems, online advertising, healthcare, and many other machine learning related tasks. While there is a lot of prior research, tight regret bounds of linear contextual bandit with infinite action sets remain open. In this paper, we address this open problem by considering the linear contextual bandit with (changing) infinite action sets. We prove a regret upper bound on the order of O(root d(2)T log T) x poly(log log T) where d is the domain dimension and T is the time horizon. Our upper bound matches the previous lower bound of Omega(root d(2)T log T) in [Li et al., 2019] up to iterated logarithmic terms.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,370,378,,,,,,,,,,,,,,,,WOS:000659893800042,0
C,"Ma, JQ; Yi, XY; Tang, WJ; Zhao, Z; Hong, LC; Chi, EH; Mei, QZ",,"Banerjee, A; Fukumizu, K",,"Ma, Jiaqi; Yi, Xinyang; Tang, Weijing; Zhao, Zhe; Hong, Lichan; Chi, Ed H.; Mei, Qiaozhu",,,Learning-to-Rank with Partitioned Preference: Fast Estimation for the Plackett-Luce Model,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We investigate the Plackett-Luce (PL) model based listwise learning-to-rank (LTR) on data with partitioned preference, where a set of items are sliced into ordered and disjoint partitions, but the ranking of items within a partition is unknown. Given N items with M partitions, calculating the likelihood of data with partitioned preference under the PL model has a time complexity of O(N + S!), where S is the maximum size of the top M - 1 partitions. This computational challenge restrains most existing PL-based listwise LTR methods to a special case of partitioned preference, top-K ranking, where the exact order of the top K items is known. In this paper, we exploit a random utility model formulation of the PL model, and propose an efficient numerical integration approach for calculating the likelihood and its gradients with a time complexity O (N + S-3). We demonstrate that the proposed method outperforms well-known LTR baselines and remains scalable through both simulation experiments and applications to real-world eXtreme Multi-Label classification tasks.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801017,0
C,"Marsden, A; Duchi, J; Valiant, G",,"Banerjee, A; Fukumizu, K",,"Marsden, Annie; Duchi, John; Valiant, Gregory",,,Misspecification in Prediction Problems and Robustness via Improper Learning,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We study probabilistic prediction games when the underlying model is misspecified, investigating the consequences of predicting using an incorrect parametric model. We show that for a broad class of loss functions and parametric families of distributions, the regret of playing a proper predictor-one from the putative model class-relative to the best predictor in the same model class has lower bound scaling at least as root gamma n, where gamma is a measure of the model misspecification to the true distribution in terms of total variation distance. In contrast, using an aggregation-based (improper) learner, one can obtain regret d log n for any underlying generating distribution, where d is the dimension of the parameter; we exhibit instances in which this is unimprovable even over the family of all learners that may play distributions in the convex hull of the parametric family. These results suggest that simple strategies for aggregating multiple learners together should be more robust, and several experiments conform to this hypothesis.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802060,0
C,"Shi, BW; Phillips, JM",,"Banerjee, A; Fukumizu, K",,"Shi, Benwei; Phillips, Jeff M.",,,A Deterministic Streaming Sketch for Ridge Regression,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We provide a deterministic space-efficient algorithm for estimating ridge regression. For n data points with d features and a large enough regularization parameter, we provide a solution within epsilon L-2 error using only O(d/epsilon) space. This is the first o(d(2)) space deterministic streaming algorithm with guaranteed solution error and risk bound for this classic problem. The algorithm sketches the covariance matrix by variants of Frequent Directions, which implies it can operate in insertion-only streams and a variety of distributed data settings. In comparisons to randomized sketching algorithms on synthetic and real-world datasets, our algorithm has less empirical error using less space and similar time.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,586,+,,,,,,,,,,,,,,,,WOS:000659893800066,0
C,"Shi, WS; Yu, Q",,"Banerjee, A; Fukumizu, K",,"Shi, Weishi; Yu, Qi",,,Active Learning with Maximum Margin Sparse Gaussian Processes,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We present a maximum-margin sparse Gaussian Process (MM-SGP) for active learning (AL) of classification models for multi-class problems. The proposed model makes novel extensions to a GP by integrating maximum-margin constraints into its learning process, aiming to further improve its predictive power while keeping its inherent capability for uncertainty quantification. The MM constraints ensure small effective size of the model, which allows MM-SGP to provide good predictive performance by using limited active data samples, a critical property for AL. Furthermore, as a Gaussian process model, MM-SGP will output both the predicted class distribution and the predictive variance, both of which are essential for defining a sampling function effective to improve the decision boundaries of a large number of classes simultaneously. Finally, the sparse nature of MM-SGP ensures that it can be efficiently trained by solving a low-rank convex dual problem. Experiment results on both synthetic and real-world datasets show the effectiveness and efficiency of the proposed AL model.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,406,+,,,,,,,,,,,,,,,,WOS:000659893800046,0
C,"Wu, F; Rebeschini, P",,"Banerjee, A; Fukumizu, K",,"Wu, Fan; Rebeschini, Patrick",,,Hadamard Wirtinger Flow for Sparse Phase Retrieval,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We consider the problem of reconstructing an n-dimensional k-sparse signal from a set of noiseless magnitude-only measurements. Formulating the problem as an unregularized empirical risk minimization task, we study the sample complexity performance of gradient descent with Hadamard parametrization, which we call Hadamard Wirtinger flow (HWF). Provided knowledge of the signal sparsity k, we prove that a single step of HWF is able to recover the support from k(x(max)*)(-2) (modulo logarithmic term) samples, where x(max)* is the largest component of the signal in magnitude. This support recovery procedure can be used to initialize existing reconstruction methods and yields algorithms with total runtime proportional to the cost of reading the data and improved sample complexity, which is linear in k when the signal contains at least one large component. We numerically investigate the performance of HWF at convergence and show that, while not requiring any explicit form of regularization nor knowledge of k, HWF adapts to the signal sparsity and reconstructs sparse signals with fewer measurements than existing gradient based methods.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801023,0
C,"Xing, Y; Song, QF; Cheng, G",,"Banerjee, A; Fukumizu, K",,"Xing, Yue; Song, Qifan; Cheng, Guang",,,Predictive Power of Nearest Neighbors Algorithm under Random Perturbation,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"This work investigates the predictive performance of the classical k Nearest Neighbors (k-NN) algorithm when the testing data are corrupted by random perturbation. The impact of corruption level on the asymptotic regret is carefully characterized and we reveal a phase-transition phenomenon that, when the corruption level of the random perturbation ! is below a critical order (i.e., small-omega regime), the asymptotic regret remains the same; when it is beyond that order (i.e., large-omega regime), the asymptotic regret deteriorates polynomially. More importantly, the regret of k-NN classifier heuristically matches the rate of minimax regret for randomly perturbed testing data, thus implies the strong robustness of k-NN against random perturbation on testing data. We show that the classical k-NN can achieve no worse predictive performance than the NN classifiers trained via the popular noise-injection strategy. Our numerical experiment also illustrates that combining k-NN component with modern learning algorithms will inherit the strong robustness of k-NN. As a technical by-product, we prove that under different model assumptions, the pre-processed 1-NN proposed in Xue and Kpotufe (2017) will achieve a suboptimal rate when the data dimension d > 4 even if k is chosen optimally in the preprocessing step.",,,,,,"Xing, Yue/0000-0001-7723-0048",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,496,+,,,,,,,,,,,,,,,,WOS:000659893800056,0
C,"Zhang, RQ; Li, YZ; De Sa, C; Devlin, S; Zhang, C",,"Banerjee, A; Fukumizu, K",,"Zhang, Ruqi; Li, Yingzhen; De Sa, Christopher; Devlin, Sam; Zhang, Cheng",,,Meta-Learning Divergences of Variational Inference,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Variational inference (VI) plays an essential role in approximate Bayesian inference due to its computational efficiency and broad applicability. Crucial to the performance of VI is the selection of the associated divergence measure, as VI approximates the intractable distribution by minimizing this divergence. In this paper we propose a meta-learning algorithm to learn the divergence metric suited for the task of interest, automating the design of VI methods. In addition, we learn the initialization of the variational parameters without additional cost when our method is deployed in the few-shot learning scenarios. We demonstrate our approach outperforms standard VI on Gaussian mixture distribution approximation, Bayesian neural network regression, image generation with variational autoencoders and recommender systems with a partial variational autoencoder.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804081,0
C,"Zhou, ZQ; Zhou, ZY; Bai, QX; Qiu, LH; Blanchet, J; Glynn, P",,"Banerjee, A; Fukumizu, K",,"Zhou, Zhengqing; Zhou, Zhengyuan; Bai, Qinxun; Qiu, Linhai; Blanchet, Jose; Glynn, Peter",,,Finite-Sample Regret Bound for Distributionally Robust Offline Tabular Reinforcement Learning,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"While reinforcement learning has witnessed tremendous success recently in a wide range of domains, robustness-or the lack thereof-remains an important issue that has not been fully explored. In this paper, we provide a distributionally robust formulation offline learning policy in tabular RL that aims to learn a policy from historical data (collected by some other behavior policy) that is robust to the future environment that can deviate from the training environment. We first develop a novel policy evaluation scheme that accurately estimates the robust value (i.e. how robust it is in a perturbed environment) of any given policy and establish its finite-sample estimation error. Building on this, we then develop a novel and minimax-optimal distributionally robust learning algorithm that achieves O-P(1/root n) regret, meaning that with high probability, the policy learned from using n training data points will be O(1/root n) close to the optimal distributionally robust policy. Finally, our simulation results demonstrate the superiority of our distributionally robust approach compared to non-robust RL algorithms.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804005,0
C,"Aglietti, V; Lu, XY; Paleyes, A; Gonzalez, J",,"Chiappa, S; Calandra, R",,"Aglietti, Virginia; Lu, Xiaoyu; Paleyes, Andrei; Gonzalez, Javier",,,Causal Bayesian Optimization,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"This paper studies the problem of globally optimizing a variable of interest that is part of a causal model in which a sequence of interventions can be performed. This problem arises in biology, operational research, communications and, more generally, in all fields where the goal is to optimize an output metric of a system of interconnected nodes. Our approach combines ideas from causal inference, uncertainty quantification and sequential decision making. In particular, it generalizes Bayesian optimization, which treats the input variables of the objective function as independent, to scenarios where causal information is available. We show how knowing the causal graph significantly improves the ability to reason about optimal decision making strategies decreasing the optimization cost while avoiding suboptimal solutions. We propose a new algorithm called Causal Bayesian Optimization (CBO). CBO automatically balances two trade-offs: the classical exploration-exploitation and the new observation-intervention, which emerges when combining real interventional data with the estimated intervention effects computed via do-calculus. We demonstrate the practical benefits of this method in a synthetic setting and in two real-world applications.",,,,,"lu, xiaoyu/GWU-6357-2022",,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3155,3164,,,,,,,,,,,,,,,,WOS:000559931300007,0
C,"Han, I; Gillenwater, J",,"Chiappa, S; Calandra, R",,"Han, Insu; Gillenwater, Jennifer",,,MAP Inference for Customized Determinantal Point Processes via Maximum Inner Product Search,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Determinantal point processes (DPPs) are a good fit for modeling diversity in many machine learning applications. For instance, in recommender systems, one might have a basic DPP defined by item features, and a customized version of this DPP for each user with features re-weighted according to user preferences. While such models perform well, they are typically applied only to relatively small datasets, because existing maximum a posteriori (MAP) approximation algorithms are expensive. In this work, we propose a new MAP algorithm: we show that, by performing a one-time preprocessing step on a basic DPP, it is possible to run an approximate version of the standard greedy MAP approximation algorithm on any customized version of the DPP in time sublinear in the number of items. Our key observation is that the core computation can be written as a maximum inner product search (MIPS), which allows us to accelerate inference via approximate MIPS structures, e.g., trees or hash tables. We provide a theoretical analysis of the algorithm's approximation quality, as well as empirical results on real-world datasets demonstrating that it is often orders of magnitude faster while sacrificing little accuracy.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2797,2806,,,,,,,,,,,,,,,,WOS:000559931301033,0
C,"Kong, ZF; Chaudhuri, K",,"Chiappa, S; Calandra, R",,"Kong, Zhifeng; Chaudhuri, Kamalika",,,The Expressive Power of a Class of Normalizing Flow Models,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Normalizing flows have received a great deal of recent attention as they allow flexible generative modeling as well as easy likelihood computation. While a wide variety of flow models have been proposed, there is little formal understanding of the representation power of these models. In this work, we study some basic normalizing flows and rigorously establish bounds on their expressive power. Our results indicate that while these flows are highly expressive in one dimension, in higher dimensions their representation power may be limited, especially when the flows have moderate depth.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3599,3608,,,,,,,,,,,,,,,,WOS:000559931301081,0
C,"Noy, A; Nayman, N; Ridnik, T; Zamir, N; Doveh, S; Friedman, I; Giryes, R; Zelnik-Manor, L",,"Chiappa, S; Calandra, R",,"Noy, Asaf; Nayman, Niv; Ridnik, Tal; Zamir, Nadav; Doveh, Sivan; Friedman, Itamar; Giryes, Raja; Zelnik-Manor, Lihi",,,"ASAP: Architecture Search, Anneal and Prune","INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Automatic methods for Neural Architecture Search (NAS) have been shown to produce state-of-the-art network models. Yet, their main drawback is the computational complexity of the search process. As some primal methods optimized over a discrete search space, thousands of days of GPU were required for convergence. A recent approach is based on constructing a differentiable search space that enables gradient-based optimization, which reduces the search time to a few days. While successful, it still includes some noncontinuous steps, e.g., the pruning of many weak connections at once. In this paper, we propose a differentiable search space that allows the annealing of architecture weights, while gradually pruning inferior operations. In this way, the search converges to a single output network in a continuous manner. Experiments on several vision datasets demonstrate the effectiveness of our method with respect to the search cost and accuracy of the achieved model. Specifically, with 0.2 GPU search days we achieve an error rate of 1.68% on CIFAR-10.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,493,502,,,,,,,,,,,,,,,,WOS:000559931302059,0
C,"Peluchetti, S; Favaro, S",,"Chiappa, S; Calandra, R",,"Peluchetti, Stefano; Favaro, Stefano",,,Infinitely deep neural networks as diffusion processes,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"When the parameters are independently and identically distributed (initialized) neural networks exhibit undesirable properties that emerge as the number of layers increases, e.g. a vanishing dependency on the input and a concentration on restrictive families of functions including constant functions. We consider parameter distributions that shrink as the number of layers increases in order to recover well-behaved stochastic processes in the limit of infinite depth. This leads to set forth a link between infinitely deep residual networks and solutions to stochastic differential equations, i.e. diffusion processes. We show that these limiting processes do not suffer from the aforementioned issues and investigate their properties.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1126,1135,,,,,,,,,,,,,,,,WOS:000559931302070,0
C,"Wu, KW; Ding, GWG; Huang, RT; Yu, YL",,"Chiappa, S; Calandra, R",,"Wu, Kaiwen; Ding, Gavin Weiguang; Huang, Ruitong; Yu, Yaoliang",,,On Minimax Optimality of GANs for Robust Mean Estimation,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Generative adversarial networks (GANs) have become one of the most popular generative modeling techniques in machine learning. In this work, we study the statistical and robust properties of GANs for Gaussian mean estimation under Huber's contamination model, where an proportion of training data may be arbitrarily corrupted. We prove that f-GAN, when equipped with appropriate discriminators, achieve optimal minimax rate, hence extending the recent result of Gao et al. (2019a). In contrast, we show that other GAN variants such as MMD-GAN (with Gaussian kernel) and W-GAN may fail to achieve minimax optimality. We further adapt f-GAN to the sparse and the unknown covariance settings. We perform numerical simulations to confirm our theoretical findings and reveal new insights on the importance of discriminators.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303084,0
C,"Yaroslavtsev, G; Zhou, S; Avdiukhin, D",,"Chiappa, S; Calandra, R",,"Yaroslavtsev, Grigory; Zhou, Samson; Avdiukhin, Dmitrii",,,BRING YOUR OWN GREEDY plus MAX: Near-Optimal 1/2-Approximations for Submodular Knapsack,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"The problem of selecting a small-size representative summary of a large dataset is a cornerstone of machine learning, optimization and data science. Motivated by applications to recommendation systems and other scenarios with query-limited access to vast amounts of data, we propose a new rigorous algorithmic framework for a standard formulation of this problem as a submodular maximization subject to a linear (knapsack) constraint. Our framework is based on augmenting all partial GREEDY solutions with the best additional item. It can be instantiated with negligible overhead in any model of computation, which allows the classic GREEDY algorithm and its variants to be implemented. We give such instantiations in the offline (GREEDY + MAX), multi-pass streaming (SIEVE + MAX) and distributed (DISTRIBUTED SIEVE + MAX) settings. Our algorithms give (1/2 - epsilon)-approximation with most other key parameters of interest being near-optimal. Our analysis is based on a new set of first-order linear differential inequalities and their robust approximate versions. Experiments on typical datasets (movie recommendations, influence maximization) confirm scalability and high quality of solutions obtained via our framework. Instance-specific approximations are typically in the 0.6-0.7 range and frequently beat even the (1 - 1/e) approximate to 0.63 worst-case barrier for polynomial-time algorithms.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303095,0
C,"Blonde, L; Kalousis, A",,"Chaudhuri, K; Sugiyama, M",,"Blonde, Lionel; Kalousis, Alexandros",,,Sample-Efficient Imitation Learning via Generative Adversarial Nets,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"GAIL is a recent successful imitation learning architecture that exploits the adversarial training procedure introduced in GANs. Albeit successful at generating behaviours similar to those demonstrated to the agent, GAIL suffers from a high sample complexity in the number of interactions it has to carry out in the environment in order to achieve satisfactory performance. We dramatically shrink the amount of interactions with the environment necessary to learn well-behaved imitation policies, by up to several orders of magnitude. Our framework, operating in the model-free regime, exhibits a significant increase in sample-efficiency over previous methods by simultaneously a) learning a self-tuned adversarially-trained surrogate reward and b) leveraging an off-policy actor-critic architecture. We show that our approach is simple to implement and that the learned agents remain remarkably stable, as shown in our experiments that span a variety of continuous control tasks. Video visualisations available at: https://youtu.be/-nCsqUJnRKU.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687903020,0
C,"Esmaeili, B; Wu, H; Jain, S; Bozkurt, A; Siddharth, N; Paige, B; Brooks, DH; Dy, J; van de Meent, JW",,"Chaudhuri, K; Sugiyama, M",,"Esmaeili, Babak; Wu, Hao; Jain, Sarthak; Bozkurt, Alican; Siddharth, N.; Paige, Brooks; Brooks, Dana H.; Dy, Jennifer; van de Meent, Jan-Willem",,,Structured Disentangled Representations,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Deep latent-variable models learn representations of high-dimensional data in an unsupervised manner. A number of recent efforts have focused on learning representations that disentangle statistically independent axes of variation by introducing modifications to the standard objective function. These approaches generally assume a simple diagonal Gaussian prior and as a result are not able to reliably disentangle discrete factors of variation. We propose a two-level hierarchical objective to control relative degree of statistical independence between blocks of variables and individual variables within blocks. We derive this objective as a generalization of the evidence lower bound, which allows us to explicitly represent the trade-offs between mutual information between data and representation, KL divergence between representation and prior, and coverage of the support of the empirical data distribution. Experiments on a variety of datasets demonstrate that our objective can not only disentangle discrete variables, but that doing so also improves disentanglement of other variables and, importantly, generalization even to unseen combinations of factors.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902059,0
C,"Karakida, R; Akaho, S; Amari, S",,"Chaudhuri, K; Sugiyama, M",,"Karakida, Ryo; Akaho, Shotaro; Amari, Shun-ichi",,,Universal Statistics of Fisher Information in Deep Neural Networks: Mean Field Approach,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"The Fisher information matrix (FIM) is a fundamental quantity to represent the characteristics of a stochastic model, including deep neural networks (DNNs). The present study reveals novel statistics of FIM that are universal among a wide class of DNNs. To this end, we use random weights and large width limits, which enables us to utilize mean field theories. We investigate the asymptotic statistics of the FIM's eigenvalues and reveal that most of them are close to zero while the maximum eigenvalue takes a huge value. Because the landscape of the parameter space is defined by the FIM, it is locally flat in most dimensions, but strongly distorted in others. Moreover, we demonstrate the potential usage of the derived statistics in learning strategies. First, small eigenvalues that induce flatness can be connected to a norm-based capacity measure of generalization ability. Second, the maximum eigenvalue that induces the distortion enables us to quantitatively estimate an appropriately sized learning rate for gradient methods to converge.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901008,0
C,"Katz, D; Shanmugam, K; Squires, C; Uhler, C",,"Chaudhuri, K; Sugiyama, M",,"Katz, Dmitriy; Shanmugam, Karthikeyan; Squires, Chandler; Uhler, Caroline",,,Size of Interventional Markov Equivalence Classes in Random DAG Models,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Directed acyclic graph (DAG) models are popular for capturing causal relationships. From observational and interventional data, a DAG model can only be determined up to its interventional Markov equivalence class (I-MEC). We investigate the size of MECs for random DAG models generated by uniformly sampling and ordering an Erdos-Renyi graph. For constant density, we show that the expected log observational MEC size asymptotically (in the number of vertices) approaches a constant. We characterize I-MEC size in a similar fashion in the above settings with high precision. We show that the asymptotic expected number of interventions required to fully identify a DAG is a constant. These results are obtained by exploiting Meek rules and coupling arguments to provide sharp upper and lower bounds on the asymptotic quantities, which are then calculated numerically up to high precision. Our results have important consequences for experimental design of interventions and the development of algorithms for causal inference.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687903030,0
C,"Li, B; Chen, CY; Liu, H; Carin, L",,"Chaudhuri, K; Sugiyama, M",,"Li, Bai; Chen, Changyou; Liu, Hao; Carin, Lawrence",,,On Connecting Stochastic Gradient MCMC and Differential Privacy,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Concerns related to data security and confidentiality have been raised when applying machine learning to real-world applications. Differential privacy provides a principled and rigorous privacy guarantee for machine learning models. While it is common to inject noise to design a model satisfying a required differential-privacy property, it is generally hard to balance the trade-off between privacy and utility. We show that stochastic gradient Markov chain Monte Carlo (SG-MCMC) a class of scalable Bayesian posterior sampling algorithms satisfies strong differential privacy, when carefully chosen stepsizes are employed. We develop theory on the performance of the proposed differentially-private SG-MCMC method. We conduct experiments to support our analysis, and show that a standard SG-MCMC sampler with minor modification can reach state-of-the-art performance in terms of both privacy and utility on Bayesian learning.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,557,566,,,,,,,,,,,,,,,,WOS:000509687900058,0
C,"Shen, ZY; Heinonen, M; Kaski, S",,"Chaudhuri, K; Sugiyama, M",,"Shen, Zheyang; Heinonen, Markus; Kaski, Samuel",,,Harmonizable mixture kernels with variational Fourier features,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"The expressive power of Gaussian processes depends heavily on the choice of kernel. In this work we propose the novel harmonizable mixture kernel (HMK), a family of expressive, interpretable, non-stationary kernels derived from mixture models on the generalized spectral representation. As a theoretically sound treatment of non-stationary kernels, HMK supports harmonizable covariances, a wide subset of kernels including all stationary and many non-stationary covariances. We also propose variational Fourier features, an inter-domain sparse GP inference framework that offers a representative set of 'inducing frequencies'. We show that harmonizable mixture kernels interpolate between local patterns, and that variational Fourier features offers a robust kernel learning framework for the new kernel family.",,,,,"Kaski, Samuel/B-6684-2008","Kaski, Samuel/0000-0003-1925-9154",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687903034,0
C,"Altosaar, J; Ranganath, R; Blei, DM",,"Storkey, A; PerezCruz, F",,"Altosaar, Jaan; Ranganath, Rajesh; Blei, David M.",,,Proximity Variational Inference,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Variational inference is a powerful approach for approximate posterior inference. However, it is sensitive to initialization and can be subject to poor local optima. In this paper, we develop proximity variational inference (pvi). pvi is a new method for optimizing the variational objective that constrains subsequent iterates of the variational parameters to robustify the optimization path. Consequently, pvi is less sensitive to initialization and optimization quirks and finds better local optima. We demonstrate our method on four proximity statistics. We study pvi on a Bernoulli factor model and sigmoid belief network fit to real and synthetic data and compare to deterministic annealing (Katahira et al., 2008). We highlight the flexibility of pvi by designing a proximity statistic for Bayesian deep learning models such as the variational autoencoder (Kingma and Welling, 2014; Rezende et al., 2014) and show that it gives better performance by reducing overpruning. pvi also yields improved predictions in a deep generative model of text. Empirically, we show that pvi consistently finds better local optima and gives better predictive performance.",,,,,"Altosaar, Jaan/AAN-6289-2020","Altosaar, Jaan/0000-0003-1294-4159",,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300205,0
C,"Chen, JX; Chang, YL; Castaldi, P; Cho, M; Hobbs, B; Dy, J",,"Storkey, A; PerezCruz, F",,"Chen, Junxiang; Chang, Yale; Castaldi, Peter; Cho, Michael; Hobbs, Brian; Dy, Jennifer",,,Crowdclustering with Partition Labels,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Crowdclustering is a practical way to incorporate domain knowledge into clustering, by combining opinions from multiple domain experts. Existing crowdclustering methods analyze binary pairwise similarity labels. However, in some applications, experts might provide partition labels. If we convert partition labels into pairwise similarity, then it would be difficult to understand the relationships between clustering solutions from different experts. In this paper, we propose a crowdclustering model that directly analyzes partition labels. The proposed model adopts a novel approach based on a modified multinomial logistic regression model, which simultaneously learns the number of clusters and determines hyper-planes that partition samples into clusters. The proposed model also learns a mapping between the latent clusters and expert labels, revealing the agreements and disagreements between experts. Experiments on benchmark data demonstrate that the proposed model simultaneously learns the number of clusters and discovers the clustering structure. An experiment on disease sub-typing problem illustrates that the proposed model helps us understand the agreement and disagreement between experts.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300118,0
C,"Cheng, CA; Boots, B",,"Storkey, A; PerezCruz, F",,"Cheng, Ching-An; Boots, Byron",,,Convergence of Value Aggregation for Imitation Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Value aggregation is a general framework for solving imitation learning problems. Based on the idea of data aggregation, it generates a policy sequence by iteratively interleaving policy optimization and evaluation in an online learning setting. While the existence of a good policy in the policy sequence can be guaranteed non-asymptotically, little is known about the convergence of the sequence or the performance of the last policy. In this paper, we debunk the common belief that value aggregation always produces a convergent policy sequence with improving performance. Moreover, we identify a critical stability condition for convergence and provide a tight non-asymptotic bound on the performance of the last policy. These new theoretical insights let us stabilize problems with regularization, which removes the inconvenient process of identifying the best policy in the policy sequence in stochastic problems.",,,,,"Cheng, Ching-An/AAZ-1802-2020","Cheng, Ching-An/0000-0002-0610-2070",,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300188,0
C,"Choromanski, K; Rowland, M; Sarlos, T; Sindhwani, V; Turner, RE; Weller, A",,"Storkey, A; PerezCruz, F",,"Choromanski, Krzysztof; Rowland, Mark; Sarlos, Tamas; Sindhwani, Vikas; Turner, Richard E.; Weller, Adrian",,,The Geometry of Random Features,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"We present an in-depth examination of the effectiveness of radial basis function kernel (beyond Gaussian) estimators based on orthogonal random feature maps. We show that orthogonal estimators outperform state-of-the-art mechanisms that use iid sampling under weak conditions for tails of the associated Fourier distributions. We prove that for the case of many dimensions, the superiority of the orthogonal transform can be accurately measured by a property we define called the charm of the kernel, and that orthogonal random features provide optimal (in terms of mean squared error) kernel estimators. We provide the first theoretical results which explain why orthogonal random features outperform unstructured on down-stream tasks such as kernel ridge regression by showing that orthogonal random features provide kernel algorithms with better spectral properties than the previous state-of-the-art. Our results enable practitioners more generally to estimate the benefits from applying orthogonal transforms.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300001,0
C,"Kalantari, R; Ghosh, J; Zhou, MY",,"Storkey, A; PerezCruz, F",,"Kalantari, Rahi; Ghosh, Joydeep; Zhou, Mingyuan",,,Nonparametric Bayesian sparse graph linear dynamical systems,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"A nonparametric Bayesian sparse graph linear dynamical system (SGLDS) is proposed to model sequentially observed multivariate data. SGLDS uses the Bernoulli-Poisson link together with a gamma process to generate an infinite dimensional sparse random graph to model state transitions. Depending on the sparsity pattern of the corresponding row and column of the graph affinity matrix, a latent state of SGLDS can be categorized as either a non-dynamic state or a dynamic one. A normal-gamma construction is used to shrink the energy captured by the non-dynamic states, while the dynamic states can be further categorized into live, absorbing, or noise-injection states, which capture different types of dynamical components of the underlying time series. The state-of-the-art performance of SGLDS is demonstrated with experiments on both synthetic and real data.",,,,,"Zhou, Mingyuan/AAE-8717-2021",,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300204,0
C,"Khanna, R; Kyrillidis, A",,"Storkey, A; PerezCruz, F",,"Khanna, Rajiv; Kyrillidis, Anastasios",,,IHT dies hard: Provable accelerated Iterative Hard Thresholding,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"We study -both in theory and practice- the use of momentum motions in classic iterative hard thresholding (IHT) methods. By simply modifying plain IHT, we investigate its convergence behavior on convex optimization criteria with non-convex constraints, under standard assumptions. In diverse scenaria, we observe that acceleration in IHT leads to significant improvements, compared to state of the art projected gradient descent and Frank-Wolfe variants. As a byproduct of our inspection, we study the impact of selecting the momentum parameter: similar to convex settings, two modes of behavior are observed -rippling and linear- depending on the level of momentum.",,,,,"Khanna, Rajiv/GPK-2566-2022","Khanna, Rajiv/0000-0003-1314-3126",,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300021,0
C,"Laude, E; Wu, T; Cremers, D",,"Storkey, A; PerezCruz, F",,"Laude, Emanuel; Wu, Tao; Cremers, Daniel",,,A Nonconvex Proximal Splitting Algorithm under Moreau-Yosida Regularization,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"We tackle highly nonconvex, nonsmooth composite optimization problems whose objectives comprise a Moreau-Yosida regularized term. Classical nonconvex proximal splitting algorithms, such as nonconvex ADMM, suffer from lack of convergence for such a problem class. To overcome this difficulty, in this work we consider a lifted variant of the Moreau-Yosida regularized model and propose a novel multiblock primal-dual algorithm that intrinsically stabilizes the dual block. We provide a complete convergence analysis of our algorithm and identify respective optimality qualifications under which stationarity of the original model is retrieved at convergence. Numerically, we demonstrate the relevance of Moreau-Yosida regularized models and the efficiency of our algorithm on robust regression as well as joint feature selection and semi-supervised learning.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300052,0
C,"Lazarus, A; Husmeier, D; Papamarkou, T",,"Storkey, A; PerezCruz, F",,"Lazarus, Alan; Husmeier, Dirk; Papamarkou, Theodore",,,Multiphase MCMC Sampling for Parameter Inference in Nonlinear Ordinary Differential Equations,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Traditionally, ODE parameter inference relies on solving the system of ODEs and assessing fit of the estimated signal with the observations. However, nonlinear ODEs often do not permit closed form solutions. Using numerical methods to solve the equations results in prohibitive computational costs, particularly when one adopts a Bayesian approach in sampling parameters from a posterior distribution. With the introduction of gradient matching, we can abandon the need to numerically solve the system of equations. Inherent in these efficient procedures is an introduction of bias to the learning problem as we no longer sample based on the exact likelihood function. This paper presents a multiphase MCMC approach that attempts to close the gap between efficiency and accuracy. By sampling using a surrogate likelihood, we accelerate convergence to the stationary distribution before sampling using the exact likelihood. We demonstrate that this method combines the efficiency of gradient matching and the accuracy of the exact likelihood scheme.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300131,0
C,"Naesseth, CA; Linderman, SW; Ranganath, R; Blei, DM",,"Storkey, A; PerezCruz, F",,"Naesseth, Christian A.; Linderman, Scott W.; Ranganath, Rajesh; Blei, David M.",,,Variational Sequential Monte Carlo,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Many recent advances in large scale probabilistic inference rely on variational methods. The success of variational approaches depends on (i) formulating a flexible parametric family of distributions, and (ii) optimizing the parameters to find the member of this family that most closely approximates the exact posterior. In this paper we present a new approximating family of distributions, the variational sequential Monte Carlo (VSMC) family, and show how to optimize it in variational inference. VSMC melds variational inference (VI) and sequential Monte Carlo (SMC), providing practitioners with flexible, accurate, and powerful Bayesian inference. The VSMC family is a variational family that can approximate the posterior arbitrarily well, while still allowing for efficient optimization of its parameters. We demonstrate its utility on state space models, stochastic volatility models for financial data, and deep Markov models of brain neural circuits.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300102,0
C,"Srinivasan, S; Gordon, G; Boots, B",,"Storkey, A; PerezCruz, F",,"Srinivasan, Siddarth; Gordon, Geoff; Boots, Byron",,,Learning Hidden Quantum Markov Models,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Hidden Quantum Markov Models (HQMMs) can be thought of as quantum probabilistic graphical models that can model sequential data. We extend previous work on HQMMs with three contributions: (1) we show how classical hidden Markov models (HMMs) can be simulated on a quantum circuit, (2) we reformulate HQMMs by relaxing the constraints for modeling HMMs on quantum circuits, and (3) we present a learning algorithm to estimate the parameters of an HQMM from data. While our algorithm requires further optimization to handle larger datasets, we are able to evaluate our algorithm using several synthetic datasets generated by valid HQMMs. We show that our algorithm learns HQMMs with the same number of hidden states and predictive accuracy as the HQMMs that generated the data, while HMMs learned with the Baum-Welch algorithm require more states to match the predictive accuracy.",,,,,"s, s/HIK-1178-2022",,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300207,0
C,"Begin, L; Germain, P; Laviolotte, F; Roy, JF",,"Gretton, A; Robert, CC",,"Begin, Luc; Germain, Pascal; Laviolotte, Francois; Roy, Jean-Francis",,,PAC-Bayesian Bounds based on the Renyi Divergence,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"We propose a simplified proof process for PAC-Bayesian generalization bounds, that allows to divide the proof in four successive inequalities, easing the customization of PAC-Bayesian theorems. We also propose a family of PAC-Bayesian bounds based on the Renyi divergence between the prior and posterior distributions, whereas most PAC-Bayesian bounds are based on the Kullback-Leibler divergence. Finally, we present an empirical evaluation of the tightness of each inequality of the simplified proof, for both the classical PAC-Bayesian bounds and those based on the Renyi divergence.",,,,,,"Germain, Pascal/0000-0003-3998-9533",,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,435,444,,,,,,,,,,,,,,,,WOS:000508662100048,0
C,"Guo, RQ; Kumar, S; Choromanski, K; Simcha, D",,"Gretton, A; Robert, CC",,"Guo, Ruiqi; Kumar, Sanjiv; Choromanski, Krzysztof; Simcha, David",,,Quantization based Fast Inner Product Search,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"We propose a quantization based approach for fast approximate Maximum Inner Product Search (MIPS). Each database vector is quantized in multiple subspaces via a set of codebooks, learned directly by minimizing the inner product quantization error. Then, the inner product of a query to a database vector is approximated as the sum of inner products with the subspace quantizers. Different from recently proposed LSH approaches to MIPS, the database vectors and queries do not need to be augmented in a higher dimensional feature space. We also provide a theoretical analysis of the proposed approach, consisting of the concentration results under mild assumptions. Furthermore, if a small set of held-out samples from the query distribution is given at the training time, we propose a modified codebook learning procedure which further improves the accuracy. Experimental results on a variety of datasets including those arising from deep neural networks show that the proposed approach significantly outperforms the existing state-of-the-art.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,482,490,,,,,,,,,,,,,,,,WOS:000508662100053,0
C,"Lee, S; Brzyski, D; Bogdan, M",,"Gretton, A; Robert, CC",,"Lee, Sangkyun; Brzyski, Damian; Bogdan, Malgorzata",,,Fast Saddle-Point Algorithm for Generalized Dantzig Selector and FDR Control with the Ordered l1-Norm,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"In this paper we propose a primal-dual proximal extragradient algorithm to solve the generalized Dantzig selector (GDS) estimation problem, based on a new convex-concave saddle-point (SP) reformulation. Our new formulation makes it possible to adopt recent developments in saddle-point optimization, to achieve the optimal O(1/k) rate of convergence. Compared to the optimal non-SP algorithms, ours do not require specification of sensitive parameters that affect algorithm performance or solution quality. We also provide a new analysis showing a possibility of local acceleration to achieve the rate of O(1/k(2)) in special cases even without strong convexity or strong smoothness. As an application, we propose a GDS equipped with the ordered l(1)-norm, showing its false discovery rate control properties in variable selection. Algorithm performance is compared between ours and other alternatives, including the linearized ADMM, Nesterov's smoothing, Nemirovski's mirror-prox, and the accelerated hybrid proximal extragradient techniques.",,,,,"Lee, Sangkyun/AAE-5272-2019","Lee, Sangkyun/0000-0001-8415-6368",,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,780,789,,,,,,,,,,,,,,,,WOS:000508662100085,0
C,"McDonald, AM; Pontil, M; Stamos, D",,"Gretton, A; Robert, CC",,"McDonald, Andrew M.; Pontil, Massimiliano; Stamos, Dimitris",,,Fitting Spectral Decay with the k-Support Norm,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"The spectral k-support norm enjoys good estimation properties in low rank matrix learning problems, empirically outperforming the trace norm. Its unit ball is the convex hull of rank k matrices with unit Frobenius norm. In this paper we generalize the norm to the spectral (k, p)-support norm, whose additional parameter p can be used to tailor the norm to the decay of the spectrum of the underlying model. We characterize the unit ball and we explicitly compute the norm. We further provide a conditional gradient method to solve regularization problems with the norm, and we derive an efficient algorithm to compute the Euclidean projection on the unit ball in the case p = infinity. In numerical experiments, we show that allowing p to vary significantly improves performance over the spectral k-support norm on various matrix completion benchmarks, and better captures the spectral decay of the underlying model.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,1061,1069,,,,,,,,,,,,,,,,WOS:000508662100115,0
C,"Nitanda, A",,"Gretton, A; Robert, CC",,"Nitanda, Atsushi",,,Accelerated Stochastic Gradient Descent for Minimizing Finite Sums,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,We propose an optimization method for minimizing the finite sums of smooth convex functions. Our method incorporates an accelerated gradient descent (AGD) and a stochastic variance reduction gradient (SVRG) in a mini-batch setting. An important feature of the method is that it can be directly applied to general convex and optimal strongly convex problems that is a weaker condition than strong convexity. We show that our method achieves a better overall complexity for the general convex problems and linear convergence for optimal strongly convex problems. Moreover we prove the fast iteration complexity of our method. Our experiments show the effectiveness of our method.,,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,195,203,,,,,,,,,,,,,,,,WOS:000508662100022,0
C,"Roy, JF; Marchand, M; Laviolette, F",,"Gretton, A; Robert, CC",,"Roy, Jean-Francis; Marchand, Mario; Laviolette, Francois",,,A Column Generation Bound Minimization Approach with PAC-Bayesian Generalization Guarantees,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"The C-bound, introduced in Lacasse et al. [2006], gives a tight upper bound on the risk of the majority vote classifier. Laviolette et al. [2011] designed a learning algorithm named MinCq that outputs a dense distribution on a finite set of base classifiers by minimizing the C-bound, together with a PAC-Bayesian generalization guarantee. In this work, we design a column generation algorithm that we call CqBoost, that optimizes the C-bound and outputs a sparse distribution on a possibly infinite set of voters. We also propose a PAC-Bayesian bound for CqBoost that holds for finite and two cases of continuous sets of base classifiers. Finally, we compare the accuracy and the sparsity of CqBoost with MinCq and other state-of-the-art boosting algorithms.",,,,,,"Marchand, Mario/0000-0002-7078-7393",,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,1241,1249,,,,,,,,,,,,,,,,WOS:000508662100135,0
C,"Sadhanala, V; Wang, YX; Tibshirani, RJ",,"Gretton, A; Robert, CC",,"Sadhanala, Veeranjaneyulu; Wang, Yu-Xiang; Tibshirani, Ryan J.",,,Graph Sparsification Approaches for Laplacian Smoothing,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Given a statistical estimation problem where regularization is performed according to the structure of a large, dense graph G, we consider fitting the statistical estimate using a sparsified surrogate graph (G) over tilde G, which shares the vertices of G but has far fewer edges, and is thus more tractable to work with computationally. We examine three types of sparsification: spectral sparsification, which can be seen as the result of sampling edges from the graph with probabilities proportional to their effective resistances, and two simpler sparsifiers, which sample edges uniformly from the graph, either globally or locally. We provide strong theoretical and experimental results, demonstrating that sparsification before estimation can give statistically sensible solutions, with significant computational savings.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,1250,1259,,,,,,,,,,,,,,,,WOS:000508662100136,0
C,"Blondel, M; Kubo, Y; Ueda, N",,"Kaski, S; Corander, J",,"Blondel, Mathieu; Kubo, Yotaro; Ueda, Naonori",,,Online Passive-Aggressive Algorithms for Non-Negative Matrix Factorization and Completion,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"Stochastic Gradient Descent (SGD) is a popular online algorithm for large-scale matrix factorization. However, SGD can often be difficult to use for practitioners, because its performance is very sensitive to the choice of the learning rate parameter. In this paper, we present non-negative passive-aggressive (NN-PA), a family of online algorithms for non-negative matrix factorization (NMF). Our algorithms are scalable, easy to implement and do not require the tedious tuning of a learning rate parameter. We demonstrate the effectiveness of our algorithms on three large-scale matrix completion problems and analyze them in the regret bound model.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,96,104,,,,,,,,,,,,,,,,WOS:000508355800011,0
C,"Eto, R; Fujimaki, R; Morinaga, S; Tamano, H",,"Kaski, S; Corander, J",,"Eto, Riki; Fujimaki, Ryohei; Morinaga, Satoshi; Tamano, Hiroshi",,,Fully-Automatic Bayesian Piecewise Sparse Linear Models,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"Piecewise linear models (PLMs) have been widely used in many enterprise machine learning problems, which assign linear experts to individual partitions on feature spaces and express whole models as patches of local experts. This paper addresses simultaneous model selection issues of PLMs; partition structure determination and feature selection of individual experts. Our contributions are mainly three-fold. First, we extend factorized asymptotic Bayesian (FAB) inference for hierarchical mixtures of experts (probabilistic PLMs). FAB inference offers penalty terms w.r.t. partition and expert complexities, and enable us to resolve the model selection issue. Second, we propose posterior optimization which significantly improves predictive accuracy. Roughly speaking, our new posterior optimization mitigates accuracy degradation due to a gap between marginal log-likelihood maximization and predictive accuracy. Third, we present an application of energy demand forecasting as well as benchmark comparisons. The experiments show our capability of acquiring compact and highly-accurate models.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,238,246,,,,,,,,,,,,,,,,WOS:000508355800027,0
C,"Talukdar, PP; Cohen, W",,"Kaski, S; Corander, J",,"Talukdar, Partha Pratim; Cohen, William",,,Scaling Graph-based Semi Supervised Learning to Large Number of Labels Using Count-Min Sketch,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"Graph-based Semi-supervised learning (SSL) algorithms have been successfully used in a large number of applications. These methods classify initially unlabeled nodes by propagating label information over the structure of graph starting from seed nodes. Graph-based SSL algorithms usually scale linearly with the number of distinct labels (m), and require O(m) space on each node. Unfortunately, there exist many applications of practical significance with very large m over large graphs, demanding better space and time complexity. In this paper, we propose MAD-SKETCH, a novel graph-based SSL algorithm which compactly stores label distribution on each node using Count-min Sketch, a randomized data structure. We present theoretical analysis showing that under mild conditions, MAD-SKETCH can reduce space complexity at each node from O(m) to O(log m), and achieve similar savings in time complexity as well. We support our analysis through experiments on multiple real world datasets. We observe that MAD-SKETCH achieves similar performance as existing state-of-the-art graph-based SSL algorithms, while requiring smaller memory footprint and at the same time achieving up to 10x speedup. We find that MAD-SKETCH is able to scale to datasets with one million labels, which is beyond the scope of existing graph-based SSL algorithms.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,940,947,,,,,,,,,,,,,,,,WOS:000508355800104,0
C,"Ajanthan, T; Gupta, K; Torr, PHS; Hartley, R; Dokania, PK",,"Banerjee, A; Fukumizu, K",,"Ajanthan, Thalaiyasingam; Gupta, Kartik; Torr, Philip H. S.; Hartley, Richard; Dokania, Puneet K.",,,Mirror Descent View for Neural Network Quantization,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Quantizing large Neural Networks (NN) while maintaining the performance is highly desirable for resource-limited devices due to reduced memory and time complexity. It is usually formulated as a constrained optimization problem and optimized via a modified version of gradient descent. In this work, by interpreting the continuous parameters (unconstrained) as the dual of the quantized ones, we introduce a Mirror Descent (MD) framework (Bubeck (2015)) for NN quantization. Specifically, we provide conditions on the projections (i.e., mapping from continuous to quantized ones) which would enable us to derive valid mirror maps and in turn the respective MD updates. Furthermore, we present a numerically stable implementation of MD that requires storing an additional set of auxiliary variables (unconstrained), and show that it is strikingly analogous to the Straight Through Estimator (STE) based method which is typically viewed as a trick to avoid vanishing gradients issue. Our experiments on CIFAR-10/100, TinyImageNet, and ImageNet classification datasets with VGG-16, ResNet-18, and MobileNetV2 architectures show that our MD variants yield state-of-the-art performance.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803039,0
C,"Arora, R; Upadhyay, J; Upadhyay, S",,"Banerjee, A; Fukumizu, K",,"Arora, Raman; Upadhyay, Jalaj; Upadhyay, Sarvagya",,,Di.erentially Private Analysis on Graph Streams,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"In this paper, we focus on answering queries, in a differentially private manner, on graph streams. We adopt the sliding window model of privacy, where we wish to perform analysis on the last W updates and ensure that privacy is preserved for the entire stream. We show that in this model, the price of ensuring differential privacy is minimal. Furthermore, since differential privacy is preserved under post-processing, our results can be used as a subroutine in many tasks, most notably solving cut functions and spectral clustering.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801044,0
C,"Basu, S; Papadigenopoulos, O; Caramanis, C; Shakkottai, S",,"Banerjee, A; Fukumizu, K",,"Basu, Soumya; Papadigenopoulos, Orestis; Caramanis, Constantine; Shakkottai, Sanjay",,,Contextual Blocking Bandits,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We study a novel variant of the multi-armed bandit problem, where at each time step, the player observes an independently sampled context that determines the arms' mean rewards. However, playing an arm blocks it (across all contexts) for a fixed number of future time steps. The above contextual setting captures important scenarios such as recommendation systems or ad placement with diverse users. This problem has been recently studied [Dickerson et al., 2018] in the full-information setting (i.e., assuming knowledge of the mean context-dependent arm rewards), where competitive ratio bounds have been derived. We focus on the bandit setting, where these means are initially unknown; we propose a UCB-based variant of the full-information algorithm that guarantees a O(log T)-regret w.r.t. an.-optimal strategy in T time steps, matching the.(log(T)) regret lower bound in this setting. Due to the time correlations caused by blocking, existing techniques for upper bounding regret fail. For proving our regret bounds, we introduce the novel concepts of delayed exploitation and opportunistic subsampling and combine them with ideas from combinatorial bandits and non-stationary Markov chains coupling.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,271,+,,,,,,,,,,,,,,,,WOS:000659893800031,0
C,"Chen, HX; Huang, ZY; Lam, H; Qian, HJ; Zhang, HF",,"Banerjee, A; Fukumizu, K",,"Chen, Haoxian; Huang, Ziyi; Lam, Henry; Qian, Huajie; Zhang, Haofeng",,,Learning Prediction Intervals for Regression: Generalization and Calibration,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We study the generation of prediction intervals in regression for uncertainty quantification. This task can be formalized as an empirical constrained optimization problem that minimizes the average interval width while maintaining the coverage accuracy across data. We strengthen the existing literature by studying two aspects of this empirical optimization. First is a general learning theory to characterize the optimality-feasibility tradeo. that encompasses Lipschitz continuity and VC-subgraph classes, which are exemplified in regression trees and neural networks. Second is a calibration machinery and the corresponding statistical theory to optimally select the regularization parameter that manages this tradeo., which bypasses the overfitting issues in previous approaches in coverage attainment. We empirically demonstrate the strengths of our interval generation and calibration algorithms in terms of testing performances compared to existing benchmarks.",,,,,"Qian, Huajie/ABC-7242-2021",,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801005,0
C,"Chuu, E; Pati, D; Bhattacharya, A",,"Banerjee, A; Fukumizu, K",,"Chuu, Eric; Pati, Debdeep; Bhattacharya, Anirban",,,A Hybrid Approximation to the Marginal Likelihood,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Computing the marginal likelihood or evidence is one of the core challenges in Bayesian analysis. While there are many established methods for estimating this quantity, they predominantly rely on using a large number of posterior samples obtained from a Markov Chain Monte Carlo (MCMC) algorithm. As the dimension of the parameter space increases, however, many of these methods become prohibitively slow and potentially inaccurate. In this paper, we propose a novel method in which we use the MCMC samples to learn a high probability partition of the parameter space and then form a deterministic approximation over each of these partition sets. This two-step procedure, which constitutes both a probabilistic and a deterministic component, is termed a Hybrid approximation to the marginal likelihood. We demonstrate its versatility in a plethora of examples with varying dimension and sample size, and we also highlight the Hybrid approximation's effectiveness in situations where there is either a limited number or only approximate MCMC samples available.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803084,0
C,"Fujisawa, M; Teshima, T; Sato, I; Sugiyama, M",,"Banerjee, A; Fukumizu, K",,"Fujisawa, Masahiro; Teshima, Takeshi; Sato, Issei; Sugiyama, Masashi",,,gamma-ABC: Outlier-Robust Approximate Bayesian Computation Based on a Robust Divergence Estimator,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Approximate Bayesian computation (ABC) is a likelihood-free inference method that has been employed in various applications. However, ABC can be sensitive to outliers if a data discrepancy measure is chosen inappropriately. In this paper, we propose to use a nearest-neighbor-based gamma-divergence estimator as a data discrepancy measure. We show that our estimator possesses a suitable theoretical robustness property called the redescending property. In addition, our estimator enjoys various desirable properties such as high flexibility, asymptotic unbiasedness, almost sure convergence, and linear-time computational complexity. Through experiments, we demonstrate that our method achieves significantly higher robustness than existing discrepancy measures.",,,,,"Sugiyama, Masashi/AEO-1176-2022","Sugiyama, Masashi/0000-0001-6658-6743",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802018,0
C,"Hayou, S; Clerico, E; He, B; Deligiannidis, G; Doucet, A; Rousseau, J",,"Banerjee, A; Fukumizu, K",,"Hayou, Soufiane; Clerico, Eugenio; He, Bobby; Deligiannidis, George; Doucet, Arnaud; Rousseau, Judith",,,Stable ResNet,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Deep ResNet architectures have achieved state of the art performance on many tasks. While they solve the problem of gradient vanishing, they might suffer from gradient exploding as the depth becomes large. Moreover, recent results have shown that ResNet might lose expressivity as the depth goes to infinity [Yang and Schoenholz, 2017, Hayou et al., 2019a]. To resolve these issues, we introduce a new class of ResNet architectures, called Stable ResNet, that have the property of stabilizing the gradient while ensuring expressivity in the infinite depth limit.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801061,0
C,"Ho, D; Le, HM; Doyle, J; Yue, YS",,"Banerjee, A; Fukumizu, K",,"Ho, Dimitar; Le, Hoang M.; Doyle, John; Yue, Yisong",,,Online Robust Control of Nonlinear Systems with Large Uncertainty,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Robust control is a core approach for controlling systems with performance guarantees that are robust to modeling error, and is widely used in real-world systems. However, current robust control approaches can only handle small system uncertainty, and thus require significant effort in system identification prior to controller design. We present an online approach that robustly controls a nonlinear system under large model uncertainty. Our approach is based on decomposing the problem into two sub-problems, robust control design (which assumes small model uncertainty) and chasing consistent models, which can be solved using existing tools from control theory and online learning, respectively. We provide a learning convergence analysis that yields a finite mistake bound on the number of times performance requirements are not met and can provide strong safety guarantees, by bounding the worst-case state deviation. To the best of our knowledge, this is the first approach for online robust control of nonlinear systems with such learning theoretic and safety guarantees. We also show how to instantiate this framework for general robotic systems, demonstrating the practicality of our approach.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804021,0
C,"Kunstner, F; Kumar, R; Schmidt, M",,"Banerjee, A; Fukumizu, K",,"Kunstner, Frederik; Kumar, Raunak; Schmidt, Mark",,,Homeomorphic-Invariance of EM: Non-Asymptotic Convergence in KL Divergence for Exponential Families via Mirror Descent,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Expectation maximization (EM) is the default algorithm for fitting probabilistic models with missing or latent variables, yet we lack a full understanding of its non-asymptotic convergence properties. Previous works show results along the lines of EM converges at least as fast as gradient descent by assuming the conditions for the convergence of gradient descent apply to EM. This approach is not only loose, in that it does not capture that EM can make more progress than a gradient step, but the assumptions fail to hold for textbook examples of EM like Gaussian mixtures. In this work we first show that for the common setting of exponential family distributions, viewing EM as a mirror descent algorithm leads to convergence rates in Kullback-Leibler (KL) divergence. Then, we show how the KL divergence is related to first-order stationarity via Bregman divergences. In contrast to previous works, the analysis is invariant to the choice of parametrization and holds with minimal assumptions. We also show applications of these ideas to local linear (and superlinear) convergence rates, generalized EM, and non-exponential family distributions.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804001,0
C,"Lange-Hegermann, M",,"Banerjee, A; Fukumizu, K",,"Lange-Hegermann, Markus",,,Linearly Constrained Gaussian Processes with Boundary Conditions,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"One goal in Bayesian machine learning is to encode prior knowledge into prior distributions, to model data efficiently. We consider prior knowledge from systems of linear partial differential equations together with their boundary conditions. We construct multi-output Gaussian process priors with realizations in the solution set of such systems, in particular only such solutions can be represented by Gaussian process regression. The construction is fully algorithmic via Grobner bases and it does not employ any approximation. It builds these priors combining two parametrizations via a pullback: the first parametrizes the solutions for the system of differential equations and the second parametrizes all functions adhering to the boundary conditions.",,,,,,"Lange-Hegermann, Markus/0000-0002-5327-4529",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801035,0
C,"Lee, K; Rengarajan, D; Kalathil, D; Shakkottai, S",,"Banerjee, A; Fukumizu, K",,"Lee, Kiyeob; Rengarajan, Desik; Kalathil, Dileep; Shakkottai, Srinivas",,,Reinforcement Learning for Mean Field Games with Strategic Complementarities,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Mean Field Games (MFG) are the class of games with a very large number of agents and the standard equilibrium concept is a Mean Field Equilibrium (MFE). Algorithms for learning MFE in dynamic MFGs are unknown in general. Our focus is on an important subclass that possess a monotonicity property called Strategic Complementarities (MFG-SC). We introduce a natural refinement to the equilibrium concept that we call Trembling-Hand-Perfect MFE (T-MFE), which allows agents to employ a measure of randomization while accounting for the impact of such randomization on their payoffs. We propose a simple algorithm for computing T-MFE under a known model. We also introduce a model-free and a model-based approach to learning T-MFE and provide sample complexities of both algorithms. We also develop a fully online learning scheme that obviates the need for a simulator. Finally, we empirically evaluate the performance of the proposed algorithms via examples motivated by real-world applications.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802093,0
C,"Liang, F; Arora, N; Tehrani, N; Li, Y; Tingley, M; Meijer, E",,"Banerjee, A; Fukumizu, K",,"Liang, Feynman; Arora, Nimar; Tehrani, Nazanin; Li, Yucen; Tingley, Michael; Meijer, Erik",,,Accelerating Metropolis-Hastings with Lightweight Inference Compilation,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"In order to construct accurate proposers for Metropolis-Hastings Markov Chain Monte Carlo, we integrate ideas from probabilistic graphical models and neural networks in a framework we call Lightweight Inference Compilation (LIC). LIC implements amortized inference within an open-universe declarative probabilistic programming language (PPL). Graph neural networks are used to parameterize proposal distributions as functions of Markov blankets, which during compilation are optimized to approximate single-site Gibbs sampling distributions. Unlike prior work in inference compilation (IC), LIC forgoes importance sampling of linear execution traces in favor of operating directly on Bayesian networks. Through using a declarative PPL, the Markov blankets of nodes (which may be non-static) are queried at inference-time to produce proposers. Experimental results show LIC can produce proposers which have less parameters, greater robustness to nuisance random variables, and improved posterior sampling in a Bayesian logistic regression and n-schools inference application.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,181,+,,,,,,,,,,,,,,,,WOS:000659893800021,0
C,"Liu, BB; Ravikumar, P; Risteski, A",,"Banerjee, A; Fukumizu, K",,"Liu, Bingbin; Ravikumar, Pradeep; Risteski, Andrej",,,Contrastive learning of strong-mixing continuous-time stochastic processes,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Contrastive learning is a family of self-supervised methods where a model is trained to solve a classification task constructed from unlabeled data. It has recently emerged as one of the leading learning paradigms in the absence of labels across many different domains (e.g. brain imaging, text, images). However, theoretical understanding of many aspects of training, both statistical and algorithmic, remain fairly elusive. In this work, we study the setting of time series|more precisely, when we get data from a strong-mixing continuous-time stochastic process. We show that a properly constructed contrastive learning task can be used to estimate the transition kernel for small-to-mid-range intervals in the diffusion case. Moreover, we give sample complexity bounds for solving this task and quantitatively characterize what the value of the contrastive loss implies for distributional closeness of the learned kernel. As a byproduct, we illuminate the appropriate settings for the contrastive distribution, as well as other hyper-parameters in this setup.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803077,0
C,"Mazzetto, A; Sam, D; Park, A; Upfal, E; Bach, SH",,"Banerjee, A; Fukumizu, K",,"Mazzetto, Alessio; Sam, Dylan; Park, Andrew; Upfal, Eli; Bach, Stephen H.",,,Semi-Supervised Aggregation of Dependent Weak Supervision Sources With Performance Guarantees,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We develop a novel method that provides theoretical guarantees for learning from weak labelers without the (mostly unrealistic) assumption that the errors of the weak labelers are independent or come from a particular family of distributions. We show a rigorous technique for efficiently selecting small subsets of the labelers so that a majority vote from such subsets has a provably low error rate. We explore several extensions of this method and provide experimental results over a range of labeled data set sizes on 45 image classification tasks. Our performance-guaranteed methods consistently match the best performing alternative, which varies based on problem difficulty. On tasks with accurate weak labelers, our methods are on average 3 percentage points more accurate than the state-of-the-art adversarial method. On tasks with inaccurate weak labelers, our methods are on average 15 percentage points more accurate than the semi-supervised Dawid-Skene model (which assumes independence).",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803082,0
C,"Obukhov, A; Rakhuba, M; Liniger, A; Huang, ZW; Georgoulis, S; Dai, DX; Van Gool, L",,"Banerjee, A; Fukumizu, K",,"Obukhov, Anton; Rakhuba, Maxim; Liniger, Alexander; Huang, Zhiwu; Georgoulis, Stamatios; Dai, Dengxin; Van Gool, Luc",,,Spectral Tensor Train Parameterization of Deep Learning Layers,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We study low-rank parameterizations of weight matrices with embedded spectral properties in the Deep Learning context. The low-rank property leads to parameter efficiency and permits taking computational shortcuts when computing mappings. Spectral properties are often subject to constraints in optimization problems, leading to better models and stability of optimization. We start by looking at the compact SVD parameterization of weight matrices and identifying redundancy sources in the parameterization. We further apply the Tensor Train (TT) decomposition to the compact SVD components, and propose a non-redundant differentiable parameterization of fixed TT-rank tensor manifolds, termed the Spectral Tensor Train Parameterization (STTP). We demonstrate the effects of neural network compression in the image classification setting and both compression and improved training stability in the generative adversarial training setting.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804029,0
C,"Teymur, O; Gorham, J; Riabiz, M; Oates, CJ",,"Banerjee, A; Fukumizu, K",,"Teymur, Onur; Gorham, Jackson; Riabiz, Marina; Oates, Chris J.",,,Optimal Quantisation of Probability Measures Using Maximum Mean Discrepancy,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Several researchers have proposed minimisation of maximum mean discrepancy (MMD) as a method to quantise probability measures, i.e., to approximate a distribution by a representative point set. We consider sequential algorithms that greedily minimise MMD over a discrete candidate set. We propose a novel non-myopic algorithm and, in order to both improve statistical efficiency and reduce computational cost, we investigate a variant that applies this technique to a mini-batch of the candidate set at each iteration. When the candidate points are sampled from the target, the consistency of these new algorithms-and their mini-batch variants-is established. We demonstrate the algorithms on a range of important computational problems, including optimisation of nodes in Bayesian cubature and the thinning of Markov chain output.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801028,0
C,"Voloshin, C; Jiang, N; Yue, YS",,"Banerjee, A; Fukumizu, K",,"Voloshin, Cameron; Jiang, Nan; Yue, Yisong",,,Minimax Model Learning,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We present a novel off-policy loss function for learning a transition model in model-based reinforcement learning. Notably, our loss is derived from the off-policy policy evaluation objective with an emphasis on correcting distribution shift. Compared to previous model-based techniques, our approach allows for greater robustness under model mis-specification or distribution shift induced by learning/evaluating policies that are distinct from the data-generating policy. We provide a theoretical analysis and show empirical improvements over existing model-based off-policy evaluation methods. We provide further analysis showing our loss can be used for off-policy optimization (OPO) and demonstrate its integration with more recent improvements in OPO.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801093,0
C,"Ahmadian, S; Epasto, A; Kumar, R; Mahdian, M",,"Chiappa, S; Calandra, R",,"Ahmadian, Sara; Epasto, Alessandro; Kumar, Ravi; Mahdian, Mohammad",,,Fair Correlation Clustering,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"In this paper, we study correlation clustering under fairness constraints. Fair variants of k-median and k-center clustering have been studied recently, and approximation algorithms using a notion called fairlet decomposition have been proposed. We obtain approximation algorithms for fair correlation clustering under several important types of fairness constraints. Our results hinge on obtaining a fairlet decomposition for correlation clustering by introducing a novel combinatorial optimization problem. We define a fairlet decomposition with cost similar to the k-median cost and this allows us to obtain approximation algorithms for a wide range of fairness constraints. We complement our theoretical results with an in-depth analysis of our algorithms on real graphs where we show that fair solutions to correlation clustering can be obtained with limited increase in cost compared to the state-of-the-art (unfair) algorithms.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,4195,4204,,,,,,,,,,,,,,,,WOS:000559931300008,0
C,"Bruel-Gabrielsson, R; Nelson, BJ; Dwaraknath, A; Skraba, P; Guibas, LJ; Carlsson, G",,"Chiappa, S; Calandra, R",,"Bruel-Gabrielsson, Rickard; Nelson, Bradley J.; Dwaraknath, Anjan; Skraba, Primoz; Guibas, Leonidas J.; Carlsson, Gunnar",,,A Topology Layer for Machine Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Topology applied to real world data using persistent homology has started to find applications within machine learning, including deep learning. We present a differentiable topology layer that computes persistent homology based on level set filtrations and edge-based filtrations. We present three novel applications: the topological layer can (i) regularize data reconstruction or the weights of machine learning models, (ii) construct a loss on the output of a deep generative network to incorporate topological priors, and (iii) perform topological adversarial attacks on deep networks trained with persistence features. The code(1) is publicly available and we hope its availability will facilitate the use of persistent homology in deep learning and other gradient based applications.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1553,1562,,,,,,,,,,,,,,,,WOS:000559931301010,0
C,"Chawla, R; Sankararaman, A; Ganesh, A; Shakkottai, S",,"Chiappa, S; Calandra, R",,"Chawla, Ronshee; Sankararaman, Abishek; Ganesh, Ayalvadi; Shakkottai, Sanjay",,,The Gossiping Insert-Eliminate Algorithm for Multi-Agent Bandits,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We consider a decentralized multi-agent Multi Armed Bandit (MAB) setup consisting of N agents, solving the same MAB instance to minimize individual cumulative regret. In our model, agents collaborate by exchanging messages through pairwise gossip style communications. We develop two novel algorithms, where each agent only plays from a subset of all the arms. Agents use the communication medium to recommend only arm-IDs (not samples), and thus update the set of arms from which they play. We establish that, if agents communicate Omega (log(T)) times through any connected pairwise gossip mechanism, then every agent's regret is a factor of order N smaller compared to the case of no collaborations. Furthermore, we show that the communication constraints only have a second order effect on the regret of our algorithm. We then analyze this second order term of the regret to derive bounds on the regret-communication tradeoffs. Finally, we empirically evaluate our algorithm and conclude that the insights are fundamental and not artifacts of our bounds. We also show a lower bound which gives that the regret scaling obtained by our algorithm cannot be improved even in the absence of any communication constraints. Our results demonstrate that even a minimal level of collaboration among agents greatly reduces regret for all agents.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3471,3480,,,,,,,,,,,,,,,,WOS:000559931300053,0
C,"Chen, MS; Li, XG; Zhao, T",,"Chiappa, S; Calandra, R",,"Chen, Minshuo; Li, Xingguo; Zhao, Tuo",,,On Generalization Bounds of a Family of Recurrent Neural Networks,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Recurrent Neural Networks (RNNs) have been widely applied to sequential data analysis. Due to their complicated modeling structures, however, the theory behind is still largely missing. To connect theory and practice, we study the generalization properties of vanilla RNNs as well as their variants, including Minimal Gated Unit (MGU), Long Short Term Memory (LSTM), and Convolutional (Cony) RNNs. Specifically, our theory is established under the PAC-Learning framework. The generalization bound is presented in terms of the spectral norms of the weight matrices and the total number of parameters. We also establish refined generalization bounds with additional norm assumptions, and draw a comparison among these bounds. We remark: (1) Our generalization bound for vanilla RNNs is significantly tighter than the best of existing results; (2) We are not aware of any other generalization bounds for MGU and LSTM RNNs in the exiting literature; (3) We demonstrate the advantages of these variants in generalization.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1233,1242,,,,,,,,,,,,,,,,WOS:000559931300057,0
C,"Ghosh, A; Ramchandran, K",,"Chiappa, S; Calandra, R",,"Ghosh, Avishek; Ramchandran, Kannan",,,Alternating Minimization Converges Super-Linearly for Mixed Linear Regression,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We address the problem of solving mixed random linear equations. We have unlabeled observations coming from multiple linear regressions, and each observation corresponds to exactly one of the regression models. The goal is to learn the linear regressors from the observations. Classically, Alternating Minimization (AM) (which is a variant of Expectation Maximization (EM)) is used to solve this problem. AM iteratively alternates between the estimation of labels and solving the regression problems with the estimated labels. Empirically, it is observed that, for a large variety of non-convex problems including mixed linear regression, AM converges at a much faster rate compared to gradient based algorithms. However, the existing theory suggests similar rate of convergence for AM and gradient based methods, failing to capture this empirical behavior. In this paper, we close this gap between theory and practice for the special case of a mixture of 2 linear regressions. We show that, provided initialized properly, AM enjoys a super-linear rate of convergence in certain parameter regimes. To the best of our knowledge, this is the first work that theoretically establishes such rate for AM. Hence, if we want to recover the unknown regressors upto an error (in l(2) norm) of epsilon, AM only takes O(log log(1/epsilon)) iterations. Furthermore, we compare AM with a gradient based heuristic algorithm empirically and show that AM dominates in iteration complexity as well as wall-clock time.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1093,1102,,,,,,,,,,,,,,,,WOS:000559931301020,0
C,"Gorbunov, E; Hanzely, F; Richtarik, P",,"Chiappa, S; Calandra, R",,"Gorbunov, Eduard; Hanzely, Filip; Richtarik, Peter",,,"A Unified Theory of SGD: Variance Reduction, Sampling, Quantization and Coordinate Descent","INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"In this paper we introduce a unified analysis of a large family of variants of proximal stochastic gradient descent (SGD) which so far have required different intuitions, convergence analyses, have different applications, and which have been developed separately in various communities. We show that our framework includes methods with and without the following tricks, and their combinations: variance reduction, importance sampling, mini-batch sampling, quantization, and coordinate sub-sampling. As a by-product, we obtain the first unified theory of SGD and randomized coordinate descent (RCD) methods, the first unified theory of variance reduced and non-variance-reduced SGD methods, and the first unified theory of quantized and non-quantized methods. A key to our approach is a parametric assumption on the iterates and stochastic gradients. In a single theorem we establish a linear convergence result under this assumption and strong-quasi convexity of the loss function. Whenever we recover an existing method as a special case, our theorem gives the best known complexity result. Our approach can be used to motivate the development of new useful methods, and offers pre-proved convergence guarantees. To illustrate the strength of our approach, we develop five new variants of SGD, and through numerical experiments demonstrate some of their properties.",,,,,"Gorbunov, Eduard/U-1740-2019","Gorbunov, Eduard/0000-0002-3370-4130",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,680,689,,,,,,,,,,,,,,,,WOS:000559931301024,0
C,"Gu, F; Askari, A; El Ghaoui, L",,"Chiappa, S; Calandra, R",,"Gu, Fangda; Askari, Armin; El Ghaoui, Laurent",,,Fenchel Lifted Networks: A Lagrange Relaxation of Neural Network Training,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Despite the recent successes of deep neural networks, the corresponding training problem remains highly non-convex and difficult to optimize. Classes of models have been proposed that introduce greater structure to the objective function at the cost of lifting the dimension of the problem. However, these lifted methods sometimes perform poorly compared to traditional neural networks. In this paper, we introduce a new class of lifted models, Fenchel lifted networks, that enjoy the same benefits as previous lifted models, without suffering a degradation in performance over classical networks. Our model represents activation functions as equivalent biconvex constraints and uses Lagrange Multipliers to arrive at a rigorous lower bound of the traditional neural network training problem. This model is efficiently trained using block-coordinate descent and is parallelizable across data points and/or layers. We compare our model against standard fully connected and convolutional networks and show that we are able to match or beat their performance.(1)",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3362,3370,,,,,,,,,,,,,,,,WOS:000559931301027,0
C,"Kisamori, K; Kanagawa, M; Yamazaki, K",,"Chiappa, S; Calandra, R",,"Kisamori, Keiichi; Kanagawa, Motonobu; Yamazaki, Keisuke",,,Simulator Calibration under Covariate Shift with Kernels,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We propose a novel calibration method for computer simulators, dealing with the problem of covariate shift. Covariate shift is the situation where input distributions for training and test are different, and ubiquitous in applications of simulations. Our approach is based on Bayesian inference with kernel mean embedding of distributions, and on the use of an importance-weighted reproducing kernel for covariate shift adaptation. We provide a theoretical analysis for the proposed method, including a novel theoretical result for conditional mean embedding, as well as empirical investigations suggesting its effectiveness in practice. The experiments include calibration of a widely used simulator for industrial manufacturing processes, where we also demonstrate how the proposed method may be useful for sensitivity analysis of model parameters.",,,,,,"Kanagawa, Motonobu/0000-0002-3948-8053",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1244,1252,,,,,,,,,,,,,,,,WOS:000559931301079,0
C,"Li, JL; Sun, YC; Su, JH; Suzuki, T; Huang, FR",,"Chiappa, S; Calandra, R",,"Li, Jingling; Sun, Yanchao; Su, Jiahao; Suzuki, Taiji; Huang, Furong",,,Understanding Generalization in Deep Learning via Tensor Methods,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Deep neural networks generalize well on unseen data though the number of parameters often far exceeds the number of training examples. Recently proposed complexity measures have provided insights to understanding the generalizability in neural networks from perspectives of PAC-Bayes, robustness, overparametrization, compression and so on. In this work, we advance the understanding of the relations between the network's architecture and its generalizability from the compression perspective. Using tensor analysis, we propose a series of intuitive, datadependent and easily-measurable properties that tightly characterize the compressibility and generalizability of neural networks; thus, in practice, our generalization bound outperforms the previous compression-based ones, especially for neural networks using tensors as their weight kernels (e.g. CNNs). Moreover, these intuitive measurements provide further insights into designing neural network architectures with properties favorable for better/guaranteed generalizability. Our experimental results demonstrate that through the proposed measurable properties, our generalization error bound matches the trend of the test error well. Our theoretical analysis further provides justifications for the empirical success and limitations of some widely-used tensor-based compression approaches. We also discover the improvements to the compressibility and robustness of current neural networks when incorporating tensor operations via our proposed layer-wise structure.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,504,514,,,,,,,,,,,,,,,,WOS:000559931302003,0
C,"Lim, JN; Yamada, M; Jitkrittum, W; Terada, Y; Matsui, S; Shimodaira, H",,"Chiappa, S; Calandra, R",,"Lim, Jen Ning; Yamada, Makoto; Jitkrittum, Wittawat; Terada, Yoshikazu; Matsui, Shigeyuki; Shimodaira, Hidetoshi",,,More Powerful Selective Kernel Tests for Feature Selection,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Refining one's hypotheses in the light of data is a common scientific practice; however, the dependency on the data introduces selection bias and can lead to specious statistical analysis. An approach for addressing this is via conditioning on the selection procedure to account for how we have used the data to generate our hypotheses, and prevent information to be used again after selection. Many selective inference (a.k.a. post-selection inference) algorithms typically take this approach but will over-condition for sake of tractability. While this practice yields well calibrated statistic tests with controlled false positive rates (FPR), it can incur a major loss in power. In our work, we extend two recent proposals for selecting features using the Maximum Mean Discrepancy and Hilbert Schmidt Independence Criterion to condition on the minimal conditioning event. We show how recent advances in multiscale bootstrap makes conditioning on the minimal selection event possible and demonstrate our proposal over a range of synthetic and real world experiments. Our results show that our proposed test is indeed more powerful in most scenarios.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,820,829,,,,,,,,,,,,,,,,WOS:000559931302013,0
C,"McAllester, D; Stratos, K",,"Chiappa, S; Calandra, R",,"McAllester, David; Stratos, Karl",,,Formal Limitations on the Measurement of Mutual Information,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Measuring mutual information from finite data is difficult. Recent work has considered variational methods maximizing a lower bound. In this paper, we prove that serious statistical limitations are inherent to any method of measuring mutual information. More specifically, we show that any distribution-free high-confidence lower bound on mutual information estimated from N samples cannot be larger than O(ln N).",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,875,883,,,,,,,,,,,,,,,,WOS:000559931302035,0
C,"Papini, M; Battistello, A; Restelli, M",,"Chiappa, S; Calandra, R",,"Papini, Matteo; Battistello, Andrea; Restelli, Marcello",,,Balancing Learning Speed and Stability in Policy Gradient via Adaptive Exploration,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"In many Reinforcement Learning (RL) applications, the goal is to find an optimal deterministic policy. However, most RL algorithms require the policy to be stochastic in order to avoid instabilities and perform a sufficient amount of exploration. Adjusting the level of stochasticity during the learning process is non-trivial, as it is difficult to assess whether the costs of random exploration will be repaid in the long run, and to contain the risk of instability. We study this problem in the context of policy gradients (PG) with Gaussian policies. Using tools from the safe PG literature, we design a surrogate objective for the policy variance that captures the effects this parameter has on the learning speed and on the quality of the final solution. Furthermore, we provide a way to optimize this objective which guarantees a stable improvement of the original performance measure. We evaluate the proposed methods on simulated continuous control tasks.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1188,1198,,,,,,,,,,,,,,,,WOS:000559931302064,0
C,"Pham, NH; Nguyen, YM; Phan, YT; Nguyen, ZH; van Dijk, Z; Tran-Dinh, Q",,"Chiappa, S; Calandra, R",,"Pham, Nhan H.; Nguyen, Ylam M.; Phan, Ydzung T.; Zphuong Ha Nguyen; van Dijk, Zxmarten; Tran-Dinh, Quoc",,,A Hybrid Stochastic Policy Gradient Algorithm for Reinforcement Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We propose a novel hybrid stochastic policy gradient estimator by combining an unbiased policy gradient estimator, the REINFORCE estimator, with another biased one, an adapted SARAH estimator for policy optimization. The hybrid policy gradient estimator is shown to be biased, but has variance reduced property. Using this estimator, we develop a new Proximal Hybrid Stochastic Policy Gradient Algorithm (ProxHSPGA) to solve a composite policy optimization problem that allows us to handle constraints or regularizers on the policy parameters. We first propose a single-looped algorithm then introduce a more practical restarting variant. We prove that both algorithms can achieve the best-known trajectory complexity O (epsilon(-3)) to attain a first-order stationary point for the composite problem which is better than existing REINFORCE/GPOMDP O (epsilon(-4)) and SVRPG O (epsilon(-10/3)) in the non-composite setting. We evaluate the performance of our algorithm on several well-known examples in reinforcement learning. Numerical results show that our algorithm outperforms two existing methods on these examples. Moreover, the composite settings indeed have some advantages compared to the non-composite ones on certain problems.",,,,,"Pham, Nhan H/AAA-6202-2022","Pham, Nhan H/0000-0002-4490-8649",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,374,384,,,,,,,,,,,,,,,,WOS:000559931302072,0
C,"Nguyen, TT; Gupta, S; Ha, H; Rana, S; Venkatesh, S",,"Chiappa, S; Calandra, R",,"Thanh Tang Nguyen; Gupta, Sunil; Ha, Huong; Rana, Santu; Venkatesh, Svetha",,,Distributionally Robust Bayesian Quadrature Optimization,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Bayesian quadrature optimization (BQO) maximizes the expectation of an expensive black-box integrand taken over a known probability distribution. In this work, we study BQO under distributional uncertainty in which the underlying probability distribution is unknown except for a limited set of its i.i.d. samples. A standard BQO approach maximizes the Monte Carlo estimate of the true expected objective given the fixed sample set. Though Monte Carlo estimate is unbiased, it has high variance given a small set of samples; thus can result in a spurious objective function. We adopt the distributionally robust optimization perspective to this problem by maximizing the expected objective under the most adversarial distribution. In particular, we propose a novel posterior sampling based algorithm, namely distributionally robust BQO (DRBQO) for this purpose. We demonstrate the empirical effectiveness of our proposed framework in synthetic and real-world problems, and characterize its theoretical convergence via Bayesian regret.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1921,1930,,,,,,,,,,,,,,,,WOS:000559931302055,0
C,"Wang, JY; Shah, NB; Ravi, R",,"Chiappa, S; Calandra, R",,"Wang, Jingyan; Shah, Nihar B.; Ravi, R.",,,Stretching the Effectiveness of MLE from Accuracy to Bias for Pairwise Comparisons,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"A number of applications (e.g., AI bot tournaments, sports, peer grading, crowdsourcing) use pairwise comparison data and the Bradley-Terry-Luce (BTL) model to evaluate a given collection of items (e.g., bots, teams, students, search results). Past work has shown that under the BTL model, the widely-used maximum-likelihood estimator (MLE) is minimax-optimal in estimating the item parameters, in terms of the mean squared error. However, another important desideratum for designing estimators is fairness. In this work, we consider one specific type of fairness, which is the notion of bias in statistics. We show that the MLE incurs a suboptimal rate in terms of bias. We then propose a simple modification to the MLE, which stretches the bounding box of the maximum-likelihood optimizer by a small constant factor from the underlying ground truth domain. We show that this simple modification leads to an improved rate in bias, while maintaining minimax-optimality in the mean squared error. In this manner, our proposed class of estimators provably improves fairness in the sense of bias without loss in accuracy.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303059,0
C,"Bellot, A; van der Schaar, M",,"Chaudhuri, K; Sugiyama, M",,"Bellot, Alexis; van der Schaar, Mihaela",,,Boosting Transfer Learning with Survival Data from Heterogeneous Domains,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Survival models derived from health care data are an important support to inform critical screening and therapeutic decisions. Most models however, do not generalize to populations outside the marginal and conditional distribution assumptions for which they were derived. This presents a significant barrier to the deployment of machine learning techniques into wider clinical practice as most medical studies are data scarce, especially for the analysis of time-to-event outcomes. In this work we propose a survival prediction model that is able to improve predictions on a small data domain of interest - such as a local hospital - by leveraging related data from other domains - such as data from other hospitals. We construct an ensemble of weak survival predictors which iteratively adapt the marginal distributions of the source and target data such that similar source patients contribute to the fit and ultimately improve predictions on target patients of interest. This represents the first boosting-based transfer learning algorithm in the survival analysis literature. We demonstrate the performance and utility of our algorithm on synthetic and real healthcare data collected at various locations.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,57,65,,,,,,,,,,,,,,,,WOS:000509687900007,0
C,"Cardoso, AR; Cummings, R",,"Chaudhuri, K; Sugiyama, M",,"Cardoso, Adrian Rivera; Cummings, Rachel",,,Differentially Private Online Submodular Minimization,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"In this paper we develop the first algorithms for online Submodular minimization that preserve differential privacy under full information feedback and bandit feedback. Our first result is in the full information setting, where the algorithm can observe the entire function after making its decision at each time step. We give an algorithm in this setting that is E-differentially private and achieves expected regret O () E over T rounds for a collection of n elements. Our second result is in the bandit setting, where the algorithm can only observe the cost incurred by its chosen set, and does not have access to the entire function. This setting is significantly more challenging due to the limited information. Our algorithm using bandit feedback is differentially private and achieves expected regret O (n3/2T2/3/epsilon).",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901072,0
C,"Chierichetti, F; Kumar, R; Lattanzi, S; Vassilvitskii, S",,"Chaudhuri, K; Sugiyama, M",,"Chierichetti, Flavio; Kumar, Ravi; Lattanzi, Silvio; Vassilvitskii, Sergei",,,"Matroids, Matchings, and Fairness","22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"The need for fairness in machine learning algorithms is increasingly critical. A recent focus has been on developing fair versions of classical algorithms, such as those for bandit learning, regression, and clustering. We extend this line of work to include algorithms for optimization subject to one or multiple matroid constraints. We map out this problem space, showing optimal solutions, approximation algorithms, or hardness results depending on the specific problem flavor. Our algorithms are efficient and empirical experiments demonstrate that fairness is achievable without a large compromise to the overall objective.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902027,0
C,"Dubois, B; Delmas, JF; Obozinski, G",,"Chaudhuri, K; Sugiyama, M",,"Dubois, Benjamin; Delmas, Jean-Francois; Obozinski, Guillaume",,,Fast Algorithms for Sparse Reduced-Rank Regression,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We consider a reformulation of Reduced-Rank Regression (RRR) and Sparse Reduced-Rank Regression (SRRR) as a non-convex non-differentiable function of a single of the two matrices usually introduced to parametrize low-rank matrix learning problems. We study the behavior of proximal gradient algorithms for the minimization of the objective. In particular, based on an analysis of the geometry of the problem, we establish that a proximal Polyak-Lojasiewicz inequality is satisfied in a neighborhood of the set of optima under a condition on the regularization parameter. We consequently derive linear convergence rates for the proximal gradient descent with line search and for related algorithms in a neighborhood of the optima. Our experiments show that our formulation leads to much faster learning algorithms for RRR and especially for SRRR.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902048,0
C,"Kargas, N; Sidiropoulos, ND",,"Chaudhuri, K; Sugiyama, M",,"Kargas, Nikos; Sidiropoulos, Nicholas D.",,,Learning Mixtures of Smooth Product Distributions: Identifiability and Algorithm,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We study the problem of learning a mixture model of non-parametric product distributions. The problem of learning a mixture model is that of finding the component distributions along with the mixing weights using observed samples generated from the mixture. The problem is well-studied in the parametric setting, i.e., when the component distributions are members of a parametric family such as Gaussian distributions. In this work, we focus on multivariate mixtures of non-parametric product distributions and propose a two-stage approach which recovers the component distributions of the mixture under a smoothness condition. Our approach builds upon the identifiability properties of the canonical polyadic (low-rank) decomposition of tensors, in tandem with Fourier and Shannon-Nyquist sampling staples from signal processing. We demonstrate the effectiveness of the approach on synthetic and real datasets.",,,,,"Kargas, Nikos/AAX-7248-2021","Kargas, Nikos/0000-0002-3798-2875",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,388,396,,,,,,,,,,,,,,,,WOS:000509687900041,0
C,"Malinsky, D; Shpitser, I; Richardson, T",,"Chaudhuri, K; Sugiyama, M",,"Malinsky, Daniel; Shpitser, Ilya; Richardson, Thomas",,,A Potential Outcomes Calculus for Identifying Conditional Path-Specific Effects,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"The do-calculus is a well-known deductive system for deriving connections between interventional and observed distributions, and has been proven complete for a number of important identifiability problems in causal inference [1, 8, 18]. Nevertheless, as it is currently defined, the do calculus is inapplicable to causal problems that involve complex nested counterfactuals which cannot be expressed in terms of the do operator. Such problems include analyses of path specific effects and dynamic treatment regimes. In this paper we present the potential outcome calculus (po-calculus), a natural generalization of do-calculus for arbitrary potential outcomes. We thereby provide a bridge between identification approaches which have their origins in artificial intelligence and statistics, respectively. We use po-calculus to give a complete identification algorithm for conditional path-specific effects with applications to problems in mediation analysis and algorithmic fairness.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687903014,0
C,"Muller, MI; Rojas, CR",,"Chaudhuri, K; Sugiyama, M",,"Mueller, Matias, I; Rojas, Cristian R.",,,Gain estimation of linear dynamical systems using Thompson Sampling,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We present the gain estimation problem for linear dynamical systems as a multi-armed bandit. This is particularly a very important engineering problem in control design, where performance guarantees are carted in terms of the largest gain of the frequency response of the system. The dynamical system is unknown and only noisy input-output data is available. In a more general setup, the noise perturbing the data is non-white and the variance at each frequency band is unknown, resulting in a two-dimensional Gaussian bandit model with unknown mean and scaled-identity covariance matrix. This model corresponds to a two-parameter exponential family. Within a bandit framework, the set of means is given by the frequency response of the system and, unlike traditional bandit problems, the goal here is to maximize the probability of choosing the arm drawing samples with the highest norm of its mean. A problem-dependent lower bound for the expected cumulative regret is derived and a matching upper bound is obtained for a Thompson-Sampling algorithm under a uniform prior over the variances and the two-dimensional means.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901060,0
C,"Rockova, V; Saha, E",,"Chaudhuri, K; Sugiyama, M",,"Rockova, Veronika; Saha, Enakshi",,,On Theory for BART,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Ensemble learning is a statistical paradigm built on the premise that many weak learners can perform exceptionally well when deployed collectively. The BART method of Chipman et al. (2010) is a prominent example of Bayesian ensemble learning, where each learner is a tree. Due to its impressive performance, BART has received a lot of attention from practitioners. Despite its wide popularity, however, theoretical studies of BART have begun emerging only very recently. Laying down foundation for the theoretical analysis of Bayesian forests, Rockova and van der Pas (2017) showed optimal posterior concentration under conditionally uniform tree priors. These priors deviate from the actual priors implemented in BART. Here, we study the exact BART prior and propose a simple modification so that it also enjoys optimality properties. To this end, we dive into the branching processes theory. We obtain tail bounds for the distribution of total progeny under heterogeneous Galton-Watson (GW) processes using their connection to random walks. We conclude with a result stating optimal rate of convergence for BART.",,,,,,"Saha, Enakshi/0000-0003-2938-539X",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902091,0
C,"Sakaue, S",,"Chaudhuri, K; Sugiyama, M",,"Sakaue, Shinsaku",,,Greedy and IHT Algorithms for Non-convex Optimization with Monotone Costs of Non-zeros,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Non-convex optimization methods, such as greedy-style algorithms and iterative hard thresholding (IHT), for l(0)-constrained minimization have been extensively studied thanks to their high empirical performances and strong guarantees. However, few works have considered non-convex optimization with general non-zero patterns; this is unfortunate since various non-zero patterns are quite common in practice. In this paper, we consider the case where non-zero patterns are specified by monotone set functions. We first prove an approximation guarantee of a cost-benefit greedy (CBG) algorithm by using the weak submodularity of the problem. We then consider an IHT-style algorithm, whose projection step uses CBG, and prove its convergence guarantee. We also provide many applications and experimental results that confirm the advantages of the algorithms introduced.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,206,215,,,,,,,,,,,,,,,,WOS:000509687900022,0
C,"Shi, HY; Gerlach, M; Diersen, I; Downey, D; Amaral, LAN",,"Chaudhuri, K; Sugiyama, M",,"Shi, Hanyu; Gerlach, Martin; Diersen, Isabel; Downey, Doug; Amaral, Luis A. N.",,,A new evaluation framework for topic modeling algorithms based on synthetic corpora,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Topic models are in widespread use in natural language processing and beyond. Here, we propose a new framework for the evaluation of probabilistic topic modeling algorithms based on synthetic corpora containing an unambiguously defined ground truth topic structure. The major innovation of our approach is the ability to quantify the agreement between the planted and inferred topic structures by comparing the assigned topic labels at the level of the tokens. In experiments, our approach yields novel insights about the relative strengths of topic models as corpus characteristics vary, and the first evidence of an undetectable phase for topic models when the planted structure is weak. We also establish the practical relevance of the insights gained for synthetic corpora by predicting the performance of topic modeling algorithms in classification tasks in real-world corpora.",,,,,"Gerlach, Martin/D-7230-2016; Downey, Douglas/B-7143-2009","Gerlach, Martin/0000-0002-0879-7865; ",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,816,826,,,,,,,,,,,,,,,,WOS:000509687900085,0
C,"Yu, Q; Li, SZ; Raviv, N; Kalan, SMM; Soltanolkotabi, M; Avestimehr, AS",,"Chaudhuri, K; Sugiyama, M",,"Yu, Qian; Li, Songze; Raviv, Netanel; Kalan, Seyed Mohammadreza Mousavi; Soltanolkotabi, Mahdi; Avestimehr, A. Salman",,,"Lagrange Coded Computing: Optimal Design for Resiliency, Security, and Privacy","22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We consider a scenario involving computations over a massive dataset stored distributedly across multiple workers, which is at the core of distributed learning algorithms. We propose Lagrange Coded Computing (LCC), a new framework to simultaneously provide (1) resiliency against stragglers that may prolong computations; (2) security against Byzantine (or malicious) workers that deliberately modify the computation for their benefit; and (3) (information-theoretic) privacy of the dataset amidst possible collusion of workers. LCC, which leverages the well-known Lagrange polynomial to create computation redundancy in a novel coded form across workers, can be applied to any computation scenario in which the function of interest is an arbitrary multivariate polynomial of the input dataset, hence covering many computations of interest in machine learning. LCC significantly generalizes prior works to go beyond linear computations. It also enables secure and private computing in distributed settings, improving the computation and communication efficiency of the state-of-the-art. Furthermore, we prove the optimality of LCC by showing that it achieves the optimal tradeoff between resiliency, security, and privacy, i.e., in terms of tolerating the maximum number of stragglers and adversaries, and providing data privacy against the maximum number of colluding workers. Finally, we show via experiments on Amazon EC2 that LCC speeds up the conventional uncoded implementation of distributed least-squares linear regression by up to 13.43x, and also achieves a 2.36x-12.65x speedup over the state-of-the-art straggler mitigation strategies.",,,,,"Li, Songze/AAU-6876-2021","Li, Songze/0000-0003-4282-3307",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901027,0
C,"Belletti, F; Beutel, A; Jain, S; Chi, EH",,"Storkey, A; PerezCruz, F",,"Belletti, Francois; Beutel, Alex; Jain, Sagar; Chi, Ed H.",,,Factorized Recurrent Neural Architectures for Longer Range Dependence,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"The ability to capture Long Range Dependence (LRD) in a stochastic process is of prime importance in the context of predictive models. A sequential model with a longer-term memory is better able contextualize recent observations. In this article, we apply the theory of LRD stochastic processes to modern recurrent architectures, such as LSTMs and GRUs, and prove they do not provide LRD under assumptions sufficient for gradients to vanish. Motivated by an information-theoretic analysis, we provide a modified recurrent neural architecture that mitigates the issue of faulty memory through redundancy while keeping the compute time constant. Experimental results on a synthetic copy task, the Youtube-8m video classification task and a recommender system show that we enable better memorization and longer-term memory.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300159,0
C,"Onak, K; Sun, XR",,"Storkey, A; PerezCruz, F",,"Onak, Krzysztof; Sun, Xiaorui",,,Probability-Revealing Samples,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"In the most popular distribution testing and parameter estimation model, one can obtain information about an underlying distribution D via independent samples from D. We introduce a model in which every sample comes with the information about the probability of selecting it. In this setting, we give algorithms for problems such as testing if two distributions are (approximately) identical, estimating the total variation distance between distributions, and estimating the support size. The sample complexity of all of our algorithms is optimal up to a constant factor for sufficiently large support size. The running times of our algorithms are near-linear in the number of samples collected. Additionally, our algorithms are robust to small multiplicative errors in probability estimates. The complexity of our model lies strictly between the complexity of the model where only independent samples are provided and the complexity of the model where additionally arbitrary probability queries are allowed. Our model finds applications where once a given element is sampled, it is easier to estimate its probability. We describe two scenarios in which all occurrences of each element are easy to explore once at least one copy of the element is detected.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300211,0
C,"Paquette, C; Lin, HZ; Drusvyatskiy, D; Mairal, J; Harchaoui, Z",,"Storkey, A; PerezCruz, F",,"Paquette, Courtney; Lin, Hongzhou; Drusvyatskiy, Dmitriy; Mairal, Julien; Harchaoui, Zaid",,,Catalyst for Gradient-based Nonconvex Optimization,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"We introduce a generic scheme to solve non-convex optimization problems using gradient-based algorithms originally designed for minimizing convex functions. Even though these methods may originally require convexity to operate, the proposed approach allows one to use them without assuming any knowledge about the convexity of the objective. In general, the scheme is guaranteed to produce a stationary point with a worst-case efficiency typical of first-order methods, and when the objective turns out to be convex, it automatically accelerates in the sense of Nesterov and achieves near-optimal convergence rate in function values. We conclude the paper by showing promising experimental results obtained by applying our approach to incremental algorithms such as SVRG and SAGA for sparse matrix factorization and for learning neural networks.",,,,,"Mairal, Julien/AAL-5611-2021",,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300065,0
C,"Saad, FA; Mansinghka, VK",,"Storkey, A; PerezCruz, F",,"Saad, Feras A.; Mansinghka, Vikash K.",,,"Temporally-Reweighted Chinese Restaurant Process Mixtures for Clustering, Imputing, and Forecasting Multivariate Time Series","INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"This article proposes a Bayesian nonparametric method for forecasting, imputation, and clustering in sparsely observed, multivariate time series data. The method is appropriate for jointly modeling hundreds of time series with widely varying, non-stationary dynamics. Given a collection of N time series, the Bayesian model first partitions them into independent clusters using a Chinese restaurant process prior. Within a cluster, all time series are modeled jointly using a novel temporally-reweighted extension of the Chinese restaurant process mixture. Markov chain Monte Carlo techniques are used to obtain samples from the posterior distribution, which are then used to form predictive inferences. We apply the technique to challenging forecasting and imputation tasks using seasonal flu data from the US Center for Disease Control and Prevention, demonstrating superior forecasting accuracy and competitive imputation accuracy as compared to multiple widely used baselines. We further show that the model discovers interpretable clusters in datasets with hundreds of time series, using macroeconomic data from the Gapminder Foundation.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300080,0
C,"Smith, MT; Alvarez, MA; Zwiessele, M; Lawrence, ND",,"Storkey, A; PerezCruz, F",,"Smith, Michael T.; Alvarez, Mauricio A.; Zwiessele, Max; Lawrence, Neil D.",,,Differentially Private Regression with Gaussian Processes,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"A major challenge for machine learning is increasing the availability of data while respecting the privacy of individuals. Here we combine the provable privacy guarantees of the differential privacy framework with the flexibility of Gaussian processes (GPs). We propose a method using GPs to provide differentially private (DP) regression. We then improve this method by crafting the DP noise covariance structure to efficiently protect the training data, while minimising the scale of the added noise. We find that this cloaking method achieves the greatest accuracy, while still providing privacy guarantees, and offers practical DP for regression over multi-dimensional inputs. Together these methods provide a starter toolkit for combining differential privacy and GPs.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300125,0
C,"Takada, M; Suzuki, T; Fujisawa, H",,"Storkey, A; PerezCruz, F",,"Takada, Masaaki; Suzuki, Taiji; Fujisawa, Hironori",,,Independently Interpretable Lasso: A New Regularizer for Sparse Regression with Uncorrelated Variables,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Sparse regularization such as l(1) regularization is a quite powerful and widely used strategy for high dimensional learning problems. The effectiveness of sparse regularization has been supported practically and theoretically by several studies. However, one of the biggest issues in sparse regularization is that its performance is quite sensitive to correlations between features. Ordinary l(1) regularization can select variables correlated with each other, which results in deterioration of not only its generalization error but also interpretability. In this paper, we propose a new regularization method, Independently Interpretable Lasso (IILasso). Our proposed regularizer suppresses selecting correlated variables, and thus each active variable independently affects the objective variable in the model. Hence, we can interpret regression coefficients intuitively and also improve the performance by avoiding overfitting. We analyze theoretical property of IILasso and show that the proposed method is much advantageous for its sign recovery and achieves almost minimax optimal convergence rate. Synthetic and real data analyses also indicate the effectiveness of IILasso.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300048,0
C,"Tan, X; Rao, V; Neville, J",,"Storkey, A; PerezCruz, F",,"Tan, Xi; Rao, Vinayak; Neville, Jennifer",,,Nested CRP with Hawkes-Gaussian Processes,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"There has been growing interest in learning social structure underlying interaction data, especially when such data consist of both temporal and textual information. In this paper, we propose a novel nonparametric Bayesian model that incorporates senders and receivers of messages into a hierarchical structure that governs the content and reciprocity of communications. We bring the nested Chinese restaurant process from nonparametric Bayesian statistics to Hawkes process models of point pattern data. By modeling senders and receivers in such a hierarchical framework, we are better able to make inferences about the authorship and audience of communications, as well as individual behavior such as favorite collaborators and top-pick words. Empirical results show that our proposed model has improved predictions about event times and clusters. In addition, the latent structure revealed by our model provides a useful qualitative understanding of the data, facilitating interesting exploratory analyses.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300135,0
C,"Chen, ZT; Poupart, P; Geng, YH",,"Gretton, A; Robert, CC",,"Chen, Zhitang; Poupart, Pascal; Geng, Yanhui",,,Online Relative Entropy Policy Search using Reproducing Kernel Hilbert Space Embeddings,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Kernel methods have been successfully applied to reinforcement learning problems to address some challenges such as high dimensional and continuous states, value function approximation and state transition probability modeling. In this paper, we develop an online policy search algorithm based on a recent state-of-the-art algorithm REPS-RKHS that uses conditional kernel embeddings. Our online algorithm inherits the advantages of REPS-RKHS, including the ability to learn non-parametric control policies for infinite horizon continuous MDPs with high-dimensional sensory representations. Different from the original REPS-RKHS algorithm which is based on batch learning, the proposed online algorithm updates the model in an online fashion and thus is able to capture and respond to rapid changes in the system dynamics. In addition, the online update operation takes constant time (i.e., independent of the sample size n), which is much more efficient computationally and allows the policy to be continuously revised. Experiments on different domains are conducted and results show that our online algorithm outperforms the original algorithm.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,573,581,,,,,,,,,,,,,,,,WOS:000508662100063,0
C,"Hauberg, S; Freifeld, O; Larsen, ABL; Fisher, JW; Hansen, LK",,"Gretton, A; Robert, CC",,"Hauberg, Soren; Freifeld, Oren; Larsen, Anders Boesen Lindbo; Fisher, John W., III; Hansen, Lars Kai",,,Dreaming More Data: Class-dependent Distributions over Diffeomorphisms for Learned Data Augmentation,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Data augmentation is a key element in training high-dimensional models. In this approach, one synthesizes new observations by applying pre-specified transformations to the original training data; e.g. new images are formed by rotating old ones. Current augmentation schemes, however, rely on manual specification of the applied transformations, making data augmentation an implicit form of feature engineering. With an eye towards true end-to-end learning, we suggest learning the applied transformations on a per-class basis. Particularly, we align image pairs within each class under the assumption that the spatial transformation between images belongs to a large class of diffeomorphisms. We then learn a class-specific probabilistic generative models of the transformations in a Riemannian submanifold of the Lie group of diffeomorphisms. We demonstrate significant performance improvements in training deep neural nets over manually-specified augmentation schemes. Our code and augmented datasets are available online.",,,,,"Hauberg, Soren/L-2104-2016","Hauberg, Soren/0000-0001-7223-877X",,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,342,350,,,,,,,,,,,,,,,,WOS:000508662100038,0
C,"Li, CL; Kandasamy, K; Poczos, B; Schneider, J",,"Gretton, A; Robert, CC",,"Li, Chun-Liang; Kandasamy, Kirthevasan; Poczos, Barnabas; Schneider, Jeff",,,High Dimensional Bayesian Optimization via Restricted Projection Pursuit Models,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Bayesian Optimization (BO) is commonly used to optimize blackbox objective functions which are expensive to evaluate. A common approach is based on using Gaussian Process (GP) to model the objective function. Applying GP to higher dimensional settings is generally difficult due to the curse of dimensionality for nonparametric regression. Existing works makes strong assumptions such as the function is low-dimensional embedding (Wang et al., 2013) or is axis-aligned additive (Kandasamy et al., 2015). In this paper, we generalize the existing assumption to a projected-additive assumption. Our generalization provides the benefits of i) greatly increasing the space of functions that can be modeled by our approach, which covers the previous works (Wang et al., 2013; Kandasamy et al., 2015) as special cases, and ii) efficiently handling the learning in a larger model space. We prove that the regret for projected-additive functions has only linear dependence on the number of dimensions in this general setting. Directly using projected-additive GP (Gilboa et al., 2013) to BO results in a non-box constraint, which is not easy to optimize. We tackle this problem by proposing a restricted-projection-pursuit GP for BO. We conduct experiments on synthetic examples and scientific and hyper-parameter tuning tasks in many cases. Our method outperforms existing approaches even when the function does not meet the projected additive assumption. Last, we study the validity of the additive and projected-additive assumption in practice.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,884,892,,,,,,,,,,,,,,,,WOS:000508662100096,0
C,"Scarlett, J; Cevher, V",,"Gretton, A; Robert, CC",,"Scarlett, Jonathan; Cevher, Volkan",,,Limits on Sparse Support Recovery via Linear Sketching with Random Expander Matrices,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Linear sketching is a powerful tool for the problem of sparse signal recovery, having numerous applications such as compressive sensing, data stream computing, graph sketching, and routing. Motivated by applications where the positions of the non-zero entries in a sparse vector are of primary interest, we consider the problem of support recovery from a linear sketch taking the form Y = X beta + Z. We focus on a widely-used expander-based construction in the columns of the measurement matrix X is an element of R-nxp are random permutations of a sparse binary vector containing d << n ones and n - d zeros. We provide a sharp characterization of the number of measurements required for an information-theoretically optimal decoder, thus permitting a precise comparison to the i.i.d. Gaussian construction. Our findings reveal both positive and negative results, showing that the performance nearly matches the Gaussian construction at moderate-to-high noise levels, while being worse by an arbitrarily large factor at low noise levels.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,149,158,,,,,,,,,,,,,,,,WOS:000508662100017,0
C,"Wang, YN; Wang, YX; Singh, A",,"Gretton, A; Robert, CC",,"Wang, Yining; Wang, Yu-Xiang; Singh, Aarti",,,Graph Connectivity in Noisy Sparse Subspace Clustering,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Subspace clustering is the problem of clustering data points into a union of low-dimensional linear/affine subspaces. It is the mathematical abstraction of many important problems in computer vision, image processing and machine learning. A line of recent work [4, 19, 24, 20] provided strong theoretical guarantee for sparse subspace clustering [4], the state-of-the-art algorithm for subspace clustering, on both noiseless and noisy data sets. It was shown that under mild conditions, with high probability no two points from different subspaces are clustered together. Such guarantee, however, is not sufficient for the clustering to be correct, due to the notorious graph connectivity problem [15]. In this paper, we investigate the graph connectivity problem for noisy sparse subspace clustering and show that a simple post-processing procedure is capable of delivering consistent clustering under certain general position or restricted eigenvalue assumptions. We also show that our condition is almost tight with adversarial noise perturbation by constructing a counter-example. These results provide the first exact clustering guarantee of noisy SSC for subspaces of dimension greater then 3.",,,,,,"Wang, Yining/0000-0001-9410-0392",,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,538,546,,,,,,,,,,,,,,,,WOS:000508662100059,0
C,"Wilson, AG; Hu, ZT; Salakhutdinov, R; Xing, EP",,"Gretton, A; Robert, CC",,"Wilson, Andrew Gordon; Hu, Zhiting; Salakhutdinov, Ruslan; Xing, Eric P.",,,Deep Kernel Learning,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"We introduce scalable deep kernels, which combine the structural properties of deep learning architectures with the non-parametric flexibility of kernel methods. Specifically, we transform the inputs of a spectral mixture base kernel with a deep architecture, using local kernel interpolation, inducing points, and structure exploiting (Kronecker and Toeplitz) algebra for a scalable kernel representation. These closed-form kernels can be used as drop-in replacements for standard kernels, with benefits in expressive power and scalability. We jointly learn the properties of these kernels through the marginal likelihood of a Gaussian process. Inference and learning cost O(n) for n training points, and predictions cost O(1) per test point. On a large and diverse collection of applications, including a dataset with 2 million examples, we show improved performance over scalable Gaussian processes with flexible kernel learning models, and stand-alone deep architectures.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,370,378,,,,,,,,,,,,,,,,WOS:000508662100041,0
C,"Bajwa, WU; Duarte, MF; Calderbank, R",,"Kaski, S; Corander, J",,"Bajwa, Waheed U.; Duarte, Marco F.; Calderbank, Robert",,,Average Case Analysis of High-Dimensional Block-Sparse Recovery and Regression for Arbitrary Designs,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"This paper studies conditions for high-dimensional inference when the set of observations is given by a linear combination of a small number of groups of columns of a design matrix, termed the block-sparse case. In this regard, it first specifies conditions on the design matrix under which most of its block submatrices are well conditioned. It then leverages this result for average-case analysis of high-dimensional block-sparse recovery and regression. In contrast to earlier works: (i) this paper provides conditions on arbitrary designs that can be explicitly computed in polynomial time, (ii) the provided conditions translate into near-optimal sealing of the number of observations with the number of active blocks of the design matrix, and (iii)the conditions suggest that the spectral norm, rather than the column/block coherences, of the design matrix fundamentally limits the performance of computational methods in high-dimensional settings.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,57,67,,,,,,,,,,,,,,,,WOS:000508355800007,0
C,"Dicker, LH",,"Kaski, S; Corander, J",,"Dicker, Lee H.",,,Sparsity and the truncated l(2)-norm,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"Sparsity is a fundamental topic in high-dimensional data analysis. Perhaps the most common measures of sparsity are the l(p)-norms, for 0 <= p < 2. In this paper, we study an alternative measure of sparsity, the truncated l(2)-norm, which is related to other l(p)-norms, but appears to have some unique and useful properties. Focusing on the n-dimensional Gaussian location model, we derive exact asymptotic minimax results for estimation over truncated l(2)-balls, which complement existing results for l(p)-balls. We then propose simple new adaptive thresholding estimators that are inspired by the truncated l(2)-norm and are adaptive asymptotic minimax over l(p)-balls (0 <= p < 2), as well as truncated l(2)-balls. Finally, we derive lower bounds on the Bayes risk of an estimator, in terms of the parameter's truncated l(2)-norm. These bounds provide necessary conditions for Bayes risk consistency in certain problems that are relevant for high-dimensional Bayesian modeling.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,159,166,,,,,,,,,,,,,,,,WOS:000508355800018,0
C,"Dworkin, L; Kearns, M; Xia, Lr",,"Kaski, S; Corander, J",,"Dworkin, Lili; Kearns, Michael; Xia, Lirong",,,Efficient Inference for Complex Queries on Complex Distributions,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"We consider problems of approximate inference in which the query of interest is given by a complex formula (such as a formula in disjunctive formal form (DNF)) over a joint distribution given by a graphical model. We give a general reduction showing that (approximate) marginal inference for a class of distributions yields approximate inference for DNF queries, and extend our techniques to accommodate even more complex queries, and dense graphical models with variational inference, under certain conditions. Our results unify and generalize classical inference techniques (which are generally restricted to simple marginal queries) and approximate counting methods such as those introduced by Karp, Luby and Madras (which are generally restricted to product distributions).",,,,,,"Xia, Lirong/0000-0002-9800-6691",,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,211,219,,,,,,,,,,,,,,,,WOS:000508355800024,0
C,"Kapicioglu, B; Rosenberg, DS; Schapire, RE; Jebara, T",,"Kaski, S; Corander, J",,"Kapicioglu, Berk; Rosenberg, David S.; Schapire, Robert E.; Jebara, Tony",,,Collaborative Ranking for Local Preferences,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"For many collaborative ranking tasks, we have access to relative preferences among subsets of items. but not to global preferences among all items. To address this, we introduce a matrix factorization framework called Collaborative Local Ranking (CLR). We justify CLR by proving a bound on its generalization error. the first such bound for collaborative ranking that we know of. We then derive a simple alternating minimization algorithm and prove that its running time is independent of the number of training examples. We apply CLR to a novel venue recommendation task and demonstrate that it outperforms state-of-the-art collaborative ranking methods on real-world data sets.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,466,474,,,,,,,,,,,,,,,,WOS:000508355800052,0
C,"Arbour, D; Dimmery, D; Rao, A",,"Banerjee, A; Fukumizu, K",,"Arbour, David; Dimmery, Drew; Rao, Anup",,,Efficient Balanced Treatment Assignments for Experimentation,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"In this work, we reframe the problem of balanced treatment assignment as optimization of a two-sample test between test and control units. Using this lens we provide an assignment algorithm that is optimal with respect to the minimum spanning tree test of Friedman and Rafsky [1979]. This assignment to treatment groups may be performed exactly in polynomial time. We provide a probabilistic interpretation of this process in terms of the most probable element of designs drawn from a determinantal point process which admits a probabilistic interpretation of the design. We provide a novel formulation of estimation as transductive inference and show how the tree structures used in design can also be used in an adjustment estimator. We conclude with a simulation study demonstrating the improved efficacy of our method.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803068,0
C,"Bao, YJ; Xiong, WJ",,"Banerjee, A; Fukumizu, K",,"Bao, Yajie; Xiong, Weijia",,,One-Round Communication Efficient Distributed M-Estimation,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Communication cost and local computation complexity are two main bottlenecks of the distributed statistical learning. In this paper, we consider the distributed M-estimation problem in both regular and sparse case and propose a novel one-round communication efficient algorithm. For regular distributed M-estimator, the asymptotic normality is provided to conduct statistical inference. For sparse distributed M-estimator, we only require solving a quadratic Lasso problem in the master machine using the same local information as the regular distributed M-estimator. Consequently, the computation complexity of the local machine is sufficiently reduced compared with the existing debiased sparse estimator. Under mild conditions, the theoretical results guarantee that our proposed distributed estimators achieve (near)optimal statistical convergence rate. The effectiveness of our proposed algorithm is verified through experiments across different M-estimation problems using both synthetic and real benchmark datasets.",,,,,,"Bao, Yajie/0000-0003-3843-7016",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,46,+,,,,,,,,,,,,,,,,WOS:000659893800006,0
C,"Han, X; Dasgupta, S; Ghosh, J",,"Banerjee, A; Fukumizu, K",,"Han, Xing; Dasgupta, Sambarta; Ghosh, Joydeep",,,Simultaneously Reconciled Quantile Forecasting of Hierarchically Related Time Series,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Many real-life applications involve simultaneously forecasting multiple time series that are hierarchically related via aggregation or disaggregation operations. For instance, commercial organizations often want to forecast inventories simultaneously at store, city, and state levels for resource planning purposes. In such applications, it is important that the forecasts, in addition to being reasonably accurate, are also consistent w.r.t one another. Although forecasting such hierarchical time series has been pursued by economists and data scientists, the current state-of-the-art models use strong assumptions, e.g., all forecasts being unbiased estimates, noise distribution being Gaussian. Besides, state-of-the-art models have not harnessed the power of modern nonlinear models, especially ones based on deep learning. In this paper, we propose using a flexible nonlinear model that optimizes quantile regression loss coupled with suitable regularization terms to maintain the consistency of forecasts across hierarchies. The theoretical framework introduced herein can be applied to any forecasting model with an underlying differentiable loss function. A proof of optimality of our proposed method is also provided. Simulation studies over a range of datasets highlight the efficacy of our approach.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,190,+,,,,,,,,,,,,,,,,WOS:000659893800022,0
C,"Le, T; Ho, N; Yamada, M",,"Banerjee, A; Fukumizu, K",,"Le, Tam; Ho, Nhat; Yamada, Makoto",,,Flow-based Alignment Approaches for Probability Measures in Different Spaces,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Gromov-Wasserstein (GW) is a powerful tool to compare probability measures whose supports are in different metric spaces. However, GW suffers from a computational drawback since it requires to solve a complex non-convex quadratic program. In this work, we consider a specific family of cost metrics, namely, tree metrics for supports of each probability measure, to develop efficient and scalable discrepancies between the probability measures. Leveraging a tree structure, we propose to align flows from a root to each support instead of pair-wise tree metrics of supports, i.e., flows from a support to another support, in GW. Consequently, we propose a novel discrepancy, named Flow-based Alignment (FlowAlign), by matching the flows of the probability measures. FlowAlign is computationally fast and scalable for large-scale applications. Further exploring the tree structure, we propose a variant of FlowAlign, named Depth-based Alignment (DepthAlign), by aligning the flows hierarchically along each depth level of the tree structures. Theoretically, we prove that both FlowAlign and DepthAlign are pseudometrics. We also derive tree-sliced variants of the proposed discrepancies for applications without prior knowledge about tree structures for probability measures, computed by averaging FlowAlign/DepthAlign using random tree metrics, adaptively sampled from supports of probability measures. Empirically, we test our proposed approaches against other variants of GW baselines on a few benchmark tasks.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804072,0
C,"Lei, Q; Nagarajan, SG; Panageas, I; Xiao, W",,"Banerjee, A; Fukumizu, K",,"Lei, Qi; Nagarajan, Sai Ganesh; Panageas, Ioannis; Wang, Xiao",,,Last Iterate Convergence in No-regret Learning: Constrained Min-max Optimization for Convex-concave Landscapes,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"In a recent series of papers it has been established that variants of Gradient Descent/Ascent and Mirror Descent exhibit last iterate convergence in convex-concave zero-sum games. Specifically, Daskalakis et al. (2018); Liang and Stokes (2018) show last iterate convergence of the so called Optimistic Gradient Descent/Ascent for the case of unconstrained min-max optimization. Moreover, in Mertikopoulos et al. (2018) the authors show that Mirror Descent with an extra gradient step displays last iterate convergence for convex-concave problems (both constrained and unconstrained), though their algorithm uses vanishing stepsizes. In this work, we show that Optimistic Multiplicative-Weights Update (OMWU) with constant stepsize, exhibits last iterate convergence locally for convex-concave games, generalizing the results of Daskalakis and Panageas (2019) where last iterate convergence of OMWU was shown only for the bilinear case. To the best of our knowledge, this is the first result about last-iterate convergence for constrained zero sum games (beyond the bilinear case) in which the dynamics use constant step-sizes.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801074,0
C,"Ma, YP; Tresp, V",,"Banerjee, A; Fukumizu, K",,"Ma, Yunpu; Tresp, Volker",,,Causal Inference under Networked Interference and Intervention Policy Enhancement,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Estimating individual treatment effects from data of randomized experiments is a critical task in causal inference. The Stable Unit Treatment Value Assumption (SUTVA) is usually made in causal inference. However, interference can introduce bias when the assigned treatment on one unit affects the potential outcomes of the neighboring units. This interference phenomenon is known as spillover effect in economics or peer effect in social science. Usually, in randomized experiments, or observational studies with interconnected units, one can only observe treatment responses under interference. Hence, the issue of how to estimate the superimposed causal effect and recover the individual treatment effect in the presence of interference becomes a challenging task. In this work, we study causal effect estimation under general network interference using Graph Neural Networks, which are powerful tools for capturing node and link dependencies in graphs. After deriving causal effect estimators, we further study intervention policy improvement on the graph under capacity constraint. We give policy regret bounds under network interference and treatment capacity constraint.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804046,0
C,"Morningstar, WR; Ham, C; Gallagher, AG; Lakshminarayanan, B; Alemi, AA; Dillon, JV",,"Banerjee, A; Fukumizu, K",,"Morningstar, Warren R.; Ham, Cusuh; Gallagher, Andrew G.; Lakshminarayanan, Balaji; Alemi, Alexander A.; Dillon, Joshua, V",,,Density of States Estimation for Out-of-Distribution Detection,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Perhaps surprisingly, recent studies have shown probabilistic model likelihoods have poor specificity for out-of-distribution (OOD) detection and often assign higher likelihoods to OOD data than in-distribution data. To ameliorate this issue we propose DoSE, the Density of States Estimator. Drawing on the statistical physics notion of density of states, the DoSE decision rule avoids direct comparison of model probabilities, and instead utilizes the probability of the model probability, or indeed the frequency of any reasonable statistic. The frequency is calculated using nonparametric density estimators (e.g., KDE and one-class SVM) which measure the typicality of various model statistics given the training data and from which we can flag test points with low typicality as anomalous. Unlike many other methods, DoSE requires neither labeled data nor OOD examples. DoSE is modular and can be trivially applied to any existing, trained model. We demonstrate DoSE's state-of-the-art performance against other unsupervised OOD detectors on previously established hard benchmarks.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803086,0
C,"Pan, H; Kondor, R",,"Banerjee, A; Fukumizu, K",,"Pan, Horace; Kondor, Risi",,,Fourier Bases for Solving Permutation Puzzles,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Traditionally, permutation puzzles such as the Rubik's Cube were often solved by heuristic search like A*-search and value based reinforcement learning methods. Both heuristic search and Q-learning approaches to solving these puzzles can be reduced to learning a heuristic/value function to decide what puzzle move to make at each step. We propose learning a value function using the irreducible representations basis (which we will also call the Fourier basis) of the puzzle's underlying group. Classical Fourier analysis on real valued functions tells us we can approximate smooth functions with low frequency basis functions. Similarly, smooth functions on finite groups can be represented by the analogous low frequency Fourier basis functions. We demonstrate the effectiveness of learning a value function in the Fourier basis for solving various permutation puzzles and show that it outperforms standard deep learning methods.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,172,+,,,,,,,,,,,,,,,,WOS:000659893800020,0
C,"Scetbon, M; Meunier, L; Atif, J; Cuturi, M",,"Banerjee, A; Fukumizu, K",,"Scetbon, Meyer; Meunier, Laurent; Atif, Jamal; Cuturi, Marco",,,Equitable and Optimal Transport with Multiple Agents,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We introduce an extension of the Optimal Transport problem when multiple costs are involved. Considering each cost as an agent, we aim to share equally between agents the work of transporting one distribution to another. To do so, we minimize the transportation cost of the agent who works the most. Another point of view is when the goal is to partition equitably goods between agents according to their heterogeneous preferences. Here we aim to maximize the utility of the least advantaged agent. This is a fair division problem. Like Optimal Transport, the problem can be cast as a linear optimization problem. When there is only one agent, we recover the Optimal Transport problem. When two agents are considered, we are able to recover Integral Probability Metrics defined by alpha-Holder functions, which include the widely-known Dudley metric. To the best of our knowledge, this is the first time a link is given between the Dudley metric and Optimal Transport. We provide an entropic regularization of that problem which leads to an alternative algorithm faster than the standard linear program.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802046,0
C,"Wang, CQ; Sun, SY; Grosse, R",,"Banerjee, A; Fukumizu, K",,"Wang, Chaoqi; Sun, Shengyang; Grosse, Roger",,,Beyond Marginal Uncertainty: How Accurately can Bayesian Regression Models Estimate Posterior Predictive Correlations?,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"While uncertainty estimation is a well-studied topic in deep learning, most such work focuses on marginal uncertainty estimates, i.e. the predictive mean and variance at individual input locations. But it is often more useful to estimate predictive correlations between the function values at different input locations. In this paper, we consider the problem of bench-marking how accurately Bayesian models can estimate predictive correlations. We first consider a downstream task which depends on posterior predictive correlations: transductive active learning (TAL). We find that TAL makes better use of models' uncertainty estimates than ordinary active learning, and recommend this as a benchmark for evaluating Bayesian models. Since TAL is too expensive and indirect to guide development of algorithms, we introduce two metrics which more directly evaluate the predictive correlations and which can be computed efficiently: meta-correlations (i.e. the correlations between the models correlation estimates and the true values), and cross-normalized likelihoods (XLL). We validate these metrics by demonstrating their consistency with TAL performance and obtain insights about the relative performance of current Bayesian neural net and Gaussian process models.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803002,0
C,"Zhang, BH; Rajan, R; Pineda, L; Lambert, N; Biedenkapp, A; Chua, K; Hutter, F; Calandra, R",,"Banerjee, A; Fukumizu, K",,"Zhang, Baohe; Rajan, Raghu; Pineda, Luis; Lambert, Nathan; Biedenkapp, Andre; Chua, Kurtland; Hutter, Frank; Calandra, Roberto",,,On the Importance of Hyperparameter Optimization for Model-based Reinforcement Learning,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Model-based Reinforcement Learning (MBRL) is a promising framework for learning control in a data-efficient manner. MBRL algorithms can be fairly complex due to the separate dynamics modeling and the subsequent planning algorithm, and as a result, they often possess tens of hyper-parameters and architectural choices. For this reason, MBRL typically requires significant human expertise before it can be applied to new problems and domains. To alleviate this problem, we propose to use automatic hyperparameter optimization (HPO). We demonstrate that this problem can be tackled effectively with automated HPO, which we demonstrate to yield significantly improved performance compared to human experts. In addition, we show that tuning of several MBRL hyperparameters dynamically, i.e. during the training itself, further improves the performance compared to using static hyperparameters which are kept fixed for the whole training. Finally, our experiments provide valuable insights into the effects of several hyperparameters, such as plan horizon or learning rate and their influence on the stability of training and resulting rewards.",,,,,,"Biedenkapp, Andre/0000-0002-8703-8559",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804080,0
C,"Zhang, YF; Yang, ZR; Wang, ZR",,"Banerjee, A; Fukumizu, K",,"Zhang, Yufeng; Yang, Zhuoran; Wang, Zhaoran",,,Provably Efficient Actor-Critic for Risk-Sensitive and Robust Adversarial RL: A Linear-Quadratic Case,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Risk-sensitivity plays a central role in artificial intelligence safety. In this paper, we study the global convergence of the actor-critic algorithm for risk-sensitive reinforcement learning (RSRL) with exponential utility, which remains challenging for policy optimization as it lacks the linearity needed to formulate policy gradient. To bypass such an issue of nonlinearity, we resort to the equivalence between RSRL and robust adversarial reinforcement learning (RARL), which is formulated as a zero-sum Markov game with a hypothetical adversary. In particular, the Nash equilibrium (NE) of such a game yields the optimal policy for RSRL, which is provably robust. We focus on a simple yet fundamental setting known as linear-quadratic (LQ) game. To attain the optimal policy, we develop a nested natural actor-critic algorithm, which provably converges to the NE of the LQ game at a sublinear rate, thus solving both RSRL and RARL. To the best knowledge, the proposed nested actor-critic algorithm appears to be the first model-free policy optimization algorithm that provably attains the optimal policy for RSRL and RARL in the LQ setting, which sheds light on more general settings.",,,,,"Zhang, Yufeng/GZL-1973-2022",,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803034,0
C,"Bogunovic, I; Krause, A; Scarlett, J",,"Chiappa, S; Calandra, R",,"Bogunovic, Ilija; Krause, Andreas; Scarlett, Jonathan",,,Corruption-Tolerant Gaussian Process Bandit Optimization,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We consider the problem of optimizing an unknown (typically non-convex) function with a bounded norm in some Reproducing Kernel Hilbert Space (RKHS), based on noisy bandit feedback. We consider a novel variant of this problem in which the point evaluations are not only corrupted by random noise, but also adversarial corruptions. We introduce an algorithm Fast-Slow GP-UCB based on Gaussian process methods, randomized selection between two instances labeled fast (but nonrobust) and slow (but robust), enlarged confidence bounds, and the principle of optimism under uncertainty. We present a novel theoretical analysis upper bounding the cumulative regret in terms of the corruption level, the time horizon, and the underlying kernel, and we argue that certain dependencies cannot be improved. We observe that distinct algorithmic ideas are required depending on whether one is required to perform well in both the corrupted and non-corrupted settings, and whether the corruption level is known or not.",,,,,"Scarlett, Jonathan/AGK-0892-2022",,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1071,1080,,,,,,,,,,,,,,,,WOS:000559931300034,0
C,"Dasgupta, S; Sabato, S",,"Chiappa, S; Calandra, R",,"Dasgupta, Sanjoy; Sabato, Sivan",,,Robust Learning from Discriminative Feature Feedback,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Recent work introduced the model of learning from discriminative feature feedback, in which a human annotator not only provides labels of instances, but also identifies discriminative features that highlight important differences between pairs of instances. It was shown that such feedback can be conducive to learning, and makes it possible to efficiently learn some concept classes that would otherwise be intractable. However, these results all relied upon perfect annotator feedback. In this paper, we introduce a more realistic, robust version of the framework, in which the annotator is allowed to make mistakes. We show how such errors can be handled algorithmically, in both an adversarial and a stochastic setting. In particular, we derive regret bounds in both settings that, as in the case of a perfect annotator, are independent of the number of features. We show that this result cannot be obtained by a naive reduction from the robust setting to the non-robust setting.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,973,981,,,,,,,,,,,,,,,,WOS:000559931300071,0
C,"Fisher, MA; Oates, CJ; Powell, C; Teckentrup, A",,"Chiappa, S; Calandra, R",,"Fisher, Matthew A.; Oates, Chris J.; Powell, Catherine; Teckentrup, Aretha",,,A Locally Adaptive Bayesian Cubature Method,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Bayesian cubature (BC) is a popular inferential perspective on the cubature of expensive integrands, wherein the integrand is emulated using a stochastic process model. Several approaches have been put forward to encode sequential adaptation (i.e. dependence on previous integrand evaluations) into this framework. However, these proposals have been limited to either estimating the parameters of a stationary covariance model or focusing computational resources on regions where large values are taken by the integrand. In contrast, many classical adaptive cubature methods are locally adaptive in the sense that they focus computational resources on spatial regions in which local error estimates are largest. The main contributions of this work are twofold; first we establish that existing BC methods do not possess local adaptivity in the sense of many classical adaptive methods and secondly, we developed a novel BC method whose behaviour, demonstrated empirically, is analogous to such methods. Finally we present evidence that the novel method provides improved cubature performance, relative to standard BC, in a detailed empirical assessment.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1265,1274,,,,,,,,,,,,,,,,WOS:000559931301003,0
C,"Gallego, J; Vani, A; Schwarzer, M; Lacoste-Julien, S",,"Chiappa, S; Calandra, R",,"Gallego, Jose; Vani, Ankit; Schwarzer, Max; Lacoste-Julien, Simon",,,GAIT: A Geometric Approach to Information Theory,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We advocate the use of a notion of entropy that reflects the relative abundances of the symbols in an alphabet, as well as the similarities between them. This concept was originally introduced in theoretical ecology to study the diversity of ecosystems. Based on this notion of entropy, we introduce geometry-aware counterparts for several concepts and theorems in information theory. Notably, our proposed divergence exhibits performance on par with state-of-the-art methods based on the Wasserstein distance, but enjoys a closed-form expression that can be computed efficiently. We demonstrate the versatility of our method via experiments on a broad range of domains: training generative models, computing image barycenters, approximating empirical measures and counting modes.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2601,2610,,,,,,,,,,,,,,,,WOS:000559931302080,0
C,"Goel, S",,"Chiappa, S; Calandra, R",,"Goel, Surbhi",,,Learning Ising and Potts Models with Latent Variables,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We study the problem of learning graphical models with latent variables. We give the first efficient algorithms for learning: 1) ferromagnetic Ising models with latent variables under arbitrary external fields, and 2) ferromagnetic Potts model with latent variables under unidirectional non-negative external field. Our algorithms have optimal dependence on the dimension but suffer from a sub-optimal dependence on the underlying sparsity of the graph. Our results rely on two structural properties of the underlying graphical models. These in turn allow us to design an influence function which can be maximized greedily to recover the structure of the underlying graphical model. These structural results may be of independent interest.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3557,3565,,,,,,,,,,,,,,,,WOS:000559931301022,0
C,"Huang, CW; Touati, A; Vincent, P; Dziugaite, GK; Lacoste, A; Courville, A",,"Chiappa, S; Calandra, R",,"Huang, Chin-Wei; Touati, Ahmed; Vincent, Pascal; Dziugaite, Gintare Karolina; Lacoste, Alexandre; Courville, Aaron",,,Stochastic Neural Network with Kronecker Flow,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Recent advances in variational inference enable the modelling of highly structured joint distributions, but are limited in their capacity to scale to the high-dimensional setting of stochastic neural networks. This limitation motivates a need for scalable parameterizations of the noise generation process, in a manner that adequately captures the dependencies among the various parameters. In this work, we address this need and present the Kronecker Flow, a generalization of the Kronecker product to invertible mappings designed for stochastic neural networks. We apply our method to variational Bayesian neural networks on predictive tasks, PAC-Bayes generalization bound estimation, and approximate Thompson sampling in contextual bandits. In all setups, our methods prove to be competitive with existing methods and better than the baselines.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,4184,4193,,,,,,,,,,,,,,,,WOS:000559931301044,0
C,"Kilbertus, N; Gomez-Rodriguez, M; Scholkopf, B; Muandet, K; Valera, I",,"Chiappa, S; Calandra, R",,"Kilbertus, Niki; Gomez-Rodriguez, Manuel; Scholkopf, Bernhard; Muandet, Krikamol; Valera, Isabel",,,Fair Decisions Despite Imperfect Predictions,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Consequential decisions are increasingly informed by sophisticated data-driven predictive models. However, consistently learning accurate predictive models requires access to ground truth labels. Unfortunately, in practice, labels may only exist conditional on certain decisions-if a loan is denied, there is not even an option for the individual to pay back the loan. In this paper, we show that, in this selective labels setting, learning to predict is suboptimal in terms of both fairness and utility. To avoid this undesirable behavior, we propose to directly learn stochastic decision policies that maximize utility under fairness constraints. In the context of fair machine learning, our results suggest the need for a paradigm shift from learning to predict to learning to decide. Experiments on synthetic and real-world data illustrate the favorable properties of learning to decide, in terms of both utility and fairness.",,,,,"Rodriguez, Manuel Gomez/AAB-5005-2021",,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,277,286,,,,,,,,,,,,,,,,WOS:000559931301071,0
C,"Kushnir, D; Mirabelli, B",,"Chiappa, S; Calandra, R",,"Kushnir, Dan; Mirabelli, Benjamin",,,Active Community Detection with Maximal Expected Model Change,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We present a novel active learning algorithm for community detection on networks. Our proposed algorithm uses a Maximal Expected Model Change (MEMC) criterion for querying network nodes label assignments. MEMC detects nodes that maximally change the community assignment likelihood model following a query. Our method is inspired by detection in the benchmark Stochastic Block Model (SBM), where we provide sample complexity analysis and empirical study with SBM and real network data for binary as well as for the multi-class settings. The analysis also covers the most challenging case of sparse degree and below-detection-threshold SBMs, where we observe a super-linear error reduction. MEMC is shown to be superior to the random selection baseline and other state-of-the-art active learners.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,724,733,,,,,,,,,,,,,,,,WOS:000559931301089,0
C,"Lu, HH; Karimireddy, SP; Ponomareva, N; Mirrokni, V",,"Chiappa, S; Calandra, R",,"Lu, Haihao; Karimireddy, Sai Praneeth; Ponomareva, Natalia; Mirrokni, Vahab",,,Accelerating Gradient Boosting Machines,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Gradient Boosting Machine (GBM) introduced by Friedman (2001) is a widely popular ensembling technique and is routinely used in competitions such as Kaggle and the KDD-Cup (Chen and Guestrin, 2016). In this work, we propose an Accelerated Gradient Boosting Machine (AGBM) by incorporating Nesterov's acceleration techniques into the design of GBM. The difficulty in accelerating GBM lies in the fact that weak (inexact) learners are commonly used, and therefore, with naive application, the errors can accumulate in the momentum term. To overcome it, we design a corrected pseudo residual that serves as a new target for fitting a weak learner, in order to perform the z-update. Thus, we are able to derive novel computational guarantees for AGBM. This is the first GBM type of algorithm with a theoretically-justified accelerated convergence rate.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,516,525,,,,,,,,,,,,,,,,WOS:000559931302019,0
C,"Rolf, E; Jordan, MI; Recht, B",,"Chiappa, S; Calandra, R",,"Rolf, Esther; Jordan, Michael I.; Recht, Benjamin",,,Post-Estimation Smoothing: A Simple Baseline for Learning with Side Information,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Observational data are often accompanied by natural structural indices, such as time stamps or geographic locations, which are meaningful to prediction tasks but are often discarded. We leverage semantically meaningful indexing data while ensuring robustness to potentially uninformative or misleading indices. We propose a post-estimation smoothing operator as a fast and effective method for incorporating structural index data into prediction. Because the smoothing step is separate from the original predictor, it applies to a broad class of machine learning tasks, with no need to retrain models. Our theoretical analysis details simple conditions under which post-estimation smoothing will improve accuracy over that of the original predictor. Our experiments on large scale spatial and temporal datasets highlight the speed and accuracy of post-estimation smoothing in practice. Together, these results illuminate a novel way to consider and incorporate the natural structure of index variables in machine learning.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1759,1768,,,,,,,,,,,,,,,,WOS:000559931302093,0
C,"Sherman, E; Arbour, D; Shpitser, I",,"Chiappa, S; Calandra, R",,"Sherman, Eli; Arbour, David; Shpitser, Ilya",,,General Identification of Dynamic Treatment Regimes Under Interference,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"In many applied fields, researchers are often interested in tailoring treatments to unit-level characteristics in order to optimize an outcome of interest. Methods for identifying and estimating treatment policies are the subject of the dynamic treatment regime literature. Separately, in many settings the assumption that data are independent and identically distributed does not hold due to inter-subject dependence. The phenomenon where a subject's outcome is dependent on his neighbor's exposure is known as interference. These areas intersect in myriad real-world settings. In this paper we consider the problem of identifying optimal treatment policies in the presence of interference. Using a general representation of interference, via Lauritzen-Wermuth-Freydenburg chain graphs (Lauritzen and Richardson, 2002), we formalize a variety of policy interventions under interference and extend existing identification theory (Tian, 2008; Sherman and Shpitser, 2018). Finally, we illustrate the efficacy of policy maximization under interference in a simulation study.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,33313513,,,,,WOS:000559931303015,0
C,"Tan, ZL; Yeom, S; Fredrikson, M; Talwalkar, A",,"Chiappa, S; Calandra, R",,"Tan, Zilong; Yeom, Samuel; Fredrikson, Matt; Talwalkar, Ameet",,,Learning Fair Representations for Kernel Models,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Fair representations are a powerful tool for satisfying fairness goals such as statistical parity and equality of opportunity in learned models. Existing techniques for learning these representations are typically model-agnostic, as they pre-process the original data such that the output satisfies some fairness criterion, and can be used with arbitrary learning methods. In contrast, we demonstrate the promise of learning a model-aware fair representation, focusing on kernel-based models. We leverage the classical sufficient dimension reduction (SDR) framework to construct representations as subspaces of the reproducing kernel Hilbert space (RKHS), whose member functions are guaranteed to satisfy a given fairness criterion. Our method supports several fairness criteria, continuous and discrete data, and multiple protected attributes. We also characterize the fairness-accuracy trade-off with a parameter that relates to the principal angles between subspaces of the RKHS. Finally, we apply our approach to obtain the first fair Gaussian process (FGP) prior for fair Bayesian learning, and show that it is competitive with, and in some cases outperforms, state-of-the-art methods on real data.",,,,,"Fredrikson, Matt/GQR-0633-2022",,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303036,0
C,"Wang, CH; Cheng, G",,"Chiappa, S; Calandra, R",,"Wang, Chi-Hua; Cheng, Guang",,,Online Batch Decision-Making with High-Dimensional Covariates,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We propose and investigate a class of new algorithms for sequential decision making that interacts with a batch of users simultaneously instead of a user at each decision epoch. This type of batch models is motivated by interactive marketing and clinical trial, where a group of people are treated simultaneously and the outcomes of the whole group are collected before the next stage of decision. In such a scenario, our goal is to allocate a batch of treatments to maximize treatment efficacy based on observed high-dimensional user covariates. We deliver a solution, named Teamwork LASSO Bandit algorithm, that resolves a batch version of explore-exploit dilemma via switching between teamwork stage and selfish stage during the whole decision process. This is made possible based on statistical properties of LASSO estimate of treatment efficacy that adapts to a sequence of batch observations. In general, a rate of optimal allocation condition is proposed to delineate the exploration and exploitation trade-off on the data collection scheme, which is sufficient for LASSO to identify the optimal treatment for observed user covariates. An upper bound on expected cumulative regret of the proposed algorithm is provided.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303069,0
C,"Banijamali, E; Abbasi-Yadkori, Y; Ghavamzadeh, M; Vlassis, N",,"Chaudhuri, K; Sugiyama, M",,"Banijamali, Ershad; Abbasi-Yadkori, Yasin; Ghavamzadeh, Mohammad; Vlassis, Nikos",,,Optimizing over a Restricted Policy Class in MDPs,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We address the problem of finding an optimal policy in a Markov decision process (MDP) under a restricted policy class defined by the convex hull of a set of base policies. This problem is of great interest in applications in which a number of reasonably good (or safe) policies are already known and we are interested in optimizing in their convex hull. We first prove that solving this problem is NP-hard. We then propose an efficient algorithm that finds a policy whose performance is almost as good as that of the best convex combination of the base policies, under the assumption that the occupancy measures of the base policies have a large overlap. The running time of the proposed algorithm is linear in the number of states and polynomial in the number of base policies. A distinct advantage of the proposed algorithm is that, apart from the computation of the occupancy measures of the base policies, it does not need to interact with the environment during the optimization process. This is especially important (i) in problems that due to concerns such as safety, we are restricted in interacting with the environment only through the (safe) base policies, and (ii) in complex systems where estimating the value of a policy can be a time consuming process.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687903010,0
C,"Bascol, K; Emonet, R; Fromont, E; Habrard, A; Metzler, G; Sebban, M",,"Chaudhuri, K; Sugiyama, M",,"Bascol, Kevin; Emonet, Remi; Fromont, Elisa; Habrard, Amaury; Metzler, Guillaume; Sebban, Marc",,,From Cost-Sensitive Classification to Tight F-measure Bounds,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"The F-measure is a classification performance measure, especially suited when dealing with imbalanced datasets, which provides a compromise between the precision and the recall of a classifier. As this measure is non convex and non linear, it is often indirectly optimized using cost-sensitive learning (that affects different costs to false positives and false negatives). In this article, we derive theoretical guarantees that give tight bounds on the best F-measure that can be obtained from cost sensitive learning. We also give an original geometric interpretation of the bounds that serves as an inspiration for CONE, a new algorithm to optimize for the F-measure. Using 10 datasets exhibiting varied class imbalance, we illustrate that our bounds are much tighter than previous work and show that CONE learns models with either superior F-measures than existing methods or comparable but in fewer iterations.",,,,,,"Emonet, Remi/0000-0002-1870-1329; Fromont, Elisa/0000-0003-0133-3491",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901030,0
C,"Bellemare, MG; Le Roux, N; Castro, PS; Moitra, S",,"Chaudhuri, K; Sugiyama, M",,"Bellemare, Marc G.; Le Roux, Nicolas; Castro, Pablo Samuel; Moitra, Subhodeep",,,Distributional reinforcement learning with linear function approximation,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Despite many algorithmic advances, our theoretical understanding of practical distributional reinforcement learning methods remains limited. One exception is Rowland et al. (2018)'s analysis of the C51 algorithm in terms of the Cramer distance, but their results only apply to the tabular setting and ignore C51's use of a softmax to produce normalized distributions. In this paper we adapt the Cramer distance to deal with arbitrary vectors. From it we derive a new distributional algorithm which is fully Cramer-based and can be combined to linear function approximation, with formal guarantees in the context of policy evaluation. In allowing the model's prediction to be any real vector, we lose the probabilistic interpretation behind the method, but otherwise maintain the appealing properties of distributional approaches. To the best of our knowledge, ours is the first proof of convergence of a distributional algorithm combined with function approximation. Perhaps surprisingly, our results provide evidence that Cramer-based distributional methods may perform worse than directly approximating the value function.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902026,0
C,"Cho, H; DeMeo, B; Peng, J; Berger, B",,"Chaudhuri, K; Sugiyama, M",,"Cho, Hyunghoon; DeMeo, Benjamin; Peng, Jian; Berger, Bonnie",,,Large-Margin Classification in Hyperbolic Space,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Representing data in hyperbolic space can effectively capture latent hierarchical relationships. To enable accurate classification of points in hyperbolic space while respecting their hyperbolic geometry, we introduce hyperbolic SVM, a hyperbolic formulation of support vector machine classifiers, and describe its theoretical connection to the Euclidean counterpart. We also generalize Euclidean kernel SVM to hyperbolic space, allowing nonlinear hyperbolic decision boundaries and providing a geometric interpretation for a certain class of indefinite kernels. Hyperbolic SVM improves classification accuracy in simulation and in real-world problems involving complex networks and word embeddings. Our work enables end-to-end analyses based on the inherent hyperbolic geometry of the data without resorting to ill-fitting tools developed for Euclidean space.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,32832915,,,,,WOS:000509687901091,0
C,"Derezinski, M; Warmuth, MK; Hsu, D",,"Chaudhuri, K; Sugiyama, M",,"Derezinski, Michal; Warmuth, Manfred K.; Hsu, Daniel",,,Correcting the bias in least squares regression with volume-rescaled sampling,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Consider linear regression where the examples are generated by an unknown distribution on R-d x R. Without any assumptions on the noise, the linear least squares solution for any i.i.d. sample will typically be biased w.r.t. the least squares optimum over the entire distribution. However, we show that if an i.i.d. sample of any size k is augmented by a certain small additional sample, then the solution of the combined sample becomes unbiased. We show this when the additional sample consists of d points drawn jointly according to the input distribution that is rescaled by the squared volume spanned by the points. Furthermore, we propose algorithms to sample from this volume-rescaled distribution when the data distribution is only known through an i.i.d sample.",,,,,,"Hsu, Daniel/0000-0002-3495-7113",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,944,953,,,,,,,,,,,,,,,,WOS:000509687900098,0
C,"Dieng, A; Liu, YM; Roy, S; Rudin, C; Volfovsky, A",,"Chaudhuri, K; Sugiyama, M",,"Dieng, Awa; Liu, Yameng; Roy, Sudeepa; Rudin, Cynthia; Volfovsky, Alexander",,,Interpretable Almost-Exact Matching for Causal Inference,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Matching methods are heavily used in the social and health sciences due to their interpretability. We aim to create the highest possible quality of treatment-control matches for categorical data in the potential outcomes framework. The method proposed in this work aims to match units on a weighted Hamming distance, taking into account the relative importance of the covariates; the algorithm aims to match units on as many relevant variables as possible. To do this, the algorithm creates a hierarchy of covariate combinations on which to match (similar to downward closure), in the process solving an optimization problem for each unit in order to construct the optimal matches. The algorithm uses a single dynamic program to solve all of the units' optimization problems simultaneously. Notable advantages of our method over existing matching procedures arc its high-quality interpretable matches, versatility in handling different data distributions that may have irrelevant variables, and ability to handle missing data by matching on as many available covariates as possible.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902051,0
C,"Hanneke, S; Yang, L",,"Chaudhuri, K; Sugiyama, M",,"Hanneke, Steve; Yang, Liu",,,Statistical Learning under Nonstationary Mixing Processes,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We study a special case of the problem of statistical learning without the i.i.d. assumption. Specifically, we suppose a learning method is presented with a sequence of data points, and required to make a prediction (e.g., a classification) for each one, and can then observe the loss incurred by this prediction. We go beyond traditional analyses, which have focused on stationary mixing processes or nonstationary product processes, by combining these two relaxations to allow nonstationary mixing processes. We are particularly interested in the case of 3-mixing processes, with the sum of changes in marginal distributions growing sublinearly in the number of samples. Under these conditions, we propose a learning method, and establish that for bounded VC subgraph classes, the cumulative excess risk grows sublinearly in the number of predictions, at a quantified rate.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901075,0
C,"Iyer, R; Bilmes, J",,"Chaudhuri, K; Sugiyama, M",,"Iyer, Rishabh; Bilmes, Jeff",,,Near Optimal Algorithms for Hard Submodular Programs with Discounted Cooperative Costs,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"In this paper, we investigate a class of sub modular problems which in general are very hard. These include minimizing a submodular cost function under combinatorial constraints, which include cuts, matchings, paths, etc., optimizing a submodular function under submodular cover and submodular knapsack constraints, and minimizing a ratio of submodular functions. All these problems appear in several real world problems but have hardness factors of Omega(root n) for general submodular cost functions. We show how we can achieve constant approximation factors when we restrict the cost functions to low rank sums of concave over modular functions. A wide variety of machine learning applications are very naturally modeled via this subclass of submodular functions. Our work therefore provides a tighter connection between theory and practice by enabling theoretically satisfying guarantees for a rich class of expressible, natural, and useful submodular cost models. We empirically demonstrate the utility of our models on real world problems of cooperative image matching and sensor placement with cooperative costs.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,276,285,,,,,,,,,,,,,,,,WOS:000509687900029,0
C,"Jankowiak, M; Karaletsos, T",,"Chaudhuri, K; Sugiyama, M",,"Jankowiak, Martin; Karaletsos, Theofanis",,,Pathwise Derivatives for Multivariate Distributions,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We exploit the link between the transport equation and derivatives of expectations to construct efficient pathwise gradient estimators for multivariate distributions. We focus on two main threads. First, we use null solutions of the transport equation to construct adaptive control variates that can be used to construct gradient estimators with reduced variance. Second, we consider the case of multivariate mixture distributions. In particular we show how to compute pathwise derivatives for mixtures of multivariate Normal distributions with arbitrary means and diagonal covariances. We demonstrate in a variety of experiments in the context of variational inference that our gradient estimators can outperform other methods, especially in high dimensions.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,333,342,,,,,,,,,,,,,,,,WOS:000509687900035,0
C,"Lopez-Lopera, AF; John, ST; Durrande, N",,"Chaudhuri, K; Sugiyama, M",,"Lopez-Lopera, Andres F.; John, S. T.; Durrande, Nicolas",,,Gaussian Process Modulated Cox Processes under Linear Inequality Constraints,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Gaussian process (GP) modulated Cox processes are widely used to model point patterns. Existing approaches require a mapping (link function) between the unconstrained GP and the positive intensity function. This commonly yields solutions that do not have a closed form or that are restricted to specific covariance functions. We introduce a novel finite approximation of GP-modulated Cox processes where positiveness conditions can be imposed directly on the GP, with no restrictions on the covariance function. Our approach can also ensure other types of inequality constraints (e.g. monotonicity, convexity), resulting in more versatile models that can be used for other classes of point processes (e.g. renewal processes). We demonstrate on both synthetic and real-world data that our framework accurately infers the intensity functions. Where monotonicity is a feature of the process, our ability to include this in the inference improves results.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902005,0
C,"Malik, D; Pananjady, A; Bhatia, K; Khamaru, K; Bartlett, PL; Wainwright, MJ",,"Chaudhuri, K; Sugiyama, M",,"Malik, Dhruv; Pananjady, Ashwin; Bhatia, Kush; Khamaru, Koulik; Bartlett, Peter L.; Wainwright, Martin J.",,,Derivative-Free Methods for Policy Optimization: Guarantees for Linear Quadratic Systems,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We study derivative-free methods for policy optimization over the class of linear policies. We focus on characterizing the convergence rate of a canonical stochastic, two-point, derivative-free method for linear-quadratic systems in which the initial state of the system is drawn at random. In particular, we show that for problems with effective dimension D, such a method converges to an epsilon-approximate solution within (O) over tilde (D/epsilon) steps, with multiplicative pre-factors that are explicit lower-order polynomial terms in the curvature parameters of the problem. Along the way, we also derive stochastic zero-order rates for a class of non-convex optimization problems.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902099,0
C,"Putta, SR; Shetty, A",,"Chaudhuri, K; Sugiyama, M",,"Putta, Sudeep Raja; Shetty, Abhishek",,,Exponential Weights on the Hypercube in Polynomial Time,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We study a general online linear optimization problem(OLO). At each round, a subset of objects from a fixed universe of n objects is chosen, and a linear cost associated with the chosen subset is incurred. To measure the performance of our algorithms, we use the notion of regret which is the difference between the total cost incurred over all iterations and the cost of the best fixed subset in hindsight. We consider Full Information and Bandit feedback for this problem. This problem is equivalent to OLO on the {0, 1}n hypercube. The Exp2 algorithm and its bandit variant are commonly used strategies for this problem. It was previously unknown if it is possible to run Exp2 on the hypercube in polynomial time. In this paper, we present a polynomial time algorithm called PolyExp for OLO on the hypercube. We show that our algorithm is equivalent Exp2 on {0, 1}n, Online Mirror Descent(OMD), Follow The Regularized Leader(FTRL) and Follow The Perturbed Leader(FTPL) algorithms. We show PolyExp achieves expected regret bound that is a factor of better than Exp2 in the full information setting under L adversarial losses. Because of the equivalence of these algorithms, this implies an improvement on Exp2's regret bound in full information. We also show matching regret lower bounds. Finally, we show how to use PolyExp on the {-1,+1}n hypercube, solving an open problem in Bubeck et al (COLT 2012).",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901099,0
C,"Rangi, A; Franceschetti, M",,"Chaudhuri, K; Sugiyama, M",,"Rangi, Anshuka; Franceschetti, Massimo",,,Online learning with feedback graphs and switching costs,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We study online learning when partial feedback information is provided following every action of the learning process, and the learner incurs switching costs for changing his actions. In this setting, the feedback information system can be represented by a graph, and previous works studied the expected regret of the learner in the case of a clique (Expert setup), or disconnected single loops (Multi-Armed Bandits (MAB)). This work provides a lower bound on the expected regret in the Partial Information (PI) setting, namely for general feedback graphs excluding the clique. Additionally, it shows that all algorithms that are optimal without switching costs are necessarily sub-optimal in the presence of switching costs, which motivates the need to design new algorithms. We propose two new algorithms: Threshold Based EXP3 and EXP3.SC. For the two special cases of symmetric PI setting and MAB, the expected regret of both of these algorithms is order optimal in the duration of the learning process. Additionally, Threshold Based EXP3 is order optimal in the switching cost, whereas EXP3.SC is not. Finally, empirical evaluations show that Threshold Based EXP3 outperforms the previously proposed order-optimal algorithms EXP3 SET in the presence of switching costs, and Batch EXP3 in the MAB setting with switching costs.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902050,0
C,"Seznec, J; Locatelli, A; Carpentier, A; Lazaric, A; Valko, M",,"Chaudhuri, K; Sugiyama, M",,"Seznec, Julien; Locatelli, Andrea; Carpentier, Alexandra; Lazaric, Alessandro; Valko, Michal",,,Rotting bandits are not harder than stochastic ones,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"In stochastic multi-armed bandits, the reward distribution of each arm is assumed to be stationary. This assumption is often violated in practice (e.g., in recommendation systems). where the reward of an arm may change whenever is selected, i.e., rested bandit setting. In this paper we consider the non-parametric rotting bandit setting. where rewards can only decrease. We introduce the filtering on expanding window average (FEW A) algorithm that constructs moving averages of increasing windows to identify arms that are more likely to return high rewards when pulled once more. We prove that for an unknown horizon T and without any knowledge on the decreasing behavior of the K arms, FEWA achieves problem-dependent regret bound of (O) over tilde (log (KT)), and a problem-independent one of (O) over tilde(root KT). Our result substantially improves over the algorithm of Levine (a al, (201 a), which suffers regret (O) over tilde ((KT2/3)-T-1/3). FEWA also matches known bounds for the stochastic bandit setting. thus showing that the rotting bandits are not harder. Finally, we report simulations confirming the theoretical improvements of FEWA.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902063,0
C,"Verma, A; Hanawal, MK; Szepesvari, C; Saligrama, V",,"Chaudhuri, K; Sugiyama, M",,"Verma, Arun; Hanawal, Manjesh K.; Szepesvari, Csaba; Saligrama, Venkatesh",,,Online Algorithm for Unsupervised Sensor Selection,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"In many security and healthcare systems, the detection and diagnosis systems use a sequence of sensors/tests. Each test outputs a prediction of the latent state and carries an inherent cost. However, the correctness of the predictions cannot be evaluated due to unavailability of the ground-truth annotations. Our objective is to learn strategies for selecting a test that gives the best trade-off between accuracy and costs in such unsupervised sensor selection (USS) problems. Clearly, learning is feasible only if ground truth can be inferred (explicitly or implicitly) from the problem structure. It is observed that this happens if the problem satisfies the 'Weak Dominance' (WD) property. We set up the USS problem as a stochastic partial monitoring problem and develop an algorithm with sub-linear regret under the WD property. We argue that our algorithm is optimal and evaluate its performance on problem instances generated from synthetic and real-world datasets.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687903023,0
C,"Yang, KK; Chen, YX; Lee, A; Yue, YS",,"Chaudhuri, K; Sugiyama, M",,"Yang, Kevin K.; Chen, Yuxin; Lee, Alycia; Yue, Yisong",,,Batched Stochastic Bayesian Optimization via Combinatorial Constraints Design,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"In many high-throughput experimental design settings, such as those common in biochemical engineering, batched queries are often more cost effective than one-by-one sequential queries. Furthermore, it is often not possible to directly choose items to query. Instead, the experimenter specifies a set of constraints that generates a library of possible items, which are then selected stochastically. Motivated by these considerations, we investigate Batched Stochastic Bayesian Optimization (BSBO), a novel Bayesian optimization scheme for choosing the constraints in order to guide exploration towards items with greater utility. We focus on site saturation mutagenesis, a prototypical setting of BSBO in biochemical engineering, and propose a natural objective function for this problem. Importantly, we show that our objective function can be efficiently decomposed as a difference of submodular functions (DS), which allows us to employ DS optimization tools to greedily identify sets of constraints that increase the likelihood of finding items with high utility. Our experimental results show that our algorithm outperforms common heuristics on both synthetic and two real protein datasets.",,,,,,"Chen, Yuxin/0000-0003-2133-140X",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687903048,0
C,"Zhang, J; Raman, P; Ji, SH; Yu, HF; Vishwanathan, SVN; Dhillon, IS",,"Chaudhuri, K; Sugiyama, M",,"Zhang, Jiong; Raman, Parameswaran; Ji, Shihao; Yu, Hsiang-Fu; Vishwanathan, S. V. N.; Dhillon, Inderjit S.",,,Extreme Stochastic Variational Inference: Distributed Inference for Large Scale Mixture Models,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Mixture of exponential family models are among the most fundamental and widely used statistical models. Stochastic variational inference (SVI), the state-of-the-art algorithm for parameter estimation in such models is inherently serial. Moreover, it requires the parameters to fit in the memory of a single processor; this poses serious limitations on scalability when the number of parameters is in billions. In this paper, we present extreme stochastic variational inference (ESVI), a distributed, asynchronous and lock-free algorithm to perform variational inference for mixture models on massive real world datasets. ESVI overcomes the limitations of SVI by requiring that each processor only access a subset of the data and a subset of the parameters, thus providing data and model parallelism simultaneously. Our empirical study demonstrates that ESVI not only outperforms VI and SVI in wallclock-time, but also achieves a better quality solution. To further speed up computation and save memory when fitting large number of topics, we propose a variant ESVI-TOPK which maintains only the top-k important topics. Empirically, we found that using top 25% topics suffices to achieve the same accuracy as storing all the topics.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,935,943,,,,,,,,,,,,,,,,WOS:000509687900097,0
C,"Alvarez-Melis, D; Jaakkola, TS; Jegelka, S",,"Storkey, A; PerezCruz, F",,"Alvarez-Melis, David; Jaakkola, Tommi S.; Jegelka, Stefanie",,,Structured Optimal Transport,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Optimal Transport has recently gained interest in machine learning for applications ranging from domain adaptation to sentence similarities or deep learning. Yet, its ability to capture frequently occurring structure beyond the ground metric is limited. In this work, we develop a nonlinear generalization of (discrete) optimal transport that is able to reflect much additional structure. We demonstrate how to leverage the geometry of this new model for fast algorithms, and explore connections and properties. Illustrative experiments highlight the benefit of the induced structured couplings for tasks in domain adaptation and natural language processing.",,,,,"Alvarez-Melis, David/AAV-1099-2021",/0000-0002-9591-8986,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300185,0
C,"Banijamali, E; Shu, R; Ghavamzadeh, M; Bui, H; Ghodsi, A",,"Storkey, A; PerezCruz, F",,"Banijamali, Ershad; Shu, Rui; Ghavamzadeh, Mohammad; Bui, Hung; Ghodsi, Ali",,,Robust Locally-Linear Controllable Embedding,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Embed-to-control (E2C) [17] is a model for solving high-dimensional optimal control problems by combining variational auto-encoders with locally-optimal controllers. However, the E2C model suffers from two major drawbacks: 1) its objective function does not correspond to the likelihood of the data sequence and 2) the variational encoder used for embedding typically has large variational approximation error, especially when there is noise in the system dynamics. In this paper, we present a new model for learning robust locally-linear controllable embedding (RCE). Our model directly estimates the predictive conditional density of the future observation given the current one, while introducing the bottleneck [11] between the current and future observations. Although the bottleneck provides a natural embedding candidate for control, our RCE model introduces additional specific structures in the generative graphical model so that the model dynamics can be robustly linearized. We also propose a principled variational approximation of the embedding posterior that takes the future observation into account, and thus, makes the variational approximation more robust against the noise. Experimental results show that RCE outperforms the E2C model, and does so significantly when the underlying dynamics is noisy.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300183,0
C,"Gottesman, O; Pan, WW; Doshi-Velez, F",,"Storkey, A; PerezCruz, F",,"Gottesman, Omer; Pan, Weiewei; Doshi-Velez, Finale",,,Weighted Tensor Decomposition for Learning Latent Variables with Partial Data,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Tensor decomposition methods are popular tools for learning latent variables given only lower-order moments of the data. However, the standard assumption is that we have sufficient data to estimate these moments to high accuracy. In this work, we consider the case in which certain dimensions of the data are not always observed-common in applied settings, where not all measurements may be taken for all observations-resulting in moment estimates of varying quality. We derive a weighted tensor decomposition approach that is computationally as efficient as the non-weighted approach, and demonstrate that it outperforms methods that do not appropriately leverage these less-observed dimensions.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300174,0
C,"Kong, DH; Bondell, H; Shen, WN",,"Storkey, A; PerezCruz, F",,"Kong, Dehan; Bondell, Howard; Shen, Weining",,,Outlier Detection and Robust Estimation in Nonparametric Regression,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"This paper studies outlier detection and robust estimation for nonparametric regression problems. We propose to include a subject-specific mean shift parameter for each data point such that a nonzero parameter will identify its corresponding data point as an outlier. We adopt a regularization approach by imposing a roughness penalty on the regression function and a shrinkage penalty on the mean shift parameter. An efficient algorithm has been proposed to solve the double penalized regression problem. We discuss a data-driven simultaneous choice of two regularization parameters based on a combination of generalized cross validation and modified Bayesian information criterion. We show that the proposed method can consistently detect the outliers. In addition, we obtain minimax-optimal convergence rates for both the regression function and the mean shift parameter under regularity conditions. The estimation procedure is shown to enjoy the oracle property in the sense that the convergence rates agree with the minimax-optimal rates when the outliers (or regression function) are known in advance. Numerical results demonstrate that the proposed method has desired performance in identifying outliers under different scenarios.",,,,,,"Bondell, Howard/0000-0001-7743-0840; Shen, Weining/0000-0003-3137-1085",,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300023,0
C,"Liau, D; Price, E; Song, Z; Yang, G",,"Storkey, A; PerezCruz, F",,"Liau, David; Price, Eric; Song, Zhao; Yang, Ger",,,Stochastic Multi-armed Bandits in Constant Space,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"We consider the stochastic bandit problem in the sublinear space setting, where one cannot record the win-loss record for all K arms. We give an algorithm using O(1) words of space with regret Sigma(K)(i=1) 1/Delta(i) log Delta(i)/Delta log T where Delta(i) is the gap between the best arm and arm i and Delta is the gap between the best and the second-best arms. If the rewards are bounded away from 0 and 1, this is within an O(log 1/Delta) factor of the optimum regret possible without space constraints.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300041,0
C,"Mokhtari, A; Hassani, H; Karbasi, A",,"Storkey, A; PerezCruz, F",,"Mokhtari, Aryan; Hassani, Hamed; Karbasi, Amin",,,Conditional Gradient Method for Stochastic Submodular Maximization: Closing the Gap,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"In this paper, we study the problem of constrained and stochastic continuous submodular maximization. Even though the objective function is not concave (nor convex) and is defined in terms of an expectation, we develop a variant of the conditional gradient method, called Stochastic Continuous Greedy, which achieves a tight approximation guarantee. More precisely, for a monotone and continuous DR-submodular function and subject to a general convex body constraint, we prove that Stochastic Continuous Greedy achieves a [(1- 1/e)OPT-epsilon] guarantee (in expectation) with O(1/epsilon(3)) stochastic gradient computations. This guarantee matches the known hardness results and closes the gap between deterministic and stochastic continuous submodular maximization. By using stochastic continuous optimization as an interface, we also provide the first (1- 1/e) tight approximation guarantee for maximizing a monotone but stochastic submodular set function subject to a general matroid constraint.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300197,0
C,"Rudin, C; Wang, YN",,"Storkey, A; PerezCruz, F",,"Rudin, Cynthia; Wang, Yining",,,Direct Learning to Rank and Rerank,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Learning-to-rank techniques have proven to be extremely useful for prioritization problems, where we rank items in order of their estimated probabilities, and dedicate our limited resources to the top-ranked items. This work exposes a serious problem with the state of learning-to-rank algorithms, which is that they are based on convex proxies that lead to poor approximations. We then discuss the possibility of exact reranking algorithms based on mathematical programming. We prove that a relaxed version of the exact problem has the same optimal solution, and provide an empirical analysis.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300082,0
C,"Silander, T; Leppa-aho, J; Jaasaari, E; Roos, T",,"Storkey, A; PerezCruz, F",,"Silander, Tomi; Leppa-aho, Janne; Jaasaari, Elias; Roos, Teemu",,,Quotient Normalized Maximum Likelihood Criterion for Learning Bayesian Network Structures,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"We introduce an information theoretic criterion for Bayesian network structure learning which we call quotient normalized maximum likelihood (qNML). In contrast to the closely related factorized normalized maximum likelihood criterion, qNML satisfies the property of score equivalence. It is also decomposable and completely free of adjustable hyperparameters. For practical computations, we identify a remarkably accurate approximation proposed earlier by Szpankowski and Weinberger. Experiments on both simulated and real data demonstrate that the new criterion leads to parsimonious models with good predictive accuracy.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300100,0
C,"Wang, YC; Theodorou, E; Verma, A; Song, L",,"Storkey, A; PerezCruz, F",,"Wang, Yichen; Theodorou, Evangelos; Verma, Apurv; Song, Le",,,A Stochastic Differential Equation Framework for Guiding Online User Activities in Closed Loop,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Recently, there is a surge of interest in using point processes to model continuous-time user activities. This framework has resulted in novel models and improved performance in diverse applications. However, most previous works focus on the open loop setting where learned models are used for predictive tasks. Typically, we are interested in the closed loop setting where a policy needs to be learned to incorporate user feedbacks and guide user activities to desirable states. Although point processes have good predictive performance, it is not clear how to use them for the challenging closed loop activity guiding task. In this paper, we propose a framework to reformulate point processes into stochastic differential equations, which allows us to extend methods from stochastic optimal control to address the activity guiding problem. We also design an efficient algorithm, and show that our method guides user activities to desired states more effectively than state-of-arts.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300113,0
C,"Zhang, HJ; Chen, PH; Ramadge, PJ",,"Storkey, A; PerezCruz, F",,"Zhang, Hejia; Chen, Po-Hsuan; Ramadge, Peter J.",,,Transfer Learning on fMRI Datasets,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"We explore transferring learning between fMRI datasets. A method is introduced to improve prediction accuracy on a primary fMRI dataset by jointly learning a model using other secondary fMRI datasets. We assume the secondary datasets are directly or indirectly linked to the primary dataset through sets of partially shared subjects. This method is particularly useful when the primary dataset is small. Using six fMRI datasets linked by various subsets of shared subjects, we show that the method yields improved performance in various predictive tasks. Our tests are performed on a variety of regions of interest in the brain and across various stimuli.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300063,0
C,"Zhang, RY; Li, CY; Chen, CY; Carin, L",,"Storkey, A; PerezCruz, F",,"Zhang, Ruiyi; Li, Chunyuan; Chen, Changyou; Carin, Lawrence",,,Learning Structural Weight Uncertainty for Sequential Decision-Making,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Learning probability distributions on the weights of neural networks (NNs) has recently proven beneficial in many applications. Bayesian methods, such as Stein variational gradient descent (SVGD), offer an elegant framework to reason about NN model uncertainty. However, by assuming independent Gaussian priors for the individual NN weights (as often applied), SVGD does not impose prior knowledge that there is often structural information (dependence) among weights. We propose efficient posterior learning of structural weight uncertainty, within an SVGD framework, by employing matrix variate Gaussian priors on NN parameters. We further investigate the learned structural uncertainty in sequential decision-making problems, including contextual bandits and reinforcement learning. Experiments on several synthetic and real datasets indicate the superiority of our model, compared with state-of-the-art methods.",,,,,"Zhang, Ruiyi/AAB-8402-2021","Carin, Lawrence/0000-0001-6277-7948",,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300119,0
C,"Heinze, C; McWilliams, B; Meinshausen, N",,"Gretton, A; Robert, CC",,"Heinze, Christina; McWilliams, Brian; Meinshausen, Nicolai",,,Dual-Loco: Distributing Statistical Estimation Using Random Projections,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"We present Dual-Loco, a communication-efficient algorithm for distributed statistical estimation. Dual-Loco assumes that the data is distributed across workers according to the features rather than the samples. It requires only a single round of communication where low-dimensional random projections are used to approximate the dependencies between features available to different workers. We show that Dual-Loco has bounded approximation error which only depends weakly on the number of workers. We compare Dual-Loco against a state-of-the-art distributed optimization method on a variety of real world datasets and show that it obtains better speedups while retaining good accuracy. In particular, Dual-Loco allows for fast cross validation as only part of the algorithm depends on the regularization parameter.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,875,883,,,,,,,,,,,,,,,,WOS:000508662100095,0
C,"Lakshminarayanan, B; Roy, DM; Teh, YW",,"Gretton, A; Robert, CC",,"Lakshminarayanan, Balaji; Roy, Daniel M.; Teh, Yee Whye",,,Mondrian Forests for Large-Scale Regression when Uncertainty Matters,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Many real-world regression problems demand a measure of the uncertainty associated with each prediction. Standard decision forests deliver efficient state-of-the-art predictive performance, but high-quality uncertainty estimates are lacking. Gaussian processes (GPs) deliver uncertainty estimates, but scaling GPs to large-scale data sets comes at the cost of approximating the uncertainty estimates. We extend Mondrian forests, first proposed by Lakshminarayanan et al. (2014) for classification problems, to the large-scale non-parametric regression setting. Using a novel hierarchical Gaussian prior that dovetails with the Mondrian forest framework, we obtain principled uncertainty estimates, while still retaining the computational advantages of decision forests. Through a combination of illustrative examples, real-world large-scale datasets, and Bayesian optimization benchmarks, we demonstrate that Mondrian forests outperform approximate GPs on large-scale regression tasks and deliver better-calibrated uncertainty assessments",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,1478,1487,,,,,,,,,,,,,,,,WOS:000508662100160,0
C,"Moritz, P; Nishihara, R; Jordan, MI",,"Gretton, A; Robert, CC",,"Moritz, Philipp; Nishihara, Robert; Jordan, Michael, I",,,A Linearly-Convergent Stochastic L-BFGS Algorithm,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"We propose a new stochastic L-BFGS algorithm and prove a linear convergence rate for strongly convex and smooth functions. Our algorithm draws heavily from a recent stochastic variant of L-BFGS proposed in Byrd et al. (2014) as well as a recent approach to variance reduction for stochastic gradient descent from Johnson and Zhang (2013). We demonstrate experimentally that our algorithm performs well on large-scale convex and non-convex optimization problems, exhibiting linear convergence and rapidly solving the optimization problems to high levels of precision. Furthermore, we show that our algorithm performs well for a wide-range of step sizes, often differing by several orders of magnitude.",,,,,,"Jordan, Michael/0000-0001-8935-817X",,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,249,258,,,,,,,,,,,,,,,,WOS:000508662100028,0
C,"Aksoylar, C; Saligrama, V",,"Kaski, S; Corander, J",,"Aksoylar, Cem; Saligrama, Venkatesh",,,Information-Theoretic Characterization of Sparse Recovery,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"We formulate sparse support recovery as a salient set identification problem and use information-theoretic analyses to characterize the recovery performance and sample complexity. We consider a very general framework where we are not restricted to linear models or specific distributions. We state non-asymptotic bounds on recovery probability and a tight mutual information formula for sample complexity. We evaluate our bounds for applications such as sparse linear regression and explicitly characterize effects of correlation or noisy features on recovery performance. We show improvements upon previous work and identify gaps between the performance of recovery algorithms and fundamental information. This illustrates a trade-off between computational complexity and sample complexity, contrasting the recovery of the support as a discrete object with signal estimation approaches.",,,,,,"Saligrama, Venkatesh/0000-0002-0675-2268",,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,38,46,,,,,,,,,,,,,,,,WOS:000508355800005,0
C,"Park, Y; Carvalho, CM; Ghosh, J",,"Kaski, S; Corander, J",,"Park, Yubin; Carvalho, Carlos M.; Ghosh, Joydeep",,,"LAMORE: A Stable, Scalable Approach to Latent Vector Autoregressive Modeling of Categorical Time Series","ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"Latent vector autoregressive models for categorical time series have a wide range of potential applications from marketing research to healthcare analytics. However, a brute-force particle filter implementation of the Expectation-Maximization (EM) algorithm often fails to estimate the maximum likelihood parameters due to the Monte Carlo approximation of the E-step and multiple local optima of the log-likelihood function. This paper proposes two auxiliary techniques that help stabilize and calibrate the estimated parameters. These two techniques, namely asymptotic mean regularization and low-resolution augmentation, do not require any additional parameter tuning, and can be implemented by modifying the brute-force EM algorithm. Experiments with simulated data show that the proposed techniques effectively stabilize the parameter estimation process. Also, experimental results using Medicare and MIMIC-II datasets illustrate various potential applications of the proposed model and methods.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,733,742,,,,,,,,,,,,,,,,WOS:000508355800081,0
C,"Parker, J; Engler, H",,"Kaski, S; Corander, J",,"Parker, Jon; Engler, Hans",,,Spoofing Large Probability Mass Functions to Improve Sampling Times and Reduce Memory Costs,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,Sampling from a probability mass function (PMF) has many applications in modern computing. This paper presents a novel lossy compression method intended for large (O(10(5))) dense PMFs that speeds up the sampling process and guarantees high fidelity sampling. This compression method closely approximates an input PMF P with another PMF Q that is easy to store and sample from. All samples are drawn from Q as opposed to the original input distribution P. We say that Q spoofs P while this switch is difficult to detect with a statistical test. The lifetime of Q is the sample size required to detect the switch from P to Q. We show how to compute a single PMF's lifetime and present numeric examples demonstrating compression rates ranging from 62% to 75% when the input PMF is not sorted and 88% to 99% when the input is already sorted. These examples have speed ups ranging from 1.47 to 2.82 compared to binary search sampling.,,,,,"Engler, Hans/HII-2937-2022",,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,743,750,,,,,,,,,,,,,,,,WOS:000508355800082,0
C,"Vats, D; Nowak, RD; Baraniuk, RG",,"Kaski, S; Corander, J",,"Vats, Divyanshu; Nowak, Robert D.; Baraniuk, Richard G.",,,Active Learning for Undirected Graphical Model Selection,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"This paper studies graphical model selection, i.e., the problem of estimating a graph of statistical relationships among a collection of random variables. Conventional graphical model selection algorithms are passive, i.e., they require all the measurements to have been collected before processing begins. We propose an active learning algorithm that uses junction tree representations to adapt future measurements based on the information gathered from prior measurements. We prove that, under certain conditions, our active learning algorithm requires fewer scalar measurements than any passive algorithm to reliably estimate a graph. A range of numerical results validate our theory and demonstrates the benefits of active learning.",,,,,"Baraniuk, Richard/ABA-1743-2020",,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,958,967,,,,,,,,,,,,,,,,WOS:000508355800106,0
C,"Adamczewski, K; Park, MJ",,"Banerjee, A; Fukumizu, K",,"Adamczewski, Kamil; Park, Mijung",,,Dirichlet Pruning for Neural Network Compression,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We introduce Dirichlet pruning, a novel post-processing technique to transform a large neural network model into a compressed one. Dirichlet pruning is a form of structured pruning which assigns the Dirichlet distribution over each layer's channels in convolutional layers (or neurons in fully-connected layers), and estimates the parameters of the distribution over these units using variational inference. The learned distribution allows us to remove unimportant units, resulting in a compact architecture containing only crucial features for a task at hand. The number of newly introduced Dirichlet parameters is only linear in the number of channels, which allows for rapid training, requiring as little as one epoch to converge. We perform extensive experiments, in particular on larger architectures such as VGG and ResNet (94% and 72% compression rate, respectively) where our method achieves the stateof-the-art compression performance and provides interpretable features as a by-product.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804039,0
C,"Akyildiz, OD; van den Burg, GJJ; Damoulas, T; Steel, MFJ",,"Banerjee, A; Fukumizu, K",,"Akyildiz, Omer Deniz; van den Burg, Gerrit J. J.; Damoulas, Theodoros; Steel, Mark F. J.",,,Probabilistic Sequential Matrix Factorization,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We introduce the probabilistic sequential matrix factorization (PSMF) method for factorizing time-varying and non-stationary datasets consisting of high-dimensional time-series. In particular, we consider nonlinear Gaussian state-space models where sequential approximate inference results in the factorization of a data matrix into a dictionary and timevarying coefficients with potentially nonlinear Markovian dependencies. The assumed Markovian structure on the coefficients enables us to encode temporal dependencies into a low-dimensional feature space. The proposed inference method is solely based on an approximate extended Kalman filtering scheme, which makes the resulting method particularly efficient. PSMF can account for temporal nonlinearities and, more importantly, can be used to calibrate and estimate generic differentiable nonlinear subspace models. We also introduce a robust version of PSMF, called rPSMF, which uses Student-t filters to handle model misspecification. We show that PSMF can be used in multiple contexts: modeling time series with a periodic subspace, robustifying changepoint detection methods, and imputing missing data in several high-dimensional time-series, such as measurements of pollutants across London.",,,,,"Steel, Mark/M-9663-2019","Steel, Mark/0000-0001-9858-9279",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804022,0
C,"Ambrogioni, L; Lin, KT; Fertig, E; Vikram, S; Hinne, M; Moore, D; van Gerven, M",,"Banerjee, A; Fukumizu, K",,"Ambrogioni, Luca; Lin, Kate; Fertig, Emily; Vikram, Sharad; Hinne, Max; Moore, Dave; van Gerven, Marcel",,,Automatic structured variational inference,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Stochastic variational inference offers an attractive option as a default method for differentiable probabilistic programming. However, the performance of the variational approach depends on the choice of an appropriate variational family. Here, we introduce automatic structured variational inference (ASVI), a fully automated method for constructing structured variational families, inspired by the closed-form update in conjugate Bayesian models. These convex-update families incorporate the forward pass of the input probabilistic program and can therefore capture complex statistical dependencies. Convex-update families have the same space and time complexity as the input probabilistic program and are therefore tractable for a very large family of models including both continuous and discrete variables. We validate our automatic variational method on a wide range of low- and high-dimensional inference problems. We find that ASVI provides a clear improvement in performance when compared with other popular approaches such as the mean-field approach and inverse autoregressive flows. We provide an open source implementation of ASVI in TensorFlow Probability.",,,,,"Hinne, Max/ABD-1486-2021","Hinne, Max/0000-0002-9279-6725",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,676,+,,,,,,,,,,,,,,,,WOS:000659893800076,0
C,"Camuto, A; Willetts, M; Roberts, S; Holmes, C; Rainforth, T",,"Banerjee, A; Fukumizu, K",,"Camuto, Alexander; Willetts, Matthew; Roberts, Stephen; Holmes, Chris; Rainforth, Tom",,,Towards a Theoretical Understanding of the Robustness of Variational Autoencoders,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We make inroads into understanding the robustness of Variational Autoencoders (VAEs) to adversarial attacks and other input perturbations. While previous work has developed algorithmic approaches to attacking and defending VAEs, there remains a lack of formalization for what it means for a VAE to be robust. To address this, we develop a novel criterion for robustness in probabilistic models: r-robustness. We then use this to construct the first theoretical results for the robustness of VAEs, deriving margins in the input space for which we can provide guarantees about the resulting reconstruction. Informally, we are able to define a region within which any perturbation will produce a reconstruction that is similar to the original reconstruction. To support our analysis, we show that VAEs trained using disentangling methods not only score well under our robustness metrics, but that the reasons for this can be interpreted through our theoretical results.",,,,,,"Rainforth, Tom/0000-0001-7939-4230",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804031,0
C,"Charikar, M; Hu, LJ",,"Banerjee, A; Fukumizu, K",,"Charikar, Moses; Hu, Lunjia",,,Approximation Algorithms for Orthogonal Non-negative Matrix Factorization,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"In the non-negative matrix factorization (NMF) problem, the input is an mxn matrix M with non-negative entries and the goal is to factorize it as M approximate to AW. The m x k matrix A and the k x n matrix W are both constrained to have non-negative entries. This is in contrast to singular value decomposition, where the matrices A and W can have negative entries but must satisfy the orthogonality constraint: the columns of A are orthogonal and the rows of W are also orthogonal. The orthogonal non-negative matrix factorization (ONMF) problem imposes both the non-negativity and the orthogonality constraints, and previous work showed that it leads to better performances than NMF on many clustering tasks. We give the first constant-factor approximation algorithm for ONMF when one or both of A and W are subject to the orthogonality constraint. We also show an interesting connection to the correlation clustering problem on bipartite graphs. Our experiments on synthetic and real-world data show that our algorithm achieves similar or smaller errors compared to previous ONMF algorithms while ensuring perfect orthogonality (many previous algorithms do not satisfy the hard orthogonality constraint).",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803030,0
C,"Ding, DS; Wei, XH; Yang, ZR; Wang, ZR; Jovanovic, MR",,"Banerjee, A; Fukumizu, K",,"Ding, Dongsheng; Wei, Xiaohan; Yang, Zhuoran; Wang, Zhaoran; Jovanovic, Mihailo R.",,,Provably Efficient Safe Exploration via Primal-Dual Policy Optimization,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We study the safe reinforcement learning problem using the constrained Markov decision processes in which an agent aims to maximize the expected total reward subject to a safety constraint on the expected total value of a utility function. We focus on an episodic setting with the function approximation where the Markov transition kernels have a linear structure but do not impose any additional assumptions on the sampling model. Designing safe reinforcement learning algorithms with provable computational and statistical efficiency is particularly challenging under this setting because of the need to incorporate both the safety constraint and the function approximation into the fundamental exploitation/exploration tradeoff. To this end, we present an Optimistic Primal-Dual Proximal Policy OPtimization (OPDOP) algorithm where the value function is estimated by combining the least-squares policy evaluation and an additional bonus term for safe exploration. We prove that the proposed algorithm achieves an (O) over tilde (dH(2.5)root T) regret and an (O) over tilde (dH(2.5)root T) constraint violation, where d is the dimension of the feature mapping, H is the horizon of each episode, and T is the total number of steps. These bounds hold when the reward/utility functions are fixed but the feedback after each episode is bandit. Our bounds depend on the capacity of the state-action space only through the dimension of the feature mapping and thus our results hold even when the number of states goes to infinity. To the best of our knowledge, we provide the first provably efficient online policy optimization algorithm for constrained Markov decision processes in the function approximation setting, with safe exploration.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804002,0
C,"Ghazi, B; Kumar, R; Manurangsi, P; Nguyen, T",,"Banerjee, A; Fukumizu, K",,"Ghazi, Badih; Kumar, Ravi; Manurangsi, Pasin; Thao Nguyen",,,Robust and Private Learning of Halfspaces,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"In this work, we study the trade-off between differential privacy and adversarial robustness under L-2-perturbations in the context of learning halfspaces. We prove nearly tight bounds on the sample complexity of robust private learning of halfspaces for a large regime of parameters. A highlight of our results is that robust and private learning is harder than robust or private learning alone. We complement our theoretical analysis with experimental results on the MNIST and USPS datasets, for a learning algorithm that is both differentially private and adversarially robust.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801092,0
C,"Guo, XY; Kulkarni, J; Li, S; Xian, JY",,"Banerjee, A; Fukumizu, K",,"Guo, Xiangyu; Kulkarni, Janardhan; Li, Shi; Xian, Jiayi",,,"Consistent k-Median: Simpler, Better and Robust",24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"In this paper we introduce and study the online consistent k-clustering with outliers problem, generalizing the non-outlier version of the problem studied by Lattanzi and Vassilvitskii (2017). We show that a simple localsearch based online algorithm can give a bicriteria constant approximation for the problem with O(k(2) log(2) (nD)) swaps of medians (recourse) in total, where D is the diameter of the metric. When restricted to the problem without outliers, our algorithm is simpler, deterministic and gives better approximation ratio and recourse, compared to that of (Lattanzi and Vassilvitskii, 2017).",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801040,0
C,"Kamath, P; Tangella, A; Sutherland, DJ; Srebro, N",,"Banerjee, A; Fukumizu, K",,"Kamath, Pritish; Tangella, Akilesh; Sutherland, Danica J.; Srebro, Nathan",,,Does Invariant Risk Minimization Capture Invariance?,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We show that the Invariant Risk Minimization (IRM) formulation of Arjovsky et al. (2019) can fail to capture natural invariances, at least when used in its practical linear form, and even on very simple problems which directly follow the motivating examples for IRM. This can lead to worse generalization on new environments, even when compared to unconstrained ERM. The issue stems from a significant gap between the linear variant (as in their concrete method IRMv1) and the full non-linear IRM formulation. Additionally, even when capturing the right invariances, we show that it is possible for IRM to learn a sub-optimal predictor, due to the loss function not being invariant across environments. The issues arise even when measuring invariance on the population distributions, but are exacerbated by the fact that IRM is extremely fragile to sampling.",,,,,"Kamath, Pritish/ABF-1354-2021",,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804086,0
C,"Lee, C; van der Schaar, M",,"Banerjee, A; Fukumizu, K",,"Lee, Changhee; van der Schaar, Mihaela",,,A Variational Information Bottleneck Approach to Multi-Omics Data Integration,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Integration of data from multiple omics techniques is becoming increasingly important in biomedical research. Due to non-uniformity and technical limitations in omics platforms, such integrative analyses on multiple omics, which we refer to as views, involve learning from incomplete observations with various view-missing patterns. This is challenging because i) complex interactions within and across observed views need to be properly addressed for optimal predictive power and ii) observations with various view-missing patterns need to be flexibly integrated. To address such challenges, we propose a deep variational information bottleneck (IB) approach for incomplete multi-view observations. Our method applies the IB framework on marginal and joint representations of the observed views to focus on intra-view and interview interactions that are relevant for the target. Most importantly, by modeling the joint representations as a product of marginal representations, we can efficiently learn from observed views with various view-missing patterns. Experiments on real-world datasets show that our method consistently achieves gain from data integration and outperforms state-of-the-art benchmarks.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801082,0
C,"Lu, YY; Meisami, A; Tewari, A",,"Banerjee, A; Fukumizu, K",,"Lu, Yangyi; Meisami, Amirhossein; Tewari, Ambuj",,,Low-Rank Generalized Linear Bandit Problems,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"In a low-rank linear bandit problem, the expected reward of an action (represented by a matrix of size d(1) x d(2)) is the inner product between the action and an unknown low-rank matrix Theta*. We propose an algorithm based on a novel combination of online-to-confidence-set conversion (Abbasi-Yadkori et al., 2012) and the exponentially weighted average forecaster constructed by a covering of low-rank matrices. In T rounds, our algorithm achieves O ((d(1) + d(2))(3/2)root rT) regret that improves upon the standard linear bandit regret bound of O (d(1)d(2)root T) when the rank of Theta*: r << min{d(1), d(2)}. We also extend our algorithmic approach to the generalized linear setting to get an algorithm which enjoys a similar bound under regularity conditions on the link function. To get around the computational intractability of covering based approaches, we propose an efficient algorithm by extending the Explore-Subspace-Then-Refine algorithm of Jun et al. (2019). Our efficient algorithm achieves O ((d(1) + d(2))(3/2)root rT) regret under a mild condition on the action set X and the r-th singular value of Theta*. Our upper bounds match the conjectured lower bound of Jun et al. (2019) for a subclass of low-rank linear bandit problems. Further, we show that existing lower bounds for the sparse linear bandit problem strongly suggest that our regret bounds are unimprovable. To complement our theoretical contributions, we also conduct experiments to demonstrate that our algorithm can greatly outperform the performance of the standard linear bandit approach when Theta* is low-rank.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,460,468,,,,,,,,,,,,,,,,WOS:000659893800052,0
C,"Mimori, T; Sasada, K; Matsui, H; Sato, I",,"Banerjee, A; Fukumizu, K",,"Mimori, Takahiro; Sasada, Keiko; Matsui, Hirotaka; Sato, Issei",,,Diagnostic Uncertainty Calibration: Towards Reliable Machine Predictions in Medical Domain,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We propose an evaluation framework for class probability estimates (CPEs) in the presence of label uncertainty, which is commonly observed as diagnosis disagreement between experts in the medical domain. We also formalize evaluation metrics for higher-order statistics, including inter-rater disagreement, to assess predictions on label uncertainty. Moreover, we propose a novel post-hoc method called alpha-calibration, that equips neural network classifiers with calibrated distributions over CPEs. Using synthetic experiments and a large-scale medical imaging application, we show that our approach significantly enhances the reliability of uncertainty estimates: disagreement probabilities and posterior CPEs.",,,,,"Matsui, Hirotaka/AED-6669-2022","Matsui, Hirotaka/0000-0002-6266-3227",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804042,0
C,"Panigrahy, R; Wang, X; Zaheer, M",,"Banerjee, A; Fukumizu, K",,"Panigrahy, Rina; Wang, Xin; Zaheer, Manzil",,,Sketch based Memory for Neural Networks,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Deep learning has shown tremendous success on a variety of problems. However, unlike traditional computational paradigm, most neural networks do not have access to a memory, which might be hampering its ability to scale to large data structures such as graphs, lookup-tables, databases. We propose a theoretical framework for a neural architecture where sketch based memory is integrated into a neural network in a uniform manner at every layer. This architecture supplements a neural layer by information accessed from the memory before feeding it to the next layer, thereby significantly expanding the capacity of the network to solve larger problem instances. We show theoretically that problems involving key-value lookup that are traditionally stored in standard databases can now be solved using neural networks augmented by our memory architecture. We also show that our memory layer can be viewed as a kernel function. We show benefits on diverse problems such as long tail image classification, language model, large graph multi hop traversal, etc. arguing that they are all build upon the classical key-value lookup problem (or the variant where the keys may be fuzzy).",,,,,"Zaheer, Manzil/ABG-6249-2021",,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803079,0
C,"Vo, TV; Wei, PF; Bergsma, W; Leong, TY",,"Banerjee, A; Fukumizu, K",,"Vo, Thanh Vinh; Wei, Pengfei; Bergsma, Wicher; Leong, Tze Yun",,,Causal Modeling with Stochastic Confounders,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"This work extends causal inference in temporal models with stochastic confounders. We propose a new approach to variational estimation of causal inference based on a representer theorem with a random input space. We estimate causal effects involving latent confounders that may be interdependent and time-varying from sequential, repeated measurements in an observational study. Our approach extends current work that assumes independent, non-temporal latent confounders with potentially biased estimators. We introduce a simple yet elegant algorithm without parametric specification on model components. Our method avoids the need for expensive and careful parameterization in deploying complex models, such as deep neural networks in existing approaches, for causal inference and analysis. We demonstrate the effectiveness of our approach on various benchmark temporal datasets.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803063,0
C,"Yin, M; Bai, Y; Wang, YX",,"Banerjee, A; Fukumizu, K",,"Yin, Ming; Bai, Yu; Wang, Yu-Xiang",,,Near-Optimal Provable Uniform Convergence in Offine Policy Evaluation for Reinforcement Learning,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"The problem of Offine Policy Evaluation (OPE) in Reinforcement Learning (RL) is a critical step towards applying RL in real life applications. Existing work on OPE mostly focus on evaluating a fixed target policy pi, which does not provide useful bounds for of offine policy learning as pi will then be data-dependent. We address this problem by simultaneously evaluating all policies in a policy class Pi - uniform convergence in OPE - and obtain nearly optimal error bounds for a number of global / local policy classes. Our results imply that the model-based planning achieves an optimal episode complexity of (O) over tilde (H-3/d(m)epsilon(2)) in identifying an epsilon-optimal policy under the time-inhomogeneous episodic MDP model (H is the planning horizon, d(m) is a quantity that reflects the exploration of the logging policy mu). To the best of our knowledge, this is the first time the optimal rate is shown to be possible for the offine RL setting and the paper is the first that systematically investigates the uniform convergence in OPE.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801088,0
C,"Zhao, SJ; Ermon, S",,"Banerjee, A; Fukumizu, K",,"Zhao, Shengjia; Ermon, Stefano",,,Right Decisions from Wrong Predictions: A Mechanism Design Alternative to Individual Calibration,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Decision makers often need to rely on imperfect probabilistic forecasts. While average performance metrics are typically available, it is difficult to assess the quality of individual forecasts and the corresponding utilities. To convey confidence about individual predictions to decision-makers, we propose a compensation mechanism ensuring that the forecasted utility matches the actually accrued utility. While a naive scheme to compensate decision-makers for prediction errors can be exploited and might not be sustainable in the long run, we propose a mechanism based on fair bets and online learning that provably cannot be exploited. We demonstrate an application showing how passengers could confidently optimize individual travel plans based on flight delay probabilities estimated by an airline.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803025,0
C,"Fiegel, C; Gabillon, V; Valko, M",,"Chiappa, S; Calandra, R",,"Fiegel, Come; Gabillon, Victor; Valko, Michal",,,Adaptive multi-fidelity optimization with fast learning rates,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"In multi-fidelity optimization, we have access to biased approximations of varying costs of the target function. In this work, we study the setting of optimizing a locally smooth function with a limited budget Lambda, where the learner has to make a trade-off between the cost and the bias of these approximations. First, we prove lower bounds for the simple regret under different assumptions on the fidelities, based on a cost-to-bias function. We then present the Kometo algorithm which achieves, with additional logarithmic factors, the same rates without any knowledge of the function smoothness and fidelity assumptions, and improves prior results. Finally, we empirically show that our algorithm outperforms prior multi-fidelity optimization methods without the knowledge of problem-dependent parameters.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3493,3501,,,,,,,,,,,,,,,,WOS:000559931301002,0
C,"Gabillon, V; Tutunov, R; Valko, M; Ammar, HB",,"Chiappa, S; Calandra, R",,"Gabillon, Victor; Tutunov, Rasul; Valko, Michal; Ammar, Haitham Bou",,,Derivative-Free & Order-Robust Optimisation,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"In this paper, we formalise order-robust optimisation as an instance of online learning minimising simple regret, and propose VROOM, a zeroth order optimisation algorithm capable of achieving vanishing regret in non-stationary environments, while recovering favorable rates under stochastic reward-generating processes. Our results are the first to target simple regret definitions in adversarial scenarios unveiling a challenge that has been rarely considered in prior work.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2293,2302,,,,,,,,,,,,,,,,WOS:000559931300012,0
C,"Huo, ZP; Pakbin, A; Chen, XH; Hurley, N; Yuan, Y; Qian, XN; Wang, ZY; Huang, S; Mortazavi, BJ",,"Chiappa, S; Calandra, R",,"Huo, Zepeng; Pakbin, Arash; Chen, Xiaohan; Hurley, Nathan; Yuan, Ye; Qian, Xiaoning; Wang, Zhangyang; Huang, Shuai; Mortazavi, Bobak J.",,,Uncertainty Quantification for Deep Context-Aware Mobile Activity Recognition and Unknown Context Discovery,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Activity recognition in wearable computing faces two key challenges: i) activity characteristics may be context-dependent and change under different contexts or situations; ii) unknown contexts and activities may occur from time to time, requiring flexibility and adaptability of the algorithm. We develop a context-aware mixture of deep models termed the alpha-beta network coupled with uncertainty quantification (UQ) based upon maximum entropy to enhance human activity recognition performance. We improve accuracy and F score by 10% by identifying high-level contexts in a data-driven way to guide model development. In order to ensure training stability, we have used a clustering-based pre-training in both public and in-house datasets, demonstrating improved accuracy through unknown context discovery.",,,,,"Huo, Zepeng/GRR-3822-2022","Huo, Zepeng/0000-0001-8920-1690",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3894,3903,,,,,,,,,,,,,,,,WOS:000559931301048,0
C,"Kapralov, M; Nouri, N; Razenshteyn, I; Velingker, A; Zandieh, A",,"Chiappa, S; Calandra, R",,"Kapralov, Michael; Nouri, Navid; Razenshteyn, Ilya; Velingker, Ameya; Zandieh, Amir",,,Scaling up Kernel Ridge Regression via Locality Sensitive Hashing,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Random binning features, introduced in the seminal paper of Rahimi and Recht '07, are an efficient method for approximating a kernel matrix using locality sensitive hashing. Random binning features provide a very simple and efficient way to approximate the Laplace kernel but unfortunately do not apply to many important classes of kernels, notably ones that generate smooth Gaussian processes, such as the Gaussian kernel and Matern kernel. In this paper we introduce a simple weighted version of random binning features, and show that the corresponding kernel function generates Gaussian processes of any desired smoothness. We show that our weighted random binning features provide a spectral approximation to the corresponding kernel matrix, leading to efficient algorithms for kernel ridge regression. Experiments on large scale regression datasets show that our method outperforms the accuracy of random Fourier features method.",,,,,,"Razenshteyn, Ilya/0000-0002-3962-721X",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,4088,4096,,,,,,,,,,,,,,,,WOS:000559931304003,0
C,"Kpotufe, S; Sriperumbudur, BK",,"Chiappa, S; Calandra, R",,"Kpotufe, Samory; Sriperumbudur, Bharath K.",,,Gaussian Sketching yields a J-L Lemma in RKHS,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"The main contribution of the paper is to show that Gaussian sketching of a kernel-Gram matrix K yields an operator whose counterpart in an RKHS H, is a random projection operator-in the spirit of Johnson-Lindenstrauss (J-L) lemma. To be precise, given a random matrix Z with i.i.d. Gaussian entries, we show that a sketch ZK corresponds to a particular random operator in (infinite-dimensional) Hilbert space H that maps functions f is an element of H to a low-dimensional space R-d, while preserving a weighted RKHS inner-product of the form < f, g >(Sigma) (=)over dot < f, Sigma(3)g >(H), where Sigma is the covariance operator induced by the data distribution. In particular, under similar assumptions as in kernel PCA (KPCA), or kernel k-means (K-k-means), well-separated subsets of feature-space {K(., x) : x is an element of X} remain well-separated after such operation, which suggests similar benefits as in KPCA and/or K-k-means, albeit at the much cheaper cost of a random projection. In particular, our convergence rates suggest that, given a large dataset {X-i}(N)(i=1) of size N, we can build the Gram matrix K on a much smaller subsample of size n << N, so that the sketch ZK is very cheap to obtain and subsequently apply as a projection operator on the original data {X-i}(N)(i=1). We verify these insights empirically on synthetic data, and on real-world clustering applications.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3928,3936,,,,,,,,,,,,,,,,WOS:000559931301086,0
C,"Kuangy, Z; Sala, F; Sohoni, N; Wu, S; Cordova-Palomera, A; Dunnmon, J; Priest, J; Re, C",,"Chiappa, S; Calandra, R",,"Kuangy, Zhaobin; Sala, Frederic; Sohoni, Nimit; Wu, Sen; Cordova-Palomera, Aldo; Dunnmon, Jared; Priest, James; Re, Christopher",,,Ivy: Instrumental Variable Synthesis for Causal Inference,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"A popular way to estimate the causal effect of a variable x on y from observational data is to use an instrumental variable (IV): a third variable z that affects y only through x. The more strongly z is associated with x, the more reliable the estimate is, but such strong IVs are difficult to find. Instead, practitioners combine more commonly available IV candidates which are not necessarily strong, or even valid, IVs into a single summary that is plugged into causal effect estimators in place of an IV. In genetic epidemiology, such approaches are known as allele scores. Allele scores require strong assumptions independence and validity of all IV candidates for the resulting estimate to be reliable. To relax these assumptions, we propose Ivy, a new method to combine IV candidates that can handle correlated and invalid IV candidates in a robust manner. Theoretically, we characterize this robustness, its limits, and its impact on the resulting causal estimates. Empirically, we show that Ivy can correctly identify the directionality of known relationships and is robust against false discovery (median effect size <= 0.025) on three real-world datasets with no causal effects, while allele scores return more biased estimates (median effect size >= 0.118).",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,398,409,,,,,,,,,,,,,,,,WOS:000559931301087,0
C,"Kwon, J; Caramanis, C",,"Chiappa, S; Calandra, R",,"Kwon, Jeongyeol; Caramanis, Constantine",,,EM Converges for a Mixture of Many Linear Regressions,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We study the convergence of the Expectation-Maximization (EM) algorithm for mixtures of linear regressions with an arbitrary number k of components. We show that as long as signal-to-noise ratio (SNR) is (Omega) over tilde (k), well-initialized EM converges to the true regression parameters. Previous results for k >= 3 have only established local convergence for the noiseless setting, i.e., where SNR is infinitely large. Our results enlarge the scope to the environment with noises, and notably, we establish a statistical error rate that is independent of the norm (or pairwise distance) of the regression parameters. In particular, our results imply exact recovery as sigma -> 0, in contrast to most previous local convergence results for EM, where the statistical error scaled with the norm of parameters. Standard moment-method approaches may be applied to guarantee we are in the region where our local convergence guarantees apply.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1727,1735,,,,,,,,,,,,,,,,WOS:000559931301092,0
C,"LeJeune, D; Dasarathy, G; Baraniuk, RG",,"Chiappa, S; Calandra, R",,"LeJeune, Daniel; Dasarathy, Gautam; Baraniuk, Richard G.",,,Thresholding Graph Bandits with GrAPL,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"In this paper, we introduce a new online decision making paradigm that we call Thresholding Graph Bandits. The main goal is to efficiently identify a subset of arms in a multi-armed bandit problem whose means are above a specified threshold. While traditionally in such problems, the arms are assumed to be independent, in our paradigm we further suppose that we have access to the similarity between the arms in the form of a graph, allowing us to gain information about the arm means with fewer samples. Such a feature is particularly relevant in modern decision making problems, where rapid decisions need to be made in spite of the large number of options available. We present GrAPL, a novel algorithm for the thresholding graph bandit problem. We demonstrate theoretically that this algorithm is effective in taking advantage of the graph structure when the structure is reflective of the distribution of the rewards. We confirm these theoretical findings via experiments on both synthetic and real data.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2476,2484,,,,,,,,,,,,,,,,WOS:000559931301098,0
C,"Oberst, M; Johansson, FD; Wei, D; Gao, T; Brat, G; Sontag, D; Varshney, KR",,"Chiappa, S; Calandra, R",,"Oberst, Michael; Johansson, Fredrik D.; Wei, Dennis; Gao, Tian; Brat, Gabriel; Sontag, David; Varshney, Kush R.",,,Characterization of Overlap in Observational Studies,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Overlap between treatment groups is required for non-parametric estimation of causal effects. If a subgroup of subjects always receives the same intervention, we cannot estimate the effect of intervention changes on that subgroup without further assumptions. When overlap does not hold globally, characterizing local regions of overlap can inform the relevance of causal conclusions for new subjects, and can help guide additional data collection. To have impact, these descriptions must be interpretable for downstream users who are not machine learning experts, such as policy makers. We formalize overlap estimation as a problem of finding minimum volume sets subject to coverage constraints and reduce this problem to binary classification with Boolean rule classifiers. We then generalize this method to estimate overlap in off-policy policy evaluation. In several real-world applications, we demonstrate that these rules have comparable accuracy to black-box estimators and provide intuitive and informative explanations that can inform policy making.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,788,797,,,,,,,,,,,,,,,,WOS:000559931302060,0
C,"Podda, M; Bacciu, D; Micheli, A",,"Chiappa, S; Calandra, R",,"Podda, Marco; Bacciu, Davide; Micheli, Alessio",,,A Deep Generative Model for Fragment-Based Molecule Generation,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Molecule generation is a challenging open problem in cheminformatics. Currently, deep generative approaches addressing the challenge belong to two broad categories, differing in how molecules are represented. One approach encodes molecular graphs as strings of text, and learns their corresponding character-based language model. Another, more expressive, approach operates directly on the molecular graph. In this work, we address two limitations of the former: generation of invalid and duplicate molecules. To improve validity rates, we develop a language model for small molecular substructures called fragments, loosely inspired by the well-known paradigm of Fragment-Based Drug Design. In other words, we generate molecules fragment by fragment, instead of atom by atom. To improve uniqueness rates, we present a frequency-based masking strategy that helps generate molecules with infrequent fragments. We show experimentally that our model largely outperforms other language model-based competitors, reaching state-of-the-art performances typical of graph-based approaches. Moreover, generated molecules display molecular properties similar to those in the training sample, even in absence of explicit task-specific supervision.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2240,2249,,,,,,,,,,,,,,,,WOS:000559931302075,0
C,"Polykovskiy, D; Vetrov, D",,"Chiappa, S; Calandra, R",,"Polykovskiy, Daniil; Vetrov, Dmitry",,,Deterministic Decoding for Discrete Data in Variational Autoencoders,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Variational autoencoders are prominent generative models for modeling discrete data. However, with flexible decoders, they tend to ignore the latent codes. In this paper, we study a VAE model with a deterministic decoder (DD-VAE) for sequential data that selects the highest-scoring tokens instead of sampling. Deterministic decoding solely relies on latent codes as the only way to produce diverse objects, which improves the structure of the learned manifold. To implement DD-VAE, we propose a new class of bounded support proposal distributions and derive Kullback-Leibler divergence for Gaussian and uniform priors. We also study a continuous relaxation of deterministic decoding objective function and analyze the relation of reconstruction accuracy and relaxation parameters. We demonstrate the performance of DD-VAE on multiple datasets, including molecular generation and optimization problems.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3046,3055,,,,,,,,,,,,,,,,WOS:000559931302077,0
C,"Reisizadeh, A; Mokhtari, A; Hassani, H; Jadbabaie, A; Pedarsani, R",,"Chiappa, S; Calandra, R",,"Reisizadeh, Amirhossein; Mokhtari, Aryan; Hassani, Hamed; Jadbabaie, Ali; Pedarsani, Ramtin",,,FedPAQ: A Communication-Efficient Federated Learning Method with Periodic Averaging and Quantization,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Federated learning is a distributed framework according to which a model is trained over a set of devices, while keeping data localized. This framework faces several systems-oriented challenges which include (i) communication bottleneck since a large number of devices upload their local updates to a parameter server, and (ii) scalability as the federated network consists of millions of devices. Due to these systems challenges as well as issues related to statistical heterogeneity of data and privacy concerns, designing a provably efficient federated learning method is of significant importance yet it remains challenging. In this paper, we present FedPAQ, a communication-efficient Federated Learning method with Periodic Averaging and Quantization. FedPAQ relies on three key features: (1) periodic averaging where models are updated locally at devices and only periodically averaged at the server; (2) partial device participation where only a fraction of devices participate in each round of the training; and (3) quantized message-passing where the edge nodes quantize their updates before uploading to the parameter server. These features address the communications and scalability challenges in federated learning. We also show that FedPAQ achieves near-optimal theoretical guarantees for strongly convex and non-convex loss functions and empirically demonstrate the communication-computation tradeoff provided by our method.",,,,,,"Pedarsani, Ramtin/0000-0003-0786-2132",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2021,2030,,,,,,,,,,,,,,,,WOS:000559931302089,0
C,"Sheth, R; Fusi, N",,"Chiappa, S; Calandra, R",,"Sheth, Rishit; Fusi, Nicolo",,,Differentiable Feature Selection by Discrete Relaxation,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"In this paper, we introduce Differentiable Feature Selection, a gradient-based search algorithm for feature selection. Our approach extends a recent result on the estimation of learnability in the sublinear data regime by showing that the calculation can be performed iteratively (i.e., in mini-batches) and in linear time and space with respect to both the number of features D and the sample size N. This, along with a discrete-to-continuous relaxation of the search domain, allows for an efficient, gradient-based search algorithm among feature subsets for very large datasets. Our algorithm utilizes higher-order correlations between features and targets for both the N > D and N < D regimes, as opposed to approaches that do not consider such correlations and/or only consider one regime. We provide experimental demonstration of the algorithm in small and large sample- and feature-size settings.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303016,0
C,"Tosh, C; Hsu, D",,"Chiappa, S; Calandra, R",,"Tosh, Christopher; Hsu, Daniel",,,Diameter-based Interactive Structure Discovery,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We introduce interactive structure discovery, a generic framework that encompasses many interactive learning settings, including active learning, top-k item identification, interactive drug discovery, and others. We adapt a recently developed active learning algorithm of Tosh and Dasgupta (2017) for interactive structure discovery, and show that the new algorithm can be made noise-tolerant and enjoys favorable query complexity bounds.",,,,,,"Hsu, Daniel/0000-0002-3495-7113",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303043,0
C,"Wang, YX; Rockova, V",,"Chiappa, S; Calandra, R",,"Wang, Yuexi; Rockova, Veronika",,,Uncertainty Quantification for Sparse Deep Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Deep learning methods continue to have a decided impact on machine learning, both in theory and in practice. Statistical theoretical developments have been mostly concerned with approximability or rates of estimation when recovering infinite dimensional objects (curves or densities). Despite the impressive array of available theoretical results, the literature has been largely silent about uncertainty quantification for deep learning. This paper takes a step forward in this important direction by taking a Bayesian point of view. We study Gaussian approximability of certain aspects of posterior distributions of sparse deep ReLU architectures in non-parametric regression. Building on tools from Bayesian non-parametrics, we provide semi-parametric Bernstein-von Mises theorems for linear and quadratic functionals, which guarantee that implied Bayesian credible regions have valid frequentist coverage. Our results provide new theoretical justifications for (Bayesian) deep learning with ReLU activation functions, highlighting their inferential potential.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303060,0
C,"Zhang, MR; Chen, L; Mokhtari, A; Hassani, H; Karbasi, A",,"Chiappa, S; Calandra, R",,"Zhang, Mingrui; Chen, Lin; Mokhtari, Aryan; Hassani, Hamed; Karbasi, Amin",,,"Quantized Frank-Wolfe: Faster Optimization, Lower Communication, and Projection Free","INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"How can we efficiently mitigate the overhead of gradient communications in distributed optimization? This problem is at the heart of training scalable machine learning models and has been mainly studied in the unconstrained setting. In this paper, we propose Quantized Frank-Wolfe (QFW), the first projection-free and communication-efficient algorithm for solving constrained optimization problems at scale. We consider both convex and non-convex objective functions, expressed as a finite-sum or more generally a stochastic optimization problem, and provide strong theoretical guarantees on the convergence rate of QFW. This is accomplished by proposing novel quantization schemes that efficiently compress gradients while controlling the noise variance intduced during this process. Finally, we empirically validate the efficiency of QFW in terms of communication and the quality of returned solution against natural baselines.",,,,,"Chen, Lin/CAH-1961-2022","Chen, Lin/0000-0003-0349-6577",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3696,3705,,,,,,,,,,,,,,,,WOS:000559931304013,0
C,"Agrawal, R; Squires, C; Yang, K; Shanmugam, K; Uhler, C",,"Chaudhuri, K; Sugiyama, M",,"Agrawal, Raj; Squires, Chandler; Yang, Karren; Shanmugam, Karthik; Uhler, Caroline",,,ABCD-Strategy: Budgeted Experimental Design for Targeted Causal Structure Discovery,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Determining the causal structure of a set of variables is critical for both scientific inquiry and decision-making. However, this is often challenging in practice due to limited interventional data. Given that randomized experiments are usually expensive to perform, we propose a general framework and theory based on optimal Bayesian experimental design to select experiments for targeted causal discovery. That is, we assume the experimenter is interested in learning some function of the unknown graph (e.g., all descendants of a target node) subject to design constraints such as limits on the number of samples and rounds of experimentation. While it is in general computationally intractable to select an optimal experimental design strategy, we provide a tractable implementation with provable guarantees on both approximation and optimization quality based on submodularity. We evaluate the efficacy of our proposed method on both synthetic and real datasets, thereby demonstrating that our method realizes considerable performance gains over baseline strategies such as random sampling.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687903047,0
C,"Bassily, R",,"Chaudhuri, K; Sugiyama, M",,"Bassily, Raef",,,Linear Queries Estimation with Local Differential Privacy,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We study the problem of estimating a set of d linear queries with respect to some unknown distribution p over a domain J = [J] based on a sensitive data set of n individuals under the constraint of local differential privacy. This problem subsumes a wide range of estimation tasks, e.g., distribution estimation and d-dimensional mean estimation. We provide new algorithms for both the offline (non-adaptive) and adaptive versions of this problem. In the offline setting, the set of queries are fixed before the algorithm starts. In the regime where n less than or similar to d(2)/log (J), our algorithms attain L-2 estimation error that is independent of d. For the special case of distribution estimation, we show that projecting the output estimate of an algorithm due to [ASZ18] on the probability simplex yields an L-2 error that depends only sub-logarithmically on J in the regime where n less than or similar to J(2)/log(J). Our bounds are within a factor of at most (log(J))(1/4) from the optimal L-2 error when n less than or similar to d(2)/log(J). These results show the possibility of accurate estimation of linear queries in the high-dimensional settings under the L-2 error criterion. In the adaptive setting, the queries are generated over d rounds; one query at a time. In each round, a query can be chosen adaptively based on all the history of previous queries and answers. We give an algorithm for this problem with optimal L-infinity estimation error (worst error in the estimated values for the queries w.r.t. the data distribution). Our bound matches a lower bound on the error for the offline version of this problem [DJW13b].",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,721,729,,,,,,,,,,,,,,,,WOS:000509687900075,0
C,"Dieng, AB; Kim, Y; Rush, AM; Blei, DM",,"Chaudhuri, K; Sugiyama, M",,"Dieng, Adji B.; Kim, Yoon; Rush, Alexander M.; Blei, David M.",,,Avoiding Latent Variable Collapse with Generative Skip Models,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Variational autoencoders (VAES) learn distributions of high-dimensional data. They model data with a deep latent-variable model and then fit the model by maximizing a lower bound of the log marginal likelihood. VAES can capture complex distributions, but they can also suffer from an issue known as 'latent variable collapse, especially if the likelihood model is powerful. Specifically, the lower bound involves an approximate posterior of the latent variables; this posterior collapses when it is set equal to the prior, i.e., when the approximate posterior is independent of the data. While VAES learn good generative models, latent variable collapse prevents them from learning useful representations. In this paper, we propose a simple new way to avoid latent variable collapse by including skip connections in our generative model; these connections enforce strong links between the latent variables and the likelihood function. We study generative skip models both theoretically and empirically. Theoretically, we prove that skip models increase the mutual information between the observations and the inferred latent variables. Empirically, we study images (MNIST and Omniglot) and text (Yahoo). Compared to existing VAE architectures, we show that generative skip models maintain similar predictive performance but lead to less collapse and provide more meaningful representations of the data.",,,,,,"Rush, Alexander/0000-0002-9900-1606",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902046,0
C,"El Karoui, N; Purdom, E",,"Chaudhuri, K; Sugiyama, M",,"El Karoui, Noureddine; Purdom, Elizabeth",,,The non-parametric bootstrap and spectral analysis in moderate and high-dimension,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We consider the properties of the bootstrap as a tool for inference concerning the eigenvalues of a sample covariance matrix computed from an n x p data matrix X. We focus on the modern framework where pin is not close to 0 but remains bounded as n and p tend to infinity. Through a mix of numerical and theoretical considerations, we show that the non-parametric bootstrap is not in general a reliable inferential tool in the setting we consider. However, in the case where the population covariance matrix is well-approximated by a finite rank matrix, the non-parametric bootstrap performs as it does in finite dimension.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902017,0
C,"Gimenez, JR; Zou, J",,"Chaudhuri, K; Sugiyama, M",,"Gimenez, Jaime Roquero; Zou, James",,,Improving the Stability of the Knockoff Procedure: Multiple Simultaneous Knockoffs and Entropy Maximization,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"The Model-X knockoff procedure has recently emerged as a powerful approach for feature selection with statistical guarantees. The advantage of knockoffs is that if we have a good model of the features X, then we can identify salient features without knowing anything about how the outcome Y depends on X. An important drawback of knockoffs is its instability: running the procedure twice can result in very different selected features, potentially leading to different conclusions. Addressing this instability is critical for obtaining reproducible and robust results. Here we present a generalization of the knockoff procedure that we call simultaneous multi-knockoffs. We show that multi-knockoffs guarantee false discovery rate (FDR) control, and are substantially more stable and powerful compared to the standard (single) knockoffs. Moreover we propose a new algorithm based on entropy maximization for generating Gaussian multiknockoffs. We validate the improved stability and power of multi-knockoffs in systematic experiments. We also illustrate how multiknockoffs can improve the accuracy of detecting genetic mutations that are causally linked to phenotypes.",,,,,"Zhang, James/HHS-8616-2022",,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902024,0
C,"Kuzborskij, I; Cella, L; Cesa-Bianchi, N",,"Chaudhuri, K; Sugiyama, M",,"Kuzborskij, Ilja; Cella, Leonardo; Cesa-Bianchi, Nicolo",,,Efficient Linear Bandits through Matrix Sketching,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We prove that two popular linear contextual bandit algorithms, OFUL and Thompson Sampling, can be made efficient using Frequent Directions, a deterministic online sketching technique. More precisely, we show that a sketch of size m allows O(md) update time for both algorithms, as opposed to Omega(d(2)) required by their non-sketched versions in general (where d is the dimension of context vectors). This computational speedup is accompanied by regret bounds of order (1 + epsilon(m))(3/2) d root T for OFUL and of order ((1 + epsilon(m))d)(3/2) root T for Thompson Sampling, where epsilon(m) is bounded by the sum of the tail eigenvalues not covered by the sketch. In particular, when the selected contexts span a subspace of dimension at most m, our algorithms have a regret bound matching that of their slower, non-sketched counterparts. Experiments on real-world datasets corroborate our theoretical results.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,177,185,,,,,,,,,,,,,,,,WOS:000509687900019,0
C,"Mokhtari, A; Ozdaglar, A; Jadbabaie, A",,"Chaudhuri, K; Sugiyama, M",,"Mokhtari, Aryan; Ozdaglar, Asuman; Jadbabaie, Ali",,,Efficient Nonconvex Empirical Risk Minimization via Adaptive Sample Size Methods,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"In this paper, we are interested in finding a local minimizer of an empirical risk minimization (ERM) problem where the loss associated with each sample is possibly a nonconvex function. Unlike traditional deterministic and stochastic algorithms that attempt to solve the ERM problem for the full training set, we propose an adaptive sample size scheme to reduce the overall computational complexity of finding a local minimum. To be more precise, we first find an approximate local minimum of the ERM problem corresponding to a small number of samples and use the uniform convergence theory to show that if the population risk is a Morse function, by properly increasing the size of training set the iterates generated by the proposed procedure always stay close to a local minimum of the corresponding ERM problem. Therefore, eventually the proposed procedure finds a local minimum of the ERM corresponding to the full training set which happens to also be close to a local minimum of the expected risk minimization problem with high probability. We formally state the conditions on the size of the initial sample set and characterize the required accuracy for obtaining an approximate local minimum to ensure that the iterates always stay in a neighborhood of a local minimum and do not get attracted to saddle points.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902055,0
C,"Pasteris, S; Vitale, F; Chan, K; Wang, SQ; Herbster, M",,"Chaudhuri, K; Sugiyama, M",,"Pasteris, Stephen; Vitale, Fabio; Chan, Kevin; Wang, Shiqiang; Herbster, Mark",,,MaxHedge: Maximising a Maximum Online,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We introduce a new online learning framework where, at each trial, the learner is required to select a subset of actions from a given known action set. Each action is associated with an energy value, a reward and a cost. The sum of the energies of the actions selected cannot exceed a given energy budget. The goal is to maximise the cumulative profit, where the profit obtained on a single trial is defined as the difference between the maximum reward among the selected actions and the sum of their costs. Action energy values and the budget are known and fixed. All rewards and costs associated with each action change over time and are revealed at each trial only after the learner's selection of actions. Our framework encompasses several online learning problems where the environment changes over time; and the solution trades-off between minimising the costs and maximising the maximum reward of the selected subset of actions, while being constrained to an action energy budget. The algorithm that we propose is efficient and general that may be specialised to multiple natural online combinatorial problems.",,,,,"Wang, Shiqiang/AAR-4091-2020","Wang, Shiqiang/0000-0003-2090-5512",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901093,0
C,"Sadhanala, V; Wang, YX; Ramdas, A; Tibshirani, RJ",,"Chaudhuri, K; Sugiyama, M",,"Sadhanala, Veeranjaneyulu; Wang, Yu-Xiang; Ramdas, Aaditya; Tibshirani, Ryan J.",,,A Higher-Order Kolmogorov-Smirnov Test,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We present an extension of the Kolmogorov-Smirnov (KS) two-sample test, which can be more sensitive to differences in the tails. Our test statistic is an integral probability metric (IPM) defined over a higher-order total variation ball, recovering the original KS test as its simplest case. We give an exact representer result for our IPM, which generalizes the fact that the original KS test statistic can be expressed in equivalent variational and CDF forms. For small enough orders (k <= 5), we develop a linear-time algorithm for computing our higher-order KS test statistic; for all others (k >= 6), we give a nearly linear-time approximation. We derive the asymptotic null distribution for our test, and show that our nearly linear-time approximation shares the same asymptotic null. Lastly, we complement our theory with numerical studies.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902069,0
C,"Sahu, AK; Zaheer, M; Kar, S",,"Chaudhuri, K; Sugiyama, M",,"Sahu, Anit Kumar; Zaheer, Manzil; Kar, Soummya",,,Towards Gradient Free and Projection Free Stochastic Optimization,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"This paper focuses on the problem of constrained stochastic optimization. A zeroth order Frank-Wolfe algorithm is proposed, which in addition to the projection-free nature of the vanilla Frank-Wolfe algorithm makes it gradient free. Under convexity and smoothness assumption, we show that the proposed algorithm converges to the optimal objective function at a rate O (1/T-1/3), where T denotes the iteration count. In particular, the primal sub-optimality gap is shown to have a dimension dependence of O (d(1/3)), which is the best known dimension dependence among all zeroth order optimization algorithms with one directional derivative per iteration. For non-convex functions, we obtain the Frank-Wolfe gap to be O (d(1/3)T (1/4)). Experiments on black-box optimization setups demonstrate the efficacy of the proposed algorithm.",,,,,"Sahu, Anit Kumar/T-2095-2019; Zaheer, Manzil/ABG-6249-2021",,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687903054,0
C,"Shaban, A; Cheng, CA; Hatch, N; Boots, B",,"Chaudhuri, K; Sugiyama, M",,"Shaban, Amirreza; Cheng, Ching-An; Hatch, Nathan; Boots, Byron",,,Truncated Back-propagation for Bilevel Optimization,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Bilevel optimization has been recently revisited for designing and analyzing algorithms in hyperparameter tuning and meta learning tasks. However, due to its nested structure, evaluating exact gradients for high dimensional problems is computationally challenging. One heuristic to circumvent this difficulty is to use the approximate gradient given by performing truncated back-propagation through the iterative optimization procedure that solves the lower-level problem. Although promising empirical performance has been reported, its theoretical properties are still unclear. In this paper, we analyze the properties of this family of approximate gradients and establish sufficient conditions for convergence. We validate this on several hyperparameter tuning and meta learning tasks. We find that optimization with the approximate gradient computed using few-step back-propagation often performs comparably to optimization with the exact gradient, while requiring far less memory and half the computation time.",,,,,"Cheng, Ching-An/AAZ-1802-2020","Cheng, Ching-An/0000-0002-0610-2070",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901080,0
C,"Tu, RB; Zhang, C; Ackermann, P; Mohan, K; Kjellstrom, H; Zhang, K",,"Chaudhuri, K; Sugiyama, M",,"Tu, Ruibo; Zhang, Cheng; Ackermann, Paul; Mohan, Karthika; Kjellstrom, Hedvig; Zhang, Kun",,,Causal Discovery in the Presence of Missing Data,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Missing data are ubiquitous in many domains such as healthcare. When these data entries are not missing completely at random, the (conditional) independence relations in the observed data may be different from those in the complete data generated by the underlying causal process. Consequently, simply applying existing causal discovery methods to the observed data may lead to wrong conclusions. In this paper, we aim at developing a causal discovery method to recover the underlying causal structure from observed data that are missing under different mechanisms, including missing completely at random (MCAR), missing at random (MAR), and missing not at random (MNAR). With missingness mechanisms represented by missingness graphs (m-graphs), we analyze conditions under which additional correction is needed to derive conditional independence/dependence relations in the complete data. Based on our analysis, we propose Missing Value PC (MVPC), which extends the PC algorithm to incorporate additional corrections. Our proposed MVPC is shown in theory to give asymptotically correct results even on data that are MAR or MNAR. Experimental results on both synthetic data and real healthcare applications illustrate that the proposed algorithm is able to find correct causal relations even in the general case of MNAR.",,,,,"W, Ackermann P/B-9714-2008","W, Ackermann P/0000-0002-5520-169X",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901084,0
C,"Ahn, S; Chertkov, M; Shin, J; Weller, A",,"Storkey, A; PerezCruz, F",,"Ahn, Sungsoo; Chertkov, Michael; Shin, Jinwoo; Weller, Adrian",,,Gauged Mini-Bucket Elimination for Approximate Inference,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Computing the partition function Z of a discrete graphical model is a fundamental inference challenge. Since this is computationally intractable, variational approximations are often used in practice. Recently, so-called gauge transformations were used to improve variational lower bounds on Z. In this paper, we propose a new gauge-variational approach, termed WMBE-G, which combines gauge transformations with the weighted mini-bucket elimination (WMBE) method. WMBE-G can provide both upper and lower bounds on Z, and is easier to optimize than the prior gauge-variational algorithm. We show that WMBE-G strictly improves the earlier WMBE approximation for symmetric models including Ising models with no magnetic field. Our experimental results demonstrate the effectiveness of WMBE-G even for generic, non-symmetric models.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300002,0
C,"Blobaum, P; Janzing, D; Washio, T; Shimizu, S; Scholkopf, B",,"Storkey, A; PerezCruz, F",,"Blobaum, Patrick; Janzing, Dominik; Washio, Takashi; Shimizu, Shohei; Scholkopf, Bernhard",,,Cause-Effect Inference by Comparing Regression Errors,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"We address the problem of inferring the causal relation between two variables by comparing the least-squares errors of the predictions in both possible causal directions. Under the assumption of an independence between the function relating cause and effect, the conditional noise distribution, and the distribution of the cause, we show that the errors are smaller in causal direction if both variables are equally scaled and the causal relation is close to deterministic. Based on this, we provide an easily applicable method that only requires a regression in both possible causal directions. The performance of this method is compared with different related causal inference methods in various artificial and real-world data sets.",,,,,"Shimizu, Shohei/B-4425-2010; Scholkopf, Bernhard/A-7570-2013","Shimizu, Shohei/0000-0002-1931-0733; Scholkopf, Bernhard/0000-0002-8177-0925",,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300095,0
C,"Chen, L; Hassani, H; Karbasi, A",,"Storkey, A; PerezCruz, F",,"Chen, Lin; Hassani, Hamed; Karbasi, Amin",,,Online Continuous Submodular Maximization,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"In this paper, we consider an online optimization process, where the objective functions are not convex (nor concave) but instead belong to a broad class of continuous submodular functions. We first propose a variant of the Frank-Wolfe algorithm that has access to the full gradient of the objective functions. We show that it achieves a regret bound of O(root T) (where T is the horizon of the online optimization problem) against a (1 - 1/e)approximation to the best feasible solution in hindsight. However, in many scenarios, only an unbiased estimate of the gradients are available. For such settings, we then propose an online stochastic gradient ascent algorithm that also achieves a regret bound of O(root T) regret, albeit against a weaker 1/2-approximation to the best feasible solution in hindsight. We also generalize our results to gamma-weakly submodular functions and prove the same sublinear regret bounds. Finally, we demonstrate the efficiency of our algorithms on a few problem instances, including non-convex/non-concave quadratic programs, multilinear extensions of submodular set functions, and D-optimal design.",,,,,"Chen, Lin/CAH-1961-2022","Chen, Lin/0000-0003-0349-6577",,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300198,0
C,"Daskalakis, C; Tzamos, C; Zampetakis, M",,"Storkey, A; PerezCruz, F",,"Daskalakis, Constantinos; Tzamos, Christos; Zampetakis, Manolis",,,Bootstrapping EM via Power EM and Convergence in the Naive Bayes Model,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"We study the convergence properties of the Expectation-Maximization algorithm in the Naive Bayes model. We show that EM can get stuck in regions of slow convergence, even when the features are binary and i.i.d. conditioning on the class label, and even under random (i.e. non worst-case) initialization. In turn, we show that EM can be boot-strapped in a pre-training step that computes a good initialization. From this initialization we show theoretically and experimentally that EM converges exponentially fast to the true model parameters. Our bootstrapping method amounts to running the EM algorithm on appropriately centered iterates of small magnitude, which as we show corresponds to effectively performing power iteration on the covariance matrix of the mixture model, although power iteration is performed under the hood by EM itself. As such, we call our bootstrapping approach power EM. Specifically for the case of two binary features, we show global exponentially fast convergence of EM, even without bootstrapping. Finally, as the Naive Bayes model is quite expressive, we show as corollaries of our convergence results that the EM algorithm globally converges to the true model parameters for mixtures of two Gaussians, recovering recent results of [XHM16, DTZ17].",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300215,0
C,"Reddi, SJ; Zaheer, M; Sra, S; Poczos, B; Salakhutdinov, R; Bach, F; Smola, AJ",,"Storkey, A; PerezCruz, F",,"Reddi, Sashank J.; Zaheer, Manzil; Sra, Suvrit; Poczos, Barnabas; Salakhutdinov, Ruslan; Bach, Francis; Smola, Alexander J.",,,A Generic Approach for Escaping Saddle points,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"A central challenge to using first-order methods for optimizing nonconvex problems is the presence of saddle points. First-order methods often get stuck at saddle points, greatly deteriorating their performance. Typically, to escape from saddles one has to use second-order methods. However, most works on second-order methods rely extensively on expensive Hessian-based computations, making them impractical in large-scale settings. To tackle this challenge, we introduce a generic framework that minimizes Hessian-based computations while at the same time provably converging to second- order critical points. Our framework carefully alternates between a first-order and a second-order sub-routine, using the latter only close to saddle points, and yields convergence results competitive to the state-of-the-art. Empirical results suggest that our strategy also enjoys a good practical performance.",,,,,"Zaheer, Manzil/ABG-6249-2021",,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300129,0
C,"Soltani, M; Hegde, C",,"Storkey, A; PerezCruz, F",,"Soltani, Mohammadreza; Hegde, Chinmay",,,Towards Provable Learning of Polynomial Neural Networks Using Low-Rank Matrix Estimation,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"We study the problem of (provably) learning the weights of a two-layer neural network with quadratic activations. In particular, we focus on the under-parametrized regime where the number of neurons in the hidden layer is (much) smaller than the dimension of the input. Our approach uses a lifting trick, which enables us to borrow algorithmic ideas from low-rank matrix estimation. In this context, we propose two novel, non-convex training algorithms which do not need any extra tuning parameters other than the number of hidden neurons. We support our algorithms with rigorous theoretical analysis, and show that the proposed algorithms enjoy linear convergence, fast running time per iteration, and near-optimal sample complexity. Finally, we complement our theoretical results with several numerical experiments.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300148,0
C,"Wu, LF; Yen, IEH; Yi, JF; Xu, FL; Lei, Q; Witbrock, MJ",,"Storkey, A; PerezCruz, F",,"Wu, Lingfei; Yen, Ian En-Hsu; Yi, Jinfeng; Xu, Fangli; Lei, Qi; Witbrock, Michael J.",,,Random Warping Series: A Random Features Method for Time-Series Embedding,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Time series data analytics has been a problem of substantial interests for decades, and Dynamic Time Warping (DTW) has been the most widely adopted technique to measure dissimilarity between time series. A number of global-alignment kernels have since been proposed in the spirit of DTW to extend its use to kernel-based estimation method such as support vector machine. However, those kernels suffer from diagonal dominance of the Gram matrix and a quadratic complexity w.r.t. the sample size. In this work, we study a family of alignment-aware positive definite (p.d.) kernels, with its feature embedding given by a distribution of Random Warping Series (RWS). The proposed kernel does not suffer from the issue of diagonal dominance while naturally enjoys a Random Features (RF) approximation, which reduces the computational complexity of existing DTW-based techniques from quadratic to linear in terms of both the number and the length of time-series. We also study the convergence of the RF approximation for the domain of time series of unbounded length. Our extensive experiments on 16 benchmark datasets demonstrate that RWS outperforms or matches state-of-the-art classification and clustering methods in both accuracy and computational time.",,,,,"Wu, Lingfei/ABC-1000-2020","Wu, Lingfei/0000-0002-3660-651X",,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300084,0
C,"Abdullah, A; Kumar, R; McGregor, A; Vassilvitskii, S; Venkatasubramanian, S",,"Gretton, A; Robert, CC",,"Abdullah, Amirali; Kumar, Ravi; McGregor, Andrew; Vassilvitskii, Sergei; Venkatasubramanian, Suresh",,,"Sketching, Embedding, and Dimensionality Reduction for Information Spaces","ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Sketching and dimensionality reduction are powerful techniques for speeding up algorithms for massive data. However unlike the rich toolbox available for the l(2)(2) distance, there are no robust results of this nature known for most popular information-theoretic measures. In this paper we show how to embed information distances like the chi(2) and Jensen-Shannon divergences efficiently in low dimensional spaces while preserving all pairwise distances. We then prove a dimensionality reduction result for the Hellinger, Jensen-Shannon, and chi(2) divergences that preserves the information geometry of the distributions, specifically, by retaining the simplex structure of the space. While our first result already implies these divergences can be explicitly embedded in the Euclidean space, retaining the simplex structure is important because it allows us to do inferences in the reduced space. We also show that these divergences can be sketched efficiently (i.e., up to a multiplicative error in sublinear space) in the aggregate streaming model. This result is exponentially stronger than known upper bounds for sketching these distances in the strict turnstile streaming model.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,948,956,,,,,,,,,,,,,,,,WOS:000508662100103,0
C,"Anandkumar, A; Jain, P; Shi, Y; Niranjan, UN",,"Gretton, A; Robert, CC",,"Anandkumar, Animashree; Jain, Prateek; Shi, Yang; Niranjan, U. N.",,,Tensor vs Matrix Methods: Robust Tensor Decomposition under Block Sparse Perturbations,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Robust tensor CP decomposition involves decomposing a tensor into low rank and sparse components. We propose a novel non-convex iterative algorithm with guaranteed recovery. It alternates between low-rank CP decomposition through gradient ascent (a variant of the tensor power method), and hard thresholding of the residual. We prove convergence to the globally optimal solution under natural incoherence conditions on the low rank component, and bounded level of sparse perturbations. We compare our method with natural baselines, viz., which apply robust matrix PCA either to the flattened tensor, or to the matrix slices of the tensor. Our method can provably handle a far greater level of perturbation when the sparse tensor is block-structured. Thus, we establish that tensor methods can tolerate a higher level of gross corruptions compared to matrix methods.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,268,276,,,,,,,,,,,,,,,,WOS:000508662100030,0
C,"Dai, B; He, N; Dai, HJ; Song, L",,"Gretton, A; Robert, CC",,"Dai, Bo; He, Niao; Dai, Hanjun; Song, Le",,,Provable Bayesian Inference via Particle Mirror Descent,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Bayesian methods are appealing in their flexibility in modeling complex data and ability in capturing uncertainty in parameters. However, when Bayes' rule does not result in tractable closed-form, most approximate inference algorithms lack either scalability or rigorous guarantees. To tackle this challenge, we propose a simple yet provable algorithm, Particle Mirror Descent (PMD), to iteratively approximate the posterior density. PMD is inspired by stochastic functional mirror descent where one descends in the density space using a small batch of data points at each iteration, and by particle filtering where one uses samples to approximate a function. We prove result of the first kind that, with m particles, PMD provides a posterior density estimator that converges in terms of KL-divergence to the true posterior in rate O(1/root m). We demonstrate competitive empirical performances of PMD compared to several approximate inference algorithms in mixture models, logistic regression, sparse Gaussian processes and latent Dirichlet allocation on large scale datasets.",,,,,"Dai, Hanjun/AAQ-8943-2021",,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,985,994,,,,,,,,,,,,,,,,WOS:000508662100107,0
C,"Gottlieb, LA; Kontorovich, A; Nisnevitch, P",,"Gretton, A; Robert, CC",,"Gottlieb, Lee-Ad; Kontorovich, Aryeh; Nisnevitch, Pinhas",,,Nearly optimal classification for semimetrics,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"We initiate the rigorous study of classification in semimetric spaces, which are point sets with a distance function that is non-negative and symmetric, but need not satisfy the triangle inequality. We define the density dimension dens and discover that it plays a central role in the statistical and algorithmic feasibility of learning in semimetric spaces. We compute this quantity for several widely used semimetrics and present nearly optimal sample compression algorithms, which are then used to obtain generalization guarantees, including fast rates. Our claim of near-optimality holds in both computational and statistical senses. When the sample has radius R and margin gamma, we show that it can be compressed down to roughly d = (R/gamma)dens points, and further that finding a significantly better compression is algorithmically intractable unless P=NP. This compression implies generalization via standard Occam-type arguments, to which we provide a nearly matching lower bound.",,,,,"Kontorovich, Aryeh/AAB-4744-2020","Kontorovich, Aryeh/0000-0001-8038-8671; Gottlieb, Lee-Ad/0000-0003-3355-351X",,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,379,388,,,,,,,,,,,,,,,,WOS:000508662100042,0
C,"Gunawardana, A; Meek, C",,"Gretton, A; Robert, CC",,"Gunawardana, Asela; Meek, Christopher",,,Universal Models of Multivariate Temporal Point Processes,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"With the rapidly increasing availability of event stream data there is growing interest in multivariate temporal point process models to capture both qualitative and quantitative features of this type of data. Recent research on multivariate point processes have focused in inference and estimation problems for restricted classes of models such as continuous time Bayesian networks, Markov jump processes, Gaussian Cox processes, and Hawkes Processes. In this paper, we study the expressive power and learnability of Graphical Event Models (GEMs) - the analogue of directed graphical models for multivariate temporal point processes. In particular, we describe a set of Graphical Event Models (GEMs) and show that this class can universally approximate any smooth multivariate temporal point process. We also describe a universal learning algorithm for this class of GEMs and show, under a mild set of assumptions, learnability results for both the dependency structures and distributions in this class. Our consistency results demonstrate the possibility of learning about both qualitative and quantitative dependencies from rich event stream data.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,556,563,,,,,,,,,,,,,,,,WOS:000508662100061,0
C,"Liu, HX; Yang, YM",,"Gretton, A; Robert, CC",,"Liu, Hanxiao; Yang, Yiming",,,Semi-Supervised Learning with Adaptive Spectral Transform,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"This paper proposes a novel nonparametric framework for semi-supervised learning and for optimizing the Laplacian spectrum of the data manifold simultaneously. Our formulation leads to a convex optimization problem that can be efficiently solved via the bundle method, and can be interpreted as to asymptotically minimize the generalization error bound of semi-supervised learning with respect to the graph spectrum. Experiments over benchmark datasets in various domains show advantageous performance of the proposed method over strong baselines.",,,,,"Liu, Han/GVT-8296-2022",,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,902,910,,,,,,,,,,,,,,,,WOS:000508662100098,0
C,"Ailon, N",,"Kaski, S; Corander, J",,"Ailon, Nir",,,Improved Bounds for Online Learning Over the Permutahedron and Other Ranking Polytopes,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"Consider the following game: There is a fixed set V of n items. At each step an adversary chooses a score function s(t) : V bar right arrow [0, 1], a learner outputs a ranking of V, and then s(t) is revealed. The learner's loss is the sum over v is an element of V, of s(t)(v) times v's position (0th, 1st, 2nd, ...) in the ranking. This problem captures, for example, online systems that iteratively present ranked lists of items to users, who then respond by choosing one (or more) sought items. The loss measures the users' burden, which increases the further the sought items are from the top. It also captures a version of online rank aggregation. We present an algorithm of expected regret O(n root OPT + n(2)), where OPT is the loss of the best (single) ranking in hindsight. This improves the previously best known algorithm of Suehiro et. al (2012) by saving a factor of Omega(root log n). We also reduce the per-step running time from O(n(2)) to O(n log n). We provide matching lower bounds. The goal of the system is to minimize its total loss after T steps. (For simplicity assume T is known in this work.) The total loss of the system is (additively) compared against that of the best (in hindsight) single ranking played throughout. The difference is called regret.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,29,37,,,,,,,,,,,,,,,,WOS:000508355800004,0
C,"Giesen, J; Kuhne, L; Laue, S",,"Kaski, S; Corander, J",,"Giesen, Joachim; Kuehne, Lars; Laue, Soeren",,,Sketching the Support of a Probability Measure,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"We want to sketch the support of a probability measure on Euclidean space from samples that have been drawn from the measure. This problem is closely related to certain manifold learning problems, where one assumes that the sample points are drawn from a manifold that is embedded in Euclidean space. Here we propose to sketch the support of the probability measure (that does not need to be a manifold) by some gradient flow complex, or more precisely by its Hasse diagram. The gradient flow is defined with respect to the distance function to the sample points. We prove that a gradient flow complex (that can be computed) is homotopy equivalent to the support of the measure for sufficiently dense samplings, and demonstrate the feasibility of our approach on real world data sets.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,257,265,,,,,,,,,,,,,,,,WOS:000508355800029,0
C,"He, P; Zhang, CS",,"Kaski, S; Corander, J",,"He, Peng; Zhang, Changshui",,,Exploiting the Limits of Structure Learning via Inherent Symmetry,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"This theoretical paper is concerned with the structure learning limit for Gaussian Markov random fields from i.i.d. samples. The common strategy is applying the Fano method to a family of restricted ensembles. The efficiency of this method, however, depends crucially on selected restricted ensembles. To break through this limitation, we analyze the whole graph ensemble from a group theoretical viewpoint. The key ingredient of our approach is the invariance of orthogonal group actions on the symmetric Kullback-Leibler divergence. We then establish the connection of the learning limit and eigenvalues of concentration matrices, which further leads to a sharper structure learning limit. To our best knowledge, this is the first paper to consider the structure learning problem via inherent symmetries of the whole ensemble. Finally, our approach can be applicable to other graphical structure learning problems.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,328,337,,,,,,,,,,,,,,,,WOS:000508355800037,0
C,"Iwata, S; Nakatsukasa, Y; Takeda, A",,"Kaski, S; Corander, J",,"Iwata, Satoru; Nakatsukasa, Yuji; Takeda, Akiko",,,Global Optimization Methods for Extended Fisher Discriminant Analysis,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"The Fisher discriminant analysis (FDA) is a common technique for binary classification. A parametrized extension, which we call the extended FDA, has been introduced from the viewpoint of robust optimization. In this work, we first give a new probabilistic interpretation of the extended FDA. We then develop algorithms for solving an optimization problem that arises from the extended FDA: computing the distance between a point and the surface of an ellipsoid. We solve this problem via the KKT points, which we show are obtained by solving a generalized eigen-value problem. We speed up the algorithm by taking advantage of the matrix structure and proving that a globally optimal solution is a KKT point with the smallest Lagrange multiplier, which can be computed efficiently as the leftmost eigenvalue. Numerical experiments illustrate the efficiency and effectiveness of the extended FDA model combined with our algorithm.",,,,,,"Nakatsukasa, Yuji/0000-0001-7911-1501",,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,411,419,,,,,,,,,,,,,,,,WOS:000508355800046,0
C,"Javdani, S; Chen, YX; Karbasi, A; Krause, A; Bagnell, JA; Srinivasa, S",,"Kaski, S; Corander, J",,"Javdani, Shervin; Chen, Yuxin; Karbasi, Amin; Krause, Andreas; Bagnell, J. Andrew; Srinivasa, Siddhartha",,,Near Optimal Bayesian Active Learning for Decision Making,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"How should we gather information to make effective decisions? We address Bayesian active learning and experimental design problems, where we sequentially select tests to reduce uncertainty about a set of hypotheses. Instead of minimizing uncertainty per se, we consider a set of overlapping decision regions of these hypotheses. Our goal is to drive uncertainty into a single decision region as quickly as possible. We identify necessary and sufficient conditions for correctly identifying a decision region that contains all hypotheses consistent with observations. We develop a novel Hyperedge Cutting (HEC) algorithm for this problem, and prove that is competitive with the intractable optimal policy. Our efficient implementation of the algorithm relies on computing subsets of the complete homogeneous symmetric polynomials. Finally, we demonstrate its effectiveness on two practical applications: approximate comparison-based learning and active localization using a robot manipulator.",,,,,,"Krause, Andreas/0000-0001-7260-9673; Chen, Yuxin/0000-0003-2133-140X",,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,430,438,,,,,,,,,,,,,,,,WOS:000508355800048,0
C,"Ma, YF; Garnett, R; Schneider, J",,"Kaski, S; Corander, J",,"Ma, Yifei; Garnett, Roman; Schneider, Jeff",,,Active Area Search via Bayesian Quadrature,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"The selection of data collection locations is a problem that has received significant research attention from classical design of experiments to various recent active learning algorithms. Typical objectives are to map an unknown function, optimize it, or find level sets in it. Each of these objectives focuses on an assessment of individual points. The introduction of set kernels has led to algorithms that instead consider labels assigned to sets of data points. In this paper we combine these two concepts and consider the problem of choosing data collection locations when the goal is to identify regions whose set of collected data would be labeled positively by a set classifier. We present an algorithm for the case where the positive class is defined in terms of a region's average function value being above some threshold with high probability, a problem we call active area search. To this end, we model the latent function using a Gaussian process and use Bayesian quadrature to estimate its integral on predefined regions. Our method is the first which directly solves the active area search problem. In experiments it outperforms previous algorithms that were developed for other active search goals.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,595,603,,,,,,,,,,,,,,,,WOS:000508355800066,0
C,"Moeller, J; Raman, P; Saha, A; Venkatasubramanian, S",,"Kaski, S; Corander, J",,"Moeller, John; Raman, Parasaran; Saha, Avishek; Venkatasubramanian, Suresh",,,A Geometric Algorithm for Scalable Multiple Kernel Learning,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"We present a geometric formulation of the Multiple Kernel Learning (MKL) problem. To do so, we reinterpret the problem of learning kernel weights as searching for a kernel that maximizes the minimum (kernel) distance between two convex polytopes. This interpretation combined with novel structural insights from our geometric formulation allows us to reduce the MKL problem to a simple optimization routine that yields provable convergence as well as quality guarantees. As a result our method scales efficiently to much larger data sets than most prior methods can handle. Empirical evaluation on eleven datasets shows that we are significantly faster and even compare favorably with a uniform unweighted combination of kernels.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,633,642,,,,,,,,,,,,,,,,WOS:000508355800070,0
C,"Bogunovic, I; Losalka, A; Krause, A; Scarlett, J",,"Banerjee, A; Fukumizu, K",,"Bogunovic, Ilija; Losalka, Arpan; Krause, Andreas; Scarlett, Jonathan",,,Stochastic Linear Bandits Robust to Adversarial Attacks,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We consider a stochastic linear bandit problem in which the rewards are not only subject to random noise, but also adversarial attacks subject to a suitable budget C (i.e., an upper bound on the sum of corruption magnitudes across the time horizon). We provide two variants of a Robust Phased Elimination algorithm, one that knows C and one that does not. Both variants are shown to attain near-optimal regret in the non-corrupted case C = 0, while incurring additional additive terms respectively having a linear and quadratic dependency on C in general. We present algorithm-independent lower bounds showing that these additive terms are nearoptimal. In addition, in a contextual setting, we revisit a setup of diverse contexts, and show that a simple greedy algorithm is provably robust with a near-optimal additive regret term, despite performing no explicit exploration and not knowing C.",,,,,"Scarlett, Jonathan/AGK-0892-2022",,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801024,0
C,"Della Vecchia, A; Mourtada, J; De Vito, E; Rosasco, L",,"Banerjee, A; Fukumizu, K",,"Della Vecchia, Andrea; Mourtada, Jaouad; De Vito, Ernesto; Rosasco, Lorenzo",,,Regularized ERM on random subspaces,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We study a natural extension of classical empirical risk minimization, where the hypothesis space is a random subspace of a given space. In particular, we consider possibly data dependent subspaces spanned by a random subset of the data, recovering as a special case Nystrom approaches for kernel methods. Considering random subspaces naturally leads to computational savings, but the question is whether the corresponding learning accuracy is degraded. These statistical-computational tradeoffs have been recently explored for the least squares loss and self-concordant loss functions, such as the logistic loss. Here, we work to extend these results to convex Lipschitz loss functions, that might not be smooth, such as the hinge loss used in support vector machines. This extension requires developing new proofs, that use different technical tools. Our main results show the existence of different settings, depending on how hard the learning problem is, for which computational efficiency can be improved with no loss in performance. Theoretical results are illustrated with simple numerical experiments.",,,,,"De Vito, Ernesto/AAX-5125-2021","De Vito, Ernesto/0000-0002-4320-3292",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804079,0
C,"Etesami, J; Trouleau, W; Kiyavash, N; Grossglauser, M; Thiran, P",,"Banerjee, A; Fukumizu, K",,"Etesami, Jalal; Trouleau, William; Kiyavash, Negar; Grossglauser, Matthias; Thiran, Patrick",,,A Variational Inference Approach to Learning Multivariate Wold Processes,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Temporal point-processes are often used for mathematical modeling of sequences of discrete events with asynchronous timestamps. We focus on a class of temporal point-process models called multivariate Wold processes (MWP). These processes are well suited to model real-world communication dynamics. Statistical inference on such processes often requires learning their corresponding parameters using a set of observed timestamps. In this work, we relax some of the restrictive modeling assumptions made in the state-of-the-art and introduce a Bayesian approach for inferring the parameters of MWP. We develop a computationally efficient variational inference algorithm that allows scaling up the approach to high-dimensional processes and long sequences of observations. Our experimental results on both synthetic and real-world datasets show that our proposed algorithm outperforms existing methods.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802047,0
C,"Gower, RM; Sebbouh, O; Loizou, N",,"Banerjee, A; Fukumizu, K",,"Gower, Robert M.; Sebbouh, Othmane; Loizou, Nicolas",,,"SGD for Structured Nonconvex Functions: Learning Rates, Minibatching and Interpolation",24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Stochastic Gradient Descent (SGD) is being used routinely for optimizing non-convex functions. Yet, the standard convergence theory for SGD in the smooth non-convex setting gives a slow sublinear convergence to a stationary point. In this work, we provide several convergence theorems for SGD showing convergence to a global minimum for non-convex problems satisfying some extra structural assumptions. In particular, we focus on two large classes of structured non-convex functions: (i) Quasar (Strongly) Convex functions (a generalization of convex functions) and (ii) functions satisfying the Polyak-Lojasiewicz condition (a generalization of strongly-convex functions). Our analysis relies on an Expected Residual condition which we show is a strictly weaker assumption than previously used growth conditions, expected smoothness or bounded variance assumptions. We provide theoretical guarantees for the convergence of SGD for different step-size selections including constant, decreasing and the recently proposed stochastic Polyak step-size. In addition, all of our analysis holds for the arbitrary sampling paradigm, and as such, we give insights into the complexity of minibatching and determine an optimal mini-batch size. Finally, we show that for models that interpolate the training data, we can dispense of our Expected Residual condition and give state-of-the-art results in this setting.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801060,0
C,"Haddadpour, F; Kamani, MM; Mokhtari, A; Mahdavi, M",,"Banerjee, A; Fukumizu, K",,"Haddadpour, Farzin; Kamani, Mohammad Mahdi; Mokhtari, Aryan; Mahdavi, Mehrdad",,,Federated Learning with Compression: Unified Analysis and Sharp Guarantees,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"In federated learning, communication cost is often a critical bottleneck to scale up distributed optimization algorithms to collaboratively learn a model from millions of devices with potentially unreliable or limited communication and heterogeneous data distributions. Two notable trends to deal with the communication overhead of federated algorithms are gradient compression and local computation with periodic communication. Despite many attempts, characterizing the relationship between these two approaches has proven elusive. We address this by proposing a set of algorithms with periodical compressed (quantized or sparsified) communication and analyze their convergence properties in both homogeneous and heterogeneous local data distributions settings. For the homogeneous setting, our analysis improves existing bounds by providing tighter convergence rates for both strongly convex and non-convex objective functions. To mitigate data heterogeneity, we introduce a local gradient tracking scheme and obtain sharp convergence rates that match the best-known communication complexities without compression for convex, strongly convex, and nonconvex settings. We complement our theoretical results by demonstrating the effectiveness of our proposed methods on real-world datasets.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802081,0
C,"Horvath, S; Klein, A; Richtarik, P; Archambeau, C",,"Banerjee, A; Fukumizu, K",,"Horvath, Samuel; Klein, Aaron; Richtarik, Peter; Archambeau, Cedric",,,Hyperparameter Transfer Learning with Adaptive Complexity,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Bayesian optimization (BO) is a sample efficient approach to automatically tune the hyperparameters of machine learning models. In practice, one frequently has to solve similar hyperparameter tuning problems sequentially. For example, one might have to tune a type of neural network learned across a series of different classification problems. Recent work on multi-task BO exploits knowledge gained from previous tuning tasks to speed up a new tuning task. However, previous approaches do not account for the fact that BO is a sequential decision making procedure. Hence, there is in general a mismatch between the number of evaluations collected in the current tuning task compared to the number of evaluations accumulated in all previously completed tasks. In this work, we enable multi-task BO to compensate for this mismatch, such that the transfer learning procedure is able to handle different data regimes in a principled way. We propose a new multi-task BO method that learns a set of ordered, non-linear basis functions of increasing complexity via nested drop-out and automatic relevance determination. Experiments on a variety of hyperparameter tuning problems show that our method improves the sample efficiency of recently published multi-task BO methods.",,,,,,"Horvath, Samuel/0000-0003-0619-9260",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801067,0
C,"Jang, S; Lee, I; Weimer, J",,"Banerjee, A; Fukumizu, K",,"Jang, Sooyong; Lee, Insup; Weimer, James",,,Improving Classifier Confidence using Lossy Label-Invariant Transformations,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Providing reliable model uncertainty estimates is imperative to enabling robust decision making by autonomous agents and humans alike. While recently there have been significant advances in confidence calibration for trained models, examples with poor calibration persist in most calibrated models. Consequently, multiple techniques have been proposed that leverage label-invariant transformations of the input (i.e., an input manifold) to improve worst-case confidence calibration. However, manifold-based confidence calibration techniques generally do not scale and/or require expensive retraining when applied to models with large input spaces (e.g., ImageNet). In this paper, we present the recursive lossy label-invariant calibration (ReCal) technique that leverages label-invariant transformations of the input that induce a loss of discriminatory information to recursively group (and calibrate) inputs - without requiring model retraining. We show that ReCal outperforms other calibration methods on multiple datasets, especially, on large-scale datasets such as ImageNet.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804084,0
C,"Laszkiewicz, M; Fischer, A; Lederer, J",,"Banerjee, A; Fukumizu, K",,"Laszkiewicz, Mike; Fischer, Asja; Lederer, Johannes",,,Thresholded Adaptive Validation: Tuning the Graphical Lasso for Graph Recovery,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Many Machine Learning algorithms are formulated as regularized optimization problems, but their performance hinges on a regularization parameter that needs to be calibrated to each application at hand. In this paper, we propose a general calibration scheme for regularized optimization problems and apply it to the graphical lasso, which is a method for Gaussian graphical modeling. The scheme is equipped with theoretical guarantees and motivates a thresholding pipeline that can improve graph recovery. Moreover, requiring at most one line search over the regularization path, the calibration scheme is computationally more efficient than competing schemes that are based on resampling. Finally, we show in simulations that our approach can improve on the graph recovery of other approaches considerably.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802027,0
C,"Liu, C; Zhu, YQ; Chaudhuri, K; Wang, YX",,"Banerjee, A; Fukumizu, K",,"Liu, Chong; Zhu, Yuqing; Chaudhuri, Kamalika; Wang, Yu-Xiang",,,Revisiting Model-Agnostic Private Learning: Faster Rates and Active Learning,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"The Private Aggregation of Teacher Ensembles (PATE) framework is one of the most promising recent approaches in differentially private learning. Existing theoretical analysis shows that PATE consistently learns any VC-classes in the realizable setting, but falls short in explaining its success in more general cases where the error rate of the optimal classifier is bounded away from zero. We fill in this gap by introducing the Tsybakov Noise Condition (TNC) and establish stronger and more interpretable learning bounds. These bounds provide new insights into when PATE works and improve over existing results even in the narrower realizable setting. We also investigate the compelling idea of using active learning for saving privacy budget. The novel components in the proofs include a more refined analysis of the majority voting classifier - which could be of independent interest and an observation that the synthetic student learning problem is nearly realizable by construction under the Tsybakov noise condition.",,,,,,"Wang, Yu-Xiang/0000-0002-6403-212X",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801007,0
C,"Maystre, L; Kumarappan, N; Butepage, J; Lalmas, M",,"Banerjee, A; Fukumizu, K",,"Maystre, Lucas; Kumarappan, Nagarjuna; Butepage, Judith; Lalmas, Mounia",,,Collaborative Classification from Noisy Labels,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We consider a setting where users interact with a collection of N items on an online platform. We are given class labels possibly corrupted by noise, and we seek to recover the true class of each item. We postulate a simple probabilistic model of the interactions between users and items, based on the assumption that users interact with classes in different proportions. We then develop a message-passing algorithm that decodes the noisy class labels efficiently. Under suitable assumptions, our method provably recovers all items' true classes in the large N limit, even when the interaction graph remains sparse. Empirically, we show that our approach is effective on several practical applications, including predicting the location of businesses, the category of consumer goods, and the language of audio content.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802002,0
C,"Mehmood, S; Ochs, P",,"Banerjee, A; Fukumizu, K",,"Mehmood, Sheheryar; Ochs, Peter",,,Differentiating the Value Function by using Convex Duality,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We consider the differentiation of the value function for parametric optimization problems. Such problems are ubiquitous in Machine Learning applications such as structured support vector machines, matrix factorization and min-min or minimax problems in general. Existing approaches for computing the derivative rely on strong assumptions of the parametric function. Therefore, in several scenarios there is no theoretical evidence that a given algorithmic differentiation strategy computes the true gradient information of the value function. We leverage a well known result from convex duality theory to relax the conditions and to derive convergence rates of the derivative approximation for several classes of parametric optimization problems in Machine Learning. We demonstrate the versatility of our approach in several experiments, including non-smooth parametric functions. Even in settings where other approaches are applicable, our duality based strategy shows a favorable performance.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804065,0
C,"Panousis, KP; Alexos, A; Theodoridis, S; Chatzis, S",,"Banerjee, A; Fukumizu, K",,"Panousis, Konstantinos P.; Alexos, Antonios; Theodoridis, Sergios; Chatzis, Sotirios",,,Local Competition and Stochasticity for Adversarial Robustness in Deep Learning,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"This work addresses adversarial robustness in deep learning by considering deep networks with stochastic local winner-takes-all (LWTA) activations. This type of network units result in sparse representations from each model layer, as the units are organized in blocks where only one unit generates a non-zero output. The main operating principle of the introduced units lies on stochastic arguments, as the network performs posterior sampling over competing units to select the winner. We combine these LWTA arguments with tools from the field of Bayesian non-parametrics, specifically the stick-breaking construction of the Indian Buffet Process, to allow for inferring the sub-part of each layer that is essential for modeling the data at hand. Then, inference is performed by means of stochastic variational Bayes. We perform a thorough experimental evaluation of our model using benchmark datasets. As we show, our method achieves high robustness to adversarial perturbations, with state-of-the-art performance in powerful adversarial attack schemes.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804064,0
C,"Perez-Salazar, S; Cummings, R",,"Banerjee, A; Fukumizu, K",,"Perez-Salazar, Sebastian; Cummings, Rachel",,,Differentially Private Online Submodular Maximization,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"In this work we consider the problem of online submodular maximization under a cardinality constraint with differential privacy (DP). A stream of T submodular functions over a common finite ground set U arrives online, and at each time-step the decision maker must choose at most k elements of U before observing the function. The decision maker obtains a profit equal to the function evaluated on the chosen set and aims to learn a sequence of sets that achieves low expected regret. In the full-information setting, we develop an (epsilon,delta)-DP algorithm with expected (1- 1/e)-regret bound of O (k(2) log vertical bar U vertical bar root T log k/delta/epsilon). This algorithm contains k ordered experts that learn the best marginal increments for each item over the whole time horizon while maintaining privacy of the functions. In the bandit setting, we provide an (epsilon, delta + O(e-T-1/3))-DP algorithm with expected (1 - 1/e)-regret bound of O (root log k/delta/epsilon (k vertical bar U vertical bar log vertical bar U vertical bar)(1/3))T-2(2/3)). One challenge for privacy in this setting is that the payoff and feedback of expert i depends on the actions taken by her i - 1 predecessors. This particular type of information leakage is not covered by post-processing, and new analysis is required. Our techniques for maintaining privacy with feedforward may be of independent interest.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801056,0
C,"Rezaabad, AL; Kalantari, R; Vishwanath, S; Zhou, MY; Tamir, JI",,"Banerjee, A; Fukumizu, K",,"Rezaabad, Ali Lotfi; Kalantari, Rahi; Vishwanath, Sriram; Zhou, Mingyuan; Tamir, Jonathan, I",,,Hyperbolic Graph Embedding with Enhanced Semi-Implicit Variational Inference,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Efficient modeling of relational data arising in physical, social, and information sciences is challenging due to complicated dependencies within the data. In this work we build off of semi-implicit graph variational auto-encoders to capture higher order statistics in a low-dimensional graph latent representation. We incorporate hyperbolic geometry in the latent space through a Poincare embedding to efficiently represent graphs exhibiting hierarchical structure. To address the naive posterior latent distribution assumptions in classical variational inference, we use semi-implicit hierarchical variational Bayes to implicitly capture posteriors of given graph data, which may exhibit heavy tails, multiple modes, skewness, and highly correlated latent structures. We show that the existing semi-implicit variational inference objective provably reduces information in the observed graph. Based on this observation, we estimate and add an additional mutual information term to the semi-implicit variational inference learning objective to capture rich correlations arising between the input and latent spaces. We show that the inclusion of this regularization term in conjunction with the Poincare embedding boosts the quality of learned high-level representations and enables more flexible and faithful graphical modeling. We experimentally demonstrate that our approach outperforms exist-ing graph variational auto-encoders both in Euclidean and in hyperbolic spaces for edge link prediction and node classification.",,,,,"Zhou, Mingyuan/AAE-8717-2021",,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804017,0
C,"Sadeghi, O; Fazel, M",,"Banerjee, A; Fukumizu, K",,"Sadeghi, Omid; Fazel, Maryam",,,Differentially Private Monotone Submodular Maximization Under Matroid and Knapsack Constraints,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Numerous tasks in machine learning and artificial intelligence have been modeled as submodular maximization problems. These problems usually involve sensitive data about individuals, and in addition to maximizing the utility, privacy concerns should be considered. In this paper, we study the general framework of nonnegative monotone submodular maximization subject to matroid or knapsack constraints in both offline and online settings. For the offline setting, we propose a differentially private (1 - kappa/e)-approximation algorithm, where kappa is an element of[0, 1] is the total curvature of the submodular set function, which improves upon prior works in terms of approximation guarantee and query complexity under the same privacy budget. In the online setting, we propose the first differentially private algorithm, and we specify the conditions under which the regret bound scales as O(root T), i.e., privacy could be ensured while maintaining the same regret bound as the optimal regret guarantee in the non-private setting.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803050,0
C,"Sakaue, S",,"Banerjee, A; Fukumizu, K",,"Sakaue, Shinsaku",,,"Differentiable Greedy Algorithm for Monotone Submodular Maximization: Guarantees, Gradient Estimators, and Applications",24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Motivated by, e.g., sensitivity analysis and end-to-end learning, the demand for differentiable optimization algorithms has been increasing. This paper presents a theoretically guaranteed differentiable greedy algorithm for monotone submodular function maximization. We smooth the greedy algorithm via randomization, and prove that it almost recovers original approximation guarantees in expectation for the cases of cardinality and kappa-extendible system constraints. We then present how to efficiently compute gradient estimators of any expected output-dependent quantities. We demonstrate the usefulness of our method by instantiating it for various applications.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,28,+,,,,,,,,,,,,,,,,WOS:000659893800004,0
C,"Shi, YY; Cornish, R",,"Banerjee, A; Fukumizu, K",,"Shi, Yuyang; Cornish, Rob",,,On Multilevel Monte Carlo Unbiased Gradient Estimation For Deep Latent Variable Models,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Standard variational schemes for training deep latent variable models rely on biased gradient estimates of the target objective. Techniques based on the Evidence Lower Bound (ELBO), and tighter variants obtained via importance sampling, produce biased gradient estimates of the true log-likelihood. The family of Reweighted Wake-Sleep (RWS) methods further relies on a biased estimator of the inference objective, which biases training of the encoder also. In this work, we show how multilevel Monte Carlo (MLMC) can provide a natural framework for debiasing these methods with two different estimators. We prove rigorously that this approach yields unbiased gradient estimators with finite variance under reasonable conditions. Furthermore, we investigate methods that can reduce variance and ensure finite variance in practice. Finally, we show empirically that the proposed unbiased estimators outperform IWAE and other debiasing method on a variety of applications at the same expected cost.",,,,,,"Shi, Yuyang/0000-0003-4383-1747",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804071,0
C,"Wang, ZY; Honorio, J",,"Banerjee, A; Fukumizu, K",,"Wang, Zhanyu; Honorio, Jean",,,The Sample Complexity of Meta Sparse Regression,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"This paper addresses the meta-learning problem in sparse linear regression with infinite tasks. We assume that the learner can access several similar tasks. The goal of the learner is to transfer knowledge from the prior tasks to a similar but novel task. For p parameters, size of the support set k, and l samples per task, we show that T is an element of O((k log p)/l) tasks are sufficient in order to recover the common support of all tasks. With the recovered support, we can greatly reduce the sample complexity for estimating the parameter of the novel task, i.e., l is an element of O(1) with respect to T and p. We also prove that our rates are minimax optimal. A key difference between meta-learning and the classical multi-task learning, is that meta-learning focuses only on the recovery of the parameters of the novel task, while multi-task learning estimates the parameter of all tasks, which requires l to grow with T. Instead, our efficient meta-learning estimator allows for l to be constant with respect to T (i.e., few-shot learning).",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802078,0
C,"Xu, K; Fjelde, TE; Sutton, C; Ge, H",,"Banerjee, A; Fukumizu, K",,"Xu, Kai; Fjelde, Tor Erlend; Sutton, Charles; Ge, Hong",,,Couplings for Multinomial Hamiltonian Monte Carlo,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Hamiltonian Monte Carlo (HMC) is a popular sampling method in Bayesian inference. Recently, Heng & Jacob (2019) studied Metropolis HMC with couplings for unbiased Monte Carlo estimation, establishing a generic parallelizable scheme for HMC. However, in practice a different HMC method, multinomial HMC, is considered as the go-to method, e.g. as part of the no-U-turn sampler. In multinomial HMC, proposed states are not limited to end-points as in Metropolis HMC; instead points along the entire trajectory can be proposed. In this paper, we establish couplings for multinomial HMC, based on optimal transport for multinomial sampling in its transition. We prove an upper bound for the meeting time - the time it takes for the coupled chains to meet - based on the notion of local contractivity. We evaluate our methods using three targets: 1;000 dimensional Gaussians, logistic regression and log-Gaussian Cox point processes. Compared to Heng & Jacob (2019), coupled multinomial HMC generally attains a smaller meeting time, and is more robust to choices of step sizes and trajectory lengths, which allows re-use of existing adaptation methods for HMC. These improvements together paves the way for a wider and more practical use of coupled HMC methods.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804040,0
C,"Alvarez-Melis, D; Mroueh, Y; Jaakkola, T",,"Chiappa, S; Calandra, R",,"Alvarez-Melis, David; Mroueh, Youssef; Jaakkola, Tommi",,,Unsupervised Hierarchy Matching with Optimal Transport over Hyperbolic Spaces,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"This paper focuses on the problem of unsupervised alignment of hierarchical data such as ontologies or lexical databases. This problem arises across areas, from natural language processing to bioinformatics, and is typically solved by appeal to outside knowledge bases and label-textual similarity. In contrast, we approach the problem from a purely geometric perspective: given only a vector-space representation of the items in the two hierarchies, we seek to infer correspondences across them. Our work derives from and interweaves hyperbolic-space representations for hierarchical data, on one hand, and unsupervised word-alignment methods, on the other. We first provide a set of negative results showing how and why Euclidean methods fail in this hyperbolic setting. We then propose a novel approach based on optimal transport over hyperbolic spaces, and show that it outperforms standard embedding alignment techniques in various experiments on cross-lingual WordNet alignment and ontology matching tasks.",,,,,"Alvarez-Melis, David/AAV-1099-2021",/0000-0002-9591-8986,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1606,1616,,,,,,,,,,,,,,,,WOS:000559931300010,0
C,"Buathong, P; Ginsbourger, D; Krityakierne, T",,"Chiappa, S; Calandra, R",,"Buathong, Poompol; Ginsbourger, David; Krityakierne, Tipaluck",,,"Kernels over Sets of Finite Sets using RKHS Embeddings, with Application to Bayesian (Combinatorial) Optimization","INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We focus on kernel methods for set-valued inputs and their application to Bayesian set optimization, notably combinatorial optimization. We investigate two classes of set kernels that both rely on Reproducing Kernel Hilbert Space embeddings, namely the Double Sum (DS) kernels recently considered in Bayesian set optimization, and a class introduced here called Deep Embedding (DE) kernels that essentially consists in applying a radial kernel on Hilbert space on top of the canonical distance induced by another kernel such as a DS kernel. We establish in particular that while DS kernels typically suffer from a lack of strict positive definiteness, vast subclasses of DE kernels built upon DS kernels do possess this property, enabling in turn combinatorial optimization without requiring to introduce a jitter parameter. Proofs of theoretical results about considered kernels are complemented by a few practicalities regarding hyperparameter fitting. We furthermore demonstrate the applicability of our approach in prediction and optimization tasks, relying both on toy examples and on two test cases from mechanical engineering and hydrogeology, respectively. Experimental results highlight the applicability and compared merits of the considered approaches while opening new perspectives in prediction and sequential design with set inputs.",,,,,"Krityakierne, Tipaluck/ABE-5702-2021","Buathong, Poompol/0000-0002-7261-3135",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2731,2740,,,,,,,,,,,,,,,,WOS:000559931300038,0
C,"Cayci, S; Eryilmaz, A; Srikant, R",,"Chiappa, S; Calandra, R",,"Cayci, Semih; Eryilmaz, Atilla; Srikant, R.",,,Budget-Constrained Bandits over General Cost and Reward Distributions,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We consider a budget-constrained bandit problem where each arm pull incurs a random cost, and yields a random reward in return. The objective is to maximize the total expected reward under a budget constraint on the total cost. The model is general in the sense that it allows correlated and potentially heavy-tailed cost-reward pairs that can take on negative values as required by many applications. We show that if moments of order (2+gamma) for some gamma > 0 exist for all cost-reward pairs, O(logB) regret is achievable for a budget B > 0. In order to achieve tight regret bounds, we propose algorithms that exploit the correlation between the cost and reward of each arm by extracting the common information via linear minimum mean-square error estimation. We prove a regret lower bound for this problem, and show that the proposed algorithms achieve tight problem-dependent regret bounds, which are optimal up to a universal constant factor in the case of jointly Gaussian cost and reward pairs.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,4388,4397,,,,,,,,,,,,,,,,WOS:000559931300046,0
C,"Chen, SY; Kasiviswanathan, SP",,"Chiappa, S; Calandra, R",,"Chen, Shiyun; Kasiviswanathan, Shiva Prasad",,,Contextual Online False Discovery Rate Control,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Multiple hypothesis testing, a situation when we wish to consider many hypotheses, is a core problem in statistical inference that arises in almost every scientific field. In this setting, controlling the false discovery rate (FDR), which is the expected proportion of type I error, is an important challenge for making meaningful inferences. In this paper, we consider a setting where an ordered (possibly infinite) sequence of hypotheses arrives in a stream, and for each hypothesis we observe a p-value along with a set of features specific to that hypothesis. The decision whether or not to reject the current hypothesis must be made immediately at each timestep, before the next hypothesis is observed. This model provides a general way of leveraging the side (contextual) information in the data to help maximize the number of discoveries while controlling the FDR. We propose a new class of powerful online testing procedures, where the rejection thresholds are learned sequentially by incorporating contextual information and previous results. We prove that any rule in this class controls online FDR under some standard assumptions. We then focus on a subclass of these procedures, based on weighting the rejection thresholds, to derive a practical algorithm that learns a parametric weight function in an online fashion to gain more discoveries. We also theoretically prove that our proposed procedures, under some easily verifiable assumptions, would lead to an increase of statistical power over a popular online testing procedure proposed by (Javanmard and Montanari, 2018). Finally, we demonstrate the superior performance of our procedure, by comparing it to state-of-the-art online multiple testing procedures, on both synthetic data and real data generated from different applications.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,952,960,,,,,,,,,,,,,,,,WOS:000559931300055,0
C,"Falahatgar, M; Ohannessian, M; Orlitsky, A; Pichapati, V",,"Chiappa, S; Calandra, R",,"Falahatgar, Moein; Ohannessian, Mesrob; Orlitsky, Alon; Pichapati, Venkatadheeraj",,,Towards Competitive N-gram Smoothing,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"N-gram models remain a fundamental component of language modeling. In data-scarce regimes, they are a strong alternative to neural models. Even when not used as-is, recent work shows they can regularize neural models. Despite this success, the effectiveness of one of the best N-gram smoothing methods, the one suggested by Kneser and Ney (1995), is not fully understood. In the hopes of explaining this performance, we study it through the lens of competitive distribution estimation: the ability to perform as well as an oracle aware of further structure in the data. We first establish basic competitive properties of Kneser-Ney smoothing. We then investigate the nature of its backoff mechanism and show that it emerges from first principles, rather than being an assumption of the model. We do this by generalizing the Good-Turing estimator to the contextual setting. This exploration leads us to a powerful generalization of Kneser-Ney, which we conjecture to have even stronger competitive properties. Empirically, it significantly improves performance on language modeling, even matching feed-forward neural models. To show that the mechanisms at play are not restricted to language modeling, we demonstrate similar gains on the task of predicting attack types in the Global Terrorism Database.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,4206,4214,,,,,,,,,,,,,,,,WOS:000559931300086,0
C,"Gultchin, L; Kusner, MJ; Kanade, V; Silva, R",,"Chiappa, S; Calandra, R",,"Gultchin, Limor; Kusner, Matt J.; Kanade, Varun; Silva, Ricardo",,,Differentiable Causal Backdoor Discovery,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Discovering the causal effect of a decision is critical to nearly all forms of decision-making. In particular, it is a key quantity in drug development, in crafting government policy, and when implementing a real-world machine learning system. Given only observational data, confounders often obscure the true causal effect. Luckily, in some cases, it is possible to recover the causal effect by using certain observed variables to adjust for the effects of confounders. However, without access to the true causal model, finding this adjustment requires brute-force search. In this work, we present an algorithm that exploits auxiliary variables, similar to instruments, in order to find an appropriate adjustment by a gradient-based optimization method. We demonstrate that it outperforms practical alternatives in estimating the true causal effect, without knowledge of the full causal graph.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3970,3978,,,,,,,,,,,,,,,,WOS:000559931301028,0
C,"Huggins, JH; Kasprzak, M; Campbell, T; Broderick, T",,"Chiappa, S; Calandra, R",,"Huggins, Jonathan H.; Kasprzak, Mikolaj; Campbell, Trevor; Broderick, Tamara",,,Validated Variational Inference via Practical Posterior Error Bounds,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Variational inference has become an increasingly attractive fast alternative to Markov chain Monte Carlo methods for approximate Bayesian inference. However, a major obstacle to the widespread use of variational methods is the lack of post-hoc accuracy measures that are both theoretically justified and computationally efficient. In this paper, we provide rigorous bounds on the error of posterior mean and uncertainty estimates that arise from full-distribution approximations, as in variational inference. Our bounds are widely applicable, as they require only that the approximating and exact posteriors have polynomial moments. Our bounds are also computationally efficient for variational inference because they require only standard values from variational objectives, straightforward analytic calculations, and simple Monte Carlo estimates. We show that our analysis naturally leads to a new and improved workflow for validated variational inference. Finally, we demonstrate the utility of our proposed workflow and error bounds on a robust regression problem and on a real-data example with a widely used multilevel hierarchical model.",,,,,,"Kasprzak, Mikolaj/0000-0003-0825-7751",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1792,1801,,,,,,,,,,,,,,,,WOS:000559931301045,0
C,"Husain, H; Balle, B; Cranko, Z; Nock, R",,"Chiappa, S; Calandra, R",,"Husain, Hisham; Balle, Borja; Cranko, Zac; Nock, Richard",,,Local Differential Privacy for Sampling,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Differential privacy (DP) is a leading privacy protection focused by design on individual privacy. In the local model of DP, strong privacy is achieved by privatizing each user's individual data before sending it to an untrusted aggregator for analysis. While in recent years local DP has been adopted for practical deployments, most research in this area focuses on problems where each individual holds a single data record. In many problems of practical interest this assumption is unrealistic since nowadays most user-owned devices collect large quantities of data (e.g. pictures, text messages, time series). We propose to model this scenario by assuming each individual holds a distribution over the space of data records, and develop novel local DP methods to sample privately from these distributions. Our main contribution is a boosting-based density estimation algorithm for learning samplers that generate synthetic data while protecting the underlying distribution of each user with local DP. We give approximation guarantees quantifying how well these samplers approximate the true distribution. Experimental results against DP kernel density estimation and DP GANs displays the quality of our results.",,,,,"Husain, Hisham/AAO-7898-2021",,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3404,3412,,,,,,,,,,,,,,,,WOS:000559931301049,0
C,"Korovina, K; Xu, SL; Kandasamy, K; Neiswanger, W; Poczos, B; Schneider, J; Xing, EP",,"Chiappa, S; Calandra, R",,"Korovina, Ksenia; Xu, Sailun; Kandasamy, Kirthevasan; Neiswanger, Willie; Poczos, Barnabas; Schneider, Jeff; Xing, Eric P.",,,ChemBO: Bayesian Optimization of Small Organic Molecules with Synthesizable Recommendations,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"In applications such as molecule design or drug discovery, it is desirable to have an algorithm which recommends new candidate molecules based on the results of past tests. These molecules first need to be synthesized and then tested for objective properties. We describe ChemBO, a Bayesian optimization framework for generating and optimizing organic molecules for desired molecular properties. While most existing data-driven methods for this problem do not account for sample efficiency or fail to enforce realistic constraints on synthesizability, our approach explores the synthesis graph in a sample-efficient way and produces synthesizable candidates. We implement ChemBO as a Gaussian process model and explore existing molecular kernels for it. Moreover, we propose a novel optimal-transport based distance and kernel that accounts for graphical information explicitly. In our experiments, we demonstrate the efficacy of the proposed approach on several molecular optimization problems.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3393,3402,,,,,,,,,,,,,,,,WOS:000559931301083,0
C,"Martens, K; Yau, C",,"Chiappa, S; Calandra, R",,"Martens, Kaspar; Yau, Christopher",,,BasisVAE: Translation-invariant feature-level clustering with Variational Autoencoders,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Variational Autoencoders (VAEs) provide a flexible and scalable framework for non-linear dimensionality reduction. However, in application domains such as genomics where data sets are typically tabular and high-dimensional, a black-box approach to dimensionality reduction does not provide sufficient insights. Common data analysis workflows additionally use clustering techniques to identify groups of similar features. This usually leads to a two-stage process, however, it would be desirable to construct a joint modelling framework for simultaneous dimensionality reduction and clustering of features. In this paper, we propose to achieve this through the Basis VAE: a combination of the VAE and a probabilistic clustering prior, which lets us learn a one-hot basis function representation as part of the decoder network. Furthermore, for scenarios where not all features are aligned, we develop an extension to handle translation-invariant basis functions. We show how a collapsed variational inference scheme leads to scalable and efficient inference for BasisVAE, demonstrated on various toy examples as well as on single-cell gene expression data.",,,,,,"Yau, Christopher/0000-0001-7615-8523",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2928,2936,,,,,,,,,,,,,,,,WOS:000559931302032,0
C,"Nitanda, A; Suzuki, T",,"Chiappa, S; Calandra, R",,"Nitanda, Atsushi; Suzuki, Taiji",,,Functional Gradient Boosting for Learning Residual-like Networks with Statistical Guarantees,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Recently, several studies have proposed progressive or sequential layer-wise training methods based on the boosting theory for deep neural networks. However, most studies lack the global convergence guarantees or require weak learning conditions that can be verified a posteriori after running methods. Moreover, generalization bounds usually have a worse dependence on the network depth. In this paper, to resolve these problems, we propose a new functional gradient boosting for learning deep residual-like networks in a layer-wise fashion with its statistical guarantees on multi-class classification tasks. In the proposed method, each residual block is recognized as a functional gradient (i.e., weak learner), and the functional gradient step is performed by stacking it on the network, resulting in a strong optimization ability. In the theoretical analysis, we show the global convergence of the method under a standard margin assumption on a data distribution instead of a weak learning condition, and we eliminate a worse dependence on the network depth in a generalization bound via a fine-grained convergence analysis. Moreover, we show that the existence of a learnable function with a large margin on a training dataset significantly improves a generalization bound. Finally, we experimentally demonstrate that our proposed method is certainly useful for learning deep residual networks.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2981,2990,,,,,,,,,,,,,,,,WOS:000559931302057,0
C,"Rowland, M; Dabney, W; Munos, R",,"Chiappa, S; Calandra, R",,"Rowland, Mark; Dabney, Will; Munos, Remi",,,Adaptive Trade-Offs in Off-Policy Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"A great variety of off-policy learning algorithms exist in the literature, and new breakthroughs in this area continue to be made, improving theoretical understanding and yielding state-of-the-art reinforcement learning algorithms. In this paper, we take a unifying view of this space of algorithms, and consider their trade-offs of three fundamental quantities: update variance, fixed-point bias, and contraction rate. This leads to new perspectives on existing methods, and also naturally yields novel algorithms for off-policy evaluation and control. We develop one such algorithm, C-trace, demonstrating that it is able to more efficiently make these trade-offs than existing methods in use, and that it can be scaled to yield state-of-the-art performance in large-scale environments.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,34,43,,,,,,,,,,,,,,,,WOS:000559931302095,0
C,"Shi, CS; Xiong, W; Shen, C; Yang, J",,"Chiappa, S; Calandra, R",,"Shi, Chengshuai; Xiong, Wei; Shen, Cong; Yang, Jing",,,Decentralized Multi-player Multi-armed Bandits with No Collision Information,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"The decentralized stochastic multi-player multi-armed bandit (MP-MAB) problem, where the collision information is not available to the players, is studied in this paper. Building on the seminal work of Boursier and Perchet (2019), we propose error correction synchronization involving communication (EC-SIC), whose regret is shown to approach that of the centralized stochastic MP-MAB with collision information. By recognizing that the communication phase without collision information corresponds to the Z-channel model in information theory, the proposed EC-SIC algorithm applies optimal error correction coding for the communication of reward statistics. A fixed message length, as opposed to the logarithmically growing one in Boursier and Perchet (2019), also plays a crucial role in controlling the communication loss. Experiments with practical Z-channel codes, such as repetition code, flip code and modified Hamming code, demonstrate the superiority of EC-SIC in both synthetic and real-world datasets.",,,,,,"Shi, Chengshuai/0000-0002-2727-8251",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303017,0
C,"Song, YX; Miao, N; Zhou, H; Yu, LT; Wang, MX; Li, L",,"Chiappa, S; Calandra, R",,"Song, Yuxuan; Miao, Ning; Zhou, Hao; Yu, Lantao; Wang, Mingxuan; Li, Lei",,,Improving Maximum Likelihood Training for Text Generation with Density Ratio Estimation,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Autoregressive sequence generative models trained by Maximum Likelihood Estimation suffer the exposure bias problem in practical finite sample scenarios. The crux is that the number of training samples for Maximum Likelihood Estimation is usually limited and the input data distributions are different at training and inference stages. Many methods have been proposed to solve the above problem (Yu et al., 2017; Lu et al., 2018), which relies on sampling from the non-stationary model distribution and suffers from high variance or biased estimations. In this paper, we propose psi-MLE, a new training scheme for autoregressive sequence generative models, which is effective and stable when operating at large sample space encountered in text generation. We derive our algorithm from a new perspective of self-augmentation and introduce bias correction with density ratio estimation. Extensive experimental results on synthetic data and real-world text generation tasks demonstrate that our method stably outperforms Maximum Likelihood Estimation and other state-of-the-art sequence generative models in terms of both quality and diversity.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303026,0
C,"Stuhmer, J; Turner, RE; Nowozin, S",,"Chiappa, S; Calandra, R",,"Stuhmer, Jan; Turner, Richard E.; Nowozin, Sebastian",,,Independent Subspace Analysis for Unsupervised Learning of Disentangled Representations,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Recently there has been an increased interest in unsupervised learning of disentangled representations using the Variational Autoencoder (VAE) framework. Most of the existing work has focused largely on modifying the variational cost function to achieve this goal. We first show that these modifications, e.g. beta-VAE, simplify the tendency of variational inference to underfit causing pathological over-pruning and over-orthogonalization of learned components. Second we propose a complementary approach: to modify the probabilistic model with a structured latent prior. This prior allows to discover latent variable representations that are structured into a hierarchy of independent vector spaces. The proposed prior has three major advantages: First, in contrast to the standard VAE normal prior the proposed prior is not rotationally invariant. This resolves the problem of unidentifiability of the standard VAE normal prior. Second, we demonstrate that the proposed prior encourages a disentangled latent representation which facilitates learning of disentangled representations. Third, extensive quantitative experiments demonstrate that the prior significantly mitigates the trade-off between reconstruction loss and disentanglement over the state of the art.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303032,0
C,"Yin, MZ; Wang, YXR; Sarkar, P",,"Chiappa, S; Calandra, R",,"Yin, Mingzhang; Wang, Y. X. Rachel; Sarkar, Purnamrita",,,A Theoretical Case Study of Structured Variational Inference for Community Detection,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Mean-field variational inference (MFVI) has been widely applied in large scale Bayesian inference. However, MFVI assumes independent distribution on the latent variables, which often leads to objective functions with many local optima, making optimization algorithms sensitive to initialization. In this paper, we study the advantage of structured variational inference in the context of a simple two-class Stochastic Blockmodel. To facilitate theoretical analysis, the variational distribution is constructed to have a simple pairwise dependency structure on the nodes of the network. We prove that, in a broad density regime and for general random initializations, unlike MFVI, the estimated class labels by structured VI converge to the ground truth with high probability, when the model parameters are known, estimated within a reasonable range or jointly optimized with the variational parameters. In addition, empirically we demonstrate structured VI is more robust compared with MFVI when the graph is sparse and the signal to noise ratio is low. The paper takes a first step towards quantifying the role of added dependency structure in variational inference for community detection.",,,,,"Yin, Mingzhang/R-5702-2018",,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303098,0
C,"Zimmert, J; Seldin, Y",,"Chiappa, S; Calandra, R",,"Zimmert, Julian; Seldin, Yevgeny",,,An Optimal Algorithm for Adversarial Bandits with Arbitrary Delays,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We propose a new algorithm for adversarial multi-armed bandits with unrestricted delays. The algorithm is based on a novel hybrid regularizer applied in the Follow the Regularized Leader (FTRL) framework. It achieves O(root kn + root D log(k)) regret guarantee, where k is the number of arms, n is the number of rounds, and D is the total delay. The result matches the lower bound within constants and requires no prior knowledge of n or D. Additionally, we propose a refined tuning of the algorithm, which achieves O(root kn + min(S)(vertical bar S vertical bar + root D((S) over bar)log(k))) regret guarantee, where S is a set of rounds excluded from delay counting, (S) over bar = [n] \ S are the counted rounds, and D-(S) over bar, is the total delay in the counted rounds. If the delays are highly unbalanced, the latter regret guarantee can be significantly tighter than the former. The result requires no advance knowledge of the delays and resolves an open problem of Thune et al. (2019). The new FTRL algorithm and its refined tuning are anytime and require no doubling, which resolves another open problem of (2019).",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3285,3293,,,,,,,,,,,,,,,,WOS:000559931304027,0
C,"Adolphs, L; Daneshmand, H; Lucchi, A; Hofmann, T",,"Chaudhuri, K; Sugiyama, M",,"Adolphs, Leonard; Daneshmand, Hadi; Lucchi, Aurelien; Hofmann, Thomas",,,Local Saddle Point Optimization: A Curvature Exploitation Approach,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Gradient-based optimization methods are the most popular choice for finding local optima for classical minimization and saddle point problems. Here, we highlight a systemic issue of gradient dynamics that arise for saddle point problems, namely the presence of undesired stable stationary points that are no local optima. We propose a novel optimization approach that exploits curvature information in order to escape from these undesired stationary points. We prove that different optimization methods, including gradient method and ADAGRAD, equipped with curvature exploitation can escape non-optimal stationary points. We also provide empirical results on common saddle point problems which confirm the advantage of using curvature exploitation.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,486,495,,,,,,,,,,,,,,,,WOS:000509687900051,0
C,"Alvarez-Melis, D; Jegelka, S; Jaakkola, TS",,"Chaudhuri, K; Sugiyama, M",,"Alvarez-Melis, David; Jegelka, Stefanie; Jaakkola, Tommi S.",,,Towards Optimal Transport with Global Invariances,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Many problems in machine learning involve calculating correspondences between sets of objects, such as point clouds or images. Discrete optimal transport provides a natural and successful approach to such tasks whenever the two sets of objects can be represented in the same space, or at least distances between them can be directly evaluated. Unfortunately neither requirement is likely to hold when object representations are learned from data. Indeed, automatically derived representations such as word embeddings are typically fixed only up to some global transformations, for example, reflection or rotation. As a result, pairwise distances across two such instances are ill-defined without specifying their relative transformation. In this work, we propose a general framework for optimal transport in the presence of latent global transformations. We cast the problem as a joint optimization over transport couplings and transformations chosen from a flexible class of invariances, propose algorithms to solve it, and show promising results in various tasks, including a popular unsupervised word translation benchmark.",,,,,"Alvarez-Melis, David/AAV-1099-2021",,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901095,0
C,"Charles, Z; Rosenberg, H; Papailiopoulos, D",,"Chaudhuri, K; Sugiyama, M",,"Charles, Zachary; Rosenberg, Harrison; Papailiopoulos, Dimitris",,,A Geometric Perspective on the Transferability of Adversarial Directions,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"State-of-the-art machine learning models frequently misclassify inputs that have been perturbed in an adversarial manner. Adversarial perturbations generated for a given input and a specific classifier often seem to be effective on other inputs and even different classifiers. In other words, adversarial perturbations seem to transfer between different inputs, models, and even different neural network architectures. In this work, we show that in the context of linear classifiers and two-layer ReLU networks, there provably exist directions that give rise to adversarial perturbations for many classifiers and data points simultaneously. We show that these transferable adversarial directions are guaranteed to exist for linear separators of a given set, and will exist with high probability for linear classifiers trained on independent sets drawn from the same distribution. We extend our results to large classes of two-layer ReLU networks. We further show that adversarial directions for ReLU networks transfer to linear classifiers while the reverse need not hold, suggesting that adversarial perturbations for more complex models are more likely to transfer to other classifiers. We validate our findings empirically, even for deeper ReLU networks.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902001,0
C,"Cohen, D; Daniely, A; Globerson, A; Elidan, G",,"Chaudhuri, K; Sugiyama, M",,"Cohen, Deborah; Daniely, Amit; Globerson, Amir; Elidan, Gal",,,Learning Rules-First Classifiers,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Complex classifiers may exhibit embarassing failures in cases where humans can easily provide a justified classification. Avoiding such failures is obviously of key importance. In this work, we focus on one such setting, where a label is perfectly predictable if the input contains certain features, or rules, and otherwise it is predictable by a linear classifier. We define a hypothesis class that captures this notion and determine its sample complexity. We also give evidence that efficient algorithms cannot achieve this sample complexity. We then derive a simple and efficient algorithm and show that its sample complexity is close to optimal, among efficient algorithms. Experiments on synthetic and sentiment analysis data demonstrate the efficacy of the method, both in terms of accuracy and interpretability.",,,,,,"Globerson, Amir/0000-0003-2557-1742",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901046,0
C,"Fernandez, T; Gretton, A",,"Chaudhuri, K; Sugiyama, M",,"Fernandez, Tamara; Gretton, Arthur",,,A maximum-mean-discrepancy goodness-of-fit test for censored data,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We introduce a kernel-based goodness-of-fit test for censored data, where observations may be missing in random time intervals: a common occurrence in clinical trials and industrial life-testing. The test statistic is straightforward to compute, as is the test threshold, and we establish consistency under the null. Unlike earlier approaches such as the Log-rank test, we make no assumptions as to how the data distribution might differ from the null, and our test has power against a very rich class of alternatives. In experiments, our test outperforms competing approaches for periodic and Weibull hazard functions (where risks are time dependent), and does not show the failure modes of tests that rely on user-defined features. Moreover, in cases where classical tests are provably most powerful, our test performs almost as well, while being more general.",,,,,,"Gretton, Arthur/0000-0003-3169-7624",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687903002,0
C,"Kohler, J; Daneshmand, H; Lucchi, A; Hofmann, T; Zhou, M; Neymeyr, K",,"Chaudhuri, K; Sugiyama, M",,"Kohler, Jonas; Daneshmand, Hadi; Lucchi, Aurelien; Hofmann, Thomas; Zhou, Ming; Neymeyr, Klaus",,,Exponential convergence rates for Batch Normalization: The power of length-direction decoupling in non-convex optimization,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Normalization techniques such as Batch Normalization have been applied successfully for training deep neural networks. Yet, despite its apparent empirical benefits, the reasons behind the success of Batch Normalization are mostly hypothetical. We here aim to provide a more thorough theoretical understanding from a classical optimization perspective. Our main contribution towards this goal is the identification of various problem instances in the realm of machine learning where Batch Normalization can provably accelerate optimization. We argue that this acceleration is due to the fact that Batch Normalization splits the optimization task into optimizing length and direction of the parameters separately. This allows gradient-based methods to leverage a favourable global structure in the loss landscape that we prove to exist in Learning Halfspace problems and neural network training with Gaussian inputs. We thereby turn Batch Normalization from an effective practical heuristic into a provably converging algorithm for these settings. Furthermore, we substantiate our analysis with empirical evidence that suggests the validity of our theoretical results in a broader context.",,,,,"Neymeyr, Klaus/R-1223-2016","Neymeyr, Klaus/0000-0002-1464-2851; Zhou, Ming/0000-0001-7096-1649",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,806,815,,,,,,,,,,,,,,,,WOS:000509687900084,0
C,"Mallasto, A; Hauberg, S; Feragen, A",,"Chaudhuri, K; Sugiyama, M",,"Mallasto, Anton; Hauberg, Soren; Feragen, Aasa",,,Probabilistic Riemannian submanifold learning with wrapped Gaussian process latent variable models,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Latent variable models (LVMs) learn probabilistic models of data manifolds lying in an ambient Euclidean space. In a number of applications, a priori known spatial constraints can shrink the ambient space into a considerably smaller manifold. Additionally, in these applications the Euclidean geometry might induce a suboptimal similarity measure, which could be improved by choosing a different metric. Euclidean models ignore such information and assign probability mass to data points that can never appear as data, and vastly different likelihoods to points that are similar under the desired metric. We propose the wrapped Gaussian process latent variable model (WGPLVM), that extends Gaussian process latent variable models to take values strictly on a given ambient Riemannian manifold, making the model blind to impossible data points. This allows non-linear, probabilistic inference of low-dimensional Riemannian submanifolds from data. Our evaluation on diverse datasets show that we improve performance on several tasks, including encoding, visualization and uncertainty quantification.",,,,,"Feragen, Aasa/G-1465-2013; Hauberg, Soren/L-2104-2016","Feragen, Aasa/0000-0002-9945-981X; Hauberg, Soren/0000-0001-7223-877X",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902043,0
C,"Mukhoty, B; Gopakumar, G; Jain, P; Kar, P",,"Chaudhuri, K; Sugiyama, M",,"Mukhoty, Bhaskar; Gopakumar, Govind; Jain, Prateek; Kar, Purushottam",,,Globally-convergent Iteratively Reweighted Least Squares for Robust Regression Problems,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We provide the first global model recovery results for the IRLS (iteratively reweighted least squares) heuristic for robust regression problems. IRLS is known to offer excellent performance, despite bad initializations and data corruption, for several parameter estimation problems. Existing analyses of IRLS frequently require careful initialization, thus offering only local convergence guarantees. We remedy this by proposing augmentations to the basic IRLS routine that not only offer guaranteed global recovery, but in practice also outperform state-of-the-art algorithms for robust regression. Our routines are more immune to hyperparameter misspecification in basic regression tasks, as well as applied tasks such as linear-armed bandit problems. Our theoretical analyses rely on a novel extension of the notions of strong convexity and smoothness to weighted strong convexity and smoothness, and establishing that sub-Gaussian designs offer bounded weighted condition numbers. These notions may be useful in analyzing other algorithms as well.",,,,,"Mukhoty, Bhaskar Pratim/AAG-8322-2020; Kar, Purushottam/W-8113-2019","Mukhoty, Bhaskar Pratim/0000-0002-8594-980X; Kar, Purushottam/0000-0003-2096-5267",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,313,322,,,,,,,,,,,,,,,,WOS:000509687900033,0
C,"Rowland, M; Hron, J; Tang, YH; Choromanski, K; Sarlos, T; Weller, A",,"Chaudhuri, K; Sugiyama, M",,"Rowland, Mark; Hron, Jiri; Tang, Yunhao; Choromanski, Krzysztof; Sarlos, Tamas; Weller, Adrian",,,Orthogonal Estimation of Wasserstein Distances,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Wasserstein distances are increasingly used in a wide variety of applications in machine learning. Sliced Wasserstein distances form an important subclass which may be estimated efficiently through one-dimensional sorting operations. In this paper, we propose a new variant of sliced Wasserstein distance, study the use of orthogonal coupling in Monte Carlo estimation of Wasserstein distances and draw connections with stratified sampling, and evaluate our approaches experimentally in a range of large-scale experiments in generative modelling and reinforcement learning.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,186,195,,,,,,,,,,,,,,,,WOS:000509687900020,0
C,"Vaswani, S; Bach, F; Schmidt, M",,"Chaudhuri, K; Sugiyama, M",,"Vaswani, Sharan; Bach, Francis; Schmidt, Mark",,,Fast and Faster Convergence of SGD for Over-Parameterized Models (and an Accelerated Perceptron),"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Modern machine learning focuses on highly expressive models that are able to fit or interpolate the data completely, resulting in zero training loss. For such models, we show that the stochastic gradients of common loss functions satisfy a strong growth condition. Under this condition, we prove that constant step-size stochastic gradient descent (SGD) with Nesterov acceleration matches the convergence rate of the deterministic accelerated method for both convex and strongly-convex functions. We also show that this condition implies that SGD can find a first-order stationary point as efficiently as full gradient descent in non-convex settings. Under interpolation, we further show that all smooth loss functions with a finite-sum structure satisfy a weaker growth condition. Given this weaker condition, we prove that SGD with a constant step-size attains the deterministic convergence rate in both the strongly-convex and convex settings. Under additional assumptions, the above results enable us to prove an O(1/k(2)) mistake bound for k iterations of a stochastic perceptron algorithm using the squared-hinge loss. Finally, we validate our theoretical findings with experiments on synthetic and real datasets.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901025,0
C,"Yadav, P; Nimishakavi, M; Yadati, N; Vashishth, S; Rajkumar, A; Talukdar, P",,"Chaudhuri, K; Sugiyama, M",,"Yadav, Prateek; Nimishakavi, Madhav; Yadati, Naganand; Vashishth, Shikhar; Rajkumar, Arun; Talukdar, Partha",,,Lovasz Convolutional Networks,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Semi-supervised learning on graph structured data has received significant attention with the recent introduction of Graph Convolution Networks (GCN). While traditional methods have focused on optimizing a loss augmented with Laplacian regularization framework, GCNs perform an implicit Laplacian type regularization to capture local graph structure. In this work, we propose Lovosz Convolutional Network (LCNs) which are capable of incorporating global graph properties. LCNs achieve this by utilizing Lovasz's orthonormal embeddings of the nodes. We analyse local and global properties of graphs and demonstrate settings where LCNs tend to work better than GCNs. We validate the proposed method on standard random graph models such as stochastic block models (SBM) and certain community structure based graphs where LCNs outperform GCNs and learn more intuitive embeddings. We also perform extensive binary and multi-class classification experiments on real world datasets to demonstrate LCN's effectiveness. In addition to simple graphs, we also demonstrate the use of LCNs on hyper-graphs by identifying settings where they are expected to work better than GCNs.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902003,0
C,"Andersen, MR; Winther, O; Hansen, LK; Poldrack, R; Koyejo, O",,"Storkey, A; PerezCruz, F",,"Andersen, Michael Riis; Winther, Ole; Hansen, Lars Kai; Poldrack, Russell; Koyejo, Oluwasanmi",,,Bayesian Structure Learning for Dynamic Brain Connectivity,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Human brain activity as measured by fMRI exhibits strong correlations between brain regions which are believed to vary over time. Importantly, dynamic connectivity has been linked to individual differences in physiology, psychology and behavior, and has shown promise as a biomarker for disease. The state of the art in computational neuroimaging is to estimate the brain networks as relatively short sliding window covariance matrices, which leads to high variance estimates, thereby resulting in high overall error. This manuscript proposes a novel Bayesian model for dynamic brain connectivity. Motivated by the underlying neuroscience, the model estimates covariances which vary smoothly over time, with an instantaneous decomposition into a collection of spatially sparse components - resulting in parsimonious and highly interpretable estimates of dynamic brain connectivity. Simulated results are presented to illustrate the performance of the model even when it is mis-specified. For real brain imaging data with unknown ground truth, in addition to qualitative evaluation, we devise a simple classification task which suggests that the estimated brain networks better capture the underlying structure.",,,,,,"Hansen, Lars Kai/0000-0003-0442-5877; Winther, Ole/0000-0002-1966-3205; Andersen, Michael/0000-0002-7411-5842; Poldrack, Russell/0000-0001-6755-0259",,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300150,0
C,"Chen, LQ; Dai, SY; Pu, YC; Zhou, EJ; Li, CY; Su, QL; Chen, CY; Carin, L",,"Storkey, A; PerezCruz, F",,"Chen, Liqun; Dai, Shuyang; Pu, Yunchen; Zhou, Erjin; Li, Chunyuan; Su, Qinliang; Chen, Changyou; Carin, Lawrence",,,Symmetric Variational Autoencoder and Connections to Adversarial Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"A new form of the variational autoencoder (VAE) is proposed, based on the symmetric Kullback-Leibler divergence. It is demonstrated that learning of the resulting symmetric VAE (sVAE) has close connections to previously developed adversarial-learning methods. This relationship helps unify the previously distinct techniques of VAE and adversarially learning, and provides insights that allow us to ameliorate shortcomings with some previously developed adversarial methods. In addition to an analysis that motivates and explains the sVAE, an extensive set of experiments validate the utility of the approach.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300070,0
C,"Cucuringu, M; Tyagi, H",,"Storkey, A; PerezCruz, F",,"Cucuringu, Mihai; Tyagi, Hemant",,,On denoising modulo 1 samples of a function,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Consider an unknown smooth function f : [0; 1] -> R, and say we are given n noisy mod 1 samples of f, i.e., y(i) = (f (x(i)) + eta(i)) mod 1 for x(i) is an element of [0, 1], where eta(i) denotes noise. Given the samples (x(i); y(i))(i=1)(n) our goal is to recover smooth, robust estimates of the clean samples f (x(i)) mod 1. We formulate a natural approach for solving this problem which works with representations of mod 1 values over the unit circle. This amounts to solving a quadratically constrained quadratic program (QCQP) with non-convex constraints involving points lying on the unit circle. Our proposed approach is based on solving its relaxation which is a trust region subproblem, and hence solvable efficiently. We demonstrate its robustness to noise via extensive simulations on several synthetic examples, and provide a detailed theoretical analysis.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300195,0
C,"Derezinski, M; Mahajan, D; Keerthi, SS; Vishwanathan, SVN; Weimer, M",,"Storkey, A; PerezCruz, F",,"Derezinski, Michal; Mahajan, Dhruv; Keerthi, S. Sathiya; Vishwanathan, S. V. N.; Weimer, Markus",,,Batch-Expansion Training: An Efficient Optimization Framework,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"We propose Batch-Expansion Training (BET), a framework for running a batch optimizer on a gradually expanding dataset. As opposed to stochastic approaches, batches do not need to be resampled i.i.d. at every iteration, thus making BET more resource efficient in a distributed setting, and when disk-access is constrained. Moreover, BET can be easily paired with most batch optimizers, does not require any parameter-tuning, and compares favorably to existing stochastic and batch methods. We show that when the batch size grows exponentially with the number of outer iterations, BET achieves optimal (O) over tilde O (1/epsilon) data-access convergence rate for strongly convex objectives. Experiments in parallel and distributed settings show that BET performs better than standard batch and stochastic approaches.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300078,0
C,"Kim, H; Teh, YW",,"Storkey, A; PerezCruz, F",,"Kim, Hyunjik; Teh, Yee Whye",,,Scaling up the Automatic Statistician: Scalable Structure Discovery using Gaussian Processes,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Automating statistical modelling is a challenging problem in artificial intelligence. The Automatic Statistician takes a first step in this direction, by employing a kernel search algorithm with Gaussian Processes (GP) to provide interpretable statistical models for regression problems. However this does not scale due to its O(N-3) running time for the model selection. We propose Scalable Kernel Composition (SKC), a scalable kernel search algorithm that extends the Automatic Statistician to bigger data sets. In doing so, we derive a cheap upper bound on the GP marginal likelihood that sandwiches the marginal likelihood with the variational lower bound. We show that the upper bound is significantly tighter than the lower bound and thus useful for model selection.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300061,0
C,"Salzo, S; Suykens, JAK; Rosasco, L",,"Storkey, A; PerezCruz, F",,"Salzo, Saverio; Suykens, Johan A. K.; Rosasco, Lorenzo",,,Solving l(p)-norm regularization with tensor kernels,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"In this paper, we discuss how a suitable family of tensor kernels can be used to efficiently solve nonparametric extensions of l(p) regularized learning methods. Our main contribution is proposing a fast dual algorithm, and showing that it allows to solve the problem efficiently. Our results contrast recent findings suggesting kernel methods cannot be extended beyond Hilbert setting. Numerical experiments confirm the effectiveness of the method.",,,,,"Salzo, Saverio/AAZ-7481-2021; Suykens, Johan/C-9781-2014","Salzo, Saverio/0000-0003-0494-9101; Suykens, Johan/0000-0002-8846-6352",,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300173,0
C,"Schmit, S; Riquelme, C",,"Storkey, A; PerezCruz, F",,"Schmit, Sven; Riquelme, Carlos",,,Human Interaction with Recommendation Systems,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Many recommendation algorithms rely on user data to generate recommendations. However, these recommendations also affect the data obtained from future users. This work aims to understand the effects of this dynamic interaction. We propose a simple model where users with heterogeneous preferences arrive over time. Based on this model, we prove that naive estimators, i.e. those which ignore this feedback loop, are not consistent. We show that consistent estimators are efficient in the presence of myopic agents. Our results are validated using extensive simulations.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300091,0
C,"Xie, JH; Qian, H; Shen, ZB; Zhang, C",,"Storkey, A; PerezCruz, F",,"Xie, Jiahao; Qian, Hui; Shen, Zebang; Zhang, Chao",,,Towards Memory-friendly Deterministic Incremental Gradient Method,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Incremental Gradient (IG) methods are classical strategies in solving finite sum minimization problems. Deterministic IG methods are particularly favorable in handling massive scale problem due to its memory-friendly data access pattern. In this paper, we propose a new deterministic variant of the IG method SVRG that blends a periodically updated full gradient with a component function gradient selected in a cyclic order. Our method uses only O(1) extra gradient storage without compromising the linear convergence. Empirical results demonstrate that the proposed method is advantageous over existing incremental gradient algorithms, especially on problems that does not fit into physical memory.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300120,0
C,"Zeng, JS; Ma, K; Yao, Y",,"Storkey, A; PerezCruz, F",,"Zeng, Jinshan; Ma, Ke; Yao, Yuan",,,Finding Global Optima in Nonconvex Stochastic Semidefinite Optimization with Variance Reduction,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"There is a recent surge of interest in nonconvex reformulations via low-rank factorization for stochastic convex semidefinite optimization problem in the purpose of efficiency and scalability. Compared with the original convex formulations, the nonconvex ones typically involve much fewer variables, allowing them to scale to scenarios with millions of variables. However, it opens a new challenge that under what conditions the nonconvex stochastic algorithms may find the global optima effectively despite their empirical success in applications. In this paper, we provide an answer that a stochastic gradient descent method with variance reduction, can be adapted to solve the nonconvex reformulation of the original convex problem, with a global linear convergence, i.e., converging to a global optimum exponentially fast, at a proper initial choice in the restricted strongly convex case. Experimental studies on both simulation and real-world applications on ordinal embedding are provided to show the effectiveness of the proposed algorithms.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300022,0
C,"Basse, G; Pillai, N; Smith, A",,"Gretton, A; Robert, CC",,"Basse, Guillaume; Pillai, Natesh; Smith, Aaron",,,Parallel Markov Chain Monte Carlo via Spectral Clustering,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"As it has become common to use many computer cores in routine applications, finding good ways to parallelize popular algorithms has become increasingly important. In this paper, we present a parallelization scheme for Markov chain Monte Carlo (MCMC) methods based on spectral clustering of the underlying state space, generalizing earlier work on parallelization of MCMC methods by state space partitioning. We show empirically that this approach speeds up MCMC sampling for multimodal distributions and that it can be usefully applied in greater generality than several related algorithms. Our algorithm converges under reasonable conditions to an 'optimal' MCMC algorithm. We also show that our approach can be asymptotically far more efficient than naive parallelization, even in situations such as completely flat target distributions where no unique optimal algorithm exists. Finally, we combine theoretical and empirical bounds to provide practical guidance on the choice of tuning parameters.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,1318,1327,,,,,,,,,,,,,,,,WOS:000508662100143,0
C,"Bloniarz, A; Wu, C; Yu, B; Talwalkar, A",,"Gretton, A; Robert, CC",,"Bloniarz, Adam; Wu, Christopher; Yu, Bin; Talwalkar, Ameet",,,Supervised neighborhoods for distributed nonparametric regression,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Techniques for nonparametric regression based on fitting small-scale local models at prediction time have long been studied in statistics and pattern recognition, but have received less attention in modern large-scale machine learning applications. In practice, such methods are generally applied to low-dimensional problems, but may falter with high-dimensional predictors if they use a Euclidean distance-based kernel. We propose a new method, Silo, for fitting prediction-time local models that uses supervised neighborhoods that adapt to the local shape of the regression surface. To learn such neighborhoods, we use a weight function between points derived from random forests. We prove the consistency of Silo, and demonstrate through simulations and real data that our method works well in both the serial and distributed settings. In the latter case, Silo learns the weighting function in a divide-and-conquer manner, entirely avoiding communication at training time.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,1450,1459,,,,,,,,,,,,,,,,WOS:000508662100157,0
C,"Rolet, A; Cuturi, M; Peyre, G",,"Gretton, A; Robert, CC",,"Rolet, Antoine; Cuturi, Marco; Peyre, Gabriel",,,Fast Dictionary Learning with a Smoothed Wasserstein Loss,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"We consider in this paper the dictionary learning problem when the observations are normalized histograms of features. This problem can be tackled using non-negative matrix factorization approaches, using typically Euclidean or Kullback-Leibler fitting errors. Because these fitting errors are separable and treat each feature on equal footing, they are blind to any similarity the features may share. We assume in this work that we have prior knowledge on these features. To leverage this side-information, we propose to use the Wasserstein (a.k.a. earth mover's or optimal transport) distance as the fitting error between each original point and its reconstruction, and we propose scalable algorithms to to so. Our methods build upon Fenchel duality and entropic regularization of Wasserstein distances, which improves not only speed but also computational stability. We apply these techniques on face images and text documents. We show in particular that we can learn dictionaries (topics) for bag-of-word representations of texts using words that may not have appeared in the original texts, or even words that come from a different language than that used in the texts.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,630,638,,,,,,,,,,,,,,,,WOS:000508662100069,0
C,"Zhou, Y; Yu, YL; Dai, W; Liang, YB; Xing, EP",,"Gretton, A; Robert, CC",,"Zhou, Yi; Yu, Yaoliang; Dai, Wei; Liang, Yingbin; Xing, Eric P.",,,On Convergence of Model Parallel Proximal Gradient Algorithm for Stale Synchronous Parallel System,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"With ever growing data volume and model size, an error-tolerant, communication efficient, yet versatile parallel algorithm has become a vital part for the success of many large-scale applications. In this work we propose msPG, an extension of the flexible proximal gradient algorithm to the model parallel and stale synchronous setting. The worker machines of msPG operate asynchronously as long as they are not too far apart, and they communicate efficiently through a dedicated parameter server. Theoretically, we provide a rigorous analysis of the various convergence properties of msPG, and a salient feature of our analysis is its seamless generality that allows both nonsmooth and nonconvex functions. Under mild conditions, we prove the whole iterate sequence of msPG converges to a critical point (which is optimal under convexity assumptions). We further provide an economical implementation of msPG, completely bypassing the need of keeping a local full model. We confirm our theoretical findings through numerical experiments.",,,,,,"Liang, Yingbin/0000-0002-8635-2992",,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,713,722,,,,,,,,,,,,,,,,WOS:000508662100078,0
C,"Crammer, K",,"Kaski, S; Corander, J",,"Crammer, Koby",,,Doubly Aggressive Selective Sampling Algorithms for Classification,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"Online selective sampling algorithms learn to perform binary classification, and additionally they decided whether to ask, or query, for a label of any given example. We introduce two stochastic linear algorithms and analyze them in the worst-case mistake-bound framework. Even though stochastic, for some inputs, our algorithms query with probability 1 and make an update even if there is no mistake, yet the margin is small, hence they are doubly aggressive. We prove bounds in the worst-case settings, which may be lower than previous bounds in some settings. Experiments with 33 document classification datasets, some with 100Ks examples, show the superiority of doubly-aggressive algorithms both in performance and number of queries.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,140,148,,,,,,,,,,,,,,,,WOS:000508355800016,0
C,"Oates, CJ; Mukherjee, S",,"Kaski, S; Corander, J",,"Oates, Chris J.; Mukherjee, Sach",,,Joint Structure Learning of Multiple Non-Exchangeable Networks,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"Several methods have recently been developed for joint structure learning of multiple (related) graphical models or networks. These methods treat individual networks as exchangeable, such that each pair of networks are equally encouraged to have similar structures. However, in many practical applications, exchangeability in this sense may not hold, as some pairs of networks may be more closely related than others, for example due to group and sub-group structure in the data. Here we present a novel Bayesian formulation that generalises joint structure learning beyond the exchangeable case. In addition to a general framework for joint learning, we (i) provide a novel default prior over the joint structure space that requires no user input; (ii) allow for latent networks; (iii) give an efficient, exact algorithm for the case of time series data and dynamic Bayesian networks. We present empirical results on non-exchangeable populations, including a real data example from biology, where cell-line-specific networks are related according to genomic features.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,687,695,,,,,,,,,,,,,,,,WOS:000508355800076,0
C,"Cassano, L; Sayed, AH",,"Banerjee, A; Fukumizu, K",,"Cassano, Lucas; Sayed, Ali H.",,,Logical Team Q-learning: An approach towards factored policies in cooperative MARL,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We address the challenge of learning factored policies in cooperative MARL scenarios. In particular, we consider the situation in which a team of agents collaborates to optimize a common cost. The goal is to obtain factored policies that determine the individual behavior of each agent so that the resulting joint policy is optimal. The main contribution of this work is the introduction of Logical Team Q-learning (LTQL). LTQL does not rely on assumptions about the environment and hence is generally applicable to any collaborative MARL scenario. We derive LTQL as a stochastic approximation to a dynamic programming method we introduce in this work. We conclude the paper by providing experiments (both in the tabular and deep settings) that illustrate the claims.",,,,,"Sayed, Ali/D-6251-2012","Sayed, Ali/0000-0002-5125-5519",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,667,+,,,,,,,,,,,,,,,,WOS:000659893800075,0
C,"Chowdhury, SR; Gopalan, A",,"Banerjee, A; Fukumizu, K",,"Chowdhury, Sayak Ray; Gopalan, Aditya",,,No-regret Algorithms for Multi-task Bayesian Optimization,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We consider multi-objective optimization (MOO) of an unknown vector-valued function in the non-parametric Bayesian optimization (BO) setting. Our aim is to maximize the expected cumulative utility of all objectives, as expressed by a given prior over a set of scalarization functions. Most existing BO algorithms do not model the fact that the multiple objectives, or equivalently, tasks can share similarities, and even the few that do lack rigorous, finite-time regret guarantees that capture explicitly inter-task structure. In this work, we address this problem by modelling inter-task dependencies using a multi-task kernel and develop two novel BO algorithms based on scalarization of the objectives. Our algorithms employ vector-valued kernel regression as a stepping stone and belong to the upper confidence bound class of algorithms. Under a smoothness assumption that the unknown vector-valued function is an element of the reproducing kernel Hilbert space associated with the multi-task kernel, we derive worst-case regret bounds for our algorithms that explicitly capture the similarities between tasks. We numerically benchmark our algorithms on both synthetic and real-life MOO problems, and show the advantages offered by learning with multi-task kernels.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802028,0
C,"Cui, K; Koeppl, H",,"Banerjee, A; Fukumizu, K",,"Cui, Kai; Koeppl, Heinz",,,Approximately Solving Mean Field Games via Entropy-Regularized Deep Reinforcement Learning,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"The recent mean field game (MFG) formalism facilitates otherwise intractable computation of approximate Nash equilibria in many-agent settings. In this paper, we consider discrete-time finite MFGs subject to finite-horizon objectives. We show that all discrete-time finite MFGs with non-constant fixed point operators fail to be contractive as typically assumed in existing MFG literature, barring convergence via fixed point iteration. Instead, we incorporate entropy-regularization and Boltzmann policies into the fixed point iteration. As a result, we obtain provable convergence to approximate fixed points where existing methods fail, and reach the original goal of approximate Nash equilibria. All proposed methods are evaluated with respect to their exploitability, on both instructive examples with tractable exact solutions and high-dimensional problems where exact methods become intractable. In high-dimensional scenarios, we apply established deep reinforcement learning methods and empirically combine fictitious play with our approximations.",,,,,,"Cui, Kai/0000-0002-2605-0386",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802032,0
C,"Deng, Z; Zhang, LJ; Ghorbani, A; Zou, J",,"Banerjee, A; Fukumizu, K",,"Deng, Zhun; Zhang, Linjun; Ghorbani, Amirata; Zou, James",,,Improving Adversarial Robustness via Unlabeled Out-of-Domain Data,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Data augmentation by incorporating cheap unlabeled data from multiple domains is a powerful way to improve prediction especially when there is limited labeled data. In this work, we investigate how adversarial robustness can be enhanced by leveraging out-of-domain unlabeled data. We demonstrate that for broad classes of distributions and classifiers, there exists a sample complexity gap between standard and robust classification. We quantify the extent to which this gap can be bridged by leveraging unlabeled samples from a shifted domain by providing both upper and lower bounds. Moreover, we show settings where we achieve better adversarial robustness when the unlabeled data come from a shifted domain rather than the same domain as the labeled data. We also investigate how to leverage out-of-domain data when some structural information, such as sparsity, is shared between labeled and unlabeled domains. Experimentally, we augment object recognition datasets (CIFAR10, CINIC-10, and SVHN) with easy-to-obtain and unlabeled out-of-domain data and demonstrate substantial improvement in the model's robustness against `1 adversarial attacks on the original domain.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803043,0
C,"Ding, ZY; Li, Q; Lu, JF; Wright, SJ",,"Banerjee, A; Fukumizu, K",,"Ding, Zhiyan; Li, Qin; Lu, Jianfeng; Wright, Stephen J.",,,Random Coordinate Underdamped Langevin Monte Carlo,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Y The Underdamped Langevin Monte Carlo (ULMC) is a popular Markov chain Monte Carlo sampling method. It requires the computation of the full gradient of the log-density at each iteration, an expensive operation if the dimension of the problem is high. We propose a sampling method called Random Coordinate ULMC (RC-ULMC), which selects a single coordinate at each iteration to be updated and leaves the other coordinates untouched. We investigate the computational complexity of RC-ULMC and compare it with the classical ULMC for strongly log-concave probability distributions. We show that RC-ULMC is always cheaper than the classical ULMC, with a significant cost reduction when the problem is highly skewed and high dimensional. Our complexity bound for RC-ULMC is also tight in terms of dimension dependence.",,,,,,"Li, Qin/0000-0001-9210-8948; DING, ZHIYAN/0000-0001-8863-403X; Lu, Jianfeng/0000-0001-6255-5165",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803027,0
C,"Gangrade, A; Kag, A; Saligrama, V",,"Banerjee, A; Fukumizu, K",,"Gangrade, Aditya; Kag, Anil; Saligrama, Venkatesh",,,Selective Classification via One-Sided Prediction,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We propose a novel method for selective classification (SC), a problem which allows a classifier to abstain from predicting some instances, thus trading off accuracy against coverage (the fraction of instances predicted). In contrast to prior gating or confidence-set based work, our proposed method optimises a collection of class-wise decoupled one-sided empirical risks, and is in essence a method for explicitly finding the largest decision sets for each class that have few false positives. This one-sided prediction (OSP) based relaxation yields an SC scheme that attains near-optimal coverage in the practically relevant high target accuracy regime, and further admits efficient implementation, leading to a flexible and principled method for SC. We theoretically derive generalization bounds for SC and OSP, and empirically we show that our scheme strongly outperforms state of the art methods in coverage at small error levels.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802062,0
C,"Gunasekar, S; Woodworth, B; Srebro, N",,"Banerjee, A; Fukumizu, K",,"Gunasekar, Suriya; Woodworth, Blake; Srebro, Nathan",,,Mirrorless Mirror Descent: A Natural Derivation of Mirror Descent,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We present a primal only derivation of Mirror Descent as a partial discretization of gradient flow on a Riemannian manifold where the metric tensor is the Hessian of the Mirror Descent potential. We contrast this discretization to Natural Gradient Descent, which is obtained by a full forward Euler discretization. This view helps shed light on the relationship between the methods and allows generalizing Mirror Descent to general Riemannian geometries, even when the metric tensor is not a Hessian, and thus there is no dual.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802076,0
C,"Jothimurugan, K; Bastani, O; Alur, R",,"Banerjee, A; Fukumizu, K",,"Jothimurugan, Kishor; Bastani, Osbert; Alur, Rajeev",,,Abstract Value Iteration for Hierarchical Reinforcement Learning,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We propose a novel hierarchical reinforcement learning framework for control with continuous state and action spaces. In our framework, the user specifies subgoal regions which are subsets of states; then, we (i) learn options that serve as transitions between these subgoal regions, and (ii) construct a high-level plan in the resulting abstract decision process (ADP). A key challenge is that the ADP may not be Markov, which we address by proposing two algorithms for planning in the ADP. Our first algorithm is conservative, allowing us to prove theoretical guarantees on its performance, which help inform the design of subgoal regions. Our second algorithm is a practical one that interweaves planning at the abstract level and learning at the concrete level. In our experiments, we demonstrate that our approach outperforms state-of-the-art hierarchical reinforcement learning algorithms on several challenging benchmarks.",,,,,"Jothimurugan, Kishor/GXW-0689-2022","Jothimurugan, Kishor/0000-0003-1448-2947",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801043,0
C,"Lyle, C; Rowland, M; Ostrovski, G; Dabney, W",,"Banerjee, A; Fukumizu, K",,"Lyle, Clare; Rowland, Mark; Ostrovski, Georg; Dabney, Will",,,On The Effect of Auxiliary Tasks on Representation Dynamics,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"While auxiliary tasks play a key role in shaping the representations learnt by reinforcement learning agents, much is still unknown about the mechanisms through which this is achieved. This work develops our understanding of the relationship between auxiliary tasks, environment structure, and representations by analysing the dynamics of temporal difference algorithms. Through this approach, we establish a connection between the spectral decomposition of the transition operator and the representations induced by a variety of auxiliary tasks. We then leverage insights from these theoretical results to inform the selection of auxiliary tasks for deep reinforcement learning agents in sparse-reward environments.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,1,+,,,,,,,,,,,,,,,,WOS:000659893800001,0
C,"Richards, D; Rabbat, M",,"Banerjee, A; Fukumizu, K",,"Richards, Dominic; Rabbat, Mike",,,Learning with Gradient Descent and Weakly Convex Losses,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We study the learning performance of gradient descent when the empirical risk is weakly convex, namely, the smallest negative eigenvalue of the empirical risk's Hessian is bounded in magnitude. By showing that this eigenvalue can control the stability of gradient descent, generalisation error bounds are proven that hold under a wider range of step sizes compared to previous work. Out of sample guarantees are then achieved by decomposing the test error into generalisation, optimisation and approximation errors, each of which can be bounded and traded off with respect to algorithmic parameters, sample size and magnitude of this eigenvalue. In the case of a two layer neural network, we demonstrate that the empirical risk can satisfy a notion of local weak convexity, specifically, the Hessian's smallest eigenvalue during training can be controlled by the normalisation of the layers, i.e., network scaling. This allows test error guarantees to then be achieved when the population risk minimiser satisfies a complexity assumption. By trading off the network complexity and scaling, insights are gained into the implicit bias of neural network scaling, which are further supported by experimental findings.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802041,0
C,"Scieur, D; Liu, L; Pumir, T; Boumal, N",,"Banerjee, A; Fukumizu, K",,"Scieur, Damien; Liu, Lewis; Pumir, Thomas; Boumal, Nicolas",,,Generalization of Quasi-Newton Methods: Application to Robust Symmetric Multisecant Updates,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Quasi-Newton (qN) techniques approximate the Newton step by estimating the Hessian using the so-called secant equations. Some of these methods compute the Hessian using several secant equations but produce non-symmetric updates. Other quasi-Newton schemes, such as BFGS, enforce symmetry but cannot satisfy more than one secant equation. We propose a new type of quasi-Newton symmetric update using several secant equations in a least-squares sense. Our approach generalizes and unifies the design of quasi-Newton updates and satisfies provable robustness guarantees.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,550,+,,,,,,,,,,,,,,,,WOS:000659893800062,0
C,"Staerman, G; Laforgue, P; Mozharovskyi, P; d'Alche-Buc, F",,"Banerjee, A; Fukumizu, K",,"Staerman, Guillaume; Laforgue, Pierre; Mozharovskyi, Pavlo; d'Alche-Buc, Florence",,,When OT meets MoM: Robust estimation of Wasserstein Distance,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Originated from Optimal Transport, the Wasserstein distance has gained importance in Machine Learning due to its appealing geometrical properties and the increasing availability of efficient approximations. It owes its recent ubiquity in generative modelling and variational inference to its ability to cope with distributions having non overlapping support. In this work, we consider the problem of estimating the Wasserstein distance between two probability distributions when observations are polluted by outliers. To that end, we investigate how to leverage a Medians of Means (MoM) approach to provide robust estimates. Exploiting the dual Kantorovitch formulation of the Wasserstein distance, we introduce and discuss novel MoM-based robust estimators whose consistency is studied under a data contamination model and for which convergence rates are provided. Beyond computational issues, the choice of the partition size, i.e., the unique parameter of theses robust estimators, is investigated in numerical experiments. Furthermore, these MoM estimators make Wasserstein Generative Adversarial Network (WGAN) robust to outliers, as witnessed by an empirical study on two benchmarks CIFAR10 and Fashion MNIST.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,136,+,,,,,,,,,,,,,,,,WOS:000659893800016,0
C,"Zhang, L; Dunn, T; Marshall, J; Olveczky, B; Linderman, S",,"Banerjee, A; Fukumizu, K",,"Zhang, Libby; Dunn, Timothy; Marshall, Jesse; Olveczky, Bence; Linderman, Scott",,,Animal pose estimation from video data with a hierarchical von Mises-Fisher-Gaussian model,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Animal pose estimation from video data is an important step in many biological studies, but current methods struggle in complex environments where occlusions are common and training data is scarce. Recent work has demonstrated improved accuracy with deep neural networks, but these methods often do not incorporate prior distributions that could improve localization. Here we present GIMBAL: a hierarchical von Mises-Fisher-Gaussian model that improves upon deep networks' estimates by leveraging spatiotemporal constraints. The spatial constraints come from the animal's skeleton, which induces a curved manifold of keypoint configurations. The temporal constraints come from the postural dynamics, which govern how angles between keypoints change over time. Importantly, the conditional conjugacy of the model permits simple and efficient Bayesian inference algorithms. We assess the model on a unique experimental dataset with video of a freely-behaving rodent from multiple viewpoints and ground-truth motion capture data for 20 keypoints. GIMBAL extends existing techniques, and in doing so offers more accurate estimates of keypoint positions, especially in challenging contexts.",,,,,,"Zhang, Elizabeth/0000-0003-2069-4081; Linderman, Scott/0000-0002-3878-9073",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803038,0
C,"Bagdasaryan, E; Veit, A; Hua, YQ; Estrin, D; Shmatikov, V",,"Chiappa, S; Calandra, R",,"Bagdasaryan, Eugene; Veit, Andreas; Hua, Yiqing; Estrin, Deborah; Shmatikov, Vitaly",,,How To Backdoor Federated Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Federated models are created by aggregating model updates submitted by participants. To protect confidentiality of the training data,the aggregator by design has no visibility into how these updates are generated. We show that this makes federated learning vulnerable to a model-poisoning attack that is significantly more powerful than poisoning attacks that target only the training data. A single or multiple malicious participants can use model replacement to introduce backdoor functionality into the joint model,e.g., modify an image classifier so that it assigns an attacker-chosen label to images with certain features, or force a word predictor to complete certain sentences with an attacker-chosen word. We evaluate model replacement under different assumptions for the standard federated-learning tasks and show that it greatly outperform straining-data poisoning. Federated learning employs secure aggregation to protect confidentiality of participants local models and thus cannot detect anomalies in participants' contributions to the joint model. To demonstrate that anomaly detection would not have been effective in any case, we also develop and evaluate a generic constrain-and-scale technique that incorporates the evasion of defenses into the attackers loss function during training.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2938,2947,,,,,,,,,,,,,,,,WOS:000559931300094,0
C,"Cella, L; Cesa-Bianchi, N",,"Chiappa, S; Calandra, R",,"Cella, Leonardo; Cesa-Bianchi, Nicolo",,,Stochastic Bandits with Delay-Dependent Payoffs,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Motivated by recommendation problems in music streaming platforms, we propose a non-stationary stochastic bandit model in which the expected reward of an arm depends on the number of rounds that have passed since the arm was last pulled. After proving that finding an optimal policy is NP-hard even when all model parameters are known, we introduce a class of ranking policies provably approximating, to within a constant factor, the expected reward of the optimal policy. We show an algorithm whose regret with respect to the best ranking policy is bounded by (O) over tilde (root kT), where k is the number of arms and T is time. Our algorithm uses only O k ln ln T) switches, which helps when switching between policies is costly. As constructing the class of learning policies requires ordering the arms according to their expectations, we also bound the number of pulls required to do so. Finally, we run experiments to compare our algorithm against UCB on different problem instances.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1168,1176,,,,,,,,,,,,,,,,WOS:000559931300047,0
C,"Cheng, CA; Lee, J; Goldberg, K; Boots, B",,"Chiappa, S; Calandra, R",,"Cheng, Ching-An; Lee, Jonathan; Goldberg, Ken; Boots, Byron",,,Online Learning with Continuous Variations Dynamic Regret and Reductions,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Online learning is a powerful tool for analyzing iterative algorithms. However, the classic adversarial setup fails to capture regularity that can exist in practice. Motivated by this observation, we establish a new setup, called Continuous Online Learning (COL), where the gradient of online loss function changes continuously across rounds with respect to the learner's decisions. We show that COL appropriately describes many interesting applications, from general equilibrium problems (EPs) to optimization in episodic MDPs. Using this new setup, we revisit the difficulty of sublinear dynamic regret. We prove a fundamental equivalence between achieving sublinear dynamic regret in COL and solving certain EPs. With this insight, we offer conditions for efficient algorithms that achieve sublinear dynamic regret, even when the losses are chosen adaptively without any a priori variation budget. Furthermore, we show for COL a reduction from dynamic regret to both static regret and convergence in the associated EP, allowing us to analyze the dynamic regret of many existing algorithms.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2218,2227,,,,,,,,,,,,,,,,WOS:000559931300059,0
C,"Foster, A; Jankowiak, M; O'Meara, M; Teh, YW; Rainforth, T",,"Chiappa, S; Calandra, R",,"Foster, Adam; Jankowiak, Martin; O'Meara, Matthew; Teh, Yee Whye; Rainforth, Tom",,,A Unified Stochastic Gradient Approach to Designing Bayesian-Optimal Experiments,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We introduce a fully stochastic gradient based approach to Bayesian optimal experimental design (BOED). Our approach utilizes variational lower bounds on the expected information gain (EIG) of an experiment that can be simultaneously optimized with respect to both the variational and design parameters. This allows the design process to be carried out through a single unified stochastic gradient ascent procedure, in contrast to existing approaches that typically construct a pointwise EIG estimator, before passing this estimator to a separate optimizer. We provide a number of different variational objectives including the novel adaptive contrastive estimation (ACE) bound. Finally, we show that our gradient-based approaches are able to provide effective design optimization in substantially higher dimensional settings than existing approaches.",,,,,,"O'Meara, Matthew/0000-0002-3128-5331; Rainforth, Tom/0000-0001-7939-4230",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2959,2968,,,,,,,,,,,,,,,,WOS:000559931301006,0
C,"Gadd, CWL; Wade, S; Boukouvalas, A",,"Chiappa, S; Calandra, R",,"Gadd, Charles W. L.; Wade, Sara; Boukouvalas, Alexis",,,Enriched mixtures of generalised Gaussian process experts,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Mixtures of experts probabilistically divide the input space into regions, where the assumptions of each expert, or conditional model, need only hold locally. Combined with Gaussian process (GP) experts, this results in a powerful and highly flexible model. We focus on alternative mixtures of GP experts, which model the joint distribution of the inputs and targets explicitly. We highlight issues of this approach in multi-dimensional input spaces, namely, poor scalability and the need for an unnecessarily large number of experts, degrading the predictive performance and increasing uncertainty. We construct a novel model to address these issues through a nested partitioning scheme that automatically infers the number of components at both levels. Multiple response types are accommodated through a generalised GP framework, while multiple input types are included through a factorised exponential family structure. We show the effectiveness of our approach in estimating a parsimonious probabilistic description of both synthetic data of increasing dimension and an Alzheimer's challenge dataset.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3144,3153,,,,,,,,,,,,,,,,WOS:000559931301011,0
C,"Harris, DG; Pensyl, T; Srinivasan, A; Trinh, K",,"Chiappa, S; Calandra, R",,"Harris, David G.; Pensyl, Thomas; Srinivasan, Aravind; Trinh, Khoa",,,Dependent randomized rounding for clustering and partition systems with knapsack constraints,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Clustering problems are fundamental to unsupervised learning. There is an increased emphasis on fairness in machine learning and AI; one representative notion of fairness is that no single demographic group should be over-represented among the cluster-centers. This, and much more general clustering problems, can be formulated with knapsack and partition constraints. We develop new randomized algorithms targeting such problems, and study two in particular: multi-knapsack median and multi-knapsack center. Our rounding algorithms give new approximation and pseudo-approximation algorithms for these problems. One key technical tool we develop and use, which may be of independent interest, is a new tail bound analogous to Feige (2006) for sums of random variables with unbounded variances. Such bounds are very useful in inferring properties of large networks using few samples.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2273,2282,,,,,,,,,,,,,,,,WOS:000559931301037,0
C,"Hess, T; Sabato, S",,"Chiappa, S; Calandra, R",,"Hess, Tom; Sabato, Sivan",,,Sequential no-Substitution k-Median-Clustering,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We study the sample-based k-median clustering objective under a sequential setting without substitutions. In this setting, an i.i.d. sequence of examples is observed. An example can be selected as a center only immediately after it is observed, and it cannot be substituted later. The goal is to select a set of centers with a good k-median cost on the distribution which generated the sequence. We provide an efficient algorithm for this setting, and show that its multiplicative approximation factor is twice the approximation factor of an efficient offine algorithm. In addition, we show that if efficiency requirements are removed, there is an algorithm that can obtain the same approximation factor as the best offine algorithm. We demonstrate in experiments the performance of the efficient algorithm on real data sets. evaluation of the downstream application of face detection. Our code is available at https://github.com/tomhess/No_Substitution_K_Median",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,962,971,,,,,,,,,,,,,,,,WOS:000559931301041,0
C,"Kim, YG; Kwon, Y; Chang, H; Paik, MC",,"Chiappa, S; Calandra, R",,"Kim, Young-Geun; Kwon, Yongchan; Chang, Hyunwoong; Paik, Myunghee Cho",,,Lipschitz Continuous Autoencoders in Application to Anomaly Detection,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Anomaly detection is the task of finding abnormal data that are distinct from normal behavior. Current deep learning-based anomaly detection methods train neural networks with normal data alone and calculate anomaly scores based on the trained model. In this work, we formalize current practices, build a theoretical framework of anomaly detection algorithms equipped with an objective function and a hypothesis space, and establish a desirable property of the anomaly detection algorithm, namely, admissibility. Admissibility implies that optimal autoencoders for normal data yield a larger reconstruction error for anomalous data than that for normal data on average. We then propose a class of admissible anomaly detection algorithms equipped with an integral probability metric-based objective function and a class of autoencoders, Lipschitz continuous autoencoders. The proposed algorithm for Wasserstein distance is implemented by minimizing an approximated Wasserstein distance with a penalty to enforce Lipschitz continuity with respect to Wasserstein distance. Through ablation studies, we demonstrate the efficacy of enforcing Lipschitz continuity of the proposed method. The proposed method is shown to be more effective in detecting anomalies than existing methods via applications to network traffic and image datasets(1).",,,,,"Kim, Young-geun/HCI-5681-2022","Kim, Young-geun/0000-0001-8910-1227",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2507,2516,,,,,,,,,,,,,,,,WOS:000559931301074,0
C,"Li, M; Soltanolkotabi, M; Oymak, S",,"Chiappa, S; Calandra, R",,"Li, Mingchen; Soltanolkotabi, Mahdi; Oymak, Samet",,,Gradient Descent with Early Stopping is Provably Robust to Label Noise for Overparameterized Neural Networks,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Modern neural networks are typically trained in an over-parameterized regime where the parameters of the model far exceed the size of the training data. Such neural networks in principle have the capacity to (over)fit any set of labels including pure noise. Despite this, somewhat paradoxically, neural network models trained via first-order methods continue to predict well on yet unseen test data. This paper takes a step towards demystifying this phenomena. Under a rich dataset model, we show that gradient descent is provably robust to noise/corruption on a constant fraction of the labels despite overparameterization. In particular, we prove that: (i) In the first few iterations where the updates are still in the vicinity of the initialization gradient descent only fits to the correct labels essentially ignoring the noisy labels. (ii) to start to overfit to the noisy labels network must stray rather far from from the initialization which can only occur after many more iterations. Together, these results show that gradient descent with early stopping is provably robust to label noise and shed light on the empirical robustness of deep networks as well as commonly adopted heuristics to prevent overfitting.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,4313,4324,,,,,,,,,,,,,,,,WOS:000559931302010,0
C,"Lorraine, J; Vicol, P; Duvenaud, D",,"Chiappa, S; Calandra, R",,"Lorraine, Jonathan; Vicol, Paul; Duvenaud, David",,,Optimizing Millions of Hyperparameters by Implicit Differentiation,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We propose an algorithm for inexpensive gradient-based hyperparameter optimization that combines the implicit function theorem (IFT) with efficient inverse Hessian approximations. We present results on the relationship between the IFT and differentiating through optimization, motivating our algorithm. We use the proposed approach to train modern network architectures with millions of weights and millions of hyperparameters. We learn a data-augmentation network-where every weight is a hyperparameter tuned for validation performance-that outputs augmented training examples; we learn a distilled dataset where every feature in each data point is a hyperparameter; and we tune millions of regularization hyperparameters. Jointly tuning weights and hyperparameters with our approach is only a few times more costly in memory and compute than standard training.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1540,1551,,,,,,,,,,,,,,,,WOS:000559931302018,0
C,"Schuster, I; Mollenhauer, M; Klus, S; Muandet, K",,"Chiappa, S; Calandra, R",,"Schuster, Ingmar; Mollenhauer, Mattes; Klus, Stefan; Muandet, Krikamol",,,Kernel Conditional Density Operators,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We introduce a novel conditional density estimation model termed the conditional density operator (CDO). It naturally captures multivariate, multimodal output densities and shows performance that is competitive with recent neural conditional density models and Gaussian processes. The proposed model is based on a novel approach to the reconstruction of probability densities from their kernel mean embeddings by drawing connections to estimation of Radon-Nikodym derivatives in the reproducing kernel Hilbert space (RKHS). We prove finite sample bounds for the estimation error in a standard density reconstruction scenario, independent of problem dimensionality. Interestingly, when a kernel is used that is also a probability density, the CDO allows us to both evaluate and sample the output density efficiently. We demonstrate the versatility and performance of the proposed model on both synthetic and real-world data.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303006,0
C,"Silva, A; Killian, T; Jimenez, IR; Son, SH; Gombolay, M",,"Chiappa, S; Calandra, R",,"Silva, Andrew; Killian, Taylor; Jimenez, Ivan Rodriguez; Son, Sung-Hyun; Gombolay, Matthew",,,Optimization Methods for Interpretable Differentiable Decision Trees in Reinforcement Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Decision trees are ubiquitous in machine learning for their ease of use and interpretability. Yet, these models are not typically employed in reinforcement learning as they cannot be updated online via stochastic gradient descent. We overcome this limitation by allowing for a gradient update over the entire tree that improves sample complexity affords interpretable policy extraction. First, we include theoretical motivation on the need for policy-gradient learning by examining the properties of gradient descent over differentiable decision trees. Second, we demonstrate that our approach equals or outperforms a neural network on all domains and can learn discrete decision trees online with average rewards up to 7x higher than a batch-trained decision tree. Third, we conduct a user study to quantify the interpretability of a decision tree, rule list, and a neural network with statistically significant results (p < 0.001).",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303022,0
C,"Skianis, K; Nikolentzos, G; Limnios, S; Vazirgiannis, M",,"Chiappa, S; Calandra, R",,"Skianis, Konstantinos; Nikolentzos, Giannis; Limnios, Stratis; Vazirgiannis, Michalis",,,Rep the Set: Neural Networks for Learning Set Representations,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"In several domains, data objects can be decomposed into sets of simpler objects. It is then natural to represent each object as the set of its components or parts. Many conventional machine learning algorithms are unable to process this kind of representations, since sets may vary in cardinality and elements lack a meaningful ordering. In this paper, we present a new neural network architecture, called RepSet, that can handle examples that are represented as sets of vectors. The proposed model computes the correspondences between an input set and some hidden sets by solving a series of network flow problems. This representation is then fed to a standard neural network architecture to produce the output. The architecture allows end-to-end gradient-based learning. We demonstrate RepSet on classification tasks, including text categorization, and graph classification, and we show that the proposed neural network achieves performance better or comparable to state-of-the-art algorithms.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303024,0
C,"Sterge, N; Sriperumbudur, B; Rosasco, L; Rudi, A",,"Chiappa, S; Calandra, R",,"Sterge, Nicholas; Sriperumbudur, Bharath; Rosasco, Lorenzo; Rudi, Alessandro",,,Gain with no Pain: Efficiency of Kernel-PCA by Nystrom Sampling,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"In this paper, we analyze a Nystrom based approach to efficient large scale kernel principal component analysis (PCA). The latter is a natural nonlinear extension of classical PCA based on considering a nonlinear feature map or the corresponding kernel. Like other kernel approaches, kernel PCA enjoys good mathematical and statistical properties but, numerically, it scales poorly with the sample size. Our analysis shows that Nystrom sampling greatly improves computational efficiency without incurring any loss of statistical accuracy. While similar effects have been observed in supervised learning, this is the first such result for PCA. Our theoretical findings are based on a combination of analytic and concentration of measure techniques. Our study is more broadly motivated by the question of understanding the interplay between statistical and computational requirements for learning.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303031,0
C,"Weber, M",,"Chiappa, S; Calandra, R",,"Weber, Melanie",,,Neighborhood Growth Determines Geometric Priors for Relational Representation Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"The problem of identifying geometric structure in heterogeneous, high-dimensional data is a cornerstone of representation learning. While there exists a large body of literature on the embeddability of canonical graphs, such as lattices or trees, the heterogeneity of the relational data typically encountered in practice limits the applicability of these classical methods. In this paper, we propose a combinatorial approach to evaluating embeddability, i.e., to decide whether a data set is best represented in Euclidean, Hyperbolic or Spherical space. Our method analyzes nearest-neighbor structures and local neighborhood growth rates to identify the geometric priors of suitable embedding spaces. For canonical graphs, the algorithm's prediction provably matches classical results. As for large, heterogeneous graphs, we introduce an efficiently computable statistic that approximates the algorithm's decision rule. We validate our method over a range of benchmark data sets and compare with recently published optimization-based embeddability methods.",,,,,,"Weber, Melanie/0000-0003-1104-7181",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303074,0
C,"Xie, YG; Wu, XX; Ward, R",,"Chiappa, S; Calandra, R",,"Xie, Yuege; Wu, Xiaoxia; Ward, Rachel",,,Linear Convergence of Adaptive Stochastic Gradient Descent,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We prove that the norm version of the adaptive stochastic gradient method (AdaGradNorm) achieves a linear convergence rate for a subset of either strongly convex functions or non-convex functions that satisfy the Polyak-Lojasiewicz (PL) inequality. The paper introduces the notion of Restricted Uniform Inequality of Gradients (RUIG)-which is a measure of the balanced-ness of the stochastic gradient norms-to depict the landscape of a function. RUIG plays a key role in proving the robustness of AdaGrad-Norm to its hyper-parameter tuning in the stochastic setting. On top of RUIG, we develop a two-stage framework to prove the linear convergence of AdaGrad-Norm without knowing the parameters of the objective functions. This framework can likely be extended to other adaptive stepsize algorithms. The numerical experiments validate the theory and suggest future directions for improvement.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303085,0
C,"Zanette, A; Brandfonbrener, D; Brunskill, E; Pirotta, M; Lazaric, A",,"Chiappa, S; Calandra, R",,"Zanette, Andrea; Brandfonbrener, David; Brunskill, Emma; Pirotta, Matteo; Lazaric, Alessandro",,,Frequentist Regret Bounds for Randomized Least-Squares Value Iteration,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We consider the exploration-exploitation dilemma in finite-horizon reinforcement learning (RL). When the state space is large or continuous, traditional tabular approaches are unfeasible and some form of function approximation is mandatory. In this paper, we introduce an optimistically-initialized variant of the popular randomized least-squares value iteration (RLSVI), a model-free algorithm where exploration is induced by perturbing the least-squares approximation of the action-value function. Under the assumption that the Markov decision process has low-rank transition dynamics, we prove that the frequentist regret of RLSVI is upper-bounded by (O) over tilde (d(2)H(2)root T) where d is the feature dimension, H is the horizon, and T is the total number of steps. To the best of our knowledge, this is the first frequentist regret analysis for randomized exploration with function approximation.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1954,1963,,,,,,,,,,,,,,,,WOS:000559931304004,0
C,"Asi, H; Duchi, JC",,"Chaudhuri, K; Sugiyama, M",,"Asi, Hilal; Duchi, John C.",,,Modeling simple structures and geometry for better stochastic optimization algorithms,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We develop procedures for solving convex stochastic optimization problems that exploit the structure and geometry of the underlying problem. Our procedures build on the model-based APRox framework that we develop in the paper [2], which highlights the importance of more careful structural modeling: as one example of this, if we seek to minimize a non-negative loss, then stochastic optimization methods should use non-negative approximations. We extend this earlier work to improve adaptivity to problem geometry via careful choices of divergence measures, highlighting both the importance of leveraging problem structure and geometry in the form of the divergence used to define stochastic updates for strong performance. Our experiments confirm our theoretical results in a range of problems, including deep learning.",,,,,,"Duchi, John/0000-0003-0045-7185",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902049,0
C,"Berthet, Q; Ellenberg, JS",,"Chaudhuri, K; Sugiyama, M",,"Berthet, Quentin; Ellenberg, Jordan S.",,,Detection of Planted Solutions for Flat Satisfiability Problems,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We study the detection problem of finding planted solutions in random instances of flat Satisfiability problems, a generalization of boolean Satisfiability formulas. We describe the properties of random instances of flat Satisfiability, as well of the optimal rates of detection of the associated hypothesis testing problem. We also study the performance of an algorithmically efficient testing procedure. We introduce a modification of our model, the light planting of solutions, and show that it is as hard as the problem of learning parity with noise. This hints strongly at the difficulty of detecting planted flat Satisfiability for a wide class of tests.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901036,0
C,"Feydy, J; Sejourne, T; Vialard, FX; Amari, S; Trouve, A; Peyre, G",,"Chaudhuri, K; Sugiyama, M",,"Feydy, Jean; Sejourne, Thibault; Vialard, Francois-Xavier; Amari, Shun-ichi; Trouve, Alain; Peyre, Gabriel",,,Interpolating between Optimal Transport and MMD using Sinkhorn Divergences,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Comparing probability distributions is a fundamental problem in data sciences. Simple norms and divergences such as the total variation and the relative entropy only compare densities in a point-wise manner and fail to capture the geometric nature of the problem. In sharp contrast, Maximum Mean Discrepancies (MMD) and Optimal Transport distances (OT) are two classes of distances between measures that take into account the geometry of the underlying space and metrize the convergence in law. This paper studies the Sinkhorn divergences, a family of geometric divergences that interpolates between MMD and OT. Relying on a new notion of geometric entropy, we provide theoretical guarantees for these divergences: positivity, convexity and metrization of the convergence in law. On the practical side, we detail a numerical scheme that enables the large scale application of these divergences for machine learning: on the GPU, gradients of the Sinkhorn loss can be computed for batches of a million samples.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902075,0
C,"Geng, Q; Ding, W; Guo, RQ; Kumar, S",,"Chaudhuri, K; Sugiyama, M",,"Geng, Quan; Ding, Wei; Guo, Ruiqi; Kumar, Sanjiv",,,Optimal Noise-Adding Mechanism in Additive Differential Privacy,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We derive the optimal (0, delta)-differentially private query-output independent noise-adding mechanism for single real-valued query function under a general cost-minimization framework. Under a mild technical condition, we show that the optimal noise probability distribution is a uniform distribution with a probability mass at the origin. We explicitly derive the optimal noise distribution for general l(p) cost functions, including l(1) (for noise magnitude) and l(2) (for noise power) cost functions, and show that the probability concentration on the origin occurs when delta > p/p+1. Our result demonstrates an improvement over the existing Gaussian mechanisms by a factor of two and three for (0, delta)-differential privacy in the high privacy regime in the context of minimizing the noise magnitude and noise power, and the gain is more pronounced in the low privacy regime. Our result is consistent with the existing result for (0,delta)-differential privacy in the discrete setting, and identifies a probability concentration phenomenon in the continuous setting.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,11,20,,,,,,,,,,,,,,,,WOS:000509687900002,0
C,"Holland, MJ",,"Chaudhuri, K; Sugiyama, M",,"Holland, Matthew J.",,,Robust descent using smoothed multiplicative noise,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"In this work, we propose a novel robust gradient descent procedure which makes use of a smoothed multiplicative noise applied directly to observations before constructing a sum of soft-truncated gradient coordinates. We show that the procedure has competitive theoretical guarantees, with the major advantage of a simple implementation that does not require an iterative sub-routine for robustification. Empirical tests reinforce the theory, showing more efficient generalization over a much wider class of data distributions.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,703,711,,,,,,,,,,,,,,,,WOS:000509687900073,0
C,"Lian, XR; Liu, J",,"Chaudhuri, K; Sugiyama, M",,"Lian, Xiangru; Liu, Ji",,,Revisit Batch Normalization: New Understanding and Refinement via Composition Optimization,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Batch Normalization (BN) has been tensively in deep learning to achieve faster training process and better resulting models. However, whether BN works strongly depends on how the batches are constructed during training, and it may not converge to a desired solution if the statistics on the batch are not close to the statistics over the whole dataset. In this paper, we try to understand BN from art optimization perspective by providing an explicit objective function associated with BN. This explicit objective function revels that: 1) BN, rather than being a new optimization Lion algorithm or trick, is creating a different objective function instead of the one in our common sense: and 2) why BN may not work well in some scenarios. We then propose a refinement of BN based on the compositional optimization technique called Full Normalization (FN) to alleviate the issues of UN when the batches arc not constructed ideally. The convergence analysis and empirical study for FN are also included in this paper.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687903032,0
C,"Nguyen, T; Shameli, A; Abbasi-Yadkori, Y; Rao, A; Kveton, B",,"Chaudhuri, K; Sugiyama, M",,"Nguyen, Tan; Shameli, Ali; Abbasi-Yadkori, Yasin; Rao, Anup; Kveton, Branislav",,,Sample Efficient Graph-Based Optimization with Noisy Observations,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We study sample complexity of optimizing hill-climbing friendly functions defined on a graph under noisy observations. We define a notion of convexity, and we show that a variant of best-arm identification can find a near-optimal solution after a small number of queries that is independent of the size of the graph. For functions that have local minima and are nearly convex, we show a sample complexity for the classical simulated annealing under noisy observations. We show effectiveness of the greedy algorithm with restarts and the simulated annealing on problems of graph based nearest neighbor classification as well as a web document re-ranking application.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687903040,0
C,"Nowak-Vila, A; Bach, F; Rudi, A",,"Chaudhuri, K; Sugiyama, M",,"Nowak-Vila, Alex; Bach, Francis; Rudi, Alessandro",,,Sharp Analysis of Learning with Discrete Losses,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"The problem of devising learning strategies for discrete losses (e.g., multilabeling, ranking) is currently addressed with methods and theoretical analyses ad-hoc for each loss. In this paper we study a least-squares framework to systematically design learning algorithms for discrete losses, with quantitative characterizations in terms of statistical and computational complexity. In particular we improve existing results by providing explicit dependence on the number of labels for a wide class of losses and faster learning rates in conditions of low-noise. Theoretical results are complemented with experiments on real datasets, showing the effectiveness of the proposed general approach.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901100,0
C,"Shen, ZB; Fang, C; Zhao, PL; Huang, JZ; Qian, H",,"Chaudhuri, K; Sugiyama, M",,"Shen, Zebang; Fang, Cong; Zhao, Peilin; Huang, Junzhou; Qian, Hui",,,Complexities in Projection-Free Stochastic Non-convex Minimization,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"For constrained nonconvex minimization problems, we propose a meta stochastic projection-free optimization algorithm, named Normalized Frank Wolfe Updating, that can take any Gradient Estimator (GE) as input. For this algorithm, we prove its convergence rate, regardless of the choice of GE. Using a sophisticated GE, this algorithm can significantly improve the Stochastic First order Oracle (SFO) complexity. Further, a new second order GE strategy is proposed to incorporate curvature information, which enjoys theoretical advantage over the first order ones. Besides, this paper also provides a lower bound of Linear-optimization Oracle (LO) queried to achieve an approximate stationary point. Simulation studies validate our analysis under various parameter settings.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902094,0
C,"Sun, MM; Li, P",,"Chaudhuri, K; Sugiyama, M",,"Sun, Mingming; Li, Ping",,,Graph to Graph: a Topology Aware Approach for Graph Structures Learning and Generation,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"This paper is concerned with the problem of learning the mapping from one graph to another graph. Primarily, we focus on the issue of how to effectively learn the topology of the source graph and then decode it to form the topology of the target graph. We embed the topology of the graph into the states of nodes by exerting a topology constraint, which results in our Topology-Flow encoder. To decoder the encoded topology, we design a conditioned graph generation model with two edge generation options, which result in the Edge-Bernoulli decoder and the Edge-Connect decoder. Experimental results on the 10-nodes simple graph dataset illustrate the substantial progress of the proposed method. The MNIST digits skeleton mapping experiment also reveals the ability of our approach to discover different typologies.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902102,0
C,"Viswanathan, K; Sachdeva, S; Tomkins, A; Ravi, S",,"Chaudhuri, K; Sugiyama, M",,"Viswanathan, Krishnamurthy; Sachdeva, Sushant; Tomkins, Andrew; Ravi, Sujith",,,Improved Semi-Supervised Learning with Multiple Graphs,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We present a new approach for graph based semi-supervised learning based on a multicomponent extension to the Gaussian MRF model. This approach models the observations on the vertices as jointly Gaussian with an inverse covariance matrix that is a weighted linear combination of multiple matrices. Building on randomized matrix trace estimation and fast Laplacian solvers, we develop fast and efficient algorithms for computing the best-fit (maximum likelihood) model and the predicted labels using gradient descent. Our model is considerably simpler, with just tens of parameters, and a single hyperparameter, in contrast with state-of-the-art approaches using deep learning techniques. Our experiments on benchmark citation networks show that the best-fit model estimated by our algorithm leads to significant improvements on all datasets compared to baseline models. Further, our performance compares favorably with several state-of-the-art methods on these datasets, and is comparable with the best performances.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687903009,0
C,"Xu, M; Quiroz, M; Kohn, R; Sisson, SA",,"Chaudhuri, K; Sugiyama, M",,"Xu, Ming; Quiroz, Matias; Kohn, Robert; Sisson, Scott A.",,,Variance reduction properties of the reparameterization trick,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"The reparameterization trick is widely used in variational inference as it yields more accurate estimates of the gradient of the variational objective than alternative approaches such as the score function method. Although there is overwhelming empirical evidence in the literature showing its success, there is relatively little research exploring why the reparameterization trick is so effective. We explore this under the idealized assumptions that the variational approximation is a mean-field Gaussian density and that the log of the joint density of the model parameters and the data is a quadratic function that depends on the variational mean. From this, we show that the marginal variances of the reparameterization gradient estimator are smaller than those of the score function gradient estimator. We apply the result of our idealized analysis to real-world examples.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902078,0
C,"Zhu, DX; Li, Z; Wang, XY; Gong, BQ; Yang, TB",,"Chaudhuri, K; Sugiyama, M",,"Zhu, Dixian; Li, Zhe; Wang, Xiaoyu; Gong, Boqing; Yang, Tianbao",,,A Robust Zero-Sum Game Framework for Pool-based Active Learning,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"In this paper, we present a novel robust zero-sum game framework for pool-based active learning grounded on advanced statistical learning theory. Pool-based active learning usually consists of two components, namely, learning of a classifier given labeled data and querying of unlabeled data for labeling. Most previous studies on active learning consider these as two separate tasks and propose various heuristics for selecting important unlabeled data for labeling, which may render the selection of unlabeled examples sub-optimal for minimizing the classification error. In contrast, the presented work formulates active learning as a unified optimization framework for learning the classifier, i.e., the querying of labels and the learning of models are unified to minimize a common objective for statistical learning. In addition, the proposed method avoids the issues of many previous algorithms such as inefficiency, sampling bias, and sensitivity to imbalanced data distribution. Besides theoretical analysis, we conduct extensive experiments on benchmark datasets and demonstrate the superior performance of the proposed active learning method over the state-of-the-art methods.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,517,526,,,,,,,,,,,,,,,,WOS:000509687900054,0
C,"Fan, JQ; Gong, WY; Li, CJ; Sun, Q",,"Storkey, A; PerezCruz, F",,"Fan, Jianqing; Gong, Wenyan; Li, Chris Junchi; Sun, Qiang",,,Statistical Sparse Online Regression: A Diffusion Approximation Perspective,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"In this paper we adopt the diffusion approximation perspective to investigate Stochastic Gradient Descent (SGD) for least squares, which allows us to characterize the exact dynamics of the online regression process. As a consequence, we show that SGD achieves the optimal rate of convergence, up to a logarithmic factor. We further show SGD combined with the trajectory average achieves a faster rate, by eliminating the logarithmic factor. We extend SGD to the high dimensional setting by proposing a two-step algorithm: a burn-in step using offine learning and a refinement step using a variant of truncated stochastic gradient descent. Under appropriate assumptions, we show the proposed algorithm produces near optimal sparse estimators. Numerical experiments lend further support to our obtained theory.",,,,,"LI, chris/HDO-6232-2022",,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300107,0
C,"Gardner, JR; Pleiss, G; Wu, RH; Weinberger, KQ; Wilson, AG",,"Storkey, A; PerezCruz, F",,"Gardner, Jacob R.; Pleiss, Geoff; Wu, Ruihan; Weinberger, Kilian Q.; Wilson, Andrew Gordon",,,Product Kernel Interpolation for Scalable Gaussian Processes,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Recent work shows that inference for Gaussian processes can be performed efficiently using iterative methods that rely only on matrix-vector multiplications (MVMs). Structured Kernel Interpolation (SKI) exploits these techniques by deriving approximate kernels with very fast MVMs. Unfortunately, such strategies suffer badly from the curse of dimensionality. We develop a new technique for MVM based learning that exploits product kernel structure. We demonstrate that this technique is broadly applicable, resulting in linear rather than exponential runtime with dimension for SKI, as well as state-of-the-art asymptotic complexity for multi-task GPs.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300147,0
C,"Jovan, F; Wyatt, J; Hawes, N",,"Storkey, A; PerezCruz, F",,"Jovan, Ferdian; Wyatt, Jeremy; Hawes, Nick",,,Efficient Bayesian Methods for Counting Processes in Partially Observable Environments,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"When sensors that count events are unreliable, the data sets that result cannot be trusted. We address this common problem by developing practical Bayesian estimators for a partially observable Poisson process (POPP). Unlike Bayesian estimation for a fully observable Poisson process (FOPP) this is non-trivial, since there is no conjugate density for a POPP and the posterior has a number of elements that grow exponentially in the number of observed intervals. We present two tractable approximations, which we combine in a switching filter. This switching filter enables efficient and accurate estimation of the posterior. We perform a detailed empirical analysis, using both simulated and real-world data.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300199,0
C,"Law, HCL; Sutherland, DJ; Sejdinovic, D; Flaxman, S",,"Storkey, A; PerezCruz, F",,"Law, Ho Chung Leon; Sutherland, Dougal J.; Sejdinovic, Dino; Flaxman, Seth",,,Bayesian Approaches to Distribution Regression,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Distribution regression has recently attracted much interest as a generic solution to the problem of supervised learning where labels are available at the group level, rather than at the individual level. Current approaches, however, do not propagate the uncertainty in observations due to sampling variability in the groups. This effectively assumes that small and large groups are estimated equally well, and should have equal weight in the final regression. We account for this uncertainty with a Bayesian distribution regression formalism, improving the robustness and performance of the model when group sizes vary. We frame our models in a neural network style, allowing for simple MAP inference using backpropagation to learn the parameters, as well as MCMC-based inference which can fully propagate uncertainty. We demonstrate our approach on illustrative toy datasets, as well as on a challenging problem of predicting age from images.",,,,,,"Sejdinovic, Dino/0000-0001-5547-9213",,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300122,0
C,"Mollenhoff, T; Ye, ZZ; Wu, T; Cremers, D",,"Storkey, A; PerezCruz, F",,"Moellenhoff, Thomas; Ye, Zhenzhang; Wu, Tao; Cremers, Daniel",,,Combinatorial Preconditioners for Proximal Algorithms on Graphs,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"We present a novel preconditioning technique for proximal optimization methods that relies on graph algorithms to construct effective preconditioners. Such combinatorial preconditioners arise from partitioning the graph into forests. We prove that certain decompositions lead to a theoretically optimal condition number. We also show how ideal decompositions can be realized using matroid partitioning and propose efficient greedy variants thereof for large-scale problems. Coupled with specialized solvers for the resulting scaled proximal subproblems, the preconditioned algorithm achieves competitive performance in machine learning and vision applications.",,,,,,"Mollenhoff, Thomas/0000-0001-7730-0843",,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300005,0
C,"Nitanda, A; Suzuki, T",,"Storkey, A; PerezCruz, F",,"Nitanda, Atsushi; Suzuki, Taiji",,,Gradient Layer: Enhancing the Convergence of Adversarial Training for Generative Models,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"We propose a new technique that boosts the convergence of training generative adversarial networks. Generally, the rate of training deep models reduces severely after multiple iterations. A key reason for this phenomenon is that a deep network is expressed using a highly non-convex finite-dimensional model, and thus the parameter gets stuck in a local optimum. Because of this, methods often suffer not only from degeneration of the convergence speed but also from limitations in the representational power of the trained network. To overcome this issue, we propose an additional layer called the gradient layer to seek a descent direction in an infinite-dimensional space. Because the layer is constructed in the infinite-dimensional space, we are not restricted by the specific model structure of finite-dimensional models. As a result, we can get out of the local optima in finite-dimensional models and move towards the global optimal function more directly. In this paper, this phenomenon is explained from the functional gradient method perspective of the gradient layer. Interestingly, the optimization procedure using the gradient layer naturally constructs the deep structure of the network. Moreover, we demonstrate that this procedure can be regarded as a discretization method of the gradient flow that naturally reduces the objective function. Finally, the method is tested using several numerical experiments, which show its fast convergence.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300106,0
C,"Shah, D; Yu, CL",,"Storkey, A; PerezCruz, F",,"Shah, Devavrat; Yu, Christina Lee",,,"Reducing Crowdsourcing to Graphon Estimation, Statistically","INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Inferring the correct answers to binary tasks based on multiple noisy answers in an unsupervised manner has emerged as the canonical question for micro-task crowdsourcing or more generally aggregating opinions. In graphon estimation, one is interested in estimating edge intensities or probabilities between nodes using a single snapshot of a graph realization. In the recent literature, there has been exciting development within both of these topics. In the context of crowdsourcing, the key intellectual challenge is to understand whether a given task can be more accurately denoised by aggregating answers collected from other different tasks. In the context of graphon estimation, precise information limits and estimation algorithms remain of interest. In this paper, we utilize a statistical reduction from crowdsourcing to graphon estimation to advance the state-of-art for both of these challenges. We use concepts from graphon estimation to design an algorithm that achieves better performance than the majority voting scheme for a setup that goes beyond the rank one models considered in the literature. We use known lower bounds for crowdsourcing to derive lower bounds for graphon estimation.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300182,0
C,"Xu, ZQ; Gao, X",,"Storkey, A; PerezCruz, F",,"Xu, Zhiqiang; Gao, Xin",,,On Truly Block Eigensolvers via Riemannian Optimization,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"We study theoretical properties of block solvers for the eigenvalue problem. Despite a recent surge of interest in such eigensolver analysis, truly block solvers have received relatively less attention, in contrast to the majority of studies concentrating on vector versions and non-truly block versions that rely on the deflation strategy. In fact, truly block solvers are more widely deployed in practice by virtue of its simplicity without compromise on accuracy. However, the corresponding theoretical analysis remains inadequate for first-order solvers, as only local and kth gap-dependent rates of convergence have been established thus far. This paper is devoted to revealing significantly better or asyet-unknown theoretical properties of such solvers. We present a novel convergence analysis in a unified framework for three types of first-order Riemannian solvers, i.e., deterministic, vanilla stochastic, and stochastic with variance reduction, that are to find top-k eigenvectors of a real symmetric matrix, in full generality. In particular, the issue of zero gaps between eigenvalues, to the best of our knowledge for the first time, is explicitly considered for these solvers, which brings new understandings, e.g., the dependence of convergence on gaps other than the k-th one. We thus propose the concept of generalized k-th gap. Three types of solvers are proved to converge to a globally optimal solution at a global, generalized k-th gap-dependent, and linear or sub-linear rate.",,,,,"Xu, Zhiqiang/AAB-7414-2022",,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300019,0
C,"Chalupka, K; Perona, P; Eberhardt, F",,"Gretton, A; Robert, CC",,"Chalupka, Krzysztof; Perona, Pietro; Eberhardt, Frederick",,,Multi-Level Cause-Effect Systems,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"We present a domain-general account of causation that applies to settings in which macro-level causal relations between two systems are of interest, but the relevant causal features are poorly understood and have to be aggregated from vast arrays of micro-measurements. Our approach generalizes that of Chalupka et al. (2015) to the setting in which the macro-level effect is not specified. We formalize the connection between micro- and macro-variables in such situations and provide a coherent framework describing causal relations at multiple levels of analysis. We present an algorithm that discovers macro-variable causes and effects from micro-level measurements obtained from an experiment. We further show how to design experiments to discover macro-variables from observational micro-variable data. Finally, we show that under specific conditions, one can identify multiple levels of causal structure. Throughout the article, we use a simulated neuroscience multi-unit recording experiment to illustrate the ideas and the algorithms.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,361,369,,,,,,,,,,,,,,,,WOS:000508662100040,0
C,"Choi, H; Meshi, O; Srebro, N",,"Gretton, A; Robert, CC",,"Choi, Heejin; Meshi, Ofer; Srebro, Nathan",,,Fast and Scalable Structural SVM with Slack Rescaling,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"We present an efficient method for training slack-rescaled structural SVM. Although finding the most violating label in a margin-rescaled formulation is often easy since the target function decomposes with respect to the structure, this is not the case for a slack-rescaled formulation, and finding the most violated label might be very difficult. Our core contribution is an efficient method for finding the most-violating-label in a slack-rescaled formulation, given an oracle that returns the most-violating-label in a (slightly modified) margin-rescaled formulation. We show that our method enables accurate and scalable training for slack-rescaled SVMs, reducing runtime by an order of magnitude compared to previous approaches to slack-rescaled SVMs.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,667,675,,,,,,,,,,,,,,,,WOS:000508662100073,0
C,"Heinonen, M; Mannerstrom, H; Rousu, J; Kaski, S; Lahdesmaki, H",,"Gretton, A; Robert, CC",,"Heinonen, Markus; Mannerstrom, Henrik; Rousu, Juho; Kaski, Samuel; Lahdesmaki, Harri",,,Non-Stationary Gaussian Process Regression with Hamiltonian Monte Carlo,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"We present a novel approach for non-stationary Gaussian process regression (GPR), where the three key parameters - noise variance, signal variance and lengthscale - can be simultaneously input-dependent. We develop gradient-based inference methods to learn the unknown function and the non-stationary model parameters, without requiring any model approximations. For inferring the full posterior distribution we use Hamiltonian Monte Carlo (HMC), which conveniently extends the analytical gradient-based GPR learning by guiding the sampling with the gradients. The MAP solution can also be learned with gradient ascent. In experiments on several synthetic datasets and in modelling of temporal gene expression, the non-stationary GPR is shown to give major improvement when modeling realistic input-dependent dynamics.",,,,,"Rousu, Juho/E-8195-2012; Kaski, Samuel/B-6684-2008; Heinonen, Markus/Q-1079-2016","Rousu, Juho/0000-0002-0705-4314; Kaski, Samuel/0000-0003-1925-9154; Mannerstrom, Henrik/0000-0001-8492-665X",,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,732,740,,,,,,,,,,,,,,,,WOS:000508662100080,0
C,"Kyrillidis, A; Bah, B; Hasheminezhad, R; Quoc, TD; Baldassarre, L; Cevher, V",,"Gretton, A; Robert, CC",,"Kyrillidis, Anastasios; Bah, Bubacarr; Hasheminezhad, Rouzbeh; Quoc Tran-Dinh; Baldassarre, Luca; Cevher, Volkan",,,Convex block-sparse linear regression with expanders - provably,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Sparse matrices are favorable objects in machine learning and optimization. When such matrices are used, in place of dense ones, the overall complexity requirements in optimization can be significantly reduced in practice, both in terms of space and run-time. Prompted by this observation, we study a convex optimization scheme for block-sparse recovery from linear measurements. To obtain linear sketches, we use expander matrices, i.e., sparse matrices containing only few non-zeros per column. Hitherto, to the best of our knowledge, such algorithmic solutions have been only studied from a non-convex perspective. Our aim here is to theoretically characterize the performance of convex approaches under such setting. Our key novelty is the expression of the recovery error in terms of the model-based norm, while assuring that solution lives in the model. To achieve this, we show that sparse model-based matrices satisfy a group version of the null-space property. Our experimental findings on synthetic and real applications support our claims for faster recovery in the convex setting - as opposed to using dense sensing matrices, while showing a competitive recovery performance.",,,,,"Tran-Dinh, Quoc/AAX-8950-2020","Tran-Dinh, Quoc/0000-0002-1077-2579",,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,19,27,,,,,,,,,,,,,,,,WOS:000508662100003,0
C,"Berg, J; Jarvisalo, M; Malone, B",,"Kaski, S; Corander, J",,"Berg, Jeremias; Jarvisalo, Matti; Malone, Brandon",,,Learning Optimal Bounded Treewidth Bayesian Networks via Maximum Satisfiability,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"Bayesian network structure learning is the well-known computationally hard problem of finding a directed acyclic graph structure that optimally describes given data. A learned structure can then be used for probabilistic inference. While exact inference in Bayesian networks is in general NP-hard, it is tractable in networks with low treewidth. This provides good motivations for developing algorithms for the NP-hard problem of learning optimal bounded treewidth Bayesian networks (BTW-BNSL). In this work, we develop a novel score-based approach to BTW-BNSL, based on casting BTW-BNSL as weighted partial Maximum satisfiability. We demonstrate empirically that the approach scales notably better than a recent exact dynamic programming algorithm for BTW-BNSL.",,,,,"Berg, Jeremias/AAA-9025-2020","Berg, Jeremias/0000-0001-7660-8061; Jarvisalo, Matti/0000-0003-2572-063X",,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,86,95,,,,,,,,,,,,,,,,WOS:000508355800010,0
C,"Eggeling, R; Roos, T; Myllymaki, P; Grosse, I",,"Kaski, S; Corander, J",,"Eggeling, Ralf; Roos, Teemu; Myllymaki, Petri; Grosse, Ivo",,,Robust learning of inhomogeneous PMMs,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"Inhomogeneous parsimonious Markov models have recently been introduced for modeling symbolic sequences, with a main application being DNA sequence analysis. Structure and parameter learning of these models has been proposed using a Bayesian approach, which entails the practically challenging choice of the prior distribution. Cross validation is a possible way of tuning the prior hyperparameters towards a specific task such as prediction or classification, but it is overly time-consuming. On this account, robust learning methods, which do not require explicit prior specification and in the absence of prior knowledge no hyperparameter tuning, are of interest. In this work, we empirically investigate the performance of robust alternatives for structure and parameter learning that extend the practical applicability of inhomogeneous parsimonious Markov models to more complex settings than before.",,,,,"Eggeling, Ralf/P-3403-2019; Myllymaki, Petri/O-4113-2014","Eggeling, Ralf/0000-0002-3583-1029; Roos, Teemu/0000-0001-9470-3759; Myllymaki, Petri/0000-0001-9095-282X",,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,229,237,,,,,,,,,,,,,,,,WOS:000508355800026,0
C,"Kaban, A",,"Kaski, S; Corander, J",,"Kaban, Ata",,,New Bounds on Compressive Linear Least Squares Regression,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"In this paper we provide a new analysis of compressive least squares regression that removes a spurious log N factor from previous bounds, where N is the number of training points. Our new bound has a clear interpretation and reveals meaningful structural properties of the linear regression problem that makes it solvable effectively in a small dimensional random subspace. In addition, the main part of our analysis does not require the compressive matrix to have the Johnson-Lindenstrauss property, or the RIP property. Instead, we only require its entries to be drawn i.i.d. from a 0-mean symmetric distribution with finite first four moments.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,448,456,,,,,,,,,,,,,,,,WOS:000508355800050,0
C,"Peltola, T; Jylanki, P; Vehtari, A",,"Kaski, S; Corander, J",,"Peltola, Tomi; Jylanki, Pasi; Vehtari, Aki",,,Expectation Propagation for Likelihoods Depending on an Inner Product of Two Multivariate Random Variables,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"We describe how a deterministic Gaussian posterior approximation can be constructed using expectation propagation (EP) for models, where the likelihood function depends on an inner product of two multivariate random variables. The family of applicable models includes a wide variety of important linear latent variable models used in statistical machine learning, such as principal component and factor analysis, their linear extensions, and errors-in-variables regression. The EP computations are facilitated by an integral transformation of the Dirac delta function, which allows transforming the multidimensional integrals over the two multivariate random variables into an analytically tractable form up to one-dimensional analytically intractable integrals that can be efficiently computed numerically. We study the resulting posterior approximations in sparse principal component analysis with Gaussian and probit likelihoods. Comparisons to Gibbs sampling and variational inference are presented.",,,,,,"Vehtari, Aki/0000-0003-2164-9469",,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,769,777,,,,,,,,,,,,,,,,WOS:000508355800085,0
C,"Sanderson, T; Scott, C",,"Kaski, S; Corander, J",,"Sanderson, Tyler; Scott, Clayton",,,Class Proportion Estimation with Application to Multiclass Anomaly Rejection,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"This work addresses two classification problems that fall under the heading of domain adaptation, wherein the distributions of training and testing examples differ. The first problem studied is that of class proportion estimation, which is the problem of estimating the class proportions in an unlabeled testing data set given labeled examples of each class. Compared to previous work on this problem, our approach has the novel feature that it does not require labeled training data from one of the classes. This property allows us to address the second domain adaptation problem, namely, multiclass anomaly rejection. Here, the goal is to design a classifier that has the option of assigning a reject label, indicating that the instance did not arise from a class present in the training data. We establish consistent learning strategies for both of these domain adaptation problems, which to our knowledge are the first of their kind. We also implement the class proportion estimation technique and demonstrate its performance on several benchmark data sets.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,850,858,,,,,,,,,,,,,,,,WOS:000508355800094,0
C,"Stathopoulos, V; Zamora-Gutierrez, V; Jones, KE; Girolami, M",,"Kaski, S; Corander, J",,"Stathopoulos, Vassilios; Zamora-Gutierrez, Veronica; Jones, Kate E.; Girolami, Mark",,,Bat Call Identification with Gaussian Process Multinomial Probit Regression and a Dynamic Time Warping Kernel,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,We study the problem of identifying bat species from echolocation calls in order to build automated bioacoustic monitoring algorithms. We employ the Dynamic Time Warping algorithm which has been successfully applied for bird flight calls identification and show that classification performance is superior to hand crafted call shape parameters used in previous research. This highlights that generic bioacoustic software with good classification rates can be constructed with little domain knowledge. We conduct a study with field data of 21 bat species from the north and central Mexico using a multinomial probit regression model with Gaussian process prior and a full EP approximation of the posterior of latent function values. Results indicate high classification accuracy across almost all classes while misclassification rate across families of species is low highlighting the common evolutionary path of echolocation in bats.,,,,,"Zamora-Gutierrez, Veronica/AAB-6146-2020; Jones, Kate/G-4768-2010","Zamora-Gutierrez, Veronica/0000-0003-0661-5180; Jones, Kate/0000-0001-5231-3293; Girolami, Mark/0000-0003-3008-253X",,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,913,921,,,,,,,,,,,,,,,,WOS:000508355800101,0
C,"Vats, D; Baraniuk, RG",,"Kaski, S; Corander, J",,"Vats, Divyanshu; Baraniuk, Richard G.",,,Path Thresholding: Asymptotically Tuning-Free High-Dimensional Sparse Regression,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"In this paper, we address the challenging problem of selecting tuning parameters for high-dimensional sparse regression. We propose a simple and computationally efficient method, called path thresholding (PaTh), that transforms any tuning parameter-dependent sparse regression algorithm into an asymptotically tuning-free sparse regression algorithm. More specifically, we prove that, as the problem size becomes large (in the number of variables and in the number of observations), PaTh performs accurate sparse regression, under appropriate conditions, without specifying a tuning parameter. In finite-dimensional settings, we demonstrate that PaTh can alleviate the computational burden of model selection algorithms by significantly reducing the search space of tuning parameters.",,,,,"Baraniuk, Richard/ABA-1743-2020",,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,948,957,,,,,,,,,,,,,,,,WOS:000508355800105,0
C,"Bozkurt, A; Esmaeili, B; Tristan, JB; Brooks, DH; Dy, JG; van de Meent, JW",,"Banerjee, A; Fukumizu, K",,"Bozkurt, Alican; Esmaeili, Babak; Tristan, Jean-Baptiste; Brooks, Dana H.; Dy, Jennifer G.; van de Meent, Jan-Willem",,,Rate-Regularization and Generalization in VAEs,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Variational autoencoders optimize an objective that combines a reconstruction loss (the distortion) and a KL term (the rate). The rate is an upper bound on the mutual information, which is often interpreted as a regularizer that controls the degree of compression. We here examine whether inclusion of the rate also acts as an inductive bias that improves generalization. We perform rate-distortion analyses that control the strength of the rate term, the network capacity, and the difficulty of the generalization problem. Decreasing the strength of the rate paradoxically improves generalization in most settings, and reducing the mutual information typically leads to underfitting. Moreover, we show that generalization continues to improve even after the mutual information saturates, indicating that the gap on the bound (i.e. the KL divergence relative to the inference marginal) affects generalization. This suggests that the standard Gaussian prior is not an inductive bias that typically aids generalization, prompting work to understand what choices of priors improve generalization in VAEs.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804066,0
C,"Chen, YN; Luo, HP; Ma, TY; Zhang, CC",,"Banerjee, A; Fukumizu, K",,"Chen, Yining; Luo, Haipeng; Ma, Tengyu; Zhang, Chicheng",,,Active Online Learning with Hidden Shifting Domains,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Online machine learning systems need to adapt to domain shifts. Meanwhile, acquiring label at every timestep is expensive. Motivated by these two challenges, we propose a surprisingly simple algorithm that adaptively balances its regret and its number of label queries in settings where the data streams are from a mixture of hidden domains. For online linear regression with oblivious adversaries, we provide a tight tradeoff that depends on the durations and dimensionalities of the hidden domains. Our algorithm can adaptively deal with interleaving spans of inputs from different domains. We also generalize our results to non-linear regression for hypothesis classes with bounded eluder dimension and adaptive adversaries. Experiments on synthetic and realistic datasets demonstrate that our algorithm achieves lower regret than uniform queries and greedy queries with equal labeling budget.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802048,0
C,"Duchi, J; Ruan, F",,"Banerjee, A; Fukumizu, K",,"Duchi, John; Ruan, Feng",,,A constrained risk inequality for general losses,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We provide a general constrained risk inequality that applies to arbitrary nondecreasing losses, extending a result of Brown and Low [Ann. Stat. 1996]. Given two distributions P-0 and P-1, we find a lower bound for the risk of estimating a parameter theta(P-1) under P-1 given an upper bound on the risk of estimating the parameter 0(P-0) under P-0. The inequality is a useful tool, as its proof relies only on the Cauchy-Schwartz inequality, it applies to general losses, including optimality gaps in stochastic convex optimization, and it transparently gives risk lower bounds on super-efficient and adaptive estimators.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801003,0
C,"Ghadikolaei, HS; Stich, SU; Jaggi, M",,"Banerjee, A; Fukumizu, K",,"Ghadikolaei, Hossein S.; Stich, Sebastian U.; Jaggi, Martin",,,LENA: Communication-Efficient Distributed Learning with Self-Triggered Gradient Uploads,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"In distributed optimization, parameter updates from the gradient computing node devices have to be aggregated in every iteration on the orchestrating server. When these updates are sent over an arbitrary commodity network, bandwidth and latency can be limiting factors. We propose a communication framework where nodes may skip unnecessary uploads. Every node locally accumulates an error vector in memory and self-triggers the upload of the memory contents to the parameter server using a significance filter. The server then uses a history of the nodes' gradients to update the parameter. We characterize the convergence rate of our algorithm in smooth settings (strongly-convex, convex, and nonconvex) and show that it enjoys the same convergence rate as when sending gradients every iteration, with substantially fewer uploads. Numerical experiments on real data indicate a significant reduction of used network resources (total communicated bits and latency), especially in large networks, compared to state-of-the-art algorithms. Our results provide important practical insights for using machine learning over resource-constrained networks, including Internet-of-Things and geo-separated datasets across the globe.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804073,0
C,"Grazzi, R; Pontil, M; Salzo, S",,"Banerjee, A; Fukumizu, K",,"Grazzi, Riccardo; Pontil, Massimiliano; Salzo, Saverio",,,Convergence Properties of Stochastic Hypergradients,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Bilevel optimization problems are receiving increasing attention in machine learning as they provide a natural framework for hyperparameter optimization and meta-learning. A key step to tackle these problems is the efficient computation of the gradient of the upper-level objective (hypergradient). In this work, we study stochastic approximation schemes for the hypergradient, which are important when the lower-level problem is empirical risk minimization on a large dataset. The method that we propose is a stochastic variant of the approximate implicit differentiation approach in (Pedregosa, 2016). We provide bounds for the mean square error of the hypergradient approximation, under the assumption that the lower-level problem is accessible only through a stochastic mapping which is a contraction in expectation. In particular, our main bound is agnostic to the choice of the two stochastic solvers employed by the procedure. We provide numerical experiments to support our theoretical analysis and to show the advantage of using stochastic hypergradients in practice.",,,,,"Salzo, Saverio/AAZ-7481-2021","Salzo, Saverio/0000-0003-0494-9101",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804060,0
C,"Lang, H; Reddy, A; Sontag, D; Vijayaraghavan, A",,"Banerjee, A; Fukumizu, K",,"Lang, Hunter; Reddy, Aravind; Sontag, David; Vijayaraghavan, Aravindan",,,Beyond Perturbation Stability: LP Recovery Guarantees for MAP Inference on Noisy Stable Instances,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Several works have shown that perturbation stable instances of the MAP inference problem in Potts models can be solved exactly using a natural linear programming (LP) relaxation. However, most of these works give few (or no) guarantees for the LP solutions on instances that do not satisfy the relatively strict perturbation stability definitions. In this work, we go beyond these stability results by showing that the LP approximately recovers the MAP solution of a stable instance even after the instance is corrupted by noise. This noisy stable model realistically fits with practical MAP inference problems: we design an algorithm for finding close stable instances, and show that several real-world instances from computer vision have nearby instances that are perturbation stable. These results suggest a new theoretical explanation for the excellent performance of this LP relaxation in practice.",,,,,"Vijayaraghavan, Aravindan/I-2257-2015",,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803065,0
C,"Sankararaman, A; Basu, S; Sankararaman, KA",,"Banerjee, A; Fukumizu, K",,"Sankararaman, Abishek; Basu, Soumya; Sankararaman, Karthik Abinav",,,Dominate or Delete: Decentralized Competing Bandits in Serial Dictatorship,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Online learning in a two-sided matching market, with demand side agents continuously competing to be matched with supply side (arms), abstracts the complex interactions under partial information on matching platforms (e.g. UpWork, TaskRabbit). We study the decentralized serial dictatorship setting, a two-sided matching market where the demand side agents have unknown and heterogeneous valuation over the supply side (arms), while the arms have known uniform preference over the demand side (agents). We design the first decentralized algorithm - UCB with Decentralized Dominant-arm Deletion (UCB-D3), for the agents, that does not require any knowledge of reward gaps or time horizon. UCB-D3 works in phases, where in each phase, agents delete dominated arms - the arms preferred by higher ranked agents, and play only from the non-dominated arms according to the UCB. At the end of the phase, agents broadcast in a decentralized fashion, their estimated preferred arms through pure exploitation. We prove both, a new regret lower bound for the decentralized serial dictatorship model, and that UCB-D3 is order optimal.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801053,0
C,"Schut, L; Key, O; McGrath, R; Costabellot, L; Sacaleanut, B; Corcoran, M; Galt, Y",,"Banerjee, A; Fukumizu, K",,"Schut, Lisa; Key, Oscar; McGrath, Rory; Costabellot, Luca; Sacaleanut, Bogdan; Corcoran, Medb; Galt, Yarin",,,Generating Interpretable Counterfactual Explanations By Implicit Minimisation of Epistemic and Aleatoric Uncertainties,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Counterfactual explanations (CEs) are a practical tool for demonstrating why machine learning classifiers make particular decisions. For CEs to be useful, it is important that they are easy for users to interpret. Existing methods for generating interpretable CEs rely on auxiliary generative models, which may not be suitable for complex datasets, and incur engineering overhead. We introduce a simple and fast method for generating interpretable CEs in a white-box setting without an auxiliary model, by using the predictive uncertainty of the classifier. Our experiments show that our proposed algorithm generates more interpretable CEs, according to IM1 scores (Van Looveren and Klaise, 2019), than existing methods. Additionally, our approach allows us to estimate the uncertainty of a CE, which may be important in safety-critical applications, such as those in the medical domain.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802015,0
C,"Xu, JM; Xu, K; Yang, DN",,"Banerjee, A; Fukumizu, K",,"Xu, Jiaming; Xu, Kuang; Yang, Dana",,,Optimal query complexity for private sequential learning against eavesdropping,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We study a learner-private sequential learning problem, motivated by the privacy and security concerns due to eavesdropping attacks. A learner tries to estimate an unknown scalar value, by sequentially querying an external database and receiving binary responses; meanwhile, a third-party adversary observes the learner's queries but not the responses. The learner's goal is to design a querying strategy with the minimum number of queries (optimal query complexity) so that she can accurately estimate the true value, while the eavesdropping adversary even with the complete knowledge of her querying strategy cannot. We develop new querying strategies and analytical techniques and use them to prove almost-matching upper and lower bounds on the optimal query complexity, obtaining a complete characterization of the optimal query complexity as a function of the estimation accuracy and the desired levels of privacy.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802075,0
C,"Astudillo, R; Frazier, PI",,"Chiappa, S; Calandra, R",,"Astudillo, Raul; Frazier, Peter I.",,,Multi-attribute Bayesian optimization with interactive preference learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We consider black-box global optimization of time-consuming-to-evaluate functions on behalf of a decision-maker (DM) whose preferences must be learned. Each feasible design is associated with a time-consuming-to-evaluate vector of attributes and each vector of attributes is assigned a utility by the DM's utility function, which may be learned approximately using preferences expressed over pairs of attribute vectors. Past work has used a point estimate of this utility function as if it were error-free within single-objective optimization. However, utility estimation errors may yield a poor suggested design. Furthermore, this approach produces a single suggested best design, whereas DMs often prefer to choose from a menu. We propose a novel multi-attribute Bayesian optimization with preference learning approach. Our approach acknowledges the uncertainty in preference estimation and implicitly chooses designs to evaluate that are good not just for a single estimated utility function but a range of likely ones. The outcome of our approach is a menu of designs and evaluated attributes from which the DM makes a final selection. We demonstrate the value and flexibility of our approach in a variety of experiments.",,,,,,"Frazier, Peter/0000-0002-3501-3341",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,4496,4506,,,,,,,,,,,,,,,,WOS:000559931300017,0
C,"Balcan, MF; Dick, T; Sharma, D",,"Chiappa, S; Calandra, R",,"Balcan, Maria-Florina; Dick, Travis; Sharma, Dravyansh",,,Learning piecewise Lipschitz functions in changing environments,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Optimization in the presence of sharp (non-Lipschitz), unpredictable (w.r.t. time and amount) changes is a challenging and largely unexplored problem of great significance. We consider the class of piecewise Lipschitz functions, which is the most general online setting considered in the literature for the problem, and arises naturally in various combinatorial algorithm selection problems where utility functions can have sharp discontinuities. The usual performance metric of 'static' regret minimizes the gap between the payoff accumulated and that of the best fixed point for the entire duration, and thus fails to capture changing environments. Shifting regret is a useful alternative, which allows for up to s environment shifts. In this work we provide an O(root sdT log T + sT(1-beta)) regret bound for beta-dispersed functions, where beta roughly quantifies the rate at which discontinuities appear in the utility functions in expectation (typically beta >= 1/2 in problems of practical interest [Balcan et al., 2019, Balcan et al., 2018a]). We also present a lower bound tight up to sub-logarithmic factors. We further obtain improved bounds when selecting from a small pool of experts. We empirically demonstrate a key application of our algorithms to online clustering problems on popular benchmarks.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303011,0
C,"Chakraborty, S; Paul, D; Das, S; Xu, J",,"Chiappa, S; Calandra, R",,"Chakraborty, Saptarshi; Paul, Debolina; Das, Swagatam; Xu, Jason",,,Entropy Weighted Power k-Means Clustering,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Despite its well-known shortcomings, k-means remains one of the most widely used approaches to data clustering. Current research continues to tackle its flaws while attempting to preserve its simplicity. Recently, the power k-means algorithm was proposed to avoid poor local minima by annealing through a family of smoother surfaces. However, the approach lacks statistical guarantees and fails in high dimensions when many features are irrelevant. This paper addresses these issues by introducing entropy regularization to learn feature relevance while annealing. We prove consistency of the proposed approach and derive a scalable majorization-minimization algorithm that enjoys closed-form updates and convergence guarantees. In particular, our method retains the same computational complexity of k-means and power k-means, but yields significant improvements over both. Its merits are thoroughly assessed on a suite of real and synthetic data experiments.",,,,,"Chakraborty, Saptarshi/ABG-3356-2020; Xu, Jason/GRR-9638-2022; XU, Jason/GPW-9039-2022","Chakraborty, Saptarshi/0000-0002-3668-637X; Paul, Debolina/0000-0002-3981-9596",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,691,700,,,,,,,,,,,,,,,,WOS:000559931300048,0
C,"Chatterji, NS; Muthukumar, V; Bartlett, PL",,"Chiappa, S; Calandra, R",,"Chatterji, Niladri S.; Muthukumar, Vidya; Bartlett, Peter L.",,,OSOM: A simultaneously optimal algorithm for multi-armed and linear contextual bandits,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We consider the stochastic linear (multi-armed) contextual bandit problem with the possibility of hidden simple multi-armed bandit structure in which the rewards are independent of the contextual information. Algorithms that are designed solely for one of the regimes are known to be sub-optimal for their alternate regime. We design a single computationally efficient algorithm that simultaneously obtains problem-dependent optimal regret rates in the simple multi-armed bandit regime and minimax optimal regret rates in the linear contextual bandit regime, without knowing a priori which of the two models generates the rewards. These results are proved under the condition of stochasticity of contextual information over multiple rounds. Our results should be viewed as a step towards principled data-dependent policy class selection for contextual bandits.",,,,,,"Muthukumar, Vidya/0000-0003-2786-7360",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1844,1853,,,,,,,,,,,,,,,,WOS:000559931300050,0
C,"Dalmasso, N; Lee, AB; Izbicki, R; Pospisil, T; Kim, I; Lin, CA",,"Chiappa, S; Calandra, R",,"Dalmasso, Niccolo; Lee, Ann B.; Izbicki, Rafael; Pospisil, Taylor; Kim, Ilmun; Lin, Chieh-An",,,Validation of Approximate Likelihood and Emulator Models for Computationally Intensive Simulations,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Complex phenomena in engineering and the sciences are often modeled with computationally intensive feed-forward simulations for which a tractable analytic likelihood does not exist. In these cases, it is sometimes necessary to estimate an approximate likelihood or fit a fast emulator model for efficient statistical inference; such surrogate models include Gaussian synthetic likelihoods and more recently neural density estimators such as autoregressive models and normalizing flows. To date, however, there is no consistent way of quantifying the quality of such a fit. Here we propose a statistical framework that can distinguish any arbitrary misspecified model from the target likelihood, and that in addition can identify with statistical confidence the regions of parameter as well as feature space where the fit is inadequate. At the heart of our approach is a two-sample test that quantifies the quality of the fit at fixed parameter values, and a global test that assesses goodness-of-fit across simulation parameters. While our general framework can incorporate any test statistic or distance metric, we specifically argue for a new two-sample test that can leverage any regression method to attain high power and provide diagnostics in complex data settings. Software for our approach is available on GitHub in Python and R",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3349,3360,,,,,,,,,,,,,,,,WOS:000559931300069,0
C,"Fang, H; Fan, ZA; Sun, YF; Friedlander, MP",,"Chiappa, S; Calandra, R",,"Fang, Huang; Fan, Zhenan; Sun, Yifan; Friedlander, Michael P.",,,Greed Meets Sparsity: Understanding and Improving Greedy Coordinate Descent for Sparse Optimization,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We consider greedy coordinate descent (GCD) for composite problems with sparsity inducing regularizers, including 1-norm regularization and non-negative constraints. Empirical evidence strongly suggests that GCD, when initialized with the zero vector, has an implicit screening ability that usually selects at each iteration coordinates that at are nonzero at the solution. Thus, for problems with sparse solutions, GCD can converge significantly faster than randomized coordinate descent. We present an improved convergence analysis of GCD for sparse optimization, and a formal analysis of its screening properties. We also propose and analyze an improved selection rule with stronger ability to produce sparse iterates. Numerical experiments on both synthetic and real-world data support our analysis and the effectiveness of the proposed selection rule.",,,,,,"Sun, Yifan/0000-0003-2475-3843",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,434,443,,,,,,,,,,,,,,,,WOS:000559931300089,0
C,"Ferdosi, M; Davoodi, AG; Mohimani, H",,"Chiappa, S; Calandra, R",,"Ferdosi, Mohsen; Davoodi, Arash Gholami; Mohimani, Hosein",,,Measuring Mutual Information Between All Pairs of Variables in Subquadratic Complexity,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Finding associations between pairs of variables in large datasets is crucial for various disciplines. The brute force method for solving this problem requires computing the mutual information between ((2)(N)) pairs. In this paper, we consider the problem of finding pairs of variables with high mutual information in sub-quadratic complexity. This problem is analogous to the nearest neighbor search, where the goal is to find pairs among N variables that are similar to each other. To solve this problem, we develop a new algorithm for finding associations based on constructing a decision tree that assigns a hash to each variable, in a way that for pairs with higher mutual information, the chance of having the same hash is higher. For any 1 <= lambda <= 2, we prove that in the case of binary data, we can reduce the number of necessary mutual information computations for finding all pairs satisfying I(X, Y) > 2 - lambda from O(N-2) to O(N-lambda), where I(X, Y) is the empirical mutual information between variables X and Y. Finally, we confirmed our theory by experiments on simulated and real data.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,4399,4408,,,,,,,,,,,,,,,,WOS:000559931301001,0
C,"Hayashi, K; Imaizumi, M; Yoshida, Y",,"Chiappa, S; Calandra, R",,"Hayashi, Kohei; Imaizumi, Masaaki; Yoshida, Yuichi",,,On Random Subsampling of Gaussian Process Regression: A Graphon-Based Analysis,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"In this paper, we study random subsampling of Gaussian process regression, one of the simplest approximation baselines, from a theoretical perspective. Although subsampling discards a large part of training data, we show provable guarantees on the accuracy of the predictive mean/variance and its generalization ability. For analysis, we consider embedding kernel matrices into graphons, which encapsulate the difference of the sample size and enables us to evaluate the approximation and generalization errors in a unified manner. The experimental results show that the subsampling approximation achieves a better trade-off regarding accuracy and runtime than the Nystrom and random Fourier expansion methods.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2055,2064,,,,,,,,,,,,,,,,WOS:000559931301038,0
C,"Li, TY; Mazoure, B; Precup, D; Rabusseau, G",,"Chiappa, S; Calandra, R",,"Li, Tianyu; Mazoure, Bogdan; Precup, Doina; Rabusseau, Guillaume",,,Efficient Planning under Partial Observability with Unnormalized Q Functions and Spectral Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Learning and planning in partially-observable domains is one of the most difficult problems in reinforcement learning. Traditional methods consider these two problems as independent, resulting in a classic two-stage paradigm: first learn the environment dynamics and then compute the optimal policy accordingly. This approach, however, disconnects the reward information from the learning of the environment model and can consequently lead to representations that are sample inefficient and time consuming for planning purpose. In this paper, we propose a novel algorithm that incorporate reward information into the representations of the environment to unify these two stages. Our algorithm is closely related to the spectral learning algorithm for predicitive state representations and offers appealing theoretical guarantees and time complexity. We empirically show on two domains that our approach is more sample and time efficient compared to classical methods.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2852,2861,,,,,,,,,,,,,,,,WOS:000559931302008,0
C,"Liang, YY; Song, Z; Wang, MD; Yang, LF; Yang, X",,"Chiappa, S; Calandra, R",,"Liang, Yingyu; Song, Zhao; Wang, Mengdi; Yang, Lin F.; Yang, Xin",,,Sketching Transformed Matrices with Applications to Natural Language Processing,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Suppose we are given a large matrix A = (a(i, j)) that cannot be stored in memory but is in a disk or is presented in a data stream. However, we need to compute a matrix decomposition of the entry-wisely transformed matrix, f(A) := (f(a(i, j))) for some function f. Is it possible to do it in a space efficient way? Many machine learning applications indeed need to deal with such large transformed matrices, for example word embedding method in NLP needs to work with the pointwise mutual information (PMI) matrix, while the entrywise transformation makes it difficult to apply known linear algebraic tools. Existing approaches for this problem either need to store the whole matrix and perform the entry-wise transformation afterwards, which is space consuming or infeasible, or need to redesign the learning method, which is application specific and requires substantial remodeling. In this paper, we first propose a space-efficient sketching algorithm for computing the product of a given small matrix with the transformed matrix. It works for a general family of transformations with provable small error bounds and thus can be used as a primitive in downstream learning tasks. We then apply this primitive to a concrete application: low-rank approximation. We show that our approach obtains small error and is efficient in both space and time. We complement our theoretical results with experiments on synthetic and real data.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,467,480,,,,,,,,,,,,,,,,WOS:000559931302011,0
C,"Liao, H; Pearlmutter, BA; Potluru, VK; Woodruff, DP",,"Chiappa, S; Calandra, R",,"Liao, Hang; Pearlmutter, Barak A.; Potluru, Vamsi K.; Woodruff, David P.",,,Automatic Differentiation of Sketched Regression,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Sketching for speeding up regression problems involves using a sketching matrix S to quickly find the approximate solution to a linear least squares regression (LLS) problem: given A of size n x d, with n >> d, along with b of size n x 1, we seek a vector y with minimal regression error MAy b112. This approximation technique is now standard in data science, and many software systems use sketched regression internally, as a component. It is often useful to calculate derivatives (gradients for the purpose of optimization, for example) of such large systems, where sketched LLS is merely a component of a larger system whose derivatives are needed. To support Automatic Differentiation (AD) of systems containing sketched LLS, we consider propagating derivatives through LLS: both propagating perturbations (forward AD) and gradients (reverse AD). AD performs accurate differentiation and is efficient for problems with a huge number of independent variables. Since we use LLSs (sketched LLS) instead of LLS for reasons of efficiency, propagation of derivatives also needs to trade accuracy for efficiency, presumably by sketching. There are two approaches for this: (a) use AD to transform the code that defines LLSs, or (b) approximate exact derivative propagation through LLS using sketching methods. We provide strong bounds on the errors produced due to these two natural forms of sketching in the context of AD, giving the first dimensionality reduction analysis for calculating the derivatives of a sketched computation. Our results crucially depend on a novel analysis of the operator norm of a sketched inverse matrix product in this context. Extensive experiments on both synthetic and real-world experiments demonstrate the efficacy of our sketched gradients.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,4367,4375,,,,,,,,,,,,,,,,WOS:000559931302012,0
C,"Stephenson, WT; Broderick, T",,"Chiappa, S; Calandra, R",,"Stephenson, William T.; Broderick, Tamara",,,Approximate Cross-Validation in High Dimensions with Guarantees,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Leave-one-out cross-validation (LOOCV) can be particularly accurate among cross-validation (CV) variants for machine learning assessment tasks e.g., assessing methods' error or variability. But it is expensive to re-fit a model N times for a dataset of size N. Previous work has shown that approximations to LOOCV can be both fast and accurate when the unknown parameter is of small, fixed dimension. But these approximations incur a running time roughly cubic in dimension - and we show that, besides computational issues, their accuracy dramatically deteriorates in high dimensions. Authors have suggested many potential and seemingly intuitive solutions, but these methods have not yet been systematically evaluated or compared. We find that all but one perform so poorly as to be unusable for approximating LOOCV. Crucially, though, we are able to show, both empirically and theoretically, that one approximation can perform well in high dimensions in cases where the high-dimensional parameter exhibits sparsity. Under interpretable assumptions, our theory demonstrates that the problem can be reduced to working within an empirically recovered (small) support. This procedure is straightforward to implement, and we prove that its running time and error depend on the (small) support size even when the full parameter dimension is large.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303030,0
C,"Sun, J; Wang, G; Giannakis, GB; Yang, QM; Yang, ZY",,"Chiappa, S; Calandra, R",,"Sun, Jun; Wang, Gang; Giannakis, Georgios B.; Yang, Qinmin; Yang, Zaiyue",,,Finite-Time Analysis of Decentralized Temporal-Difference Learning with Linear Function Approximation,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Motivated by the emerging use of multi-agent reinforcement learning (MARL) in various engineering applications, we investigate the policy evaluation problem in a fully decentralized setting, using temporal-difference (TD) learning with linear function approximation to handle large state spaces in practice. The goal of a group of agents is to collaboratively learn the value function of a given policy from locally private rewards observed in a shared environment, through exchanging local estimates with neighbors. Despite their simplicity and widespread use, our theoretical understanding of such decentralized TD learning algorithms remains limited. Existing results were obtained based on i.i.d. data samples, or by imposing an 'additional' projection step to control the 'gradient' bias incurred by the Markovian observations. In this paper, we provide a finite-sample analysis of the fully decentralized TD(0) learning under both i.i.d. as well as Markovian samples, and prove that all local estimates converge linearly to a neighborhood of the optimum. The resultant error bounds are the first of its type-in the sense that they hold under the most practical assumptions - which is made possible by means of a novel multi-step Lyapunov analysis.",,,,,"Yang, Zaiyue/AHB-2796-2022",,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303034,0
C,"Yin, M; Wang, YX",,"Chiappa, S; Calandra, R",,"Yin, Ming; Wang, Yu-Xiang",,,Asymptotically Efficient Off-Policy Evaluation for Tabular Reinforcement Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We consider the problem of off-policy evaluation for reinforcement learning, where the goal is to estimate the expected reward of a target policy pi using offline data collected by running a logging policy mu. Standard importance-sampling based approaches for this problem suffer from a variance that scales exponentially with time horizon H, which motivates a splurge of recent interest in alternatives that break the Curse of Horizon (Liu et aL, 2018a; Xie et aL, 2019). In particular, it was shown that a marginalized importance sampling (MIS) approach can be used to achieve an estimation error of order O(H-3/n) in mean square error (MSE) under an episodic Markov Decision Process model with finite states and potentially infinite actions. The MSE bound however is still a factor of H away from a Cramer-Rao lower bound of order Omega(H-2/n). In this paper, we prove that with a simple modification to the MIS estimator, we can asymptotically attain the Cramer-Rao lower bound, provided that the action space is finite. We also provide a general method for constructing MIS estimators with high-probability error bounds.",,,,,,"Wang, Yu-Xiang/0000-0002-6403-212X; Yin, Ming/0000-0001-6458-0751",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303099,0
C,"Zhuo, JC; Lei, Q; Dimakis, A; Caramanis, C",,"Chiappa, S; Calandra, R",,"Zhuo, Jiacheng; Lei, Qi; Dimakis, Alexandros; Caramanis, Constantine",,,Communication-Efficient Asynchronous Stochastic Frank-Wolfe over Nuclear-norm Balls,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Large-scale machine learning training suffers from two prior challenges, specifically for nuclear-norm constrained problems with distributed systems: the synchronization slowdown due to the straggling workers, and high communication costs. In this work, we propose an asynchronous Stochastic Frank Wolfe (SFW-asyn) method, which, for the first time, solves the two problems simultaneously, while successfully maintaining the same convergence rate as the vanilla SFW. We implement our algorithm in python (with MPI) to run on Amazon EC2, and demonstrate that SFW-asyn yields speed-ups almost linear to the number of machines compared to the vanilla SFW.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1464,1473,,,,,,,,,,,,,,,,WOS:000559931304025,0
C,"Bird, A; Williams, CKI; Hawthorne, C",,"Chaudhuri, K; Sugiyama, M",,"Bird, Alex; Williams, Christopher K., I; Hawthorne, Christopher",,,Multi-Task Time Series Analysis applied to Drug Response Modelling,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Time series models such as dynamical systems are frequently fitted to a cohort of data, ignoring variation between individual entities such as patients. In this paper we show how these models can be personalised to an individual level while retaining statistical power, via use of multi-task learning (MTL). To our knowledge this is a novel development of MTL which applies to time series both with and without control inputs. The modelling framework is demonstrated on a physiological drug response problem which results in improved predictive accuracy and uncertainty estimation over existing state-of-the-art models.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902023,0
C,"Holland, MJ",,"Chaudhuri, K; Sugiyama, M",,"Holland, Matthew J.",,,Classification using margin pursuit,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"In this work, we study a new approach to optimizing the margin distribution realized by binary classifiers, in which the learner searches the hypothesis space in such a way that a pre-set margin level ends up being a distribution-robust estimator of the margin location. This procedure is easily implemented using gradient descent, and admits finite-sample bounds on the excess risk under unbounded inputs, yielding competitive rates under mild assumptions. Empirical tests on real-world benchmark data reinforce the basic principles highlighted by the theory.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,712,720,,,,,,,,,,,,,,,,WOS:000509687900074,0
C,"Li, CL; Chang, WC; Mroueh, Y; Yang, YM; Poczos, B",,"Chaudhuri, K; Sugiyama, M",,"Li, Chun-Liang; Chang, Wei-Cheng; Mroueh, Youssef; Yang, Yiming; Poczos, Barnabas",,,Implicit Kernel Learning,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Kernels are powerful and versatile tools in machine learning and statistics. Although the notion of universal kernels and characteristic kernels has been studied, kernel selection still greatly influences the empirical performance. While learning the kernel in a data driven way has been investigated, in this paper we explore learning the spectral distribution of kernel via implicit generative models parametrized by deep neural networks. We called our method Implicit Kernel Learning (IKL). The proposed framework is simple to train and inference is performed via sampling random Fourier features. We investigate two applications of the proposed IKL as examples, including generative adversarial networks with MMD (MMD GAN) and standard supervised learning. Empirically, MMD GAN with IKL outperforms vanilla predefined kernels on both image and text generation benchmarks; using IKL with Random Kitchen Sinks also leads to substantial improvement over existing state-of-the-art kernel learning algorithms on popular supervised learning benchmarks. Theory and conditions for using IKL in both applications are also studied as well as connections to previous state-of-the-art methods.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902006,0
C,"Martens, K; Titsias, MK; Yau, C",,"Chaudhuri, K; Sugiyama, M",,"Martens, Kaspar; Titsias, Michalis K.; Yau, Christopher",,,Augmented Ensemble MCMC sampling in Factorial Hidden Markov Models,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Bayesian inference for factorial hidden Markov models is challenging due to the exponentially sized latent variable space. Standard Monte Carlo samplers can have difficulties effectively exploring the posterior landscape and are often restricted to exploration around localised regions that depend on initialisation. We introduce a general purpose ensemble Markov Chain Monte Carlo (MCMC) technique to improve on existing poorly mixing samplers. This is achieved by combining parallel tempering and an auxiliary variable scheme to exchange information between the chains in an efficient way. The latter exploits a genetic algorithm within an augmented Gibbs sampler. We compare our technique with various existing samplers in a simulation study as well as in a cancer genomics application, demonstrating the improvements obtained by our augmented ensemble approach.",,,,,,"Yau, Christopher/0000-0001-7615-8523",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902042,0
C,"Mroueh, Y; Sercu, T; Raj, A",,"Chaudhuri, K; Sugiyama, M",,"Mroueh, Youssef; Sercu, Tom; Raj, Anant",,,Sobolev Descent,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We study a simplification of CAN training: the problem of transporting particles from a source to a target distribution. Starting; from the Sobolev CAN critic, part of the gradient regularized GAN family. we show a strong relation with Optimal 'transport (01). Specifically with the less popular dynamic formulation OT that finds a path of distributions from source to target minimizing a kinetic energy. We introduce Sobolev descent that constructs similar paths by following gradient flows of a critic function in a kernel space or parametrized by a neural network In the kernel version, we show convergence to the target distribution in the MMD sense. We show in and experiments that regularization has an important role in favoring smooth transitions between distributions, avoiding large gradients from the critic. This analysis in a simplified particle setting provides insight in paths to equilibrium in GANs.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687903003,0
C,"Saha, A; Gopalan, A",,"Chaudhuri, K; Sugiyama, M",,"Saha, Aadirupa; Gopalan, Aditya",,,Active Ranking with Subset-wise Preferences,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We consider the problem of probably approximately correct (PAC) ranking n items by adaptively eliciting subset-wise preference feedback. At each round, the learner chooses a subset of k items and observes stochastic feedback indicating preference information of the winner (most preferred) item of the chosen subset drawn according to a Plackett-Luce (PL) subset choice model unknown a priori. The objective is to identify an epsilon-optimal ranking of the n items with probability at least 1 - delta. When the feedback in each subset round is a single Plackett-Luce-sampled item, we show (epsilon,delta)-PAC algorithms with a sample complexity of O(n/epsilon 2 ln n/delta) rounds, which we establish as being order-optimal by exhibiting a matching sample complexity lower bound of Omega (n/epsilon 2 ln n/delta)-In this shows that there is essentially no improvement possible from the pairwise comparisons setting (k = 2). When, however, it is possible to elicit top-m (<= k) ranking feedback according to the PL model from each adaptively chosen subset of size k, we show that an (epsilon, delta)-PAC ranking sample complexity of O (n/m epsilon 2 ln n/delta) is achievable with explicit algorithms, which represents an m-wise reduction in sample complexity compared to the pairwise case. This again turns out to be order-wise unimprovable across the class of symmetric ranking algorithms. Our algorithms rely on a novel pivot trick to maintain only n itemwise score estimates, unlike O(n(2)) pairwise score estimates that has been used in prior work. We report results of numerical experiments that corroborate our findings.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687903038,0
C,"Shu, R; Bui, HH; Whang, J; Ermon, S",,"Chaudhuri, K; Sugiyama, M",,"Shu, Rui; Bui, Hung H.; Whang, Jay; Ermon, Stefano",,,Training Variational Autoencoders with Buffered Stochastic Variational Inference,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"The recognition network in deep latent variable models such as variational autoencoders (VAEs) relies on amortized inference for efficient posterior approximation that can scale up to large datasets. However, this technique has also been demonstrated to select suboptimal variational parameters, often resulting in considerable additional error called the amortization gap. To close the amortization gap and improve the training of the generative model, recent works have introduced an additional refinement step that applies stochastic variational inference (SVI) to improve upon the variational parameters returned by the amortized inference model. In this paper, we propose the Buffered Stochastic Variational Inference (BSVI), a new refinement procedure that makes use of SVI's sequence of intermediate variational proposal distributions and their corresponding importance weights to construct a new generalized importance-weighted lower bound. We demonstrate empirically that training the variational autoencoders with BSVI consistently out-performs SVI, yielding an improved training procedure for VAEs.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902019,0
C,"Sun, YF; Jeong, H; Nutini, J; Schmidt, M",,"Chaudhuri, K; Sugiyama, M",,"Sun, Yifan; Jeong, Halyun; Nutini, Julie; Schmidt, Mark",,,Are we there yet? Manifold identification of gradient-related proximal methods,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"In machine learning, models that generalize better often generate outputs that lie on a low-dimensional manifold. Recently, several works have separately shown finite-time manifold identification by some proximal methods. In this work we provide a unified view by giving a simple condition under which any proximal method using a constant step size can achieve finite-iteration manifold detection. For several key methods (FISTA, DRS, ADMM, SVRG, SAGA, and RDA) we give an iteration bound, characterized in terms of their variable convergence rate and a problem-dependent constant that indicates problem degeneracy. For popular models, this constant is related to certain data assumptions, which gives intuition as to when lower active set complexity may be expected in practice.",,,,,,"Sun, Yifan/0000-0003-2475-3843",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901016,0
C,"Tompkins, A; Senanayake, R; Morere, P; Ramos, F",,"Chaudhuri, K; Sugiyama, M",,"Tompkins, Anthony; Senanayake, Ransalu; Morere, Philippe; Ramos, Fabio",,,Black Box Quantiles for Kernel Learning,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Kernel methods have been successfully used in various domains to model nonlinear patterns. However, the structure of the kernels is typically handcrafted for each dataset based on the experience of the data analyst. In this paper, we present a novel technique to learn kernels that best fit the data. We exploit the measure-theoretic view of a shift invariant kernel given by the Bochner's theorem, and automatically learn the measure in terms of a parameterized quantile function. This flexible black box quantile function, evaluated on Quasi-Monte Carlo samples, builds up quasi-random Fourier feature maps that can approximate arbitrary kernels. The proposed method is not only general enough to be used in any kernel machine, but can also be combined with other kernel design techniques. We learn expressive kernels on a variety of datasets, verifying the methods ability to automatically discover complex patterns without being guided by human expert knowledge.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901049,0
C,"El Halabi, M; Bach, F; Cevher, V",,"Storkey, A; PerezCruz, F",,"El Halabi, Marwa; Bach, Francis; Cevher, Volkan",,,Combinatorial Penalties: Which structures are preserved by convex relaxations?,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"We consider the homogeneous and the non-homogeneous convex relaxations for combinatorial penalty functions defined on support sets. Our study identifies key differences in the tightness of the resulting relaxations through the notion of the lower combinatorial envelope of a set-function along with new necessary conditions for support identification. We then propose a general adaptive estimator for convex monotone regularizers, and derive new sufficient conditions for support recovery in the asymptotic setting.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300162,0
C,"Foster, DJ; Reichman, D; Sridharan, K",,"Storkey, A; PerezCruz, F",,"Foster, Dylan J.; Reichman, Daniel; Sridharan, Karthik",,,Inference in Sparse Graphs with Pairwise Measurements and Side Information,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"We consider the statistical problem of recovering a hidden ground truth binary labeling for the vertices of a graph up to low Hamming error from noisy edge and vertex measurements. We present new algorithms and a sharp finite-sample analysis for this problem on trees and sparse graphs with poor expansion properties such as hypergrids and ring lattices. Our method generalizes and improves over that of Globerson et al. (2015), who introduced the problem for two-dimensional grid lattices. For trees we provide a simple, efficient, algorithm that infers the ground truth with optimal Hamming error has optimal sample complexity and implies recovery results for all connected graphs. Here, the presence of side information is critical to obtain a non-trivial recovery rate. We then show how to adapt this algorithm to tree decompositions of edge-subgraphs of certain graph families such as lattices, resulting in optimal recovery error rates that can be obtained efficiently The thrust of our analysis is to 1) use the tree decomposition along with edge measurements to produce a small class of viable vertex labelings and 2) apply an analysis influenced by statistical learning theory to show that we can infer the ground truth from this class using vertex measurements. We show the power of our method in several examples including hypergrids, ring lattices, and the Newman-Watts model for small world graphs. For two-dimensional grids, our results improve over Globerson et al. (2015) by obtaining optimal recovery in the constant-height regime.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300189,0
C,"Ge, J; Wang, ZR; Wang, MD; Liu, H",,"Storkey, A; PerezCruz, F",,"Ge, Jason; Wang, Zhaoran; Wang, Mengdi; Liu, Han",,,Minimax-Optimal Privacy-Preserving Sparse PCA in Distributed Systems,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"This paper proposes a distributed privacy-preserving sparse PCA (DPS-PCA) algorithm that generates a minimax-optimal sparse PCA estimator under differential privacy constraints. In a distributed optimization framework, data providers can use this algorithm to collaboratively analyze the union of their data sets while limiting the disclosure of their private information. DPS-PCA can recover the leading eigenspace of the population covariance at a geometric convergence rate, and simultaneously achieves the optimal minimax statistical error for high-dimensional data. Our algorithm provides fine-tuned control over the tradeoff between estimation accuracy and privacy preservation. Numerical simulations demonstrate that DPS-PCA significantly outperforms other privacy-preserving PCA methods in terms of estimation accuracy and computational efficiency.",,,,,"Wang, Zhaoran/P-7113-2018",,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300166,0
C,"Imaizumi, M; Maehara, T; Yoshida, Y",,"Storkey, A; PerezCruz, F",,"Imaizumi, Masaaki; Maehara, Takanori; Yoshida, Yuichi",,,Statistically Efficient Estimation for Non-Smooth Probability Densities,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"We investigate statistical efficiency of estimators for non-smooth density functions. The density estimation problem appears in various situations, and it is intensively used in statistics and machine learning. The statistical efficiencies of estimators, i.e., their convergence rates, play a central role in advanced statistical analysis. Although estimators and their convergence rates for smooth density functions are well investigated in the literature, those for non-smooth density functions remain elusive despite their importance in application fields. In this paper, we propose new estimators for non-smooth density functions by employing the notion of Szemeredi partitions from graph theory. We derive convergence rates of the proposed estimators. One of them has the optimal convergence rate in minimax sense, and the other has slightly worse convergence rate but runs in polynomial time. Experimental results support the theoretical performance of our estimators.",,,,,,"Maehara, Takanori/0000-0002-2101-1484",,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300103,0
C,"Izmailov, PA; Novikov, AV; Kropotov, DA",,"Storkey, A; PerezCruz, F",,"Izmailov, Pavel A.; Novikov, Alexander V.; Kropotov, Dmitry A.",,,Scalable Gaussian Processes with Billions of Inducing Inputs via Tensor Train Decomposition,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"We propose a method (TT-GP) for approximate inference in Gaussian Process (GP) models. We build on previous scalable GP research including stochastic variational inference based on inducing inputs, kernel interpolation, and structure exploiting algebra. The key idea of our method is to use Tensor Train decomposition for variational parameters, which allows us to train GPs with billions of inducing inputs and achieve state-of-the-art results on several benchmarks. Further, our approach allows for training kernels based on deep neural networks without any modifications to the underlying GP model. A neural network learns a multidimensional embedding for the data, which is used by the GP to make the final prediction. We train GP and neural network parameters end-to-end without pretraining, through maximization of GP marginal likelihood. We show the efficiency of the proposed approach on several regression and classification benchmark datasets including MNIST, CIFAR-10, and Airline.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300077,0
C,"Ma, YZ; Nowak, R; Rigollet, P; Zhang, XZ; Zhu, XJ",,"Storkey, A; PerezCruz, F",,"Ma, Yuzhe; Nowak, Robert; Rigollet, Philippe; Zhang, Xuezhou; Zhu, Xiaojin",,,Teacher Improves Learning by Selecting a Training Subset,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"We call a learner super-teachable if a teacher can trim down an iid training set while making the learner learn even better. We provide sharp super-teaching guarantees on two learners: the maximum likelihood estimator for the mean of a Gaussian, and the large margin classifier in 1D. For general learners, we provide a mixed-integer nonlinear programming-based algorithm to find a super teaching set. Empirical experiments show that our algorithm is able to find good super-teaching sets for both regression and classification problems.",,,,,"Ma, Yuzhe/CAF-7203-2022; Zhang, Xuezhou/ABD-8993-2021",,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300143,0
C,"Shvartsman, M; Sundaram, N; Aoi, M; Charles, A; Willke, TL; Cohen, JD",,"Storkey, A; PerezCruz, F",,"Shvartsman, Michael; Sundaram, Narayanan; Aoi, Mikio; Charles, Adam; Willke, Theodore L.; Cohen, Jonathan D.",,,Matrix-normal models for fMRI analysis,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Multivariate analysis of fMRI data has benefited substantially from advances in machine learning. Most recently, a range of probabilistic latent variable models applied to fMRI data have been successful in a variety of tasks, including identifying similarity patterns in neural data, combining multi-subject datasets, and mapping between brain and behavior. Although these methods share some underpinnings, they have been developed as distinct methods, with distinct algorithms and software tools. We show how the matrix-variate normal (MN) formalism can unify some of these methods into a single framework. In doing so, we gain the ability to reuse noise modeling assumptions, algorithms, and code across models. Our primary theoretical contribution shows how some of these methods can be written as instantiations of the same model, allowing us to generalize them to flexibly modeling structured residual covariances. Our formalism permits novel model variants and improved estimation strategies for SRM and RSA using substantially fewer parameters. We empirically demonstrate advantages of our two new methods: for MN-RSA, we show up to 10x improvement in run-time, up to 6x improvement in RMSE, and more conservative behavior under the null. For MN-SRM, our method grants a modest improvement to out-of-sample reconstruction while relaxing the orthonormality constraint of SRM. We also provide a software prototyping tool for MN models that can flexibly reuse residual covariance assumptions and algorithms across models.",,,,,,"Charles, Adam/0000-0002-9045-3489",,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300200,0
C,"Wang, BL; Sekhon, A; Qi, YJ",,"Storkey, A; PerezCruz, F",,"Wang, Beilun; Sekhon, Arshdeep; Qi, Yanjun",,,Fast and Scalable Learning of Sparse Changes in High-Dimensional Gaussian Graphical Model Structure,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"We focus on the problem of estimating the change in the dependency structures of two p-dimensional Gaussian Graphical models (GGMs). Previous studies for sparse change estimation in GGMs involve expensive and difficult non-smooth optimization. We propose a novel method, DIFFEE for estimating DIFFerential networks via an Elementary Estimator under a high-dimensional situation. DIFFEE is solved through a faster and closed form solution that enables it to work in large-scale settings. We conduct a rigorous statistical analysis showing that surprisingly DIFFEE achieves the same asymptotic convergence rates as the state-of-the-art estimators that are much more difficult to compute. Our experimental results on multiple synthetic datasets and one real-world data about brain connectivity show strong performance improvements over baselines, as well as significant computational benefits.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300177,0
C,"Ye, YT; Ju, C; Lei, LH",,"Storkey, A; PerezCruz, F",,"Ye, Yuting; Ju, Cheng; Lei, Lihua",,,HONES: A Fast and Tuning-free Homotopy Method For Online Newton Step,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"In this article, we develop and analyze a homotopy continuation method, referred to as HONES, for solving the sequential generalized projections in Online Newton Step (Hazan et al., 2006b), as well as the generalized problem known as sequential standard quadratic programming. HONES is fast, tuning-free, error-free (up to machine error) and adaptive to the solution sparsity. This is confirmed by both careful theoretical analysis and extensive experiments on both synthetic and real data.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300210,0
C,"Feraud, R; Allesiardo, R; Urvoy, T; Clerot, F",,"Gretton, A; Robert, CC",,"Feraud, Raphael; Allesiardo, Robin; Urvoy, Tanguy; Clerot, Fabrice",,,Random Forest for the Contextual Bandit Problem,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"To address the contextual bandit problem, we propose an online random forest algorithm. The analysis of the proposed algorithm is based on the sample complexity needed to find the optimal decision stump. Then, the decision stumps are recursively stacked in a random collection of decision trees, BANDIT FOREST. We show that the proposed algorithm is optimal up to logarithmic factors. The dependence of the sample complexity upon the number of contextual variables is logarithmic. The computational cost of the proposed algorithm with respect to the time horizon is linear. These analytical results allow the proposed algorithm to be efficient in real applications, where the number of events to process is huge, and where we expect that some contextual variables, chosen from a large set, have potentially non-linear dependencies with the rewards. In the experiments done to illustrate the theoretical analysis, BANDIT FOREST obtain promising results in comparison with state-of-the-art algorithms.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,93,101,,,,,,,,,,,,,,,,WOS:000508662100011,0
C,"Jamieson, K; Talwalkar, A",,"Gretton, A; Robert, CC",,"Jamieson, Kevin; Talwalkar, Ameet",,,Non-stochastic Best Arm Identification and Hyperparameter Optimization,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Motivated by the task of hyperparameter optimization, we introduce the non-stochastic best-arm identification problem. We identify an attractive algorithm for this setting that makes no assumptions on the convergence behavior of the arms' losses, has no free-parameters to adjust, provably outperforms the uniform allocation baseline in favorable conditions, and performs comparably (up to log factors) otherwise. Next, by leveraging the iterative nature of many learning algorithms, we cast hyperparameter optimization as an instance of non-stochastic best-arm identification. Our empirical results show that, by allocating more resources to promising hyperparameter settings, our approach achieves comparable test accuracies an order of magnitude faster than the uniform strategy. The robustness and simplicity of our approach makes it well-suited to ultimately replace the uniform strategy currently used in most machine learning software packages.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,240,248,,,,,,,,,,,,,,,,WOS:000508662100027,0
C,"Nishihara, R; Lopez-Paz, D; Bottou, L",,"Gretton, A; Robert, CC",,"Nishihara, Robert; Lopez-Paz, David; Bottou, Leon",,,No Regret Bound for Extreme Bandits,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Algorithms for hyperparameter optimization abound, all of which work well under different and often unverifiable assumptions. Motivated by the general challenge of sequentially choosing which algorithm to use, we study the more specific task of choosing among distributions to use for random hyperparameter optimization. This work is naturally framed in the extreme bandit setting, which deals with sequentially choosing which distribution from a collection to sample in order to minimize (maximize) the single best cost (reward). Whereas the distributions in the standard bandit setting are primarily characterized by their means, a number of subtleties arise when we care about the minimal cost as opposed to the average cost. For example, there may not be a well-defined best distribution as there is in the standard bandit setting. The best distribution depends on the rewards that have been obtained and on the remaining time horizon. Whereas in the standard bandit setting, it is sensible to compare policies with an oracle which plays the single best arm, in the extreme bandit setting, there are multiple sensible oracle models. We define a sensible notion of extreme regret in the extreme bandit setting, which parallels the concept of regret in the standard bandit setting. We then prove that no policy can asymptotically achieve no extreme regret.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,259,267,,,,,,,,,,,,,,,,WOS:000508662100029,0
C,"Tang, K; Ruozzi, N; Belanger, D; Jebara, T",,"Gretton, A; Robert, CC",,"Tang, Kui; Ruozzi, Nicholas; Belanger, David; Jebara, Tony",,,Bethe Learning of Graphical Models via MAP Decoding,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Many machine learning tasks require fitting probabilistic models over structured objects, such as pixel grids, matchings, and graph edges. Maximum likelihood estimation (MLE) for such domains is challenging due to the intractability of computing partition functions. One can resort to approximate marginal inference in conjunction with gradient descent, but such algorithms require careful tuning. Alternatively, in frameworks such as the structured support vector machine (SVM-Struct), discriminative functions are learned by iteratively applying efficient maximum a posteriori (MAP) decoders. We introduce MLE-Struct, a method for learning discrete exponential family models using the Bethe approximation to the partition function. Remarkably, this problem can also be reduced to iterative (MAP) decoding. This connection emerges by combining the Bethe approximation with the Frank-Wolfe (FW) algorithm on a convex dual objective, which circumvents the intractable partition function. Our method can learn both generative and conditional models and is substantially faster and easier to implement than existing MLE approaches while still relying on the same black-box interface to MAP decoding as SVM-Struct. We perform competitively on problems in denoising, segmentation, matching, and new datasets of roommate assignments and news and financial time series.",,,,,,"Ruozzi, Nicholas/0000-0002-4262-2698",,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,1096,1104,,,,,,,,,,,,,,,,WOS:000508662100119,0
C,"Mladenov, M; Globerson, A; Kersting, K",,"Kaski, S; Corander, J",,"Mladenov, Martin; Globerson, Amir; Kersting, Kristian",,,Efficient Lifting of MAP LP Relaxations Using k-Locality,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"Inference in large scale graphical models is an important task in many domains, and in particular for probabilistic relational models (e.g,. Markov logic networks). Such models often exhibit considerable symmetry, and it is a challenge to devise algorithms that exploit this symmetry to speed up inference. Here we address this task in the context of the MAP inference problem and its linear programming relaxations. We show that symmetry in these problems can be discovered using an elegant algorithm known as the k-dimensional Weisfeiler-Lehman (k-WL) algorithm. We run k-WL on the original graphical model, and not on the far larger graph of the linear program (LP) as proposed in earlier work in the field. Furthermore, the algorithm is polynomial and thus far more practical than other previous approaches which rely on orbit partitions that are GI complete to find. The fact that k-WL can be used in this manner follows from the recently introduced notion of k-local LPs and their relation to Sherali Adams relaxations of graph automorphisms. Finally, for relational models such as Markov logic networks, the benefits of our approach are even more dramatic, as we can discover symmetries in the original domain graph, as opposed to running lifting on the much larger grounded model.",,,,,,"Globerson, Amir/0000-0003-2557-1742",,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,623,632,,,,,,,,,,,,,,,,WOS:000508355800069,0
C,"Ranganath, R; Gerrish, S; Blei, DM",,"Kaski, S; Corander, J",,"Ranganath, Rajesh; Gerrish, Sean; Blei, David M.",,,Black Box Variational Inference,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"Variational inference has become a widely used method to approximate posteriors in complex latent variables models. However, deriving a variational inference algorithm generally requires significant model-specific analysis. These efforts can hinder and deter us from quickly developing and exploring a variety of models for a problem at hand. In this paper, we present a black box variational inference algorithm, one that can be quickly applied to many models with little additional derivation. Our method is based on a stochastic optimization of the variational objective where the noisy gradient is computed from Monte Carlo samples from the variational distribution. We develop a number of methods to reduce the variance of the gradient, always maintaining the criterion that we want to avoid difficult model-based derivations. We evaluate our method against the corresponding black box sampling based methods. We find that our method reaches better predictive likelihoods much faster than sampling methods. Finally, we demonstrate that Black Box Variational Inference lets us easily explore a wide space of models by quickly constructing and evaluating several models of longitudinal healthcare data.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,814,822,,,,,,,,,,,,,,,,WOS:000508355800090,0
C,"Zhou, JTY; Tsang, IW; Pang, SJL; Tan, MK",,"Kaski, S; Corander, J",,"Zhou, Joey Tianyi; Tsang, Ivor W.; Pang, Sinno Jialin; Tan, Mingkui",,,Heterogeneous Domain Adaptation for Multiple Classes,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"In this paper, we present an efficient multi-class heterogeneous domain adaptation method, where data from source and target domains are represented by heterogeneous features of different dimensions. Specifically, we propose to reconstruct a sparse feature transformation matrix to map the weight vector of classifiers learned from the source domain to the target domain. We cast this learning task as a compressed sensing problem, where each binary classifier induced from multiple classes can be deemed as a measurement sensor. Based on the compressive sensing theory, the estimation error of the transformation matrix decreases with the increasing number of classifiers. Therefore, to guarantee reconstruction performance, we construct sufficiently many binary classifiers based on the error correcting output coding. Extensive experiments are conducted on both a toy dataset and three real-world datasets to verify the superiority of our proposed method over existing state-of-the-art HDA methods in terms of prediction accuracy.",,,,,"PAN, Sinno Jialin/P-6696-2014","PAN, Sinno Jialin/0000-0001-6565-3836; Tsang, Ivor/0000-0001-8095-4637",,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,1095,1103,,,,,,,,,,,,,,,,WOS:000508355800121,0
C,"Achterhold, J; Stueckler, J",,"Banerjee, A; Fukumizu, K",,"Achterhold, Jan; Stueckler, Joerg",,,Explore the Context: Optimal Data Collection for Context-Conditional Dynamics Models,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"In this paper, we learn dynamics models for parametrized families of dynamical systems with varying properties. The dynamics models are formulated as stochastic processes conditioned on a latent context variable which is inferred from observed transitions of the respective system. The probabilistic formulation allows us to compute an action sequence which, for a limited number of environment interactions, optimally explores the given system within the parametrized family. This is achieved by steering the system through transitions being most informative for the context variable. We demonstrate the effectiveness of our method for exploration on a non-linear toyproblem and two well-known reinforcement learning environments.",,,,,"Stckler, Jrg/AAK-9145-2021","Stckler, Jrg/0000-0002-2328-4363",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804027,0
C,"Alimisis, F; Orvieto, A; Becigneul, G; Lucchi, A",,"Banerjee, A; Fukumizu, K",,"Alimisis, Foivos; Orvieto, Antonio; Becigneul, Gary; Lucchi, Aurelien",,,Momentum Improves Optimization on Riemannian Manifolds,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We develop a new Riemannian descent algorithm that relies on momentum to improve over existing first-order methods for geodesically convex optimization. In contrast, accelerated convergence rates proved in prior work have only been shown to hold for geodesically strongly-convex objective functions. We further extend our algorithm to geodesically weakly-quasi-convex objectives. Our proofs of convergence rely on a novel estimate sequence that illustrates the dependency of the convergence rate on the curvature of the manifold. We validate our theoretical results empirically on several optimization problems defined on the sphere and on the manifold of positive definite matrices.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801064,0
C,"Arias-Castro, E; Jiang, H",,"Banerjee, A; Fukumizu, K",,"Arias-Castro, Ery; Jiang, He",,,On the Consistency of Metric and Non-Metric K-medoids,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We establish the consistency of K-medoids in the context of metric spaces. We start by proving that K-medoids is asymptotically equivalent to K-means restricted to the support of the underlying distribution under general conditions, including a wide selection of loss functions. This asymptotic equivalence, in turn, enables us to apply the work of Parna (1986) on the consistency of K-means. This general approach applies also to non-metric settings where only an ordering of the dissimilarities is available. We consider two types of ordinal information: one where all quadruple comparisons are available; and one where only triple comparisons are available. We provide some numerical experiments to illustrate our theory.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803003,0
C,"Haussmann, M; Gerwinn, S; Look, A; Rakitsch, B; Kandemir, M",,"Banerjee, A; Fukumizu, K",,"Haussmann, Manuel; Gerwinn, Sebastian; Look, Andreas; Rakitsch, Barbara; Kandemir, Melih",,,Learning Partially Known Stochastic Dynamics with Empirical PAC Bayes,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Neural Stochastic Differential Equations model a dynamical environment with neural nets assigned to their drift and diffusion terms. The high expressive power of their nonlinearity comes at the expense of instability in the identification of the large set of free parameters. This paper presents a recipe to improve the prediction accuracy of such models in three steps: i) accounting for epistemic uncertainty by assuming probabilistic weights, ii) incorporation of partial knowledge on the state dynamics, and iii) training the resultant hybrid model by an objective derived from a PAC-Bayesian generalization bound. We observe in our experiments that this recipe effectively translates partial and noisy prior knowledge into an improved model fit.",,,,,,"Kandemir, Melih/0000-0001-6293-3656",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,478,+,,,,,,,,,,,,,,,,WOS:000659893800054,0
C,"Mehta, N; Liang, KJ; Verma, VK; Carin, L",,"Banerjee, A; Fukumizu, K",,"Mehta, Nikhil; Liang, Kevin J.; Verma, Vinay K.; Carin, Lawrence",,,Continual Learning using a Bayesian Nonparametric Dictionary of Weight Factors,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Naively trained neural networks tend to experience catastrophic forgetting in sequential task settings, where data from previous tasks are unavailable. A number of methods, using various model expansion strategies, have been proposed recently as possible solutions. However, determining how much to expand the model is left to the practitioner, and often a constant schedule is chosen for simplicity, regardless of how complex the incoming task is. Instead, we propose a principled Bayesian nonparametric approach based on the Indian Buffet Process (IBP) prior, letting the data determine how much to expand the model complexity. We pair this with a factorization of the neural network's weight matrices. Such an approach allows the number of factors of each weight matrix to scale with the complexity of the task, while the IBP prior encourages sparse weight factor selection and factor reuse, promoting positive knowledge transfer between tasks. We demonstrate the effectiveness of our method on a number of continual learning benchmarks and analyze how weight factors are allocated and reused throughout the training.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,100,+,,,,,,,,,,,,,,,,WOS:000659893800012,0
C,"Qian, JA; Wu, YR; Zhuang, BJ; Wang, SJ; Xiao, J",,"Banerjee, A; Fukumizu, K",,"Qian, Jiang; Wu, Yuren; Zhuang, Bojin; Wang, Shaojun; Xiao, Jing",,,Understanding Gradient Clipping In Incremental Gradient Methods,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We provide a theoretical analysis on how gradient clipping affects the convergence of the incremental gradient methods on minimizing an objective function that is the sum of a large number of component functions. We show that clipping on gradients of component functions leads to bias on the descent direction, which is affected by the clipping threshold, the norms of gradients of component functions, together with the angles between gradients of component functions and the full gradient. We then propose some sufficient conditions under which the increment gradient methods with gradient clipping can be shown to be convergent under the more general relaxed smoothness assumption. We also empirically observe that the angles between gradients of component functions and the full gradient generally decrease as the batchsize increases, which may help to explain why larger batchsizes generally lead to faster convergence in training deep neural networks with gradient clipping.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801081,0
C,"Shekelyan, M; Cormode, G",,"Banerjee, A; Fukumizu, K",,"Shekelyan, Michael; Cormode, Graham",,,Sequential Random Sampling Revisited: Hidden Shuffle Method,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Random sampling (without replacement) is ubiquitously employed to obtain a representative subset of the data. Unlike common methods, sequential methods report samples in ascending order of index without keeping track of previous samples. This enables lightweight iterators that can jump directly from one sampled position to the next. Previously, sequential methods focused on drawing from the distribution of gap sizes, which requires intricate algorithms that are difficult to validate and can be slow in the worst-case. This can be avoided by a new method, the Hidden Shuffle. The name mirrors the fact that although the algorithm does not resemble shuffliing, its correctness can be proven by conceptualising the sampling process as a random shuffle. The Hidden Shuffle algorithm stores just a handful of values, can be implemented in few lines of code, offers strong worst-case guarantees and is shown to be faster than state-of-the-art methods while using comparably few random variates.",,,,,"Shekelyan, Michael/ADM-7191-2022","Shekelyan, Michael/0000-0002-6500-2192",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804038,0
C,"Sinha, D; Sankararama, KA; Kazerouni, A; Avadhanula, V",,"Banerjee, A; Fukumizu, K",,"Sinha, Deeksha; Sankararama, Karthik Abinav; Kazerouni, Abbas; Avadhanula, Vashist",,,Multi-Armed Bandits with Cost Subsidy,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"In this paper, we consider a novel variant of the multi-armed bandit (MAB) problem, MAB with cost subsidy, which models many real-life applications where the learning agent has to pay to select an arm and is concerned about optimizing cumulative costs and rewards. We present two applications, intelligent SMS routing problem and ad audience optimization problem faced by several businesses (especially online platforms), and show how our problem uniquely captures key features of these applications. We show that naive generalizations of existing MAB algorithms like Upper Confidence Bound and Thompson Sampling do not perform well for this problem. We then establish a fundamental lower bound on the performance of any online learning algorithm for this problem, highlighting the hardness of our problem in comparison to the classical MAB problem. We also present a simple variant of explore-then-commit and establish near-optimal regret bounds for this algorithm. Lastly, we perform extensive numerical simulations to understand the behavior of a suite of algorithms for various instances and recommend a practical guide to employ different algorithms.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803062,0
C,"Stich, SU; Mohtashami, A; Jaggi, M",,"Banerjee, A; Fukumizu, K",,"Stich, Sebastian U.; Mohtashami, Amirkeivan; Jaggi, Martin",,,Critical Parameters for Scalable Distributed Learning with Large Batches and Asynchronous Updates,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"It has been experimentally observed that the efficiency of distributed training with stochastic gradient (SGD) depends decisively on the batch size and-in asynchronous implementations-on the gradient staleness. Especially, it has been observed that the speedup saturates beyond a certain batch size and/or when the delays grow too large. We identify a data-dependent parameter that explains the speedup saturation in both these settings. Our comprehensive theoretical analysis, for strongly convex, convex and non-convex settings, unifies and generalized prior work directions that often focused on only one of these two aspects. In particular, our approach allows us to derive improved speedup results under frequently considered sparsity assumptions. Our insights give rise to theoretically based guidelines on how the learning rates can be adjusted in practice. We show that our results are tight and illustrate key findings in numerical experiments.",,,,,,"Jaggi, Martin/0000-0003-1579-5558",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804083,0
C,"Vandegar, M; Kagan, M; Wehenkel, A; Louppe, G",,"Banerjee, A; Fukumizu, K",,"Vandegar, Maxime; Kagan, Michael; Wehenkel, Antoine; Louppe, Gilles",,,Neural Empirical Bayes: Source Distribution Estimation and its Applications to Simulation-Based Inference,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We revisit empirical Bayes in the absence of a tractable likelihood function, as is typical in scientific domains relying on computer simulations. We investigate how the empirical Bayesian can make use of neural density estimators first to use all noise-corrupted observations to estimate a prior or source distribution over uncorrupted samples, and then to perform single-observation posterior inference using the fitted source distribution. We propose an approach based on the direct maximization of the log-marginal likelihood of the observations, examining both biased and de-biased estimators, and comparing to variational approaches. We find that, up to symmetries, a neural empirical Bayes approach recovers ground truth source distributions. With the learned source distribution in hand, we show the applicability to likelihood-free inference and examine the quality of the resulting posterior estimates. Finally, we demonstrate the applicability of Neural Empirical Bayes on an inverse problem from collider physics.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802054,0
C,"Yuan, XT; Li, P",,"Banerjee, A; Fukumizu, K",,"Yuan, Xiaotong; Li, Ping",,,Stability and Risk Bounds of Iterative Hard Thresholding,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"The Iterative Hard Thresholding (IHT) algorithm is one of the most popular and promising greedy pursuit methods for high-dimensional statistical estimation under cardinality constraint. The existing analysis of IHT mostly focuses on parameter estimation and sparsity recovery consistency. From the perspective of statistical learning theory, another fundamental question is how well the IHT estimation would perform on unseen samples. The answer to this question is important for understanding the generalization ability of IHT yet has remaind elusive. In this paper, we investigate this problem and develop a novel generalization theory for IHT from the viewpoint of algorithmic stability. Our theory reveals that: 1) under natural conditions on the empirical risk function over n samples of dimension p, IHT with sparsity level k enjoys an (O) over tilde (n(-1/2)root k log(n) log(p)) rate of convergence in sparse excess risk; and 2) a fast rate of order (O) over tilde (n(-1) k(log(3) (n) + log(p))) can be derived for strongly convex risk function under certain strong-signal conditions. The results have been substantialized to sparse linear regression and logistic regression models along with numerical evidence provided to support our theory.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802009,0
C,"Zhang, C; Cinelli, C; Chen, B; Pearl, J",,"Banerjee, A; Fukumizu, K",,"Zhang, Chi; Cinelli, Carlos; Chen, Bryant; Pearl, Judea",,,Exploiting equality constraints in causal inference,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Assumptions about equality of effects are commonly made in causal inference tasks. For example, the well-known differencein-differences method assumes that confounding remains constant across time periods. Similarly, it is not unreasonable to assume that causal effects apply equally to units undergoing interference. Finally, sensitivity analysis often hypothesizes equality among existing and unaccounted for confounders. Despite the ubiquity of these equality constraints, modern identification methods have not leveraged their presence in a systematic way. In this paper, we develop a novel graphical criterion that extends the well-known method of generalized instrumental sets to exploit such additional constraints for causal identification in linear models. We further demonstrate how it solves many diverse problems found in the literature in a general way, including differencein-differences, interference, as well as benchmarking in sensitivity analysis.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893802001,0
C,"Adhikary, S; Srinivasan, S; Gordon, G; Boots, B",,"Chiappa, S; Calandra, R",,"Adhikary, Sandesh; Srinivasan, Siddarth; Gordon, Geoff; Boots, Byron",,,Expressiveness and Learning of Hidden Quantum Markov Models,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Extending classical probabilistic reasoning using the quantum mechanical view of probability has been of recent interest, particularly in the development of hidden quantum Markov models (HQMMs) to model stochastic processes. However, there has been little progress in characterizing the expressiveness of such models and learning them from data. We tackle these problems by showing that HQMMs are a special subclass of the general class of observable operator models (OOMs) that do not suffer from the negative probability problem by design. We also provide a feasible retraction-based learning algorithm for HQMMs using constrained gradient descent on the Stiefel manifold of model parameters. We demonstrate that this approach is faster and scales to larger models than previous learning algorithms.",,,,,"s, s/HIK-1178-2022",,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,4151,4160,,,,,,,,,,,,,,,,WOS:000559931300006,0
C,"Fan, XH; Li, B; Sisson, SA",,"Chiappa, S; Calandra, R",,"Fan, Xuhui; Li, Bin; Sisson, Scott A.",,,Online Binary Space Partitioning Forests,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"The Binary Space Partitioning-Tree (BSP-Tree) process was recently proposed as an efficient strategy for space partitioning tasks. Because it uses more than one dimension to partition the space, the BSP-Tree Process is more efficient and flexible than conventional axis-aligned cutting strategies. However, due to its batch learning setting, it is not well suited to large-scale classification and regression problems. In this paper, we develop an online B SP-Forest framework to address this limitation. With the arrival of new data, the resulting online algorithm can simultaneously expand the space coverage and refine the partition structure, with guaranteed universal consistency for both classification and regression problems. The effectiveness and competitive performance of the online BSP-Forest is verified via simulations on real-world datasets.",,,,,"Fan, Xu/GSE-2196-2022",,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,527,536,,,,,,,,,,,,,,,,WOS:000559931300088,0
C,"Fatras, K; Zine, Y; Flamary, R; Gribonval, R; Courty, N",,"Chiappa, S; Calandra, R",,"Fatras, Kilian; Zine, Younes; Flamary, Remi; Gribonval, Remi; Courty, Nicolas",,,Learning with minibatch Wasserstein : asymptotic and gradient properties,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Optimal transport distances are powerful tools to compare probability distributions and have found many applications in machine learning. Yet their algorithmic complexity prevents their direct use on large scale datasets. To overcome this challenge, practitioners compute these distances on minibatches i.e. they average the outcome of several smaller optimal transport problems. We propose in this paper an analysis of this practice, which effects are not well understood so far. We notably argue that it is equivalent to an implicit regularization of the original problem, with appealing properties such as unbiased estimators, gradients and a concentration bound around the expectation, but also with defects such as loss of distance property. Along with this theoretical analysis, we also conduct empirical experiments on gradient flows, GANs or color transfer that highlight the practical interest of this strategy.",,,,,"Flamary, Rmi/AAC-1958-2022","Flamary, Rmi/0000-0002-4212-6627",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2131,2140,,,,,,,,,,,,,,,,WOS:000559931300093,0
C,"Garber, D; Kretzu, B",,"Chiappa, S; Calandra, R",,"Garber, Dan; Kretzu, Ben",,,Improved Regret Bounds for Projection-free Bandit Convex Optimization,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We revisit the challenge of designing online algorithms for the bandit convex optimization problem (BCO) which are also scalable to high dimensional problems. Hence, we consider algorithms that are projection-free, i.e., based on the conditional gradient method whose only access to the feasible decision set is through a linear optimization oracle (as opposed to other methods which require potentially much more computationally-expensive subprocedures, such as computing Euclidean projections). We present the first such algorithm that attains O (T-3/4) expected regret using only O(T) overall calls to the linear optimization oracle, in expectation, where T is the number of prediction rounds. This improves over the O (T-4/5) expected regret bound recently obtained by Chen et al. (2019), and actually matches the current best regret bound for projection-free online learning in the full information setting.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2196,2205,,,,,,,,,,,,,,,,WOS:000559931301013,0
C,"Kim, C; Klabjan, D",,"Chiappa, S; Calandra, R",,"Kim, Cheolmin; Klabjan, Diego",,,Stochastic Variance-Reduced Algorithms for PCA with Arbitrary Mini-Batch Sizes,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We present two stochastic variance-reduced PCA algorithms and their convergence analyses. By deriving explicit forms of step size, epoch length and batch size to ensure the optimal runtime, we show that the proposed algorithms can attain the optimal runtime with any batch sizes. Also, we establish global convergence of the algorithms based on a novel approach, which studies the optimality gap as a ratio of two expectation terms. The framework in our analysis is general and can be used to analyze other stochastic variance-reduced PCA algorithms and improve their analyses. Moreover, we introduce practical implementations of the algorithms which do not require hyper-parameters. The experimental results show that the proposed methodsd outperform other stochastic variance-reduced PCA algorithms regardless of the batch size.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,4302,4311,,,,,,,,,,,,,,,,WOS:000559931301076,0
C,"Kumar, A; Poole, B; Murphy, K",,"Chiappa, S; Calandra, R",,"Kumar, Abhishek; Poole, Ben; Murphy, Kevin",,,Regularized Autoencoders via Relaxed Injective Probability Flow,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Invertible flow-based generative models are an effective method for learning to generate samples, while allowing for tractable likelihood computation and inference. However, the invertibility requirement restricts models to have the same latent dimensionality as the inputs. This imposes significant architectural, memory, and computational costs, making them more challenging to scale than other classes of generative models such as Variational Autoencoders (VAEs). We propose a generative model based on probability flows that does away with the bijectivity requirement on the model and only assumes injectivity. This also provides another perspective on regularized autoencoders (RAEs), with our final objectives resembling RAEs with specific regularizers that are derived by lower bounding the probability flow objective. We empirically demonstrate the promise of the proposed model, improving over VAEs and AEs in terms of sample quality.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,4292,4300,,,,,,,,,,,,,,,,WOS:000559931301088,0
C,"Lee, JN; Pacchiano, A; Jordan, MI",,"Chiappa, S; Calandra, R",,"Lee, Jonathan N.; Pacchiano, Aldo; Jordan, Michael I.",,,Convergence Rates of Smooth Message Passing with Rounding in Entropy-Regularized MAP Inference,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Maximum a posteriori (MAP) inference is a fundamental computational paradigm for statistical inference. In the setting of graphical models, MAP inference entails solving a combinatorial optimization problem to find the most likely configuration of the discrete-valued model. Linear programming (LP) relaxations in the Sherali-Adams hierarchy are widely used to attempt to solve this problem, and smooth message passing algorithms have been proposed to solve regularized versions of these LPs with great success. This paper leverages recent work in entropy-regularized LPs to analyze convergence rates of a class of edge-based smooth message passing algorithms to epsilon-optimality in the relaxation. With an appropriately chosen regularization constant, we present a theoretical guarantee on the number of iterations sufficient to recover the true integral MAP solution when the LP is tight and the solution is unique.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,3003,3013,,,,,,,,,,,,,,,,WOS:000559931301096,0
C,"Magnusson, M; Andersen, MR; Jonasson, J; Vehtari, A",,"Chiappa, S; Calandra, R",,"Magnusson, Mans; Andersen, Michael Riis; Jonasson, Johan; Vehtari, Aki",,,Leave-One-Out Cross-Validation for Bayesian Model Comparison in Large Data,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Recently, new methods for model assessment, based on subsampling and posterior approximations, have been proposed for scaling leave-one-out cross-validation (LOO) to large datasets. Although these methods work well for estimating predictive performance for individual models, they are less powerful in model comparison. We propose an efficient method for estimating differences in predictive performance by combining fast approximate LOO surrogates with exact LOO subsampling using the difference estimator and supply proofs with regards to scaling characteristics. The resulting approach can be orders of magnitude more efficient than previous approaches, as well as being better suited to model comparison.",,,,,,"Vehtari, Aki/0000-0003-2164-9469; Andersen, Michael/0000-0002-7411-5842",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,341,350,,,,,,,,,,,,,,,,WOS:000559931302027,0
C,"Neglia, G; Xu, C; Towsley, D; Calbi, G",,"Chiappa, S; Calandra, R",,"Neglia, Giovanni; Xu, Chuan; Towsley, Don; Calbi, Gianmarco",,,Decentralized gradient methods: does topology matter?,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Consensus-based distributed optimization methods have recently been advocated as alternatives to parameter server and ring all-reduce paradigms for large scale training of machine learning models. In this case, each worker maintains a local estimate of the optimal parameter vector and iteratively updates it by averaging the estimates obtained from its neighbors, and applying a correction on the basis of its local dataset. While theoretical results suggest that worker communication topology should have strong impact on the number of epochs needed to converge, previous experiments have shown the opposite conclusion. This paper sheds lights on this apparent contradiction and show how sparse topologies can lead to faster convergence even in the absence of communication delays.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2348,2357,,,,,,,,,,,,,,,,WOS:000559931302054,0
C,"Pedregosa, F; Negiar, G; Askari, A; Jaggi, M",,"Chiappa, S; Calandra, R",,"Pedregosa, Fabian; Negiar, Geoffrey; Askari, Armin; Jaggi, Martin",,,Linearly Convergent Frank-Wolfe with Backtracking Line-Search,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Structured constraints in Machine Learning have recently brought the Frank-Wolfe (FW) family of algorithms back into the spotlight. While the classical FW algorithm has poor local convergence properties, Away-steps FW and Pairwise FW have emerged as improved variants with faster convergence. However, these improved variants suffer from two practical limitations: they require at each iteration to solve a 1-dimensional minimization problem to set the step-size and also require the Frank-Wolfe linear subproblems to be solved exactly. In this paper, we propose variants of Away-steps and Pairwise FW that lift both restrictions simultaneously. The proposed methods set the step-size based on a sufficient decrease condition, and do not require prior knowledge of the objective. Furthermore, they inherit all the favorable convergence properties of the exact line-search version, including linear convergence for strongly convex functions over polytopes. Benchmarks on different machine learning problems illustrate large performance gains of the proposed variants.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1,9,,,,,,,,,,,,,,,,WOS:000559931302069,0
C,"Prasad, A; Balakrishnan, S; Ravikumar, P",,"Chiappa, S; Calandra, R",,"Prasad, Adarsh; Balakrishnan, Sivaraman; Ravikumar, Pradeep",,,A Robust Univariate Mean Estimator is All You Need,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We study the problem of designing estimators when the data has heavy-tails and is corrupted by outliers. In such an adversarial setup, we aim to design statistically optimal estimators for flexible non-parametric distribution classes such as distributions with bounded-2k moments and symmetric distributions. Our primary workhorse is a conceptually simple reduction from multivariate estimation to univariate estimation. Using this reduction, we design estimators which are optimal in both heavy-tailed and contaminated settings. Our estimators achieve an optimal dimension independent bias in the contaminated setting, while also simultaneously achieving high-probability error guarantees with optimal sample complexity. These results provide some of the first such estimators for a broad range of problems including Mean Estimation, Sparse Mean Estimation, Covariance Estimation, Sparse Covariance Estimation and Sparse PCA.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,4034,4043,,,,,,,,,,,,,,,,WOS:000559931302081,0
C,"Sadeghi, O; Fazel, M",,"Chiappa, S; Calandra, R",,"Sadeghi, Omid; Fazel, Maryam",,,Online Continuous DR-Submodular Maximization with Long-Term Budget Constraints,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"In this paper, we study a class of online optimization problems with long-term budget constraints where the objective functions are not necessarily concave (nor convex), but they instead satisfy the Diminishing Returns (DR) property. In this online setting, a sequence of monotone DR-submodular objective functions and linear budget functions arrive over time and assuming a limited total budget, the goal is to take actions at each time, before observing the utility and budget function arriving at that round, to achieve sub-linear regret bound while the total budget violation is sub-linear as well. Prior work has shown that achieving sub-linear regret and total budget violation simultaneously is impossible if the utility and budget functions are chosen adversarially. Therefore, we modify the notion of regret by comparing the agent against the best fixed decision in hindsight which satisfies the budget constraint proportionally over any window of length W. We propose the Online Saddle Point Hybrid Gradient (OSPHG) algorithm to solve this class of online problems. For W = T, we recover the aforementioned impossibility result. However, if W is sub-linear in T, we show that it is possible to obtain sub-linear bounds for both the regret and the total budget violation.",,,,,,"Sadeghi, Omid/0000-0003-2813-1670",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,4410,4418,,,,,,,,,,,,,,,,WOS:000559931302099,0
C,"Soori, S; Mischenko, K; Mokhtari, A; Dehnavi, MM; Gurbuzbalaban, M",,"Chiappa, S; Calandra, R",,"Soori, Saeed; Mischenko, Konstantin; Mokhtari, Aryan; Dehnavi, Maryam Mehri; Gurbuzbalaban, Mert",,,DAve-QN: A Distributed Averaged Quasi-Newton Method with Local Superlinear Convergence Rate,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"In this paper, we consider distributed algorithms for solving the empirical risk minimization problem under the master/worker communication model. We develop a distributed asynchronous quasi-Newton algorithm that can achieve superlinear convergence. To our knowledge, this is the first distributed asynchronous algorithm with superlinear convergence guarantees. Our algorithm is communication-efficient in the sense that at every iteration the master node and workers communicate vectors of size O(p), where p is the dimension of the decision variable. The proposed method is based on a distributed asynchronous averaging scheme of decision vectors and gradients in a way to effectively capture the local Hessian information of the objective function. Our convergence theory supports asynchronous computations subject to both bounded delays and unbounded delays with a bounded time-average. Unlike in the majority of asynchronous optimization literature, we do not require choosing smaller stepsize when delays are huge. We provide numerical experiments that match our theoretical results and showcase significant improvement comparing to state-of-the-art distributed algorithms.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303027,0
C,"Vojnovic, M; Yun, SY; Zhou, K",,"Chiappa, S; Calandra, R",,"Vojnovic, Milan; Yun, Se-Young; Zhou, Kaifang",,,Convergence Rates of Gradient Descent and MM Algorithms for Bradley-Terry Models,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We present tight convergence rate bounds for gradient descent and MM algorithms for maximum likelihood (ML) estimation and maximum a posteriori probability (MAP) estimation of a popular Bayesian inference method, for Bradley-Terry models of ranking data. Our results show that MM algorithms have the same convergence rate, up to a constant factor, as gradient descent algorithms with optimal constant step size. For the ML estimation objective, the convergence is linear with the rate crucially determined by the algebraic connectivity of the matrix of item pair co-occurrences in observed comparison data. For the MAP estimation objective, we show that the convergence rate is also linear, with the rate determined by a parameter of the prior distribution in a way that can make convergence arbitrarily slow for small values of this parameter. The limit of small values of this parameter corresponds to a flat, non-informative prior distribution.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303058,0
C,"Wenger, J; Kjellstrom, H; Triebel, R",,"Chiappa, S; Calandra, R",,"Wenger, Jonathan; Kjellstroem, Hedvig; Triebel, Rudolph",,,Non-Parametric Calibration for Classification,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Many applications of classification methods not only require high accuracy but also reliable estimation of predictive uncertainty. However, while many current classification frameworks, in particular deep neural networks, achieve high accuracy, they tend to incorrectly estimate uncertainty. In this paper, we propose a method that adjusts the confidence estimates of a general classifier such that they approach the probability of classifying correctly. In contrast to existing approaches, our calibration method employs a non-parametric representation using a latent Gaussian process, and is specifically designed for multi-class classification. It can be applied to any classifier that outputs confidence estimates and is not limited to neural networks. We also provide a theoretical analysis regarding the over- and underconfidence of a classifier and its relationship to calibration, as well as an empirical outlook for calibrated active learning. In experiments we show the universally strong performance of our method across different classifiers and benchmark data sets, in particular for state-of-the art neural network architectures.",,,,,"Triebel, Rudolph/ABG-8692-2020","Triebel, Rudolph/0000-0002-7975-036X",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303078,0
C,"Yan, LF; Kleijn, WB; Abhayapala, TD",,"Chiappa, S; Calandra, R",,"Yan, Longfei; Kleijn, W. Bastiaan; Abhayapala, Thushara D.",,,A Linear-time Independence Criterion Based on a Finite Basis Approximation,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Detection of statistical dependence between random variables is an essential component in many machine learning algorithms. We propose a novel independence criterion for two random variables with linear-time complexity. We establish that our independence criterion is an upper bound of the Hirschfeld-Gebelein-Renyi maximum correlation coefficient between tested variables. A finite set of basis functions is employed to approximate the mapping functions that can achieve the maximal correlation. Using classic benchmark experiments based on independent component analysis, we demonstrate that our independence criterion performs comparably with the state-of-the-art quadratic-time kernel dependence measures like the Hilbert-Schmidt Independence Criterion, while being more efficient in computation. The experimental results also show that our independence criterion outperforms another contemporary linear-time kernel dependence measure, the Finite Set Independence Criterion. The potential application of our criterion in deep neural networks is validated experimentally.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303090,0
C,"Zhao, H; Rai, P; Du, L; Buntine, W; Phung, D; Zhou, MY",,"Chiappa, S; Calandra, R",,"Zhao, He; Rai, Piyush; Du, Lan; Buntine, Wray; Phung, Dinh; Zhou, Mingyuan",,,Variational Autoencoders for Sparse and Overdispersed Discrete Data,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Many applications, such as text modelling, high-throughput sequencing, and recommender systems, require analysing sparse, high-dimensional, and overdispersed discrete (count or binary) data. Recent deep probabilistic models based on variational autoencoders (VAE) have shown promising results on discrete data but may have inferior modelling performance due to the insufficient capability in modelling overdispersion and model misspecification. To address these issues, we develop a VAE-based framework using the negative binomial distribution as the data distribution. We also provide an analysis of its properties vis-a-vis other models. We conduct extensive experiments on three problems from discrete data analysis: text analysis/topic modelling, collaborative filtering, and multi-label learning. Our models outperform state-of-the-art approaches on these problems, while also capturing the phenomenon of overdispersion more effectively.(1)",,,,,"Zhao, He/GON-4192-2022; Du, Lan/AAY-1249-2021; Zhou, Mingyuan/AAE-8717-2021","Zhao, He/0000-0003-0894-2265; Buntine, Wray/0000-0001-9292-1015; Du, Lan/0000-0002-9925-0223",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1684,1693,,,,,,,,,,,,,,,,WOS:000559931304018,0
C,"Agrawal, R; Campbell, T; Huggins, J; Broderick, T",,"Chaudhuri, K; Sugiyama, M",,"Agrawal, Raj; Campbell, Trevor; Huggins, Jonathan; Broderick, Tamara",,,Data-dependent compression of random features for large-scale kernel approximation,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Kernel methods offer the flexibility to learn complex relationships in modern, large data sets while enjoying strong theoretical guarantees on quality. Unfortunately, these methods typically require cubic running time in the data set size, a prohibitive cost in the largedata setting. Random feature maps (RFMs) and the Nystrom method both consider low-rank approximations to the kernel matrix as a potential solution. But, in order to achieve desirable theoretical guarantees, the former may require a prohibitively large number of features J(+), and the latter may be prohibitively expensive for high-dimensional problems. We propose to combine the simplicity and generality of RFMs with a data-dependent feature selection scheme to achieve desirable theoretical approximation properties of Nystrom with just O(log J(+)) features. Our key insight is to begin with a large set of random features, then reduce them to a small number of weighted features in a data-dependent, computationally efficient way, while preserving the statistical guarantees of using the original large set of features. We demonstrate the efficacy of our method with theory and experiments including on a data set with over 50 million observations. In particular, we show that our method achieves small kernel matrix approximation error and better test set accuracy with provably fewer random features than state-of-the-art methods.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901090,0
C,"Bu, F; Xu, S; Heller, K; Volfovsky, A",,"Chaudhuri, K; Sugiyama, M",,"Bu, Fan; Xu, Sonia; Heller, Katherine; Volfovsky, Alexander",,,SMOGS: Social Network Metrics of Game Success,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"In this paper we propose a novel metric of basketball game success, derived from a team's dynamic social network of game play. We combine ideas from random effects models for network links with taking a multi-resolution stochastic process approach to model passes between teammates. These passes can be viewed as directed dynamic relational links in a network. Multiplicative latent factors are introduced to study higher-order patterns in players' interactions that distinguish a successful game from a loss. Parameters are estimated using a Markov chain Monte Carlo sampler. Results in simulation experiments suggest that the sampling scheme is effective in recovering the parameters. We also apply the model to the first high-resolution optical tracking data set collected in college basketball games. The learned latent factors demonstrate significant differences between players' passing and receiving patterns in a loss, as opposed to a win. Our model is applicable to team sports other than basketball, as well as other time-varying network observations.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902047,0
C,"Chen, ZH; Li, XG; Yang, L; Haupt, J; Zhao, T",,"Chaudhuri, K; Sugiyama, M",,"Chen, Zhehui; Li, Xingguo; Yang, Lin; Haupt, Jarvis; Zhao, Tuo",,,On Constrained Nonconvex Stochastic Optimization: A Case Study for Generalized Eigenvalue Decomposition,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We study constrained nonconvex optimization problems in machine learning and signal processing. It is well-known that these problems can be rewritten to a min-max problem in a Lagrangian form. However, due to the lack of convexity, their landscape is not well understood and how to find the stable equilibria of the Lagrangian function is still unknown. To bridge the gap, we study the landscape of the Lagrangian function. Further, we define a special class of Lagrangian functions. They enjoy the following two properties: 1.Equilibria are either stable or unstable (Formal definition in Section 2); 2.Stable equilibria correspond to the global optima of the original problem. We show that a generalized eigenvalue (GEV) problem, including canonical correlation analysis and other problems as special examples, belongs to the class. Specifically, we characterize its stable and unstable equilibria by leveraging an invariant group and symmetric property (more details in Section 3). Motivated by these neat geometric structures, we propose a simple, efficient, and stochastic primal-dual algorithm solving the online GEV problem. Theoretically, under sufficient conditions, we establish an asymptotic rate of convergence and obtain the first sample complexity result for the online GEV problem by diffusion approximations, which are widely used in applied probability. Numerical results are also provided to support our theory.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,916,925,,,,,,,,,,,,,,,,WOS:000509687900095,0
C,"Goel, G; Wierman, A",,"Chaudhuri, K; Sugiyama, M",,"Goel, Gautam; Wierman, Adam",,,An Online Algorithm for Smoothed Regression and LQR Control,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We consider Online Convex Optimization (OCO) in the setting where the costs are m-strongly convex and the online learner pays a switching cost for changing decisions between rounds. We show that the recently proposed Online Balanced Descent (OBD) algorithm is constant competitive in this setting, with competitive ratio 3 + O(1/m), irrespective of the ambient dimension. Additionally, we show that when the sequence of cost functions is 6-smooth, OBD has near-optimal dynamic regret and maintains strong per-round accuracy. We demonstrate the generality of our approach by showing that the OBD framework can be used to construct competitive algorithms for a variety of online problems across learning and control, including online variants of ridge regression, logistic regression, maximum likelihood estimation, and LQR control.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902057,0
C,"Lin, Y; Song, Z; Yang, LF",,"Chaudhuri, K; Sugiyama, M",,"Lin, Yibo; Song, Zhao; Yang, Lin F.",,,Towards a Theoretical Understanding of Hashing-Based Neural Nets,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Parameter reduction has been an important topic in deep learning due to the ever-increasing size of deep neural network models and the need to train and run them on resource limited machines. Despite many efforts in this area, there were no rigorous theoretical guarantees on why existing neural net compression methods should work. In this paper, we provide provable guarantees on some hashing-based parameter reduction methods in neural nets. First, we introduce a neural net compression scheme based on random linear sketching (which is usually implemented efficiently via hashing), and show that the sketched (smaller) network is able to approximate the original network on all input data coming from any smooth and well-conditioned low-dimensional manifold. The sketched network can also be trained directly via back-propagation. Next, we study the previously proposed HashedNets architecture and show that the optimization landscape of one-hidden-layer HashedNets has a local strong convexity property similar to a normal fully connected neural network. We complement our theoretical results with empirical verifications.(1)",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,127,137,,,,,,,,,,,,,,,,WOS:000509687900014,0
C,"Locatelli, A; Carpentier, A; Valko, M",,"Chaudhuri, K; Sugiyama, M",,"Locatelli, Andrea; Carpentier, Alexandra; Valko, Michal",,,Active multiple matrix completion with adaptive confidence sets,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"In this work, we formulate a new multi-task active learning setting in which the learner's goal is to solve multiple matrix completion problems simultaneously. At each round, the learner can choose from which matrix it receives a sample from an entry drawn uniformly at random. Our main practical motivation is market segmentation, where the matrices represent different regions with different preferences of the customers. The challenge in this setting is that each of the matrices can be of a different size and also of a different rank which is unknown. We provide and analyze a new algorithm, MALocate that is able to adapt to the unknown ranks of the different matrices. We then give a lower-bound showing that our strategy is minimax-optimal and demonstrate its performance with synthetic experiments.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901086,0
C,"Reddi, SJ; Kale, S; Yu, FL; Holtmann-Rice, D; Chen, J; Kumar, S",,"Chaudhuri, K; Sugiyama, M",,"Reddi, Sashank J.; Kale, Satyen; Yu, Felix; Holtmann-Rice, Dan; Chen, Jiecao; Kumar, Sanjiv",,,Stochastic Negative Mining for Learning with Large Output Spaces,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We consider the problem of retrieving the most relevant labels for a given input when the size of the output space is very large. Retrieval methods are modeled as set-valued classifiers which output a small set of classes for each input, and a mistake is made if the label is not in the output set. Despite its practical importance, a statistically principled, yet practical solution to this problem is largely missing. To this end, we first define a family of surrogate losses and show that they are calibrated and convex under certain conditions on the loss parameters and data distribution, thereby establishing a statistical and analytical basis for using these losses. Furthermore, we identify a particularly intuitive class of loss functions in the aforementioned family and show that they are amenable to practical implementation in the large output space setting (i.e. computation is possible without evaluating scores of all labels) by developing a technique called Stochastic Negative Mining. We also provide generalization error bounds for the losses in the family. Finally, we conduct experiments which demonstrate that Stochastic Negative Mining yields benefits over commonly used negative sampling approaches.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901102,0
C,"Zhou, Y; Gram-Hansen, BJ; Kohn, T; Rainforth, T; Yang, H; Wood, F",,"Chaudhuri, K; Sugiyama, M",,"Zhou, Yuan; Gram-Hansen, Bradley J.; Kohn, Tobias; Rainforth, Tom; Yang, Hongseok; Wood, Frank",,,LF-PPL: A Low-Level First Order Probabilistic Programming Language for Non-Differentiable Models,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We develop a new Low-level, First-order Probabilistic Programming Language (LF-PPL) suited for models containing a mix of continuous, discrete, and/or piecewise-continuous variables. The key success of this language and its compilation scheme is in its ability to automatically distinguish parameters the density function is discontinuous with respect to, while further providing runtime checks for boundary crossings. This enables the introduction of new inference engines that are able to exploit gradient information, while remaining efficient for models which are not everywhere differentiable. We demonstrate this ability by incorporating a discontinuous Hamiltonian Monte Carlo (DHMC) inference engine that is able to deliver automated and efficient inference for non-differentiable models. Our system is backed up by a mathematical formalism that ensures that any model expressed in this language has a density with measure zero discontinuities to maintain the validity of the inference engine.",,,,,"Yang, Hongseok/AAC-4471-2020",,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,148,157,,,,,,,,,,,,,,,,WOS:000509687900016,0
C,"Abbati, G; Tosi, A; Osborne, MA; Flaxman, S",,"Storkey, A; PerezCruz, F",,"Abbati, Gabriele; Tosi, Alessandra; Osborne, Michael A.; Flaxman, Seth",,,AdaGeo: Adaptive Geometric Learning for Optimization and Sampling,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Gradient-based optimization and Markov Chain Monte Carlo sampling can be found at the heart of several machine learning methods. In high-dimensional settings, well-known issues such as slow-mixing, non-convexity and correlations can hinder the algorithms' efficiency. In order to overcome these difficulties, we propose AdaGeo, a preconditioning framework for adaptively learning the geometry of the parameter space during optimization or sampling. In particular, we use the Gaussian process latent variable model (GP-LVM) to represent a lowerdimensional embedding of the parameters, identifying the underlying Riemannian manifold on which the optimization or sampling is taking place. Samples or optimization steps are consequently proposed based on the geometry of the manifold. We apply our framework to stochastic gradient descent, stochastic gradient Langevin dynamics, and stochastic gradient Riemannian Langevin dynamics, and show performance improvements for both optimization and sampling.",,,,,,"Osborne, Michael/0000-0003-1959-012X",,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300025,0
C,"Futami, F; Sato, I; Sugiyama, M",,"Storkey, A; PerezCruz, F",,"Futami, Futoshi; Sato, Issei; Sugiyama, Masashi",,,Variational Inference based on Robust Divergences,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Robustness to outliers is a central issue in real-world machine learning applications. While replacing a model to a heavy-tailed one (e.g., from Gaussian to Student-t) is a standard approach for robustification, it can only be applied to simple models. In this paper, based on Zellner's optimization and variational formulation of Bayesian inference, we propose an outlier-robust pseudo-Bayesian variational method by replacing the Kullback-Leibler divergence used for data fitting to a robust divergence such as the beta- and gamma-divergences. An advantage of our approach is that superior but complex models such as deep networks can also be handled. We theoretically prove that, for deep networks with ReLU activation functions, the influence function in our proposed method is bounded, while it is unbounded in the ordinary variational inference. This implies that our proposed method is robust to both of input and output outliers, while the ordinary variational method is not. We experimentally demonstrate that our robust variational method outperforms ordinary variational inference in regression and classification with deep networks.",,,,,"Sugiyama, Masashi/AEO-1176-2022","Sugiyama, Masashi/0000-0001-6658-6743",,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300086,0
C,"Lim, SH; Calvez, G",,"Storkey, A; PerezCruz, F",,"Lim, Shiau Hong; Calvez, Gregory",,,Labeled Graph Clustering via Projected Gradient Descent,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Advances in recovering low-rank matrices from noisy observations have led to tractable algorithms for clustering from general pair-wise labels with provable performance guarantees. Based on convex relaxation, it has been shown that the ground truth clusters can be recovered with high probability under a generalized stochastic block model by solving a semidefinite program. Although tractable, the algorithm is typically too slow for sufficiently large problems in practice. Inspired by recent advances in non-convex approaches to low-rank recovery problems, we propose an algorithm based on projected gradient descent that enjoys similar provable guarantees as the convex counterpart, but can be orders of magnitude faster. Our theoretical results are further supported by encouraging empirical results.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300208,0
C,"Narasimhan, H",,"Storkey, A; PerezCruz, F",,"Narasimhan, Harikrishna",,,Learning with Complex Loss Functions and Constraints,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"We develop a general approach for solving constrained classification problems, where the loss and constraints are defined in terms of a general function of the confusion matrix. We are able to handle complex, non-linear loss functions such as the F-measure, G-mean or H-mean, and constraints ranging from budget limits, to constraints for fairness, to bounds on complex evaluation metrics. Our approach builds on the framework of Narasimhan et al. (2015) for unconstrained classification with complex losses, and reduces the constrained learning problem to a sequence of cost-sensitive learning tasks. We provide algorithms for two broad families of problems, involving convex and fractional-convex losses, subject to convex constraints. Our algorithms are statistically consistent, generalize an existing approach for fair classification, and readily apply to multiclass problems. Experiments on a variety of tasks demonstrate the efficacy of our methods.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300172,0
C,"Yu, R; Li, GY; Liu, Y",,"Storkey, A; PerezCruz, F",,"Yu, Rose; Li, Guangyu; Liu, Yan",,,Tensor Regression Meets Gaussian Processes,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Low-rank tensor regression, a new model class that learns high-order correlation from data, has recently received considerable attention. At the same time, Gaussian processes (GP) are well-studied machine learning models for structure learning. In this paper, we demonstrate interesting connections between the two, especially for multi-way data analysis. We show that low-rank tensor regression is essentially learning a multi-linear kernel in Gaussian processes, and the low-rank assumption translates to the constrained Bayesian inference problem. We prove the oracle inequality and derive the average case learning curve for the equivalent GP model. Our finding implies that low-rank tensor regression, though empirically successful, is highly dependent on the eigenvalues of covariance functions as well as variable correlations.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300051,0
C,"Bartels, S; Hennig, P",,"Gretton, A; Robert, CC",,"Bartels, Simon; Hennig, Philipp",,,Probabilistic Approximate Least-Squares,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Least-squares and kernel-ridge / Gaussian process regression are among the foundational algorithms of statistics and machine learning. Famously, the worst-case cost of exact non-parametric regression grows cubically with the data-set size; but a growing number of approximations have been developed that estimate good solutions at lower cost. These algorithms typically return point estimators, without measures of uncertainty. Leveraging recent results casting elementary linear algebra operations as probabilistic inference, we propose a new approximate method for nonparametric least-squares that affords a probabilistic uncertainty estimate over the error between the approximate and exact least-squares solution (this is not the same as the posterior variance of the associated Gaussian process regressor). This allows estimating the error of the least-squares solution on a subset of the data relative to the full-data solution. The uncertainty can be used to control the computational effort invested in the approximation. Our algorithm has linear cost in the data-set size, and a simple formal form, so that it can be implemented with a few lines of code in programming languages with linear algebra functionality.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,676,684,,,,,,,,,,,,,,,,WOS:000508662100074,0
C,"Basse, GW; Soufiani, HA; Lambert, D",,"Gretton, A; Robert, CC",,"Basse, Guillaume W.; Soufiani, Hossein Azari; Lambert, Diane",,,Randomization and The Pernicious Effects of Limited Budgets on Auction Experiments,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Buyers (e.g., advertisers) often have limited financial and processing resources, and so their participation in auctions is throttled. Changes to auctions may affect bids or throttling and any change may affect what winners pay. This paper shows that if an A/B experiment affects only bids, then the observed treatment effect is unbiased when all the bidders in an auction are randomly assigned to A or B but it can be severely biased otherwise, even in the absence of throttling. Experiments that affect throttling algorithms can also be badly biased, but the bias can be substantially reduced if the budget for each advertiser in the experiment is allocated to separate pots for the A and B arms of the experiment.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,1412,1420,,,,,,,,,,,,,,,,WOS:000508662100153,0
C,"Gonzalez, J; Dai, ZW; Hennig, P; Lawrence, N",,"Gretton, A; Robert, CC",,"Gonzalez, Javier; Dai, Zhenwen; Hennig, Philipp; Lawrence, Neil",,,Batch Bayesian Optimization via Local Penalization,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"The popularity of Bayesian optimization methods for efficient exploration of parameter spaces has lead to a series of papers applying Gaussian processes as surrogates in the optimization of functions. However, most proposed approaches only allow the exploration of the parameter space to occur sequentially. Often, it is desirable to simultaneously propose batches of parameter values to explore. This is particularly the case when large parallel processing facilities are available. These could either be computational or physical facets of the process being optimized. Batch methods, however, require the modeling of the interaction between the different evaluations in the batch, which can be expensive in complex scenarios. We investigate this issue and propose a highly effective heuristic based on an estimate of the function's Lipschitz constant that captures the most important aspect of this interaction-local repulsion-at negligible computational overhead. A penalized acquisition function is used to collect batches of points minimizing the non-parallelizable computational effort. The resulting algorithm compares very well, in run-time, with much more elaborate alternatives.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,648,657,,,,,,,,,,,,,,,,WOS:000508662100071,0
C,"Landrieu, L; Obozinski, G",,"Gretton, A; Robert, CC",,"Landrieu, Loic; Obozinski, Guillaume",,,Cut Pursuit: fast algorithms to learn piecewise constant functions,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,We propose working-set/greedy algorithms to efficiently find the solutions to convex optimization problems penalized respectively by the total variation and the Mumford Shah boundary size. Our algorithms exploit the piecewise constant structure of the level-sets of the solutions by recursively splitting them using graph cuts. We obtain significant speed up on images that can be approximated with few level-sets compared to state-of-the-art algorithms.,,,,,", landrieu/N-5735-2017",", landrieu/0000-0002-7738-8141",,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,1384,1393,,,,,,,,,,,,,,,,WOS:000508662100150,0
C,"Matthews, AGD; Hensman, J; Turner, RE; Ghahramani, Z",,"Gretton, A; Robert, CC",,"Matthews, Alexander G. de G.; Hensman, James; Turner, Richard E.; Ghahramani, Zoubin",,,On Sparse Variational Methods and the Kullback-Leibler Divergence between Stochastic Processes,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"The variational framework for learning inducing variables (Titsias, 2009a) has had a large impact on the Gaussian process literature. The framework may be interpreted as minimizing a rigorously defined Kullback-Leibler divergence between the approximating and posterior processes. To our knowledge this connection has thus far gone unremarked in the literature. In this paper we give a substantial generalization of the literature on this topic. We give a new proof of the result for infinite index sets which allows inducing points that are not data points and likelihoods that depend on all function values. We then discuss augmented index sets and show that, contrary to previous works, marginal consistency of augmentation is not enough to guarantee consistency of variational inference with the original model. We then characterize an extra condition where such a guarantee is obtainable. Finally we show how our framework sheds light on interdomain sparse approximations and sparse approximations for Cox processes.",,,,,,"Hensman, James/0000-0002-4989-3589",,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,231,239,,,,,,,,,,,,,,,,WOS:000508662100026,0
C,"Ritchie, D; Stuhlmuller, A; Goodman, ND",,"Gretton, A; Robert, CC",,"Ritchie, Daniel; Stuhlmueller, Andreas; Goodman, Noah D.",,,C3: Lightweight Incrementalized MCMC for Probabilistic Programs using Continuations and Callsite Caching,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Lightweight, source-to-source transformation approaches to implementing MCMC for probabilistic programming languages are popular for their simplicity, support of existing deterministic code, and ability to execute on existing fast runtimes [1]. However, they are also inefficient, requiring a complete re-execution of the program on every Metropolis Hastings proposal. We present a new extension to the lightweight approach, C3, which enables efficient, incrementalized re-execution of MH proposals. C3 is based on two core ideas: transforming probabilistic programs into continuation passing style (CPS), and caching the results of function calls. It is particularly effective at speeding up recursive programs with many local latent variables. We show that on several common such models, C3 reduces proposal runtime by 20-100x, in some cases reducing runtime complexity from linear in model size to constant. We also demonstrate nearly an order of magnitude speedup on a complex inverse procedural modeling application.",,,,,,"Ritchie, Daniel/0000-0002-8253-0069",,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,28,37,,,,,,,,,,,,,,,,WOS:000508662100004,0
C,"Nakajima, S; Sugiyama, M",,"Kaski, S; Corander, J",,"Nakajima, Shinichi; Sugiyama, Masashi",,,Analysis of Empirical MAP and Empirical Partially Bayes: Can They be Alternatives to Variational Bayes?,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"Variational Bayesian (VB) learning is known to be a promising approximation to Bayesian learning with computational efficiency. However, in some applications, e.g., large-scale collaborative filtering and tensor factorization, VB is still computationally too costly. In such cases, looser approximations such as MAP estimation and partially Bayesian (PB) learning, where a part of the parameters are point-estimated, seem attractive. In this paper, we theoretically investigate the behavior of the MAP and the PB solutions of matrix factorization. A notable finding is that the global solutions of MAP and PB in the empirical Bayesian scenario, where the hyperparameters are also estimated from observation, are trivial and useless, while their local solutions behave similarly to the global solution of VB. This suggests that empirical MAP and empirical PB with local search can be alternatives to empirical VB equipped with the useful automatic relevance determination property. Experiments support our theory.",,,,,"Sugiyama, Masashi/AEO-1176-2022","Sugiyama, Masashi/0000-0001-6658-6743",,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,20,28,,,,,,,,,,,,,,,,WOS:000508355800003,0
C,"Rosenfeld, N; Meshi, O; Tarlow, D; Globerson, A",,"Kaski, S; Corander, J",,"Rosenfeld, Nir; Meshi, Ofer; Tarlow, Danny; Globerson, Amir",,,Learning Structured Models with the AUC Loss and Its Generalizations,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"Many problems involve the prediction of multiple, possibly dependent labels. The structured output prediction framework builds predictors that take these dependencies into account and use them to improve accuracy. In many such tasks, performance is evaluated by the Area Under the ROC Curve (AUC). While a framework for optimizing the AUC loss for unstructured models exists, it does not naturally extend to structured models. In this work, we propose a representation and learning formulation for optimizing structured models over the AUC loss, show how our approach generalizes the unstructured case, and provide algorithms for solving the resulting inference and learning problems. We also explore several new variants of the AUC measure which naturally arise from our formulation. Finally, we empirically show the utility of our approach in several domains.",,,,,,"Globerson, Amir/0000-0003-2557-1742",,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,841,849,,,,,,,,,,,,,,,,WOS:000508355800093,0
C,"Brubach, B; Grammel, N; Ma, W; Srinivasan, A",,"Banerjee, A; Fukumizu, K",,"Brubach, Brian; Grammel, Nathaniel; Ma, Will; Srinivasan, Aravind",,,Follow Your Star: New Frameworks for Online Stochastic Matching with Known and Unknown Patience,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We study several generalizations of the Online Bipartite Matching problem. We consider settings with stochastic rewards, patience constraints, and weights (considering both vertex- and edge-weighted variants). We introduce a stochastic variant of the patience-constrained problem, where the patience is chosen randomly according to some known distribution and is not known in advance. We also consider stochastic arrival settings (i.e. the nature in which the online vertices arrive is determined by a known random process), which are natural settings that are able to beat the hard worst-case bounds of adversarial arrivals. We design black-box algorithms for star graphs under various models of patience, which solve the problem optimally for deterministic or geometrically-distributed patience, and yield a 1=2-approximation for any patience distribution. These star graph algorithms are then used as black boxes to solve the online matching problems under different arrival settings. We show improved (or first-known) competitive ratios for these problems. We also present negative results that include formalizing the concept of a stochasticity gap for LP upper bounds on these problems, showing some new stochasticity gaps for popular LPs, and bounding the worst-case performance of some greedy approaches.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803046,0
C,"Camuto, A; Willetts, M; Paige, B; Holmes, C; Roberts, S",,"Banerjee, A; Fukumizu, K",,"Camuto, Alexander; Willetts, Matthew; Paige, Brooks; Holmes, Chris; Roberts, Stephen",,,Learning Bijective Feature Maps for Linear ICA,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Separating high-dimensional data like images into independent latent factors, i.e independent component analysis (ICA), remains an open research problem. As we show, existing probabilistic deep generative models (DGMs), which are tailor-made for image data, under-perform on non-linear ICA tasks. To address this, we propose a DGM which combines bijective feature maps with a linear ICA model to learn interpretable latent structures for high-dimensional data. Given the complexities of jointly training such a hybrid model, we introduce novel theory that constrains linear ICA to lie close to the manifold of orthogonal rectangular matrices, the Stiefel manifold. By doing so we create models that converge quickly, are easy to train, and achieve better unsupervised latent factor discovery than flow-based models, linear ICA, and Variational Autoencoders on images.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804041,0
C,"Kerdreux, T; d'Aspremont, A; Pokutta, S",,"Banerjee, A; Fukumizu, K",,"Kerdreux, Thomas; d'Aspremont, Alexandre; Pokutta, Sebastian",,,Projection-Free Optimization on Uniformly Convex Sets,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"The Frank-Wolfe method solves smooth constrained convex optimization problems at a generic sublinear rate of O(1/T), and it (or its variants) enjoys accelerated convergence rates for two fundamental classes of constraints: polytopes and strongly-convex sets. Uniformly convex sets non-trivially subsume strongly convex sets and form a large variety of curved convex sets commonly encountered in machine learning and signal processing. For instance, the l(p)-balls are uniformly convex for all p > 1, but strongly convex for p is an element of]1, 2] only. We show that these sets systematically induce accelerated convergence rates for the original Frank-Wolfe algorithm, which continuously interpolate between known rates. Our accelerated convergence rates emphasize that it is the curvature of the constraint sets - not just their strong convexity - that leads to accelerated convergence rates. These results also importantly highlight that the Frank-Wolfe algorithm is adaptive to much more generic constraint set structures, thus explaining faster empirical convergence. Finally, we also show accelerated convergence rates when the set is only locally uniformly convex around the optima and provide similar results in online linear optimization.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,19,+,,,,,,,,,,,,,,,,WOS:000659893800003,0
C,"Ma, W; Simchi-Levi, D",,"Banerjee, A; Fukumizu, K",,"Ma, Will; Simchi-Levi, David",,,Reaping the Benefits of Bundling under High Production Costs,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"It is well-known that selling different goods in a single bundle can significantly increase revenue. However, bundling is no longer profitable if the goods have high production costs. To overcome this challenge, we introduce a new mechanism, Pure Bundling with Disposal for Cost (PBDC), where after buying the bundle, the customer is allowed to return any subset of goods for their costs. We provide two types of guarantees on the profit of PBDC mechanisms relative to the optimum in the presence of production costs, under the assumption that customers have valuations which are additive over the items and drawn independently. We first provide a distribution-dependent guarantee which shows that PBDC earns at least 1 6c(2/3) of the optimal profit, where c denotes the coefficient of variation of the welfare random variable. c approaches 0 if there are a large number of items whose individual valuations have bounded coefficients of variation, and our constants improve upon those from the classical result of Bakos and Brynjolfsson (1999) without costs. We then provide a distribution-free guarantee which shows that either PBDC or individual sales earns at least 1/5.2 times the optimal profit, generalizing and improving the constant of 1/6 from the celebrated result of Babaioff et al. (2014). Conversely, we also provide the best-known upper bound on the performance of any partitioning mechanism (which captures both individual sales and pure bundling), of 1/1.19 times the optimal profit, improving on the previouslyknown upper bound of 1/1.08. Finally, we conduct simulations under the same playing field as the extensive numerical study of Chu et al. (2011), which confirm that PBDC outperforms other simple pricing schemes overall.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801063,0
C,"Miller, J; Rabusseau, G; Terilla, J",,"Banerjee, A; Fukumizu, K",,"Miller, Jacob; Rabusseau, Guillaume; Terilla, John",,,Tensor Networks for Probabilistic Sequence Modeling,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Tensor networks are a powerful modeling framework developed for computational manybody physics, which have only recently been applied within machine learning. In this work we utilize a uniform matrix product state (u-MPS) model for probabilistic modeling of sequence data. We first show that u-MPS enable sequence-level parallelism, with lengthn sequences able to be evaluated in depth O(log n). We then introduce a novel generative algorithm giving trained u-MPS the ability to efficiently sample from a wide variety of conditional distributions, each one defined by a regular expression. Special cases of this algorithm correspond to autoregressive and fill-in-the-blank sampling, but more complex regular expressions permit the generation of richly structured data in a manner that has no direct analogue in neural generative models. Experiments on sequence modeling with synthetic and real text data show u-MPS outperforming a variety of baselines and effectively generalizing their predictions in the presence of limited data.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803069,0
C,"O'Leary, J; Wang, GY; Jacob, PE",,"Banerjee, A; Fukumizu, K",,"O'Leary, John; Wang, Guanyang; Jacob, Pierre E.",,,Maximal Couplings of the Metropolis-Hastings Algorithm,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Couplings play a central role in the analysis of Markov chain Monte Carlo algorithms and appear increasingly often in the algorithms themselves, e.g. in convergence diagnostics, parallelization, and variance reduction techniques. Existing couplings of the Metropolis-Hastings algorithm handle the proposal and acceptance steps separately and fall short of the upper bound on one-step meeting probabilities given by the coupling inequality. This paper introduces maximal couplings which achieve this bound while retaining the practical advantages of current methods. We consider the properties of these couplings and examine their behavior on a selection of numerical examples.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893801050,0
C,"Sreekumar, S; Zhang, ZX; Goldfeld, Z",,"Banerjee, A; Fukumizu, K",,"Sreekumar, Sreejith; Zhang, Zhengxin; Goldfeld, Ziv",,,Non-asymptotic Performance Guarantees for Neural Estimation of f-Divergences,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Statistical distances (SDs), which quantify the dissimilarity between probability distributions, are central to machine learning and statistics. A modern method for estimating such distances from data relies on parametrizing a variational form by a neural network (NN) and optimizing it. These estimators are abundantly used in practice, but corresponding performance guarantees are partial and call for further exploration. In particular, there seems to be a fundamental tradeoff between the two sources of error involved: approximation and estimation. While the former needs the NN class to be rich and expressive, the latter relies on controlling complexity. This paper explores this tradeoff by means of non-asymptotic error bounds, focusing on three popular choices of SDs-Kullback-Leibler divergence, chi-squared divergence, and squared Hellinger distance. Our analysis relies on non-asymptotic function approximation theorems and tools from empirical process theory. Numerical results validating the theory are also provided.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804004,0
C,"Wei, CY; Jafarnia-Jahromi, M; Luo, HP; Jain, R",,"Banerjee, A; Fukumizu, K",,"Wei, Chen-Yu; Jafarnia-Jahromi, Mehdi; Luo, Haipeng; Jain, Rahul",,,Learning Infinite-horizon Average-reward MDPs with Linear Function Approximation,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"We develop several new algorithms for learning Markov Decision Processes in an infinite-horizon average-reward setting with linear function approximation. Using the optimism principle and assuming that the MDP has a linear structure, we first propose a computationally inefficient algorithm with optimal (O) over tilde(root T) regret and another computationally efficient variant with (O) over tilde (T-3/4) regret, where T is the number of interactions. Next, taking inspiration from adversarial linear bandits, we develop yet another efficient algorithm with (O) over tilde(root T) regret under a different set of assumptions, improving the best existing result by Hao et al. (2021) with (O) over tilde (T-2/3) regret. Moreover, we draw a connection between this algorithm and the Natural Policy Gradient algorithm proposed by Kakade (2002), and show that our analysis improves the sample complexity bound recently given by Agarwal et al. (2020).",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893803061,0
C,"Yang, ZL; Chen, ZX; Cai, T; Chen, XY; Li, B; Tian, YD",,"Banerjee, A; Fukumizu, K",,"Yang, Zhuolin; Chen, Zhaoxi; Cai, Tiffany (Tianhui); Chen, Xinyun; Li, Bo; Tian, Yuandong",,,Understanding Robustness in Teacher-Student Setting: A New Perspective,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"Adversarial examples have appeared as a ubiquitous property of machine learning models where bounded adversarial perturbation could mislead the models to make arbitrarily incorrect predictions. Such examples provide a way to assess the robustness of machine learning models as well as a proxy for understanding the model training process. There have been extensive studies trying to explain the existence of adversarial examples and provide ways to improve model robustness, e.g., adversarial training. Different from prior works that mostly focus on models trained on datasets with predefined labels, we leverage the teacher-student framework and assume a teacher model, or oracle, to provide the labels for given instances. In this setting, we extend Tian (2019) in the case of low-rank input data, and show that student specialization (the trained student neuron is highly correlated with certain teacher neuron at the same layer) still happens within the input subspace, but the teacher and student nodes could differ wildly out of the data subspace, which we conjecture leads to adversarial examples. Extensive experiments show that student specialization correlates strongly with model robustness in different scenarios, including students trained via standard training, adversarial training, confidence-calibrated adversarial training, and training with the robust feature dataset. Our studies could shed light on the future exploration of adversarial examples, and potential approaches to enhance model robustness via principled data augmentation.",,,,,"Chen, Xinyun/ABZ-9877-2022","Chen, Zhaoxi/0000-0003-3998-7044",,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804003,0
C,"Ying, JX; Cardoso, JVD; Palomar, D",,"Banerjee, A; Fukumizu, K",,"Ying, Jiaxi; Cardoso, Jose Vinicius de Miranda; Palomar, Daniel",,,Minimax Estimation of Laplacian Constrained Precision Matrices,24TH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS (AISTATS),Proceedings of Machine Learning Research,,,,24th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 13-15, 2021",ELECTR NETWORK,,,,,"This paper considers the problem of high-dimensional sparse precision matrix estimation under Laplacian constraints. We prove that the Laplacian constraints bring favorable properties for estimation: the Gaussian maximum likelihood estimator exists and is unique almost surely on the basis of one observation, irrespective of the dimension. We establish the optimal rate of convergence under Frobenius norm by the derivation of the minimax lower and upper bounds. The minimax lower bound is obtained by applying Le Cam-Assouad's method with a novel construction of a subparameter space of multivariate normal distributions. The minimax upper bound is established by designing an adaptive l(1)-norm regularized maximum likelihood estimation method and quantifying the rate of convergence. We prove that the proposed estimator attains the optimal rate of convergence with an overwhelming probability. Numerical experiments demonstrate the effectiveness of the proposed estimator.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2021,130,,,,,,,,,,,,,,,,,,,,,,,WOS:000659893804050,0
C,"Baldin, N; Berthet, Q",,"Chiappa, S; Calandra, R",,"Baldin, Nicolai; Berthet, Quentin",,,Statistical and Computational Rates in Graph Logistic Regression,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We consider the problem of graph logistic regression, based on partial observation of a large network, and on side information associated to its vertices. The generative model is formulated as a matrix logistic regression. The performance of the model is analyzed in a high-dimensional regime under a structural assumption. The optimal statistical rates are derived, and an estimator based on penalized maximum likelihood is shown to attain it. The algorithmic aspects of this problem are also studied, and optimal rates under computational constraints are derived, and shown to differ from the information-theoretic rates - under a complexity assumption.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2719,2729,,,,,,,,,,,,,,,,WOS:000559931300032,0
C,"Ha, W; Fountoulakis, K; Mahoney, MW",,"Chiappa, S; Calandra, R",,"Ha, Wooseok; Fountoulakis, Kimon; Mahoney, Michael W.",,,Statistical guarantees for local graph clustering,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Local graph clustering methods aim to find small clusters in very large graphs. These methods take as input a graph and a seed node, and they return as output a good cluster in a running time that depends on the size of the output cluster but that is independent of the size of the input graph. In this paper, we adopt a statistical perspective on local graph clustering, and we analyze the performance of the l(1)-regularized PageRank method for the recovery of a single target cluster, given a seed node inside the cluster. Assuming the target cluster has been generated by a random model, we present two results. In the first, we show that the optimal support of l(1)-regularized PageRank recovers the full target cluster, with bounded false positives. In the second, we show that if the seed node is connected solely to the target cluster then the optimal support of l(1)-regularized PageRank recovers exactly the target cluster. We also show empirically that l(1)-regularized PageRank has a state-of-the-art performance on many real graphs, demonstrating the superiority of the method.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2687,2696,,,,,,,,,,,,,,,,WOS:000559931301030,0
C,"Laborde, M; Oberman, A",,"Chiappa, S; Calandra, R",,"Laborde, M.; Oberman, A.",,,A Lyapunov analysis for accelerated gradient methods: from deterministic to stochastic case,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Recent work by Su, Boyd and Candes made a connection between Nesterov's accelerated gradient descent method and an ordinary differential equation (ODE). We show that this connection can be extended to the case of stochastic gradients, and develop Lyapunov function based convergence rates proof for Nesterov's accelerated stochastic gradient descent. In the gradient case, we show Nesterov's method arises as a straightforward discretization of a modified ODE. Established Lyapunov analysis is used to recover the accelerated rates of convergence in both continuous and discrete time. Moreover, the Lyapunov analysis can be extended to the case of stochastic gradients. The result is a unified approach to acceleration in both continuous and discrete time, and in for both stochastic and full gradients.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,602,611,,,,,,,,,,,,,,,,WOS:000559931301093,0
C,"Li, BC; Ma, M; Giannakis, GB",,"Chiappa, S; Calandra, R",,"Li, Bingcong; Ma, Meng; Giannakis, Georgios B.",,,On the Convergence of SARAH and Beyond,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"The main theme of this work is a unifying algorithm, LoopLess SARAH (L2S) for problems formulated as summation of n individual loss functions. L2S broadens a recently developed variance reduction method known as SARAH. To find an 6 -accurate solution, L2S enjoys a complexity of 0 ((n+K) ln(1/6) for strongly convex problems. For convex problems, when adopting an n -dependent step size, the complexity of L2S is O(n + 710; while for more frequently adopted n -independent step size, the complexity is 0(n + n/c). Distinct from SARAH, our theoretical findings support an n -independent step size in convex problems without extra assumptions. For nonconvex problems, the complexity of L2S is O(n + AFVE). Our numerical tests on neural networks suggest that L2S can have better generalization properties than SARAH. Along with L2S, our side results include the linear convergence of the last iteration for SARAH in strongly convex problems.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,223,232,,,,,,,,,,,,,,,,WOS:000559931302001,0
C,"Ma, P; Zhang, XL; Xing, X; Ma, JY; Mahoney, MW",,"Chiappa, S; Calandra, R",,"Ma, Ping; Zhang, Xinlian; Xing, Xin; Ma, Jingyi; Mahoney, Michael W.",,,Asymptotic Analysis of Sampling Estimators for Randomized Numerical Linear Algebra Algorithms,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"The statistical analysis of Randomized Numerical Linear Algebra (RandNLA) algorithms within the past few years has mostly focused on their performance as point estimators. However, this is insufficient for conducting statistical inference, e.g., constructing confidence intervals and hypothesis testing, since the distribution of the estimator is lacking. In this article, we develop asymptotic analysis to derive the distribution of RandNLA sampling estimators for the least-squares problem. In particular, we derive the asymptotic distribution of a general sampling estimator with arbitrary sampling probabilities. The analysis is conducted in two complementary settings, i.e., when the objective of interest is to approximate the full sample estimator or is to infer the underlying ground truth model parameters. For each setting, we show that the sampling estimator is asymptotically normally distributed under mild regularity conditions. Moreover, the sampling estimator is asymptotically unbiased in both settings. Based on our asymptotic analysis, we use two criteria, the Asymptotic Mean Squared Error (AMSE) and the Expected Asymptotic Mean Squared Error (EAMSE), to identify optimal sampling probabilities. Several of these optimal sampling probability distributions are new to the literature, e.g., the root leverage sampling estimator and the predictor length sampling estimator. Our theoretical results clarify the role of leverage in the sampling process, and our empirical results demonstrate improvements over existing methods.",,,,,"Xing, Xin/AAW-7605-2021",,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1026,1034,,,,,,,,,,,,,,,,WOS:000559931302025,0
C,"Majzoubi, M; Choromanska, A",,"Chiappa, S; Calandra, R",,"Majzoubi, Maryam; Choromanska, Anna",,,LdSM: Logarithm-depth Streaming Multi-label Decision Trees,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We consider multi-label classification where the goal is to annotate each data point with the most relevant subset of labels from an extremely large label set. Efficient annotation can be achieved with balanced tree predictors, i.e. trees with logarithmic-depth in the label complexity, whose leaves correspond to labels. Designing prediction mechanism with such trees for real data applications is non-trivial as it needs to accommodate sending examples to multiple leaves while at the same time sustain high prediction accuracy. In this paper we develop the LdSM algorithm for the construction and training of multi-label decision trees, where in every node of the tree we optimize a novel objective function that favors balanced splits, maintains high class purity of children nodes, and allows sending examples to multiple directions but with a penalty that prevents tree over-growth. Each node of the tree is trained once the previous node is completed leading to a streaming approach for training We analyze the proposed objective theoretically and show that minimizing it leads to pure and balanced data splits. Furthermore, we show a boosting theorem that captures its connection to the multi-label classification error. Experimental results on benchmark data sets demonstrate that our approach achieves high prediction accuracy and low prediction time and position LdSM as a competitive tool among existing state-of-the-art approaches.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,4247,4256,,,,,,,,,,,,,,,,WOS:000559931302028,0
C,"Mayekar, P; Tyagi, H",,"Chiappa, S; Calandra, R",,"Mayekar, Prathamesh; Tyagi, Himanshu",,,RATQ: A Universal Fixed-Length Quantizer for Stochastic Optimization,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We present Rotated Adaptive Tetra-iterated Quantizer (RATQ), a fixed-length quantizer for gradients in first order stochastic optimization. RATQ is easy to implement and involves only a Hadamard transform computation and adaptive uniform quantization with appropriately chosen dynamic ranges. For noisy gradients with almost surely bounded Euclidean norms, we establish an information theoretic lower bound for optimization accuracy using finite precision gradients and show that RATQ almost attains this lower bound. For mean square bounded noisy gradients, we use a gain-shape quantizer which separately quantizes the Euclidean norm and uses RATQ to quantize the normalized unit norm vector. We establish lower bounds for performance of any optimization procedure and shape quantizer, when used with a uniform gain quantizer. Finally, we propose an adaptive quantizer for gain which when used with RATQ for shape quantizer outperforms uniform gain quantization and is, in fact, close to optimal.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,1399,1408,,,,,,,,,,,,,,,,WOS:000559931302034,0
C,"Meng, CL; Song, Y; Song, JM; Ermon, S",,"Chiappa, S; Calandra, R",,"Meng, Chenlin; Song, Yang; Song, Jiaming; Ermon, Stefano",,,Gaussianization Flows,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Iterative Gaussianization is a fixed-point iteration procedure that can transform any continuous random vector into a Gaussian one. Based on iterative Gaussianization, we propose a new type of normalizing flow model that enables both efficient computation of likelihoods and efficient inversion for sample generation. We demonstrate that these models, named Gaussianization flows, are universal approximators for continuous probability distributions under some regularity conditions. Because of this guaranteed expressivity, they can capture multimodal target distributions without compromising the efficiency of sample generation. Experimentally, we show that Gaussianization flows achieve better or comparable performance on several tabular datasets compared to other efficiently invertible flow models such as Real NVP, Glow and FFJORD. In particular, Gaussianization flows are easier to initialize, demonstrate better robustness with respect to different transformations of the training data, and generalize better on small training sets.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,4336,4344,,,,,,,,,,,,,,,,WOS:000559931302040,0
C,"Mishchenko, K; Kovalev, D; Shulgin, E; Richtarik, P; Malitsky, Y",,"Chiappa, S; Calandra, R",,"Mishchenko, Konstantin; Kovalev, Dmitry; Shulgin, Egor; Richtarik, Peter; Malitsky, Yura",,,Revisiting Stochastic Extragradient,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We fix a fundamental issue in the stochastic extragradient method by providing a new sampling strategy that is motivated by approximating implicit updates. Since the existing stochastic extragradient algorithm, called Mirror-Prox, of (Juditsky et al., 2011) diverges on a simple bilinear problem when the domain is not bounded, we prove guarantees for solving variational inequality that go beyond existing settings. Furthermore, we illustrate numerically that the proposed variant converges faster than many other methods on bilinear saddle-point problems. We also discuss how extragradient can be applied to training Generative Adversarial Networks (GANs) and how it compares to other methods. Our experiments on GANs demonstrate that the introduced approach may make the training faster in terms of data passes, while its higher iteration complexity makes the advantage smaller.",,,,,,"Mishchenko, Konstantin/0000-0002-5241-7292",,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,4573,4581,,,,,,,,,,,,,,,,WOS:000559931302044,0
C,"Moore, T; Wong, WK",,"Chiappa, S; Calandra, R",,"Moore, Travis; Wong, Weng-Keen",,,The Quantile Snapshot Scan: Comparing Quantiles of Spatial Data from Two Snapshots in Time,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"We introduce the Quantile Snapshot Scan (Qsnap), a spatial scan algorithm which identifies spatial regions that differ the most between two snapshots in time. Qsnap is designed for spatial data with a numeric response and a vector of associated covariates for each spatial data point. Qsnap focuses on differences involving a specific quantile of the data distribution. A naive implementation of Qsnap is too computationally expensive for large datasets but our novel incremental update provides an order of magnitude speedup. We demonstrate Qsnap's effectiveness over an extensive set of experiments on simulated data. In addition, we apply Qsnap to two real-world problems: discovering bird migration paths and identifying regions with dramatic changes in drought conditions.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,2677,2685,,,,,,,,,,,,,,,,WOS:000559931302049,0
C,"Uziel, G; El-Yaniv, R",,"Chiappa, S; Calandra, R",,"Uziel, Guy; El-Yaniv, Ran",,,Long-and Short-Term Forecasting for Portfolio Selection with Transaction Costs,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,In this paper we focus on the problem of on-line portfolio selection with transaction costs. We tackle this problem using a novel approach for combining the predictions of long-term experts with those of short-term experts so as to effectively reduce transaction costs. We prove that the new strategy maintains bounded regret relative to the performance of the best possible combination (switching times) of the long-and short-term experts. We empirically validate our approach on several standard benchmark datasets. These studies indicate that the proposed approach achieves state-of-the-art performance.,,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303049,0
C,"Ward, WOC; Ryder, T; Prangle, D; Alvarez, MA",,"Chiappa, S; Calandra, R",,"Ward, Wil O. C.; Ryder, Tom; Prangle, Dennis; Alvarez, Mauricio A.",,,Black-Box Inference for Non-Linear Latent Force Models,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 108",Proceedings of Machine Learning Research,,,,23rd International Conference on Artificial Intelligence and Statistics (AISTATS),"AUG 26-28, 2020",ELECTR NETWORK,,,,,"Latent force models are systems whereby there is a mechanistic model describing the dynamics of the system state, with some unknown forcing term that is approximated with a Gaussian process. If such dynamics are non-linear, it can be difficult to estimate the posterior state and forcing term jointly, particularly when there are system parameters that also need estimating. This paper uses black-box variational inference to jointly estimate the posterior, designing a multivariate extension to local inverse autoregressive flows as a flexible approximator of the system. We compare estimates on systems where the posterior is known, demonstrating the effectiveness of the approximation, and apply to problems with non-linear dynamics, multi-output systems and models with non-Gaussian likelihoods.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2020,108,,,,,,,,,,,,,,,,,,,,,,,WOS:000559931303072,0
C,"Dikov, G; Bayer, J",,"Chaudhuri, K; Sugiyama, M",,"Dikov, Georgi; Bayer, Justin",,,Bayesian Learning of Neural Network Architectures,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"In this paper we propose a Bayesian method for estimating architectural parameters of neural networks, namely layer size and network depth. We do this by learning concrete distributions over these parameters. Our results show that regular networks with a learnt structure can generalise better on small datasets, while fully stochastic networks can be more robust to parameter initialisation. The proposed method relies on standard neural variational learning and, unlike randomised architecture search, does not require a retraining of the model, thus keeping the computational overhead at minimum.",,,,,,"van der Smagt, Patrick/0000-0003-4418-4916",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,730,738,,,,,,,,,,,,,,,,WOS:000509687900076,0
C,"Dupont, E; Suresha, S",,"Chaudhuri, K; Sugiyama, M",,"Dupont, Emilien; Suresha, Suhas",,,Probabilistic Semantic Inpainting with Pixel Constrained CNNs,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Semantic inpainting is the task of inferring, missing pixels in an image pixels and high level image semantics. Most semantic inpainting algorithms are deterministic: an image with missing regions, a single inpainted image is generated. However, there are often several plausible inpaintings for a given missing region. In this paper, we propose a method to perform probabilistic semantic inpainting by building a model, based on PixelCNNs, that learns a distribution of images conditioned on a subset of visible pixels. Experiments on the MNIST and CelebA datasets show that our methods produdes diverse and realistic inpaintings.",,,,,,"Suresha, Suhas/0000-0002-6842-5748",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902032,0
C,"Garg, N; Johari, R",,"Chaudhuri, K; Sugiyama, M",,"Garg, Nikhil; Johari, Ramesh",,,Designing Optimal Binary Rating Systems,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Modern online platforms rely on effective rating systems to learn about items. We consider the optimal design of rating systems that collect binary feedback after transactions. We make three contributions. First, we formalize the performance of a rating system as the speed with which it recovers the true underlying ranking on items (in a large deviations sense), accounting for both items' underlying match rates and the platform's preferences. Second, we provide an efficient algorithm to compute the binary feedback system that yields the highest such performance. Finally, we show how this theoretical perspective can be used to empirically design an implementable, approximately optimal rating system, and validate our approach using real-world experimental data collected on Amazon Mechanical Turk.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901101,0
C,"Hegde, P; Heinonen, M; Lahdesmaki, H; Kaski, S",,"Chaudhuri, K; Sugiyama, M",,"Hegde, Pashupati; Heinonen, Markus; Lahdesmaki, Harri; Kaski, Samuel",,,Deep learning with differential Gaussian process flows,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We propose a novel deep learning paradigm of differential flows that learn a stochastic differential equation transformations of inputs prior to a standard classification or regression function. The key property of differential Gaussian processes is the warping of inputs through infinitely deep, but infinitesimal, differential fields, that generalise discrete layers into a dynamical system. We demonstrate excellent results as compared to deep Gaussian processes and Bayesian neural networks.",,,,,"Kaski, Samuel/B-6684-2008","Kaski, Samuel/0000-0003-1925-9154",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901089,0
C,"Hiranandani, G; Boodaghians, S; Mehta, R; Koyejo, O",,"Chaudhuri, K; Sugiyama, M",,"Hiranandani, Gaurush; Boodaghians, Shant; Mehta, Ruta; Koyejo, Oluwasanmi",,,Performance Metric Elicitation from Pairwise Classifier Comparisons,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Given a binary prediction problem, which performance metric should the classifier optimize? We address this question by formalizing the problem of Metric Elicitation. The goal of metric elicitation is to discover the performance metric of a practitioner, which reflects her innate rewards (costs) for correct (incorrect) classification. In particular, we focus on eliciting binary classification performance metrics from pairwise feedback, where a practitioner is queried to provide relative preference between two classifiers. By exploiting key geometric properties of the space of confusion matrices, we obtain provably query efficient algorithms for eliciting linear and linear-fractional performance metrics. We further show that our method is robust to feedback and finite sample noise.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,371,379,,,,,,,,,,,,,,,,WOS:000509687900039,0
C,"Hsu, K; Ramos, F",,"Chaudhuri, K; Sugiyama, M",,"Hsu, Kelvin; Ramos, Fabio",,,Bayesian Learning of Conditional Kernel Mean Embeddings for Automatic Likelihood-Free Inference,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"In likelihood-free settings where likelihood evaluations are intractable, approximate Bayesian computation (ABC) addresses the formidable inference task to discover plausible parameters of simulation programs that explain the observations. However, they demand large quantities of simulation calls. Critically, hyperparameters that determine measures of simulation discrepancy crucially balance inference accuracy and sample efficiency, yet are difficult to tune. In this paper, we present kernel embedding likelihood-free inference (KELFI), a holistic framework that automatically learns model hyperparameters to improve inference accuracy given limited simulation budget. By leveraging likelihood smoothness with conditional mean embeddings, we nonparametrically approximate likelihoods and posteriors as surrogate densities and sample from closed-form posterior mean embeddings, whose hyperparameters are learned under its approximate marginal likelihood. Our modular framework demonstrates improved accuracy and efficiency on challenging inference problems in ecology.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902070,0
C,"Liu, LF; Liu, LP",,"Chaudhuri, K; Sugiyama, M",,"Liu, Linfeng; Liu, Li-Ping",,,Amortized Variational Inference with Graph Convolutional Networks for Gaussian Processes,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"GP Inference on large datasets is computationally expensive, especially when the observation likelihood is non-Gaussian. To reduce the computation, many recent variational inference methods define the variational distribution based on a small number of inducing points. These methods have a hard tradeoff between distribution flexibility and computational efficiency. In this paper, we focus on the approximation of GP posterior at a local level: we define a reusable template to approximate the posterior at neighborhoods while maintaining a global approximation. We first construct a variational distribution such that the inference for a data point considers only its neighborhood, thereby separating the calculation for each data point. We then train Graph Convolutional Networks as a reusable model to run inference for each data point. Comparing to previous methods, our method greatly reduces the number of parameters and also the number of optimization iterations. In empirical evaluations, the proposed method significantly speeds up the inference and often gets more accurate results than competing methods.",,,,,"Liu, Linfeng/GYA-1116-2022",,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687902035,0
C,"Ma, YY; Ganapathiraman, V; Zhang, XH",,"Chaudhuri, K; Sugiyama, M",,"Ma, Yingyi; Ganapathiraman, Vignesh; Zhang, Xinhua",,,Learning Invariant Representations with Kernel Warping,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Invariance is an effective prior that has been extensively used to bias supervised learning with a given representation of data. In order to learn invariant representations, wavelet and scattering based methods hard code invariance over the entire sample space, hence restricted to a limited range of transformations. Kernels based on Haar integration also work only on a group of transformations. In this work, we break this limitation by designing a new representation learning algorithm that incorporates invariances beyond transformation. Our approach, which is based on warping the kernel in a data-dependent fashion, is computationally efficient using random features, and leads to a deep kernel through multiple layers. We apply it to convolutional kernel networks and demonstrate its stability.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901005,0
C,"Nitanda, A; Suzuki, T",,"Chaudhuri, K; Sugiyama, M",,"Nitanda, Atsushi; Suzuki, Taiji",,,Stochastic Gradient Descent with Exponential Convergence Rates of Expected Classification Errors,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"We consider stochastic gradient descent and its averaging variant for binary classification problems in a reproducing kernel Hilbert space. In traditional analysis using a consistency property of loss functions, it is known that the expected classification error converges more slowly than the expected risk even when assuming a low noise condition on the conditional label probabilities. Consequently, the resulting rate is sublinear. Therefore, it is important to consider whether much faster convergence of the expected classification error can be achieved. In recent research, an exponential convergence rate for stochastic gradient descent was shown under a strong low noise condition but provided theoretical analysis was limited to the squared loss function, which is somewhat inadequate for binary classification tasks. In this paper, we show an exponential convergence of the expected classification error in the final phase of the stochastic gradient descent for a wide class of differentiable convex loss functions under similar assumptions. As for the averaged stochastic gradient descent, we show that the same convergence rate holds from the early phase of training. In experiments, we verify our analyses on the L-2-regularized logistic regression.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,,,,,,WOS:000509687901048,0
C,"Redko, I; Courty, N; Flamary, R; Tuia, D",,"Chaudhuri, K; Sugiyama, M",,"Redko, Ievgen; Courty, Nicolas; Flamary, Remi; Tuia, Devis",,,Optimal Transport for Multi-source Domain Adaptation under Target Shift,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"In this paper, we tackle the problem of reducing discrepancies between multiple domains, i.e. multi-source domain adaptation, and consider it under the target shift assumption: in all domains we aim to solve a classification problem with the same output classes, but with different labels proportions. This problem, generally ignored in the vast majority of domain adaptation papers, is nevertheless critical in real-world applications, and we theoretically show its impact on the success of the adaptation. Our proposed method is based on optimal transport, a theory that has been successfully used to tackle adaptation problems in machine learning. The introduced approach, Joint Class Proportion and Optimal Transport (JCPOT), performs multi-source adaptation and target shift correction simultaneously by learning the class probabilities of the unlabeled target sample and the coupling allowing to align two (or more) probability distributions. Experiments on both synthetic and real-world data (satellite image pixel classification) task show the superiority of the proposed method over the state-of-the-art.",,,,,"Flamary, Rmi/AAC-1958-2022; Tuia, Devis/AAE-9339-2019","Flamary, Rmi/0000-0002-4212-6627; Tuia, Devis/0000-0003-0374-2459",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,849,858,,,,,,,,,,,,,,,,WOS:000509687900088,0
C,"Xie, R; Wang, ZY; Bai, SY; Ma, P; Zhong, WX",,"Chaudhuri, K; Sugiyama, M",,"Xie, Rui; Wang, Zengyan; Bai, Shuyang; Ma, Ping; Zhong, Wenxuan",,,Online Decentralized Leverage Score Sampling for Streaming Multidimensional Time Series,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Estimating the dependence structure of multidimensional time series data in real-time is challenging. With large volumes of streaming data, the problem becomes more difficult when the multidimensional data are collected asynchronously across distributed nodes, which motivates us to sample representative data points from streams. We propose a leverage score sampling (LSS) method for efficient online inference of the streaming vector autoregressive (VAR) model. We define the leverage score for the streaming VAR model so that the LSS method selects informative data points in real-time with statistical guarantees of parameter estimation efficiency. Moreover, our LSS method can be directly deployed in an asynchronous decentralized environment, e.g., a sensor network without a fusion center, and produce asynchronous consensus online parameter estimation over time. By exploiting the temporal dependence structure of the VAR model, the LSS method selects samples independently on each dimension and thus is able to update the estimation asynchronously. We illustrate the effectiveness of the LSS method in synthetic, gas sensor and seismic datasets.",,,,,"Ma, Ping/M-7746-2015; Bai, Shuyang/AAW-3846-2021","Ma, Ping/0000-0002-5728-3596; ",,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,,,,,,,,,,,,,31187096,,,,,WOS:000509687902036,0
C,"Zhang, RY; Wen, Z; Chen, CY; Fang, C; Yu, T; Carin, L",,"Chaudhuri, K; Sugiyama, M",,"Zhang, Ruiyi; Wen, Zheng; Chen, Changyou; Fang, Chen; Yu, Tong; Carin, Lawrence",,,Scalable Thompson Sampling via Optimal Transport,"22ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 89",Proceedings of Machine Learning Research,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 16-18, 2019","Naha, JAPAN",,,,,"Thompson sampling (TS) is a class of algorithms for sequential decision making, in which a posterior distribution is maintained over a reward model. However, calculating exact posterior distributions is intractable for all but the simplest models. Development of computationally-efficiently approximate methods for the posterior distribution is consequently a crucial problem for scalable TS with complex models, such as neural networks. In this paper, we use distribution optimization techniques to approximate the posterior distribution, solved via Wasserstein gradient flows. Based on the framework, a principled particle-optimization algorithm is developed for TS to approximate the posterior efficiently. Our approach is scalable and does not make explicit distribution assumptions on posterior approximations. Extensive experiments on both synthetic and real large-scale data demonstrate the superior performance of the proposed methods.",,,,,"Zhang, Ruiyi/AAB-8402-2021; wen, zheng/HII-3705-2022",,,,,,,,,,,,,,2640-3498,,,,,,2019,89,,,,,,87,96,,,,,,,,,,,,,,,,WOS:000509687900010,0
C,"Chen, NT; Klushyn, A; Kurle, R; Jiang, XY; Bayer, J; van der Smagt, P",,"Storkey, A; PerezCruz, F",,"Chen, Nutan; Klushyn, Alexej; Kurle, Richard; Jiang, Xueyan; Bayer, Justin; van der Smagt, Patrick",,,Metrics for Deep Generative Models,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Neural samplers such as variational autoencoders (VAEs) or generative adversarial networks (GANs) approximate distributions by transforming samples from a simple random source-the latent space-to samples from a more complex distribution represented by a dataset. While the manifold hypothesis implies that a dataset contains large regions of low density, the training criterions of VAEs and GANs will make the latent space densely covered. Consequently points that are separated by low-density regions in observation space will be pushed together in latent space, making stationary distances poor proxies for similarity. We transfer ideas from Riemannian geometry to this setting, letting the distance between two points be the shortest path on a Riemannian manifold induced by the transformation. The method yields a principled distance measure, provides a tool for visual inspection of deep generative models, and an alternative to linear interpolation in latent space. In addition, it can be applied for robot movement generalization using previously learned skills. The method is evaluated on a synthetic dataset with known ground truth; on a simulated robot arm dataset; on human motion capture data; and on a generative model of handwritten digits.",,,,,,"van der Smagt, Patrick/0000-0003-4418-4916",,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300161,0
C,"Hu, SX; Obozinski, G",,"Storkey, A; PerezCruz, F",,"Hu, Shell Xu; Obozinski, Guillaume",,,SDCA-Powered Inexact Dual Augmented Lagrangian Method for Fast CRF Learning,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"We propose an efficient dual augmented Lagrangian formulation to learn conditional random fields (CRF). Our algorithm, which can be interpreted as an inexact gradient descent algorithm on the multiplier, does not require to perform global inference iteratively, and requires only a fixed number of stochastic clique-wise updates at each epoch to obtain a sufficiently good estimate of the gradient w.r.t. the Lagrange multipliers. We prove that the proposed algorithm enjoys global linear convergence for both the primal and the dual objectives. Our experiments show that the proposed algorithm outperforms state-of-the-art baselines in terms of speed of convergence.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300104,0
C,"Sankararaman, KA; Slivkins, A",,"Storkey, A; PerezCruz, F",,"Sankararaman, Karthik Abinav; Slivkins, Aleksandrs",,,Combinatorial Semi-Bandits with Knapsacks,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"We unify two prominent lines of work on multi-armed bandits: bandits with knapsacks and combinatorial semi-bandits. The former concerns limited resources consumed by the algorithm, e.g., limited supply in dynamic pricing. The latter allows a huge number of actions but assumes combinatorial structure and additional feedback to make the problem tractable. We define a common generalization, support it with several motivating examples, and design an algorithm for it. Our regret bounds are comparable with those for BwK and combinatorial semi-bandits.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300184,0
C,"Yamada, M; Umezu, Y; Fukumizu, K; Takeuchi, I",,"Storkey, A; PerezCruz, F",,"Yamada, Makoto; Umezu, Yuta; Fukumizu, Kenji; Takeuchi, Ichiro",,,Post Selection Inference with Kernels,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Finding a set of statistically significant features from complex data (e.g., nonlinear and/or multi-dimensional output data) is important for scientific discovery and has a number of practical applications including biomarker discovery. In this paper, we propose a kernel-based post-selection inference (PSI) algorithm that can find a set of statistically significant features from non-linearly related data. Specifically, our PSI algorithm is based on independence measures, and we call it the Hilbert-Schmidt Independence Criterion (HSIC)-based PSI algorithm (hsicInf). The novelty of hsicInf is that it can handle non-linearity and/or multi-variate/multi-class outputs through kernels. Through synthetic experiments, we show that hsicInf can find a set of statistically significant features for both regression and classification problems. We applied hsicInf to real-world datasets and show that it can successfully identify important features.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300017,0
C,"Yan, BW; Sarkar, P; Cheng, XY",,"Storkey, A; PerezCruz, F",,"Yan, Bowei; Sarkar, Purnamrita; Cheng, Xiuyuan",,,Provable Estimation of the Number of Blocks in Block Models,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"Community detection is a fundamental unsupervised learning problem for unlabeled networks which has a broad range of applications. Many community detection algorithms assume that the number of clusters r is known apriori. In this paper, we propose an approach based on semi-definite relaxations, which does not require prior knowledge of model parameters like many existing convex relaxation methods and recovers the number of clusters and the clustering matrix exactly under a broad parameter regime, with probability tending to one. On a variety of simulated and real data experiments, we show that the proposed method often outperforms state-of-the-art techniques for estimating the number of clusters.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300124,0
C,"Yang, TB; Li, Z; Zhang, LJ",,"Storkey, A; PerezCruz, F",,"Yang, Tianbao; Li, Zhe; Zhang, Lijun",,,A Simple Analysis for Exp-concave Empirical Minimization with Arbitrary Convex Regularizer,"INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 84",Proceedings of Machine Learning Research,,,,21st International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 09-11, 2018","Lanzarote, SPAIN",,,,,"In this paper, we present a simple analysis of fast rates with high probability of empirical minimization for stochastic composite optimization over a finite-dimensional bounded convex set with exponentially concave loss functions and an arbitrary convex regularization. To the best of our knowledge, this result is the first of its kind. As a byproduct, we can directly obtain the fast rate with high probability for exponentially concave empirical risk minimization with and without any convex regularization, which not only extends existing results of empirical risk minimization but also provides a unified framework for analyzing exponentially concave empirical risk minimization with and without any convex regularization. Our proof is very simple only exploiting the covering number of a finite-dimensional bounded set and a concentration inequality of random vectors.",,,,,,,,,,,,,,,,,,,2640-3498,,,,,,2018,84,,,,,,,,,,,,,,,,,,,,,,,WOS:000509385300047,0
C,"Asteris, M; Kyrillidis, A; Papailiopoulos, D; Dimakis, AG",,"Gretton, A; Robert, CC",,"Asteris, Megasthenis; Kyrillidis, Anastasios; Papailiopoulos, Dimitris; Dimakis, Alexandros G.",,,Bipartite Correlation Clustering: Maximizing Agreements,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"In Bipartite Correlation Clustering (BCC) we are given a complete bipartite graph G with '+' and '-' edges, and we seek a vertex clustering that maximizes the number of agreements: the number of all '+' edges within clusters plus all '-' edges cut across clusters. BCC is known to be NP-hard [5]. We present a novel approximation algorithm for k-BCC, a variant of BCC with an upper bound k on the number of clusters. Our algorithm outputs a k-clustering that provably achieves a number of agreements within a multiplicative (1 - delta)-factor from the optimal, for any desired accuracy delta. It relies on solving a combinatorially constrained bilinear maximization on the bi-adjacency matrix of G. It runs in time exponential in k and 1/delta, but linear in the size of the input. Further, we show that in the (unconstrained) BCC setting, an (1 - delta)-approximation can be achieved by O(delta - 1) clusters regardless of the size of the graph. In turn, our k-BCC algorithm implies an Efficient PTAS for the BCC objective of maximizing agreements.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,121,129,,,,,,,,,,,,,,,,WOS:000508662100014,0
C,"Camoriano, R; Angles, T; Rudi, A; Rosasco, L",,"Gretton, A; Robert, CC",,"Camoriano, Raffaello; Angles, Tomas; Rudi, Alessandro; Rosasco, Lorenzo",,,NYTRO: When Subsampling Meets Early Stopping,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Early stopping is a well known approach to reduce the time complexity for performing training and model selection of large scale learning machines. On the other hand, memory/space (rather than time) complexity is the main constraint in many applications, and randomized subsampling techniques have been proposed to tackle this issue. In this paper we ask whether early stopping and subsampling ideas can be combined in a fruitful way. We consider the question in a least squares regression setting and propose a form of randomized iterative regularization based on early stopping and subsampling. In this context, we analyze the statistical and computational properties of the proposed method. Theoretical results are complemented and validated by a thorough experimental analysis.",,,,,"Camoriano, Raffaello/GWN-0743-2022",,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,1403,1411,,,,,,,,,,,,,,,,WOS:000508662100152,0
C,"Hu, WC; Frazier, PI",,"Gretton, A; Robert, CC",,"Hu, Weici; Frazier, Peter, I",,,Bayes-Optimal Effort Allocation in Crowdsourcing: Bounds and Index Policies,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"We consider effort allocation in crowdsourcing, where we wish to assign labeling tasks to imperfect homogeneous crowd workers to maximize overall accuracy in a continuous-time Bayesian setting, subject to budget and time constraints. The Bayes-optimal policy for this problem is the solution to a partially observable Markov decision process, but the curse of dimensionality renders the computation infeasible. Based on the Lagrangian Relaxation technique in Adelman & Mersereau (2008), we provide a computationally tractable instance-specific upper bound on the value of this Bayes-optimal policy, which can in turn be used to bound the optimality gap of any other sub-optimal policy. In an approach similar in spirit to the Whittle index for restless multi-armed bandits, we provide an index policy for effort allocation in crowdsourcing and demonstrate numerically that it outperforms other stateof-arts and performs close to optimal solution.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,324,332,,,,,,,,,,,,,,,,WOS:000508662100036,0
C,"Meirom, EA; Kisilev, P",,"Gretton, A; Robert, CC",,"Meirom, Eli A.; Kisilev, Pavel",,,NuC-MKL: A Convex Approach to Non Linear Multiple Kernel Learning,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Multiple Kernel Learning (MKL) methods are known for their effectiveness in solving classification and regression problems involving multi-modal data. Many MKL approaches use linear combination of base kernels, resulting in somewhat limited feature representations. Several non-linear MKL formulations were proposed recently. They provide much higher dimensional feature spaces, and, therefore, richer representations. However, these methods often lead to nonconvex optimization and to intractable number of optimization parameters. In this paper, we propose a new non-linear MKL method that utilizes nuclear norm regularization and leads to convex optimization problem. The proposed Nuclear-norm-Constrained MKL (NuC-MKL) algorithm converges faster, and requires smaller number of calls to an SVM solver, as compared to other competing methods. Moreover, the number of the model support vectors in our approach is usually much smaller, as compared to other methods. This suggests that our algorithm is more resilient to overfitting. We test our algorithm on several known benchmarks, and show that it equals or outperforms the state-of-the-art MKL methods on all these data sets.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,610,619,,,,,,,,,,,,,,,,WOS:000508662100067,0
C,"Russo, D; Zou, J",,"Gretton, A; Robert, CC",,"Russo, Daniel; Zou, James",,,Controlling Bias in Adaptive Data Analysis Using Information Theory,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 51",JMLR Workshop and Conference Proceedings,,,,19th International Conference on Artificial Intelligence and Statistics (AISTATS),"MAY 09-11, 2016","Cadiz, SPAIN",,,,,"Modern big data settings often involve messy, high-dimensional data, where it is not clear a priori what are the right questions to ask. To extract the most insights from a dataset, the analyst typically needs to engage in an iterative process of adaptive data analysis. The choice of analytics to be performed next depends on the results of the previous analyses on the same data. It is commonly recognized that such adaptivity (also called researcher degrees of freedom), even if well-intentioned, can lead to false discoveries, contributing to the crisis of reproducibility in science. In this paper, we propose a general information-theoretic framework to quantify and provably bound the bias of arbitrary adaptive analysis process. We prove that our mutual information based bound is tight in natural models. We show how this framework can give rigorous insights into when commonly used feature selection protocols (e.g. rank selection) do and do not lead to biased estimation. We also show how recent insights from differential privacy emerge from this framework when the analyst is assumed to be adversarial, though our bounds applies in more general settings. We illustrate our results with simple simulations.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2016,51,,,,,,1232,1240,,,,,,,,,,,,,,,,WOS:000508662100134,0
C,"Chen, JH; Yang, TB; Zhu, SH",,"Kaski, S; Corander, J",,"Chen, Jianhui; Yang, Tianbao; Zhu, Shenghuo",,,Efficient Low-Rank Stochastic Gradient Descent Methods for Solving Semidefinite Programs,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"We propose a low-rank stochastic gradient descent (LR-SGD) method for solving a class of semidefinite programming (SDP) problems. LR-SGD has clear computational advantages over the standard SGD peers as its iterative projection step (a SDP problem) can be solved in an efficient manner. Specifically, LR-SGD constructs a low-rank stochastic gradient and computes an optimal solution to the projection step via analyzing the low-rank structure of its stochastic gradient. Moreover, our theoretical analysis shows the universal existence of arbitrary low-rank stochastic gradients which in turn validates the rationale of the LR-SGD method. Since LR-SGD is a SGD based method, it achieves the optimal convergence rates of the standard SGD methods. The presented experimental results demonstrate the efficiency and effectiveness of the LR-SGD method.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,122,130,,,,,,,,,,,,,,,,WOS:000508355800014,0
C,"Gornitz, N; Porbadnigk, AK; Binder, A; Sannelli, C; Braun, M; Muller, KR; Kloft, M",,"Kaski, S; Corander, J",,"Goernitz, Nico; Porbadnigk, Anne K.; Binder, Alexander; Sannelli, Claudia; Braun, Mikio; Mueller, Klaus-Robert; Kloft, Marius",,,Learning and Evaluation in Presence of Non-i.i.d. Label Noise,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"In many real-world applications, the simplified assumption of independent and identically distributed noise breaks down, and labels can have structured, systematic noise. For example, in brain-computer interface applications, training data is often the result of lengthy experimental sessions, where the attention levels of participants can change over the course of the experiment. In such application cases, structured label noise will cause problems because most machine learning methods assume independent and identically distributed label noise. In this paper, we present a novel methodology for learning and evaluation in presence of systematic label noise. The core of which is a novel extension of support vector data description / one-class SVM that can incorporate latent variables. Controlled simulations on synthetic data and a real-world EEG experiment with 20 subjects from the domain of brain-computer-interfacing show that our method achieves accuracies that go beyond the state of the art.",,,,,"Mueller, Klaus-Robert/Y-3547-2019","Mueller, Klaus-Robert/0000-0002-3861-7685; Binder, Alexander/0000-0001-9605-6209",,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,293,302,,,,,,,,,,,,,,,,WOS:000508355800033,0
C,"Izbicki, R; Lee, AB; Schafer, CM",,"Kaski, S; Corander, J",,"Izbicki, Rafael; Lee, Ann B.; Schafer, Chad M.",,,High-Dimensional Density Ratio Estimation with Extensions to Approximate Likelihood Computation,"ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 33",JMLR Workshop and Conference Proceedings,,,,17th International Conference on Artificial Intelligence and Statistics (AISTATS),"APR 22-25, 2014","Reykjavik, ICELAND",,,,,"The ratio between two probability density functions is an important component of various tasks, including selection bias correction, novelty detection and classification. Recently, several estimators of this ratio have been proposed. Most of these methods fail if the sample space is high-dimensional, and hence require a dimension reduction step, the result of which can be a significant loss of information. Here we propose a simple-to-implement, fully nonparametric density ratio estimator that expands the ratio in terms of the cigenfunctions of a kernel-based operator; these functions reflect the underlying geometry of the data (e.g., submanifold structure), often leading to better estimates without an explicit dimension reduction step. We show how our general framework can be extended to address another important problem, the estimation of a likelihood function in situations where that function cannot be well-approximated by an analytical form. One is often faced with this situation when performing statistical inference with data from the sciences, due the complexity of the data and of the processes that generated those data. We emphasize applications where using existing likelihood-free methods of inference would be challenging due to the high dimensionality of the sample space, but where our spectral series method yields a reasonable estimate of the likelihood function. We provide theoretical guarantees and illustrate the effectiveness of our proposed method with numerical experiments.",,,,,,,,,,,,,,,,,,,1938-7288,,,,,,2014,33,,,,,,420,429,,,,,,,,,,,,,,,,WOS:000508355800047,0
